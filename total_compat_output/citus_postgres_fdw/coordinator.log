2023-11-25 04:50:20.386 UTC [1094384] LOG:  00000: number of prepared transactions has not been configured, overriding
2023-11-25 04:50:20.386 UTC [1094384] DETAIL:  max_prepared_transactions is now set to 200
2023-11-25 04:50:20.386 UTC [1094384] LOCATION:  AdjustMaxPreparedTransactions, transaction_management.c:761
2023-11-25 04:50:20.403 UTC [1094384] LOG:  00000: starting PostgreSQL 15.3 on x86_64-pc-linux-gnu, compiled by gcc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0, 64-bit
2023-11-25 04:50:20.403 UTC [1094384] LOCATION:  PostmasterMain, postmaster.c:1189
2023-11-25 04:50:20.403 UTC [1094384] LOG:  00000: listening on IPv4 address "127.0.0.1", port 57636
2023-11-25 04:50:20.403 UTC [1094384] LOCATION:  StreamServerPort, pqcomm.c:582
2023-11-25 04:50:20.403 UTC [1094384] LOG:  00000: listening on Unix socket "/tmp/.s.PGSQL.57636"
2023-11-25 04:50:20.403 UTC [1094384] LOCATION:  StreamServerPort, pqcomm.c:577
2023-11-25 04:50:20.404 UTC [1094387] LOG:  00000: database system was shut down at 2023-11-25 04:50:20 UTC
2023-11-25 04:50:20.404 UTC [1094387] LOCATION:  StartupXLOG, xlog.c:4928
2023-11-25 04:50:20.407 UTC [1094384] LOG:  00000: database system is ready to accept connections
2023-11-25 04:50:20.407 UTC [1094384] LOCATION:  reaper, postmaster.c:3117
2023-11-25 04:50:21.164 UTC [1094454] LOG:  00000: starting maintenance daemon on database 16384 user 10
2023-11-25 04:50:21.164 UTC [1094454] CONTEXT:  Citus maintenance daemon for database 16384 user 10
2023-11-25 04:50:21.164 UTC [1094454] LOCATION:  CitusMaintenanceDaemonMain, maintenanced.c:373
2023-11-25 04:50:21.175 UTC [1094384] LOG:  00000: received SIGHUP, reloading configuration files
2023-11-25 04:50:21.175 UTC [1094384] LOCATION:  SIGHUP_handler, postmaster.c:2769
2023-11-25 04:50:21.176 UTC [1094384] LOG:  00000: parameter "citus.metadata_sync_interval" changed to "3000"
2023-11-25 04:50:21.176 UTC [1094384] LOCATION:  ProcessConfigFileInternal, guc-file.l:453
2023-11-25 04:50:21.176 UTC [1094384] LOG:  00000: parameter "citus.metadata_sync_retry_interval" changed to "500"
2023-11-25 04:50:21.176 UTC [1094384] LOCATION:  ProcessConfigFileInternal, guc-file.l:453
2023-11-25 04:50:21.376 UTC [1094488] ERROR:  55000: disabling the first worker node in the metadata is not allowed
2023-11-25 04:50:21.376 UTC [1094488] DETAIL:  Citus uses the first worker node in the metadata for certain internal operations when replicated tables are modified. Synchronous mode ensures that all nodes have the same view of the first worker node, which is used for certain locking operations.
2023-11-25 04:50:21.376 UTC [1094488] HINT:  You can force disabling node, SELECT citus_disable_node('localhost', 57637, synchronous:=true);
2023-11-25 04:50:21.376 UTC [1094488] LOCATION:  citus_disable_node, node_metadata.c:542
2023-11-25 04:50:21.376 UTC [1094488] STATEMENT:  SELECT citus_disable_node('localhost', 57637);
2023-11-25 04:50:21.432 UTC [1094488] ERROR:  XX000: cannot remove or disable the node localhost:57638 because because it contains the only shard placement for shard 1220001
2023-11-25 04:50:21.432 UTC [1094488] DETAIL:  One of the table(s) that prevents the operation complete successfully is public.cluster_management_test
2023-11-25 04:50:21.432 UTC [1094488] HINT:  To proceed, either drop the tables or use undistribute_table() function to convert them to local tables
2023-11-25 04:50:21.432 UTC [1094488] LOCATION:  ErrorIfNodeContainsNonRemovablePlacements, node_metadata.c:1947
2023-11-25 04:50:21.432 UTC [1094488] STATEMENT:  SELECT master_remove_node('localhost', 57638);
2023-11-25 04:50:21.440 UTC [1094488] ERROR:  XX000: cannot remove or disable the node localhost:57638 because because it contains the only shard placement for shard 1220001
2023-11-25 04:50:21.440 UTC [1094488] DETAIL:  One of the table(s) that prevents the operation complete successfully is public.cluster_management_test
2023-11-25 04:50:21.440 UTC [1094488] HINT:  To proceed, either drop the tables or use undistribute_table() function to convert them to local tables
2023-11-25 04:50:21.440 UTC [1094488] LOCATION:  ErrorIfNodeContainsNonRemovablePlacements, node_metadata.c:1947
2023-11-25 04:50:21.440 UTC [1094488] STATEMENT:  SELECT citus_remove_node('localhost', 57638);
2023-11-25 04:50:21.440 UTC [1094488] ERROR:  XX000: cannot remove or disable the node localhost:57638 because because it contains the only shard placement for shard 1220001
2023-11-25 04:50:21.440 UTC [1094488] DETAIL:  One of the table(s) that prevents the operation complete successfully is public.cluster_management_test
2023-11-25 04:50:21.440 UTC [1094488] HINT:  To proceed, either drop the tables or use undistribute_table() function to convert them to local tables
2023-11-25 04:50:21.440 UTC [1094488] LOCATION:  ErrorIfNodeContainsNonRemovablePlacements, node_metadata.c:1947
2023-11-25 04:50:21.440 UTC [1094488] STATEMENT:  SELECT citus_disable_node('localhost', 57638);
2023-11-25 04:50:21.440 UTC [1094488] ERROR:  XX000: node at "localhost.noexist:2345" does not exist
2023-11-25 04:50:21.440 UTC [1094488] LOCATION:  ModifiableWorkerNode, node_metadata.c:732
2023-11-25 04:50:21.440 UTC [1094488] STATEMENT:  SELECT citus_disable_node('localhost.noexist', 2345);
2023-11-25 04:50:21.544 UTC [1094488] ERROR:  42501: permission denied for function master_add_inactive_node
2023-11-25 04:50:21.544 UTC [1094488] LOCATION:  aclcheck_error, aclchk.c:3650
2023-11-25 04:50:21.544 UTC [1094488] STATEMENT:  SELECT 1 FROM master_add_inactive_node('localhost', 57638 + 1);
2023-11-25 04:50:21.544 UTC [1094488] ERROR:  42501: permission denied for function master_activate_node
2023-11-25 04:50:21.544 UTC [1094488] LOCATION:  aclcheck_error, aclchk.c:3650
2023-11-25 04:50:21.544 UTC [1094488] STATEMENT:  SELECT 1 FROM master_activate_node('localhost', 57638 + 1);
2023-11-25 04:50:21.544 UTC [1094488] ERROR:  42501: permission denied for function citus_disable_node
2023-11-25 04:50:21.544 UTC [1094488] LOCATION:  aclcheck_error, aclchk.c:3650
2023-11-25 04:50:21.544 UTC [1094488] STATEMENT:  SELECT 1 FROM citus_disable_node('localhost', 57638 + 1);
2023-11-25 04:50:21.544 UTC [1094488] ERROR:  42501: permission denied for function master_remove_node
2023-11-25 04:50:21.544 UTC [1094488] LOCATION:  aclcheck_error, aclchk.c:3650
2023-11-25 04:50:21.544 UTC [1094488] STATEMENT:  SELECT 1 FROM master_remove_node('localhost', 57638 + 1);
2023-11-25 04:50:21.544 UTC [1094488] ERROR:  42501: permission denied for function master_add_node
2023-11-25 04:50:21.544 UTC [1094488] LOCATION:  aclcheck_error, aclchk.c:3650
2023-11-25 04:50:21.544 UTC [1094488] STATEMENT:  SELECT 1 FROM master_add_node('localhost', 57638 + 1);
2023-11-25 04:50:21.544 UTC [1094488] ERROR:  42501: permission denied for function master_add_secondary_node
2023-11-25 04:50:21.544 UTC [1094488] LOCATION:  aclcheck_error, aclchk.c:3650
2023-11-25 04:50:21.544 UTC [1094488] STATEMENT:  SELECT 1 FROM master_add_secondary_node('localhost', 57638 + 2, 'localhost', 57638);
2023-11-25 04:50:21.544 UTC [1094488] ERROR:  42501: permission denied for function master_update_node
2023-11-25 04:50:21.544 UTC [1094488] LOCATION:  aclcheck_error, aclchk.c:3650
2023-11-25 04:50:21.544 UTC [1094488] STATEMENT:  SELECT master_update_node(nodeid, 'localhost', 57638 + 3) FROM pg_dist_node WHERE nodeport = 57638;
2023-11-25 04:50:21.544 UTC [1094488] ERROR:  XX000: operation is not allowed
2023-11-25 04:50:21.544 UTC [1094488] HINT:  Run the command with a superuser.
2023-11-25 04:50:21.544 UTC [1094488] LOCATION:  EnsureSuperUser, metadata_utility.c:2299
2023-11-25 04:50:21.544 UTC [1094488] STATEMENT:  SELECT 1 FROM master_add_node('localhost', 57638);
2023-11-25 04:50:21.596 UTC [1094516] ERROR:  XX000: cannot remove or disable the node localhost:57638 because because it contains the only shard placement for shard 1220001
2023-11-25 04:50:21.596 UTC [1094516] DETAIL:  One of the table(s) that prevents the operation complete successfully is public.cluster_management_test
2023-11-25 04:50:21.596 UTC [1094516] HINT:  To proceed, either drop the tables or use undistribute_table() function to convert them to local tables
2023-11-25 04:50:21.596 UTC [1094516] LOCATION:  ErrorIfNodeContainsNonRemovablePlacements, node_metadata.c:1947
2023-11-25 04:50:21.596 UTC [1094516] STATEMENT:  SELECT master_remove_node('localhost', 57638);
2023-11-25 04:50:21.613 UTC [1094516] ERROR:  XX000: cannot remove or disable the node localhost:57638 because because it contains the only shard placement for shard 1220001
2023-11-25 04:50:21.613 UTC [1094516] DETAIL:  One of the table(s) that prevents the operation complete successfully is public.cluster_management_test
2023-11-25 04:50:21.613 UTC [1094516] HINT:  To proceed, either drop the tables or use undistribute_table() function to convert them to local tables
2023-11-25 04:50:21.613 UTC [1094516] LOCATION:  ErrorIfNodeContainsNonRemovablePlacements, node_metadata.c:1947
2023-11-25 04:50:21.613 UTC [1094516] STATEMENT:  SELECT master_remove_node('localhost', 57638);
2023-11-25 04:50:21.630 UTC [1094516] ERROR:  XX000: node group 5 does not have a primary node
2023-11-25 04:50:21.630 UTC [1094516] LOCATION:  LookupNodeForGroup, metadata_cache.c:1189
2023-11-25 04:50:21.630 UTC [1094516] STATEMENT:  SELECT * FROM cluster_management_test;
2023-11-25 04:50:21.643 UTC [1094516] ERROR:  XX000: there is a shard placement in node group 5 but there are no nodes in that group
2023-11-25 04:50:21.643 UTC [1094516] LOCATION:  LookupNodeForGroup, metadata_cache.c:1181
2023-11-25 04:50:21.643 UTC [1094516] STATEMENT:  SELECT * FROM cluster_management_test;
2023-11-25 04:50:21.650 UTC [1094516] WARNING:  01000: could not find any shard placements for shardId 1220001
2023-11-25 04:50:21.650 UTC [1094516] LOCATION:  ShardPlacementList, metadata_cache.c:1247
2023-11-25 04:50:21.650 UTC [1094516] WARNING:  01000: could not find any shard placements for shardId 1220003
2023-11-25 04:50:21.650 UTC [1094516] LOCATION:  ShardPlacementList, metadata_cache.c:1247
2023-11-25 04:50:21.650 UTC [1094516] WARNING:  01000: could not find any shard placements for shardId 1220005
2023-11-25 04:50:21.650 UTC [1094516] LOCATION:  ShardPlacementList, metadata_cache.c:1247
2023-11-25 04:50:21.650 UTC [1094516] WARNING:  01000: could not find any shard placements for shardId 1220007
2023-11-25 04:50:21.650 UTC [1094516] LOCATION:  ShardPlacementList, metadata_cache.c:1247
2023-11-25 04:50:21.650 UTC [1094516] WARNING:  01000: could not find any shard placements for shardId 1220009
2023-11-25 04:50:21.650 UTC [1094516] LOCATION:  ShardPlacementList, metadata_cache.c:1247
2023-11-25 04:50:21.650 UTC [1094516] WARNING:  01000: could not find any shard placements for shardId 1220011
2023-11-25 04:50:21.650 UTC [1094516] LOCATION:  ShardPlacementList, metadata_cache.c:1247
2023-11-25 04:50:21.650 UTC [1094516] WARNING:  01000: could not find any shard placements for shardId 1220013
2023-11-25 04:50:21.650 UTC [1094516] LOCATION:  ShardPlacementList, metadata_cache.c:1247
2023-11-25 04:50:21.650 UTC [1094516] WARNING:  01000: could not find any shard placements for shardId 1220015
2023-11-25 04:50:21.650 UTC [1094516] LOCATION:  ShardPlacementList, metadata_cache.c:1247
2023-11-25 04:50:21.678 UTC [1094516] ERROR:  XX000: cannot remove or disable the node localhost:57638 because because it contains the only shard placement for shard 1220001
2023-11-25 04:50:21.678 UTC [1094516] DETAIL:  One of the table(s) that prevents the operation complete successfully is public.cluster_management_test
2023-11-25 04:50:21.678 UTC [1094516] HINT:  To proceed, either drop the tables or use undistribute_table() function to convert them to local tables
2023-11-25 04:50:21.678 UTC [1094516] LOCATION:  ErrorIfNodeContainsNonRemovablePlacements, node_metadata.c:1947
2023-11-25 04:50:21.678 UTC [1094516] STATEMENT:  SELECT master_remove_node('localhost', 57638);
2023-11-25 04:50:21.906 UTC [1094554] ERROR:  XX000: primaries must be added to the default cluster
2023-11-25 04:50:21.906 UTC [1094554] LOCATION:  AddNodeMetadata, node_metadata.c:2141
2023-11-25 04:50:21.906 UTC [1094554] STATEMENT:  SELECT master_add_node('localhost', 9999, nodecluster => 'olap');
2023-11-25 04:50:21.907 UTC [1094554] ERROR:  XX000: group 14 already has a primary node
2023-11-25 04:50:21.907 UTC [1094554] LOCATION:  AddNodeMetadata, node_metadata.c:2130
2023-11-25 04:50:21.907 UTC [1094554] STATEMENT:  SELECT master_add_node('localhost', 9999, groupid => 14, noderole => 'primary');
2023-11-25 04:50:21.910 UTC [1094554] ERROR:  P0001: there cannot be two primary nodes in a group
2023-11-25 04:50:21.910 UTC [1094554] CONTEXT:  PL/pgSQL function citus_internal.pg_dist_node_trigger_func() line 10 at RAISE
2023-11-25 04:50:21.910 UTC [1094554] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:21.910 UTC [1094554] STATEMENT:  INSERT INTO pg_dist_node (nodename, nodeport, groupid, noderole)
	  VALUES ('localhost', 5000, 14, 'primary');
2023-11-25 04:50:21.911 UTC [1094554] ERROR:  P0001: there cannot be two primary nodes in a group
2023-11-25 04:50:21.911 UTC [1094554] CONTEXT:  PL/pgSQL function citus_internal.pg_dist_node_trigger_func() line 18 at RAISE
2023-11-25 04:50:21.911 UTC [1094554] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:21.911 UTC [1094554] STATEMENT:  UPDATE pg_dist_node SET noderole = 'primary'
	  WHERE groupid = 14 AND nodeport = 9998;
2023-11-25 04:50:21.911 UTC [1094554] ERROR:  23514: new row for relation "pg_dist_node" violates check constraint "primaries_are_only_allowed_in_the_default_cluster"
2023-11-25 04:50:21.911 UTC [1094554] DETAIL:  Failing row contains (24, 1000, localhost, 5000, default, f, t, primary, olap, f, t).
2023-11-25 04:50:21.911 UTC [1094554] LOCATION:  ExecConstraints, execMain.c:2019
2023-11-25 04:50:21.911 UTC [1094554] STATEMENT:  INSERT INTO pg_dist_node (nodename, nodeport, groupid, noderole, nodecluster)
	  VALUES ('localhost', 5000, 1000, 'primary', 'olap');
2023-11-25 04:50:21.911 UTC [1094554] ERROR:  23514: new row for relation "pg_dist_node" violates check constraint "primaries_are_only_allowed_in_the_default_cluster"
2023-11-25 04:50:21.911 UTC [1094554] DETAIL:  Failing row contains (16, 14, localhost, 57637, default, f, t, primary, olap, f, t).
2023-11-25 04:50:21.911 UTC [1094554] LOCATION:  ExecConstraints, execMain.c:2019
2023-11-25 04:50:21.911 UTC [1094554] STATEMENT:  UPDATE pg_dist_node SET nodecluster = 'olap'
	  WHERE nodeport = 57637;
2023-11-25 04:50:21.912 UTC [1094554] ERROR:  XX000: node at "localhost:2000" does not exist
2023-11-25 04:50:21.912 UTC [1094554] LOCATION:  GroupForNode, node_metadata.c:801
2023-11-25 04:50:21.912 UTC [1094554] STATEMENT:  SELECT master_add_secondary_node('localhost', 9993, 'localhost', 2000);
2023-11-25 04:50:21.914 UTC [1094554] ERROR:  P0002: node 100 not found
2023-11-25 04:50:21.914 UTC [1094554] LOCATION:  citus_update_node, node_metadata.c:1216
2023-11-25 04:50:21.914 UTC [1094554] STATEMENT:  SELECT master_update_node(100, 'localhost', 8000);
2023-11-25 04:50:21.914 UTC [1094554] ERROR:  55000: there is already another node with the specified hostname and port
2023-11-25 04:50:21.914 UTC [1094554] LOCATION:  citus_update_node, node_metadata.c:1207
2023-11-25 04:50:21.914 UTC [1094554] STATEMENT:  SELECT master_update_node(16, 'localhost', 57638);
2023-11-25 04:50:21.985 UTC [1094554] ERROR:  XX000: only the 'shouldhaveshards' property can be set using this function
2023-11-25 04:50:21.985 UTC [1094554] LOCATION:  citus_set_node_property, node_metadata.c:695
2023-11-25 04:50:21.985 UTC [1094554] STATEMENT:  SELECT * from master_set_node_property('localhost', 57638, 'bogusproperty', false);
2023-11-25 04:50:22.028 UTC [1094554] ERROR:  XX000: do not sync metadata in transaction block when the sync mode is nontransactional
2023-11-25 04:50:22.028 UTC [1094554] HINT:  resync after SET citus.metadata_sync_mode TO 'transactional'
2023-11-25 04:50:22.028 UTC [1094554] LOCATION:  ActivateNodeList, node_metadata.c:1061
2023-11-25 04:50:22.028 UTC [1094554] STATEMENT:  SELECT start_metadata_sync_to_all_nodes();
2023-11-25 04:50:22.074 UTC [1094554] ERROR:  XX000: do not add node in transaction block when the sync mode is nontransactional
2023-11-25 04:50:22.074 UTC [1094554] HINT:  add the node after SET citus.metadata_sync_mode TO 'transactional'
2023-11-25 04:50:22.074 UTC [1094554] LOCATION:  citus_add_node, node_metadata.c:322
2023-11-25 04:50:22.074 UTC [1094554] STATEMENT:  SELECT citus_add_node('localhost', 57637);
2023-11-25 04:50:22.171 UTC [1094583] ERROR:  42601: syntax error at or near ""123"" at character 37
2023-11-25 04:50:22.171 UTC [1094583] LOCATION:  scanner_yyerror, scan.l:1188
2023-11-25 04:50:22.171 UTC [1094583] STATEMENT:  CREATE ROLE create_role_sysid SYSID "123";
2023-11-25 04:50:22.538 UTC [1094640] ERROR:  42704: role "nonexisting_role_2" does not exist
2023-11-25 04:50:22.538 UTC [1094640] LOCATION:  get_role_oid, acl.c:5184
2023-11-25 04:50:22.538 UTC [1094640] STATEMENT:  GRANT existing_role_1, nonexisting_role_1 TO existing_role_2, nonexisting_role_2;
2023-11-25 04:50:22.547 UTC [1094644] ERROR:  42704: role "nonexisting_role_1" does not exist
2023-11-25 04:50:22.547 UTC [1094644] LOCATION:  DropRole, user.c:951
2023-11-25 04:50:22.547 UTC [1094644] STATEMENT:  DROP ROLE existing_role_1, existing_role_2, nonexisting_role_1, nonexisting_role_2;
2023-11-25 04:50:23.237 UTC [1094773] ERROR:  42P16: cannot distribute relation "data_load_test"
2023-11-25 04:50:23.237 UTC [1094773] DETAIL:  Relation "data_load_test" contains data.
2023-11-25 04:50:23.237 UTC [1094773] HINT:  Empty your table before distributing it.
2023-11-25 04:50:23.237 UTC [1094773] LOCATION:  EnsureLocalTableEmpty, create_distributed_table.c:2032
2023-11-25 04:50:23.237 UTC [1094773] STATEMENT:  SELECT create_distributed_table('data_load_test', 'col1', 'append');
2023-11-25 04:50:23.245 UTC [1094773] ERROR:  42P16: cannot distribute relation "data_load_test"
2023-11-25 04:50:23.245 UTC [1094773] DETAIL:  Relation "data_load_test" contains data.
2023-11-25 04:50:23.245 UTC [1094773] HINT:  Empty your table before distributing it.
2023-11-25 04:50:23.245 UTC [1094773] LOCATION:  EnsureLocalTableEmpty, create_distributed_table.c:2032
2023-11-25 04:50:23.245 UTC [1094773] STATEMENT:  SELECT create_distributed_table('data_load_test', 'col1', 'range');
2023-11-25 04:50:23.389 UTC [1094773] ERROR:  0A000: cannot create a citus table from a catalog table
2023-11-25 04:50:23.389 UTC [1094773] LOCATION:  ErrorIfTableIsACatalogTable, create_distributed_table.c:1968
2023-11-25 04:50:23.389 UTC [1094773] STATEMENT:  SELECT create_distributed_table('pg_class', 'relname');
2023-11-25 04:50:23.390 UTC [1094773] ERROR:  0A000: cannot create a citus table from a catalog table
2023-11-25 04:50:23.390 UTC [1094773] LOCATION:  ErrorIfTableIsACatalogTable, create_distributed_table.c:1968
2023-11-25 04:50:23.390 UTC [1094773] STATEMENT:  SELECT create_reference_table('pg_class');
2023-11-25 04:50:23.401 UTC [1094773] ERROR:  XX000: 0 is outside the valid range for parameter "shard_count" (1 .. 64000)
2023-11-25 04:50:23.401 UTC [1094773] LOCATION:  create_distributed_table, create_distributed_table.c:276
2023-11-25 04:50:23.401 UTC [1094773] STATEMENT:  SELECT create_distributed_table('shard_count_table_2', 'a', shard_count:=0);
2023-11-25 04:50:23.401 UTC [1094773] ERROR:  XX000: -100 is outside the valid range for parameter "shard_count" (1 .. 64000)
2023-11-25 04:50:23.401 UTC [1094773] LOCATION:  create_distributed_table, create_distributed_table.c:276
2023-11-25 04:50:23.401 UTC [1094773] STATEMENT:  SELECT create_distributed_table('shard_count_table_2', 'a', shard_count:=-100);
2023-11-25 04:50:23.401 UTC [1094773] ERROR:  XX000: 64001 is outside the valid range for parameter "shard_count" (1 .. 64000)
2023-11-25 04:50:23.401 UTC [1094773] LOCATION:  create_distributed_table, create_distributed_table.c:276
2023-11-25 04:50:23.401 UTC [1094773] STATEMENT:  SELECT create_distributed_table('shard_count_table_2', 'a', shard_count:=64001);
2023-11-25 04:50:23.401 UTC [1094773] ERROR:  XX000: Cannot use colocate_with with a table and shard_count at the same time
2023-11-25 04:50:23.401 UTC [1094773] LOCATION:  create_distributed_table, create_distributed_table.c:237
2023-11-25 04:50:23.401 UTC [1094773] STATEMENT:  SELECT create_distributed_table('shard_count_table_2', 'a', shard_count:=12, colocate_with:='shard_count');
2023-11-25 04:50:23.448 UTC [1094773] LOG:  00000: performing blocking isolate_tenant_to_new_shard 
2023-11-25 04:50:23.448 UTC [1094773] LOCATION:  SplitShard, shard_split.c:507
2023-11-25 04:50:23.448 UTC [1094773] STATEMENT:  SELECT 1 FROM isolate_tenant_to_new_shard('shard_split_table', 5, shard_transfer_mode => 'block_writes');
2023-11-25 04:50:23.449 UTC [1094773] LOG:  00000: creating child shards for isolate_tenant_to_new_shard
2023-11-25 04:50:23.449 UTC [1094773] LOCATION:  BlockingShardSplit, shard_split.c:571
2023-11-25 04:50:23.449 UTC [1094773] STATEMENT:  SELECT 1 FROM isolate_tenant_to_new_shard('shard_split_table', 5, shard_transfer_mode => 'block_writes');
2023-11-25 04:50:23.457 UTC [1094773] LOG:  00000: performing copy for isolate_tenant_to_new_shard
2023-11-25 04:50:23.457 UTC [1094773] LOCATION:  BlockingShardSplit, shard_split.c:577
2023-11-25 04:50:23.457 UTC [1094773] STATEMENT:  SELECT 1 FROM isolate_tenant_to_new_shard('shard_split_table', 5, shard_transfer_mode => 'block_writes');
2023-11-25 04:50:23.458 UTC [1094773] LOG:  00000: creating auxillary structures (indexes, stats, replicaindentities, triggers) for isolate_tenant_to_new_shard
2023-11-25 04:50:23.458 UTC [1094773] LOCATION:  BlockingShardSplit, shard_split.c:587
2023-11-25 04:50:23.458 UTC [1094773] STATEMENT:  SELECT 1 FROM isolate_tenant_to_new_shard('shard_split_table', 5, shard_transfer_mode => 'block_writes');
2023-11-25 04:50:23.458 UTC [1094773] LOG:  00000: marking deferred cleanup of source shard(s) for isolate_tenant_to_new_shard
2023-11-25 04:50:23.458 UTC [1094773] LOCATION:  BlockingShardSplit, shard_split.c:609
2023-11-25 04:50:23.458 UTC [1094773] STATEMENT:  SELECT 1 FROM isolate_tenant_to_new_shard('shard_split_table', 5, shard_transfer_mode => 'block_writes');
2023-11-25 04:50:23.459 UTC [1094773] LOG:  00000: creating foreign key constraints (if any) for isolate_tenant_to_new_shard
2023-11-25 04:50:23.459 UTC [1094773] LOCATION:  BlockingShardSplit, shard_split.c:625
2023-11-25 04:50:23.459 UTC [1094773] STATEMENT:  SELECT 1 FROM isolate_tenant_to_new_shard('shard_split_table', 5, shard_transfer_mode => 'block_writes');
2023-11-25 04:50:23.521 UTC [1094773] ERROR:  0A000: cannot distribute a temporary table
2023-11-25 04:50:23.521 UTC [1094773] LOCATION:  ErrorIfTemporaryTable, create_distributed_table.c:1950
2023-11-25 04:50:23.521 UTC [1094773] STATEMENT:  select create_distributed_table('temp_table', 'a');
2023-11-25 04:50:23.521 UTC [1094773] ERROR:  0A000: cannot distribute a temporary table
2023-11-25 04:50:23.521 UTC [1094773] LOCATION:  ErrorIfTemporaryTable, create_distributed_table.c:1950
2023-11-25 04:50:23.521 UTC [1094773] STATEMENT:  select create_reference_table('temp_table');
2023-11-25 04:50:24.543 UTC [1095005] WARNING:  0A000: table "uniq_cns_append_tables" has a UNIQUE or EXCLUDE constraint
2023-11-25 04:50:24.543 UTC [1095005] DETAIL:  UNIQUE constraints, EXCLUDE constraints, and PRIMARY KEYs on append-partitioned tables cannot be enforced.
2023-11-25 04:50:24.543 UTC [1095005] HINT:  Consider using hash partitioning.
2023-11-25 04:50:24.543 UTC [1095005] LOCATION:  ErrorIfUnsupportedConstraint, table.c:2975
2023-11-25 04:50:24.545 UTC [1095005] WARNING:  0A000: table "excl_cns_append_tables" has a UNIQUE or EXCLUDE constraint
2023-11-25 04:50:24.545 UTC [1095005] DETAIL:  UNIQUE constraints, EXCLUDE constraints, and PRIMARY KEYs on append-partitioned tables cannot be enforced.
2023-11-25 04:50:24.545 UTC [1095005] HINT:  Consider using hash partitioning.
2023-11-25 04:50:24.545 UTC [1095005] LOCATION:  ErrorIfUnsupportedConstraint, table.c:2975
2023-11-25 04:50:24.553 UTC [1095005] ERROR:  0A000: cannot create constraint on "pk_on_non_part_col"
2023-11-25 04:50:24.553 UTC [1095005] DETAIL:  Distributed relations cannot have UNIQUE, EXCLUDE, or PRIMARY KEY constraints that do not include the partition column (with an equality operator if EXCLUDE).
2023-11-25 04:50:24.553 UTC [1095005] LOCATION:  ErrorIfUnsupportedConstraint, table.c:3021
2023-11-25 04:50:24.553 UTC [1095005] STATEMENT:  SELECT create_distributed_table('pk_on_non_part_col', 'partition_col', 'hash');
2023-11-25 04:50:24.601 UTC [1095005] ERROR:  23505: duplicate key value violates unique constraint "pk_on_non_part_col_pkey_365000"
2023-11-25 04:50:24.601 UTC [1095005] DETAIL:  Key (other_col)=(1) already exists.
2023-11-25 04:50:24.601 UTC [1095005] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:24.601 UTC [1095005] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:24.601 UTC [1095005] STATEMENT:  INSERT INTO pk_on_non_part_col VALUES (1,1);
2023-11-25 04:50:24.610 UTC [1095005] ERROR:  0A000: cannot create constraint on "uq_on_non_part_col"
2023-11-25 04:50:24.610 UTC [1095005] DETAIL:  Distributed relations cannot have UNIQUE, EXCLUDE, or PRIMARY KEY constraints that do not include the partition column (with an equality operator if EXCLUDE).
2023-11-25 04:50:24.610 UTC [1095005] LOCATION:  ErrorIfUnsupportedConstraint, table.c:3021
2023-11-25 04:50:24.610 UTC [1095005] STATEMENT:  SELECT create_distributed_table('uq_on_non_part_col', 'partition_col', 'hash');
2023-11-25 04:50:24.612 UTC [1095005] ERROR:  0A000: cannot create constraint on "ex_on_non_part_col"
2023-11-25 04:50:24.612 UTC [1095005] DETAIL:  Distributed relations cannot have UNIQUE, EXCLUDE, or PRIMARY KEY constraints that do not include the partition column (with an equality operator if EXCLUDE).
2023-11-25 04:50:24.612 UTC [1095005] LOCATION:  ErrorIfUnsupportedConstraint, table.c:3021
2023-11-25 04:50:24.612 UTC [1095005] STATEMENT:  SELECT create_distributed_table('ex_on_non_part_col', 'partition_col', 'hash');
2023-11-25 04:50:24.645 UTC [1095005] ERROR:  23P01: conflicting key value violates exclusion constraint "ex_on_non_part_col_other_col_excl_365004"
2023-11-25 04:50:24.645 UTC [1095005] DETAIL:  Key (other_col)=(1) conflicts with existing key (other_col)=(1).
2023-11-25 04:50:24.645 UTC [1095005] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:24.645 UTC [1095005] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:24.645 UTC [1095005] STATEMENT:  INSERT INTO ex_on_non_part_col VALUES (1,1);
2023-11-25 04:50:24.701 UTC [1095005] ERROR:  23505: duplicate key value violates unique constraint "uq_two_columns_partition_col_other_col_key_365016"
2023-11-25 04:50:24.701 UTC [1095005] DETAIL:  Key (partition_col, other_col)=(1, 1) already exists.
2023-11-25 04:50:24.701 UTC [1095005] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:24.701 UTC [1095005] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:24.701 UTC [1095005] STATEMENT:  INSERT INTO uq_two_columns (partition_col, other_col) VALUES (1,1);
2023-11-25 04:50:24.784 UTC [1095005] ERROR:  0A000: cannot create constraint on "pk_on_two_non_part_cols"
2023-11-25 04:50:24.784 UTC [1095005] DETAIL:  Distributed relations cannot have UNIQUE, EXCLUDE, or PRIMARY KEY constraints that do not include the partition column (with an equality operator if EXCLUDE).
2023-11-25 04:50:24.784 UTC [1095005] LOCATION:  ErrorIfUnsupportedConstraint, table.c:3021
2023-11-25 04:50:24.784 UTC [1095005] STATEMENT:  SELECT create_distributed_table('pk_on_two_non_part_cols', 'partition_col', 'hash');
2023-11-25 04:50:24.811 UTC [1095005] ERROR:  23505: duplicate key value violates unique constraint "pk_on_two_non_part_cols_pkey_365020"
2023-11-25 04:50:24.811 UTC [1095005] DETAIL:  Key (other_col, other_col_2)=(1, 1) already exists.
2023-11-25 04:50:24.811 UTC [1095005] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:24.811 UTC [1095005] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:24.811 UTC [1095005] STATEMENT:  INSERT INTO pk_on_two_non_part_cols VALUES (1,1,1);
2023-11-25 04:50:24.845 UTC [1095005] ERROR:  23P01: conflicting key value violates exclusion constraint "ex_on_part_col_partition_col_excl_365024"
2023-11-25 04:50:24.845 UTC [1095005] DETAIL:  Key (partition_col)=(1) conflicts with existing key (partition_col)=(1).
2023-11-25 04:50:24.845 UTC [1095005] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:24.845 UTC [1095005] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:24.845 UTC [1095005] STATEMENT:  INSERT INTO ex_on_part_col (partition_col, other_col) VALUES (1,2);
2023-11-25 04:50:24.858 UTC [1095005] ERROR:  23P01: conflicting key value violates exclusion constraint "ex_on_two_columns_partition_col_other_col_excl_365028"
2023-11-25 04:50:24.858 UTC [1095005] DETAIL:  Key (partition_col, other_col)=(1, 1) conflicts with existing key (partition_col, other_col)=(1, 1).
2023-11-25 04:50:24.858 UTC [1095005] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:24.858 UTC [1095005] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:24.858 UTC [1095005] STATEMENT:  INSERT INTO ex_on_two_columns (partition_col, other_col) VALUES (1,1);
2023-11-25 04:50:24.875 UTC [1095005] ERROR:  23P01: conflicting key value violates exclusion constraint "ex_on_two_columns_prt_partition_col_other_col_excl_365032"
2023-11-25 04:50:24.875 UTC [1095005] DETAIL:  Key (partition_col, other_col)=(1, 101) conflicts with existing key (partition_col, other_col)=(1, 101).
2023-11-25 04:50:24.875 UTC [1095005] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:24.875 UTC [1095005] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:24.875 UTC [1095005] STATEMENT:  INSERT INTO ex_on_two_columns_prt (partition_col, other_col) VALUES (1,101);
2023-11-25 04:50:24.878 UTC [1095005] ERROR:  0A000: cannot create constraint on "ex_wrong_operator"
2023-11-25 04:50:24.878 UTC [1095005] DETAIL:  Distributed relations cannot have UNIQUE, EXCLUDE, or PRIMARY KEY constraints that do not include the partition column (with an equality operator if EXCLUDE).
2023-11-25 04:50:24.878 UTC [1095005] LOCATION:  ErrorIfUnsupportedConstraint, table.c:3021
2023-11-25 04:50:24.878 UTC [1095005] STATEMENT:  SELECT create_distributed_table('ex_wrong_operator', 'partition_col', 'hash');
2023-11-25 04:50:24.905 UTC [1095005] ERROR:  23P01: conflicting key value violates exclusion constraint "ex_overlaps_other_col_partition_col_excl_365039"
2023-11-25 04:50:24.905 UTC [1095005] DETAIL:  Key (other_col, partition_col)=(["2016-01-15 00:00:00","2016-02-01 00:00:00"], ["2016-01-01 00:00:00","2016-02-01 00:00:00"]) conflicts with existing key (other_col, partition_col)=(["2016-01-01 00:00:00","2016-02-01 00:00:00"], ["2016-01-01 00:00:00","2016-02-01 00:00:00"]).
2023-11-25 04:50:24.905 UTC [1095005] CONTEXT:  while executing command on localhost:57638
2023-11-25 04:50:24.905 UTC [1095005] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:24.905 UTC [1095005] STATEMENT:  INSERT INTO ex_overlaps (partition_col, other_col) VALUES ('[2016-01-01 00:00:00, 2016-02-01 00:00:00]', '[2016-01-15 00:00:00, 2016-02-01 00:00:00]');
2023-11-25 04:50:24.942 UTC [1095005] ERROR:  23505: duplicate key value violates unique constraint "uq_two_columns_named_uniq_365048"
2023-11-25 04:50:24.942 UTC [1095005] DETAIL:  Key (partition_col, other_col)=(1, 1) already exists.
2023-11-25 04:50:24.942 UTC [1095005] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:24.942 UTC [1095005] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:24.942 UTC [1095005] STATEMENT:  INSERT INTO uq_two_columns_named (partition_col, other_col) VALUES (1,1);
2023-11-25 04:50:24.954 UTC [1095005] ERROR:  23P01: conflicting key value violates exclusion constraint "ex_on_part_col_named_exclude_365052"
2023-11-25 04:50:24.954 UTC [1095005] DETAIL:  Key (partition_col)=(1) conflicts with existing key (partition_col)=(1).
2023-11-25 04:50:24.954 UTC [1095005] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:24.954 UTC [1095005] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:24.954 UTC [1095005] STATEMENT:  INSERT INTO ex_on_part_col_named (partition_col, other_col) VALUES (1,2);
2023-11-25 04:50:24.966 UTC [1095005] ERROR:  23P01: conflicting key value violates exclusion constraint "ex_on_two_columns_named_exclude_365056"
2023-11-25 04:50:24.966 UTC [1095005] DETAIL:  Key (partition_col, other_col)=(1, 1) conflicts with existing key (partition_col, other_col)=(1, 1).
2023-11-25 04:50:24.966 UTC [1095005] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:24.966 UTC [1095005] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:24.966 UTC [1095005] STATEMENT:  INSERT INTO ex_on_two_columns_named (partition_col, other_col) VALUES (1,1);
2023-11-25 04:50:24.980 UTC [1095005] ERROR:  23P01: conflicting key value violates exclusion constraint "ex_multiple_excludes_excl1_365060"
2023-11-25 04:50:24.980 UTC [1095005] DETAIL:  Key (partition_col, other_col)=(1, 1) conflicts with existing key (partition_col, other_col)=(1, 1).
2023-11-25 04:50:24.980 UTC [1095005] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:24.980 UTC [1095005] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:24.980 UTC [1095005] STATEMENT:  INSERT INTO ex_multiple_excludes (partition_col, other_col, other_other_col) VALUES (1,1,2);
2023-11-25 04:50:24.981 UTC [1095005] ERROR:  23P01: conflicting key value violates exclusion constraint "ex_multiple_excludes_excl2_365060"
2023-11-25 04:50:24.981 UTC [1095005] DETAIL:  Key (partition_col, other_other_col)=(1, 1) conflicts with existing key (partition_col, other_other_col)=(1, 1).
2023-11-25 04:50:24.981 UTC [1095005] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:24.981 UTC [1095005] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:24.981 UTC [1095005] STATEMENT:  INSERT INTO ex_multiple_excludes (partition_col, other_col, other_other_col) VALUES (1,2,1);
2023-11-25 04:50:24.982 UTC [1095005] ERROR:  0A000: cannot create constraint on "ex_wrong_operator_named"
2023-11-25 04:50:24.982 UTC [1095005] DETAIL:  Distributed relations cannot have UNIQUE, EXCLUDE, or PRIMARY KEY constraints that do not include the partition column (with an equality operator if EXCLUDE).
2023-11-25 04:50:24.982 UTC [1095005] LOCATION:  ErrorIfUnsupportedConstraint, table.c:3021
2023-11-25 04:50:24.982 UTC [1095005] STATEMENT:  SELECT create_distributed_table('ex_wrong_operator_named', 'partition_col', 'hash');
2023-11-25 04:50:24.996 UTC [1095005] ERROR:  23P01: conflicting key value violates exclusion constraint "ex_overlaps_operator_named_exclude_365067"
2023-11-25 04:50:24.996 UTC [1095005] DETAIL:  Key (other_col, partition_col)=(["2016-01-15 00:00:00","2016-02-01 00:00:00"], ["2016-01-01 00:00:00","2016-02-01 00:00:00"]) conflicts with existing key (other_col, partition_col)=(["2016-01-01 00:00:00","2016-02-01 00:00:00"], ["2016-01-01 00:00:00","2016-02-01 00:00:00"]).
2023-11-25 04:50:24.996 UTC [1095005] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:24.996 UTC [1095005] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:24.996 UTC [1095005] STATEMENT:  INSERT INTO ex_overlaps_named (partition_col, other_col) VALUES ('[2016-01-01 00:00:00, 2016-02-01 00:00:00]', '[2016-01-15 00:00:00, 2016-02-01 00:00:00]');
2023-11-25 04:50:25.147 UTC [1095103] ERROR:  2BP01: cannot drop table raw_table_1 because other objects depend on it
2023-11-25 04:50:25.147 UTC [1095103] DETAIL:  constraint raw_table_2_user_id_fkey on table raw_table_2 depends on table raw_table_1
2023-11-25 04:50:25.147 UTC [1095103] HINT:  Use DROP ... CASCADE to drop the dependent objects too.
2023-11-25 04:50:25.147 UTC [1095103] LOCATION:  reportDependentObjects, dependency.c:1189
2023-11-25 04:50:25.147 UTC [1095103] STATEMENT:  DROP TABLE raw_table_1;
2023-11-25 04:50:25.383 UTC [1095188] ERROR:  0A000: complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
2023-11-25 04:50:25.383 UTC [1095188] LOCATION:  JoinOrderList, multi_join_order.c:310
2023-11-25 04:50:25.383 UTC [1095188] STATEMENT:  INSERT INTO agg_results_third (user_id, value_1_agg)
	SELECT user_id, array_length(events_table, 1)
	FROM (
	  SELECT user_id, array_agg(event ORDER BY time) AS events_table
	  FROM (
	    SELECT u.user_id, e.event_type::text AS event, e.time
	    FROM users_table AS u,
	         events_table AS e
	    WHERE u.user_id != e.user_id
	      AND u.user_id >= 10
	      AND u.user_id <= 25
	      AND e.event_type IN (100, 101, 102)
	  ) t
	  GROUP BY user_id
	) q;
2023-11-25 04:50:25.386 UTC [1095188] ERROR:  0A000: complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
2023-11-25 04:50:25.386 UTC [1095188] LOCATION:  JoinOrderList, multi_join_order.c:310
2023-11-25 04:50:25.386 UTC [1095188] STATEMENT:  INSERT INTO agg_results_third (user_id, value_1_agg, value_2_agg )
	SELECT user_id, sum(array_length(events_table, 1)), length(hasdone_event)
	FROM (
	  SELECT
	    t1.user_id,
	    array_agg(event ORDER BY time) AS events_table,
	    COALESCE(hasdone_event, 'Has not done event') AS hasdone_event
	  FROM (
	    (
	      SELECT u.user_id, 'step=>1'::text AS event, e.time
	      FROM users_table AS u,
	          events_table AS e
	      WHERE  u.user_id != e.user_id
	      AND u.user_id >= 10
	      AND u.user_id <= 25
	      AND e.event_type IN (100, 101, 102)
	    )
	    UNION
	    (
	      SELECT u.user_id, 'step=>2'::text AS event, e.time
	      FROM users_table AS u,
	         events_table AS e
	      WHERE  u.user_id = e.user_id
	      AND u.user_id >= 10
	      AND u.user_id <= 25
	      AND e.event_type IN (103, 104, 105)
	    )
	  ) t1 LEFT JOIN (
	      SELECT DISTINCT user_id,
	        'Has done event'::TEXT AS hasdone_event
	      FROM  events_table AS e
	      WHERE  e.user_id >= 10
	      AND e.user_id <= 25
	      AND e.event_type IN (106, 107, 108)
	  ) t2 ON (t1.user_id = t2.user_id)
	  GROUP BY  t1.user_id, hasdone_event
	) t GROUP BY user_id, hasdone_event;
2023-11-25 04:50:25.390 UTC [1095188] ERROR:  XX000: the query contains a join that requires repartitioning
2023-11-25 04:50:25.390 UTC [1095188] HINT:  Set citus.enable_repartition_joins to on to enable repartitioning
2023-11-25 04:50:25.390 UTC [1095188] LOCATION:  JobExecutorType, multi_server_executor.c:68
2023-11-25 04:50:25.390 UTC [1095188] STATEMENT:  INSERT INTO agg_results_third (user_id, value_1_agg, value_2_agg )
	SELECT user_id, sum(array_length(events_table, 1)), length(hasdone_event)
	FROM (
	  SELECT
	    t1.user_id,
	    array_agg(event ORDER BY time) AS events_table,
	    COALESCE(hasdone_event, 'Has not done event') AS hasdone_event
	  FROM (
	    (
	      SELECT u.user_id, 'step=>1'::text AS event, e.time
	      FROM users_table AS u,
	          events_table AS e
	      WHERE  u.user_id = e.user_id
	      AND u.user_id >= 10
	      AND u.user_id <= 25
	      AND e.event_type IN (100, 101, 102)
	    )
	    UNION
	    (
	      SELECT u.user_id, 'step=>2'::text AS event, e.time
	      FROM users_table AS u,
	         events_table AS e
	      WHERE  u.user_id = e.event_type
	      AND u.user_id >= 10
	      AND u.user_id <= 25
	      AND e.event_type IN (103, 104, 105)
	    )
	  ) t1 LEFT JOIN (
	      SELECT DISTINCT user_id,
	        'Has done event'::TEXT AS hasdone_event
	      FROM  events_table AS e
	      WHERE  e.user_id >= 10
	      AND e.user_id <= 25
	      AND e.event_type IN (106, 107, 108)
	  ) t2 ON (t1.user_id = t2.user_id)
	  GROUP BY  t1.user_id, hasdone_event
	) t GROUP BY user_id, hasdone_event;
2023-11-25 04:50:25.417 UTC [1095188] ERROR:  0A000: complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
2023-11-25 04:50:25.417 UTC [1095188] LOCATION:  JoinOrderList, multi_join_order.c:310
2023-11-25 04:50:25.417 UTC [1095188] STATEMENT:  INSERT INTO agg_results_third (user_id, value_1_agg, value_2_agg)
	SELECT
	  user_id,
	  avg(array_length(events_table, 1)) AS event_average,
	  count_pay
	  FROM (
	  SELECT
	  subquery_1.user_id,
	  array_agg(event ORDER BY time) AS events_table,
	  COALESCE(count_pay, 0) AS count_pay
	  FROM
	  (
	    (SELECT
	      users_table.user_id,
	      'action=>1'AS event,
	      events_table.time
	    FROM
	      users_table,
	      events_table
	    WHERE
	      users_table.user_id = events_table.user_id AND
	      users_table.user_id >= 10 AND
	      users_table.user_id <= 70 AND
	      events_table.event_type > 10 AND events_table.event_type < 12
	      )
	    UNION
	    (SELECT
	      users_table.user_id,
	      'action=>2'AS event,
	      events_table.time
	    FROM
	      users_table,
	      events_table
	    WHERE
	      users_table.user_id != events_table.user_id AND
	      users_table.user_id >= 10 AND
	      users_table.user_id <= 70 AND
	      events_table.event_type > 12 AND events_table.event_type < 14
	    )
	  ) AS subquery_1
	  LEFT JOIN
	    (SELECT
	       user_id,
	      COUNT(*) AS count_pay
	    FROM
	      users_table
	    WHERE
	      user_id >= 10 AND
	      user_id <= 70 AND
	      users_table.value_1 > 15 AND users_table.value_1 < 17
	    GROUP BY
	      user_id
	    HAVING
	      COUNT(*) > 1) AS subquery_2
	  ON
	    subquery_1.user_id = subquery_2.user_id
	  GROUP BY
	    subquery_1.user_id,
	    count_pay) AS subquery_top
	WHERE
	  array_ndims(events_table) > 0
	GROUP BY
	  count_pay, user_id
	ORDER BY
	  count_pay;
2023-11-25 04:50:25.434 UTC [1095188] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:25.434 UTC [1095188] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:25.434 UTC [1095188] STATEMENT:  INSERT INTO agg_results_third (user_id, agg_time, value_2_agg)
	SELECT
	    user_id,
	    user_lastseen,
	    array_length(event_array, 1)
	FROM (
	    SELECT
	        user_id,
	        max(u.time) as user_lastseen,
	        array_agg(event_type ORDER BY u.time) AS event_array
	    FROM (
	        SELECT user_id, time
	        FROM users_table
	        WHERE
	        user_id >= 10 AND
	        user_id <= 70 AND
	        users_table.value_1 > 10 AND users_table.value_1 < 12
	        ) u LEFT JOIN LATERAL (
	          SELECT event_type, time
	          FROM events_table
	          WHERE user_id != u.user_id AND
	          events_table.event_type > 10 AND events_table.event_type < 12
	        ) t ON true
	        GROUP BY user_id
	) AS shard_union
	ORDER BY user_lastseen DESC;
2023-11-25 04:50:25.434 UTC [1095188] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:25.434 UTC [1095188] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:25.434 UTC [1095188] STATEMENT:  INSERT INTO agg_results_third (user_id, agg_time, value_2_agg)
	SELECT
	    user_id,
	    user_lastseen,
	    array_length(event_array, 1)
	FROM (
	    SELECT
	        user_id,
	        max(u.time) as user_lastseen,
	        array_agg(event_type ORDER BY u.time) AS event_array
	    FROM (
	        SELECT user_id, time
	        FROM users_table
	        WHERE
	        user_id >= 10 AND
	        user_id <= 70 AND
	        users_table.value_1 > 10 AND users_table.value_1 < 12
	        ) u LEFT JOIN LATERAL (
	          SELECT event_type, time
	          FROM events_table
	          WHERE event_type = u.user_id AND
	          events_table.event_type > 10 AND events_table.event_type < 12
	        ) t ON true
	        GROUP BY user_id
	) AS shard_union
	ORDER BY user_lastseen DESC;
2023-11-25 04:50:25.437 UTC [1095188] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:25.437 UTC [1095188] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:25.437 UTC [1095188] STATEMENT:  INSERT INTO agg_results_third (user_id, agg_time, value_2_agg)
	SELECT
	    user_id,
	    user_lastseen,
	    array_length(event_array, 1)
	FROM (
	    SELECT
	        user_id,
	        max(u.time) as user_lastseen,
	        array_agg(event_type ORDER BY u.time) AS event_array
	    FROM (
	        SELECT user_id, time, value_3 as val_3
	        FROM users_table
	        WHERE
	        user_id >= 10 AND
	        user_id <= 70 AND
	        users_table.value_1 > 10 AND users_table.value_1 < 12
	        ) u LEFT JOIN LATERAL (
	          SELECT event_type, time
	          FROM events_table
	          WHERE event_type = u.val_3 AND
	          events_table.event_type > 10 AND events_table.event_type < 12
	        ) t ON true
	        GROUP BY user_id
	) AS shard_union
	ORDER BY user_lastseen DESC;
2023-11-25 04:50:25.462 UTC [1095188] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:25.462 UTC [1095188] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:25.462 UTC [1095188] STATEMENT:  INSERT INTO agg_results_third(user_id, value_2_agg)
	SELECT user_id, value_2 FROM users_table WHERE
	  value_1 > 101 AND value_1 < 110
	  AND value_2 >= 5
	  AND EXISTS (SELECT user_id FROM events_table WHERE event_type>101  AND event_type < 110 AND value_3 > 100 AND user_id!=users_table.user_id);
2023-11-25 04:50:25.463 UTC [1095188] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:25.463 UTC [1095188] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:25.463 UTC [1095188] STATEMENT:  INSERT INTO agg_results_third(user_id, value_2_agg)
	SELECT user_id, value_2 FROM users_table WHERE
	  value_1 > 101 AND value_1 < 110
	  AND value_2 >= 5
	  AND EXISTS (SELECT user_id FROM events_table WHERE event_type>101  AND event_type < 110 AND value_3 > 100 AND event_type = users_table.user_id);
2023-11-25 04:50:25.464 UTC [1095188] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:25.464 UTC [1095188] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:25.464 UTC [1095188] STATEMENT:  INSERT INTO agg_results_third(user_id, value_2_agg)
	SELECT user_id, value_2 FROM users_table WHERE
	  value_1 = 101
	  AND value_2 >= 5
	  AND NOT EXISTS (SELECT user_id FROM events_table WHERE event_type=101 AND value_3 > 100 AND user_id!=users_table.user_id);
2023-11-25 04:50:25.464 UTC [1095188] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:25.464 UTC [1095188] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:25.464 UTC [1095188] STATEMENT:  INSERT INTO agg_results_third(user_id, value_2_agg)
	SELECT user_id, value_2 FROM users_table WHERE
	  value_1 = 101
	  AND value_2 >= 5
	  AND NOT EXISTS (SELECT user_id FROM events_table WHERE event_type=101 AND value_3 > 100 AND event_type=users_table.user_id);
2023-11-25 04:50:25.465 UTC [1095188] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:25.465 UTC [1095188] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:25.465 UTC [1095188] STATEMENT:  INSERT INTO agg_results_third(user_id, value_2_agg)
	SELECT user_id, value_2 FROM users_table WHERE
	  value_1 > 100
	  AND value_2 >= 5
	  AND  EXISTS (SELECT user_id FROM events_table WHERE event_type!=100 AND value_3 > 100 AND user_id=users_table.user_id)
	  AND  EXISTS (SELECT user_id FROM events_table WHERE event_type=101 AND value_3 > 100 AND user_id!=users_table.user_id);
2023-11-25 04:50:25.466 UTC [1095188] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:25.466 UTC [1095188] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:25.466 UTC [1095188] STATEMENT:  INSERT INTO agg_results_third(user_id, value_2_agg)
	SELECT user_id, value_2 FROM users_table WHERE
	  value_2 >= 5
	  AND  EXISTS (SELECT user_id FROM events_table WHERE event_type > 100 AND event_type <= 300 AND value_3 > 100 AND user_id!=users_table.user_id)
	  AND  NOT EXISTS (SELECT user_id FROM events_table WHERE event_type > 300 AND event_type <= 350  AND value_3 > 100 AND user_id=users_table.user_id);
2023-11-25 04:50:25.469 UTC [1095188] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:25.469 UTC [1095188] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:25.469 UTC [1095188] STATEMENT:  INSERT INTO agg_results_third(user_id, value_2_agg)
	  SELECT user_id,
	         value_2
	  FROM   users_table
	  WHERE  value_1 > 100
	         AND value_1 < 124
	         AND value_2 >= 5
	         AND EXISTS (SELECT user_id
	                     FROM   events_table
	                     WHERE  event_type > 100
	                            AND event_type < 124
	                            AND value_3 > 100
	                            AND user_id != users_table.user_id
	                     GROUP  BY user_id
	                     HAVING Count(*) > 2);
2023-11-25 04:50:25.469 UTC [1095188] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:25.469 UTC [1095188] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:25.469 UTC [1095188] STATEMENT:  INSERT INTO agg_results_third(user_id, value_2_agg)
	  SELECT user_id,
	         value_2
	  FROM   users_table
	  WHERE  value_1 > 100
	         AND value_1 < 124
	         AND value_2 >= 5
	         AND EXISTS (SELECT user_id
	                     FROM   events_table
	                     WHERE  event_type > 100
	                            AND event_type < 124
	                            AND value_3 > 100
	                            AND event_type = users_table.user_id
	                     GROUP  BY user_id
	                     HAVING Count(*) > 2);
2023-11-25 04:50:25.470 UTC [1095188] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:25.470 UTC [1095188] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:25.470 UTC [1095188] STATEMENT:  INSERT INTO agg_results_third(user_id, value_2_agg)
	  SELECT user_id,
	         value_2
	  FROM   users_table
	  WHERE  value_1 > 100
	         AND value_1 < 124
	         AND value_2 >= 5
	         AND EXISTS (SELECT user_id
	                     FROM   events_table
	                     WHERE  event_type > 100
	                            AND event_type < 124
	                            AND value_3 > 100
	                            AND user_id = users_table.value_1
	                     GROUP  BY user_id
	                     HAVING Count(*) > 2);
2023-11-25 04:50:25.496 UTC [1095188] ERROR:  0A000: complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
2023-11-25 04:50:25.496 UTC [1095188] LOCATION:  JoinOrderList, multi_join_order.c:310
2023-11-25 04:50:25.496 UTC [1095188] STATEMENT:  INSERT INTO agg_results_third(user_id, value_1_agg, value_3_agg)
	SELECT
	    users_table.user_id, users_table.value_1, prob
	FROM
	   users_table
	        JOIN
	   (SELECT
	      ma.user_id, (GREATEST(coalesce(ma.value_4 / 250, 0.0) + GREATEST(1.0))) / 2 AS prob
	    FROM
	      users_table AS ma, events_table as short_list
	    WHERE
	      short_list.user_id != ma.user_id and ma.value_1 < 50 and short_list.event_type < 50
	    ) temp
	  ON users_table.user_id = temp.user_id
	  WHERE users_table.value_1 < 50;
2023-11-25 04:50:25.498 UTC [1095188] ERROR:  XX000: the query contains a join that requires repartitioning
2023-11-25 04:50:25.498 UTC [1095188] HINT:  Set citus.enable_repartition_joins to on to enable repartitioning
2023-11-25 04:50:25.498 UTC [1095188] LOCATION:  JobExecutorType, multi_server_executor.c:68
2023-11-25 04:50:25.498 UTC [1095188] STATEMENT:  INSERT INTO agg_results_third(user_id, value_1_agg, value_3_agg)
	SELECT
	    users_table.user_id, users_table.value_1, prob
	FROM
	   users_table
	        JOIN
	   (SELECT
	      ma.user_id, (GREATEST(coalesce(ma.value_4 / 250, 0.0) + GREATEST(1.0))) / 2 AS prob
	    FROM
	      users_table AS ma, events_table as short_list
	    WHERE
	      short_list.user_id = ma.value_2 and ma.value_1 < 50 and short_list.event_type < 50
	    ) temp
	  ON users_table.user_id = temp.user_id
	  WHERE users_table.value_1 < 50;
2023-11-25 04:50:25.533 UTC [1095186] ERROR:  23505: duplicate key value violates unique constraint "raw_events_second_user_id_value_1_key_13300004"
2023-11-25 04:50:25.533 UTC [1095186] DETAIL:  Key (user_id, value_1)=(1, 10) already exists.
2023-11-25 04:50:25.533 UTC [1095186] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:25.533 UTC [1095186] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:25.533 UTC [1095186] STATEMENT:  INSERT INTO raw_events_second  SELECT * FROM raw_events_first;
2023-11-25 04:50:25.578 UTC [1095186] ERROR:  42883: function multi_insert_select.evaluate_on_master(integer) does not exist
2023-11-25 04:50:25.578 UTC [1095186] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:50:25.578 UTC [1095186] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:25.578 UTC [1095186] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:25.578 UTC [1095186] STATEMENT:  INSERT INTO raw_events_second (user_id, value_1)
	SELECT
	  user_id, evaluate_on_master(value_1)
	FROM
	  raw_events_first
	WHERE
	  user_id = 0;
2023-11-25 04:50:25.601 UTC [1095186] ERROR:  23505: duplicate key value violates unique constraint "raw_events_second_user_id_value_1_key_13300007"
2023-11-25 04:50:25.601 UTC [1095186] DETAIL:  Key (user_id, value_1)=(9, 90) already exists.
2023-11-25 04:50:25.601 UTC [1095186] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:25.601 UTC [1095186] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:25.601 UTC [1095186] STATEMENT:  INSERT INTO raw_events_second (user_id, value_1, value_3)
	SELECT
	   user_id, value_1, value_3
	FROM
	   raw_events_first
	WHERE
	   user_id = 9 OR user_id = 16
	RETURNING *;
2023-11-25 04:50:25.622 UTC [1095186] ERROR:  23505: duplicate key value violates unique constraint "agg_events_user_id_value_1_agg_key_13300008"
2023-11-25 04:50:25.622 UTC [1095186] DETAIL:  Key (user_id, value_1_agg)=(1, 10) already exists.
2023-11-25 04:50:25.622 UTC [1095186] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:25.622 UTC [1095186] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:25.622 UTC [1095186] STATEMENT:  INSERT INTO agg_events (value_3_agg, value_4_agg, value_1_agg, user_id)
	SELECT
	   sum(value_3), count(value_4), sum(value_1), user_id
	FROM
	   raw_events_first
	GROUP BY
	   value_2, user_id
	RETURNING *;
2023-11-25 04:50:25.629 UTC [1095186] ERROR:  23505: duplicate key value violates unique constraint "agg_events_user_id_value_1_agg_key_13300008"
2023-11-25 04:50:25.629 UTC [1095186] DETAIL:  Key (user_id, value_1_agg)=(1, 10) already exists.
2023-11-25 04:50:25.629 UTC [1095186] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:25.629 UTC [1095186] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:25.629 UTC [1095186] STATEMENT:  INSERT INTO agg_events
	            (value_1_agg,
	             user_id)
	SELECT SUM(value_1),
	       id
	FROM   (SELECT raw_events_second.user_id AS id,
	               raw_events_second.value_1
	        FROM   raw_events_first,
	               raw_events_second
	        WHERE  raw_events_first.user_id = raw_events_second.user_id) AS foo
	GROUP  BY id
	ORDER  BY id;
2023-11-25 04:50:25.632 UTC [1095186] ERROR:  23505: duplicate key value violates unique constraint "agg_events_user_id_value_1_agg_key_13300008"
2023-11-25 04:50:25.632 UTC [1095186] DETAIL:  Key (user_id, value_1_agg)=(1, 10) already exists.
2023-11-25 04:50:25.632 UTC [1095186] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:25.632 UTC [1095186] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:25.632 UTC [1095186] STATEMENT:  INSERT INTO agg_events
	            (value_4_agg,
	             value_1_agg,
	             user_id)
	SELECT v4,
	       v1,
	       id
	FROM   (SELECT SUM(raw_events_second.value_4) AS v4,
	               SUM(raw_events_first.value_1) AS v1,
	               raw_events_second.user_id      AS id
	        FROM   raw_events_first,
	               raw_events_second
	        WHERE  raw_events_first.user_id = raw_events_second.user_id
	        GROUP  BY raw_events_second.user_id) AS foo
	ORDER  BY id;
2023-11-25 04:50:26.019 UTC [1095186] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:26.019 UTC [1095186] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:26.019 UTC [1095186] STATEMENT:  INSERT INTO agg_events (user_id)
	 SELECT
	   raw_events_first.user_id
	 FROM
	   raw_events_first LEFT JOIN raw_events_second ON raw_events_first.user_id = raw_events_second.value_1;
2023-11-25 04:50:26.102 UTC [1095186] ERROR:  0A000: cannot perform distributed planning on this query
2023-11-25 04:50:26.102 UTC [1095186] DETAIL:  Cartesian products are currently unsupported
2023-11-25 04:50:26.102 UTC [1095186] LOCATION:  JoinSequenceArray, multi_physical_planner.c:3589
2023-11-25 04:50:26.102 UTC [1095186] STATEMENT:  INSERT INTO agg_events
	             (user_id)
	 SELECT raw_events_second.user_id
	 FROM   raw_events_first,
	        raw_events_second
	 WHERE  raw_events_first.user_id = raw_events_first.value_1;
2023-11-25 04:50:26.102 UTC [1095186] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:26.102 UTC [1095186] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:26.102 UTC [1095186] STATEMENT:  INSERT INTO agg_events (user_id)
	 SELECT
	   raw_events_first.user_id
	 FROM
	   raw_events_first LEFT JOIN raw_events_second ON raw_events_first.value_1 = raw_events_second.value_1;
2023-11-25 04:50:26.199 UTC [1095186] ERROR:  XX000: EXPLAIN ANALYZE is currently not supported for INSERT ... SELECT commands via coordinator
2023-11-25 04:50:26.199 UTC [1095186] LOCATION:  NonPushableInsertSelectExplainScan, multi_explain.c:252
2023-11-25 04:50:26.199 UTC [1095186] STATEMENT:  EXPLAIN (costs off, analyze on)
	 INSERT INTO agg_events (user_id)
	 SELECT
	   raw_events_first.user_id
	 FROM
	   raw_events_first INNER JOIN raw_events_second ON raw_events_first.value_1 = raw_events_second.value_1;
2023-11-25 04:50:26.201 UTC [1095186] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:26.201 UTC [1095186] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:26.201 UTC [1095186] STATEMENT:  INSERT INTO agg_events (user_id)
	SELECT
	  raw_events_first.user_id
	FROM
	  raw_events_first LEFT JOIN raw_events_second ON raw_events_first.user_id = raw_events_second.value_1
	WHERE
	  raw_events_first.user_id = 10;
2023-11-25 04:50:26.215 UTC [1095186] ERROR:  0A000: complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
2023-11-25 04:50:26.215 UTC [1095186] LOCATION:  JoinOrderList, multi_join_order.c:310
2023-11-25 04:50:26.215 UTC [1095186] STATEMENT:  INSERT INTO agg_events
	            (value_4_agg,
	             value_1_agg,
	             user_id)
	SELECT v4,
	       v1,
	       id
	FROM   (SELECT SUM(raw_events_second.value_4) AS v4,
	               SUM(raw_events_first.value_1) AS v1,
	               raw_events_second.user_id      AS id
	        FROM   raw_events_first,
	               raw_events_second
	        WHERE  raw_events_first.user_id != raw_events_second.user_id
	        GROUP  BY raw_events_second.user_id) AS foo;
2023-11-25 04:50:26.219 UTC [1095186] ERROR:  22004: the partition column of table multi_insert_select.agg_events cannot be NULL
2023-11-25 04:50:26.219 UTC [1095186] LOCATION:  ShardIdForTuple, multi_copy.c:2592
2023-11-25 04:50:26.219 UTC [1095186] STATEMENT:  INSERT INTO agg_events
	            (value_4_agg,
	             value_1_agg,
	             user_id)
	SELECT v4,
	       v1,
	       id
	FROM   (SELECT SUM(raw_events_second.value_4) AS v4,
	               SUM(raw_events_first.value_1) AS v1,
	               raw_events_second.value_3      AS id
	        FROM   raw_events_first,
	               raw_events_second
	        WHERE  raw_events_first.user_id = raw_events_second.user_id
	        GROUP  BY raw_events_second.value_3) AS foo;
2023-11-25 04:50:26.219 UTC [1095186] ERROR:  22004: the partition column of table multi_insert_select.raw_events_second should have a value
2023-11-25 04:50:26.219 UTC [1095186] LOCATION:  NonPushableInsertSelectExecScan, insert_select_executor.c:153
2023-11-25 04:50:26.219 UTC [1095186] STATEMENT:  INSERT INTO raw_events_second
	            (value_1)
	SELECT value_1
	FROM   raw_events_first;
2023-11-25 04:50:26.220 UTC [1095186] ERROR:  22004: the partition column of table multi_insert_select.raw_events_second should have a value
2023-11-25 04:50:26.220 UTC [1095186] LOCATION:  NonPushableInsertSelectExecScan, insert_select_executor.c:153
2023-11-25 04:50:26.220 UTC [1095186] STATEMENT:  INSERT INTO raw_events_second
	            (value_1)
	SELECT user_id
	FROM   raw_events_first;
2023-11-25 04:50:26.221 UTC [1095186] ERROR:  22004: the partition column value cannot be NULL
2023-11-25 04:50:26.221 UTC [1095186] CONTEXT:  while executing command on localhost:57638
2023-11-25 04:50:26.221 UTC [1095186] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:26.221 UTC [1095186] STATEMENT:  INSERT INTO raw_events_second
	            (user_id)
	SELECT value_1
	FROM   raw_events_first;
2023-11-25 04:50:26.248 UTC [1095186] ERROR:  22004: the partition column value cannot be NULL
2023-11-25 04:50:26.248 UTC [1095186] CONTEXT:  while executing command on localhost:57638
2023-11-25 04:50:26.248 UTC [1095186] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:26.248 UTC [1095186] STATEMENT:  INSERT INTO agg_events
	            (value_3_agg,
	             value_4_agg,
	             value_1_agg,
	             value_2_agg,
	             user_id)
	SELECT SUM(value_3),
	       Count(value_4),
	       user_id,
	       SUM(value_1),
	       Avg(value_2)
	FROM   raw_events_first
	GROUP  BY user_id;
2023-11-25 04:50:26.251 UTC [1095186] ERROR:  22004: the partition column value cannot be NULL
2023-11-25 04:50:26.251 UTC [1095186] CONTEXT:  while executing command on localhost:57638
2023-11-25 04:50:26.251 UTC [1095186] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:26.251 UTC [1095186] STATEMENT:  INSERT INTO agg_events
	            (value_3_agg,
	             value_4_agg,
	             value_1_agg,
	             value_2_agg,
	             user_id)
	SELECT SUM(value_3),
	       Count(value_4),
	       user_id,
	       SUM(value_1),
	       value_2
	FROM   raw_events_first
	GROUP  BY user_id,
	          value_2;
2023-11-25 04:50:26.352 UTC [1095186] ERROR:  0A000: could not run distributed query with GROUPING SETS, CUBE, or ROLLUP
2023-11-25 04:50:26.352 UTC [1095186] HINT:  Consider using an equality filter on the distributed table's partition column.
2023-11-25 04:50:26.352 UTC [1095186] LOCATION:  DeferErrorIfQueryNotSupported, multi_logical_planner.c:945
2023-11-25 04:50:26.352 UTC [1095186] STATEMENT:  INSERT INTO agg_events
	            (user_id,
	             value_1_agg,
	             value_2_agg)
	SELECT user_id,
	       Sum(value_1) AS sum_val1,
	       Sum(value_2) AS sum_val2
	FROM   raw_events_second
	GROUP  BY grouping sets ( ( user_id ), ( value_1 ), ( user_id, value_1 ), ( ) );
2023-11-25 04:50:27.215 UTC [1095186] ERROR:  0A000: INSERT ... SELECT into an append-distributed table is not supported
2023-11-25 04:50:27.215 UTC [1095186] LOCATION:  NonPushableInsertSelectSupported, insert_select_planner.c:1572
2023-11-25 04:50:27.215 UTC [1095186] STATEMENT:  INSERT INTO insert_append_table (user_id, value_4)
	SELECT user_id, 1 FROM raw_events_second LIMIT 5;
2023-11-25 04:50:27.339 UTC [1095186] ERROR:  XX000: value too long for type character(1)
2023-11-25 04:50:27.339 UTC [1095186] LOCATION:  ReportCopyError, multi_copy.c:1193
2023-11-25 04:50:27.339 UTC [1095186] STATEMENT:  INSERT INTO coerce_agg(user_id, value_1_agg)
	SELECT *
	FROM (
	  SELECT user_id, value_1
	  FROM coerce_events
	) AS ftop
	LIMIT 5;
2023-11-25 04:50:27.360 UTC [1095186] ERROR:  XX000: value too long for type character(1)
2023-11-25 04:50:27.360 UTC [1095186] LOCATION:  ReportCopyError, multi_copy.c:1193
2023-11-25 04:50:27.360 UTC [1095186] STATEMENT:  INSERT INTO coerce_agg(user_id, value_1_agg)
	SELECT *
	FROM (
	  SELECT user_id, value_1
	  FROM coerce_events
	) AS ftop
	LIMIT 5;
2023-11-25 04:50:27.403 UTC [1095186] ERROR:  23514: new row for relation "coerce_agg_13300067" violates check constraint "small_number_13300067"
2023-11-25 04:50:27.403 UTC [1095186] DETAIL:  Failing row contains (10, 10).
2023-11-25 04:50:27.403 UTC [1095186] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:27.403 UTC [1095186] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:27.403 UTC [1095186] STATEMENT:  INSERT INTO coerce_agg(user_id, value_1_agg)
	SELECT *
	FROM (
	  SELECT user_id, value_1
	  FROM coerce_events
	) AS ftop;
2023-11-25 04:50:27.468 UTC [1095186] ERROR:  0A000: modifying the partition value of rows is not allowed
2023-11-25 04:50:27.468 UTC [1095186] LOCATION:  ErrorIfOnConflictNotSupported, multi_router_planner.c:1255
2023-11-25 04:50:27.468 UTC [1095186] STATEMENT:  INSERT INTO agg_events AS ae
	            (
	                        user_id,
	                        value_1_agg,
	                        agg_time
	            )
	SELECT user_id,
	       value_1,
	       time
	FROM   raw_events_first
	ON conflict (user_id, value_1_agg)
	DO UPDATE
	   SET    user_id = 42
	RETURNING user_id, value_1_agg;
2023-11-25 04:50:28.015 UTC [1096381] ERROR:  42P01: relation "users_copy_table" does not exist at character 26
2023-11-25 04:50:28.015 UTC [1096381] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 04:50:28.015 UTC [1096381] STATEMENT:  SELECT SUM(value_3) FROM users_copy_table;
2023-11-25 04:50:28.358 UTC [1096381] ERROR:  21000: more than one row returned by a subquery used as an expression
2023-11-25 04:50:28.358 UTC [1096381] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:28.358 UTC [1096381] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:28.358 UTC [1096381] STATEMENT:  UPDATE users_test_table as utt
	SET    value_1 = 3
	WHERE value_2 > (SELECT value_3 FROM events_test_table as ett WHERE utt.user_id = ett.user_id);
2023-11-25 04:50:28.359 UTC [1096381] ERROR:  0A000: only reference tables may be queried when targeting a reference table with multi shard UPDATE/DELETE queries with multiple tables 
2023-11-25 04:50:28.359 UTC [1096381] LOCATION:  MultiShardUpdateDeleteSupported, multi_router_planner.c:1294
2023-11-25 04:50:28.359 UTC [1096381] STATEMENT:  UPDATE users_reference_copy_table
	SET    value_2 = 5
	FROM   events_test_table
	WHERE  users_reference_copy_table.user_id = events_test_table.user_id;
2023-11-25 04:50:28.359 UTC [1096381] ERROR:  0A000: a join with USING causes an internal naming conflict, use ON instead
2023-11-25 04:50:28.359 UTC [1096381] LOCATION:  MultiShardUpdateDeleteSupported, multi_router_planner.c:1279
2023-11-25 04:50:28.359 UTC [1096381] STATEMENT:  UPDATE events_test_table
	SET value_2 = users_test_table.user_id
	FROM users_test_table
	FULL OUTER JOIN events_test_table e2 USING (user_id)
	WHERE e2.user_id = events_test_table.user_id RETURNING events_test_table.value_2;
2023-11-25 04:50:28.378 UTC [1096381] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:28.378 UTC [1096381] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:28.378 UTC [1096381] STATEMENT:  UPDATE users_test_table
	SET    value_2 = (SELECT value_3
	                  FROM   users_test_table);
2023-11-25 04:50:28.378 UTC [1096381] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:28.378 UTC [1096381] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:28.378 UTC [1096381] STATEMENT:  UPDATE users_test_table
	SET value_2 = 2
	WHERE
	  value_2 >
	          (SELECT
	              max(value_2)
	           FROM
	              events_test_table
	           WHERE
	              users_test_table.user_id > events_test_table.user_id AND
	              users_test_table.value_1 = events_test_table.value_1
	           GROUP BY
	              user_id
	          );
2023-11-25 04:50:28.388 UTC [1096381] ERROR:  0A000: functions used in the WHERE/ON/WHEN clause of modification queries on distributed tables must not be VOLATILE
2023-11-25 04:50:28.388 UTC [1096381] LOCATION:  TargetlistAndFunctionsSupported, multi_router_planner.c:616
2023-11-25 04:50:28.388 UTC [1096381] STATEMENT:  UPDATE users_test_table
	SET    value_2 = 5
	FROM   events_test_table
	WHERE  users_test_table.user_id = events_test_table.user_id * random();
2023-11-25 04:50:28.388 UTC [1096381] ERROR:  0A000: functions used in UPDATE queries on distributed tables must not be VOLATILE
2023-11-25 04:50:28.388 UTC [1096381] LOCATION:  TargetlistAndFunctionsSupported, multi_router_planner.c:575
2023-11-25 04:50:28.388 UTC [1096381] STATEMENT:  UPDATE users_test_table
	SET    value_2 = 5 * random()
	FROM   events_test_table
	WHERE  users_test_table.user_id = events_test_table.user_id;
2023-11-25 04:50:28.388 UTC [1096381] ERROR:  0A000: functions used in UPDATE queries on distributed tables must not be VOLATILE
2023-11-25 04:50:28.388 UTC [1096381] LOCATION:  SingleShardUpdateDeleteSupported, multi_router_planner.c:1330
2023-11-25 04:50:28.388 UTC [1096381] STATEMENT:  UPDATE users_test_table
	SET    value_1 = 3
	WHERE  user_id = 1 AND value_1 IN (SELECT value_1
	                                   FROM users_test_table
	                                   WHERE user_id = 1 AND value_2 > random());
2023-11-25 04:50:28.389 UTC [1096381] ERROR:  0A000: functions used in UPDATE queries on distributed tables must not be VOLATILE
2023-11-25 04:50:28.389 UTC [1096381] LOCATION:  SingleShardUpdateDeleteSupported, multi_router_planner.c:1330
2023-11-25 04:50:28.389 UTC [1096381] STATEMENT:  UPDATE users_test_table
	SET    value_2 = subquery.random FROM (SELECT user_id, random()
	                                       FROM events_test_table) subquery
	WHERE  users_test_table.user_id = subquery.user_id;
2023-11-25 04:50:28.415 UTC [1096381] ERROR:  0A000: cannot push down this subquery
2023-11-25 04:50:28.415 UTC [1096381] DETAIL:  Shards of relations in subquery need to have 1-to-1 shard partitioning
2023-11-25 04:50:28.415 UTC [1096381] LOCATION:  ErrorIfUnsupportedShardDistribution, multi_physical_planner.c:2432
2023-11-25 04:50:28.415 UTC [1096381] STATEMENT:  UPDATE users_test_table
	SET    value_2 = 5
	FROM   events_test_table_2
	WHERE  users_test_table.user_id = events_test_table_2.user_id;
2023-11-25 04:50:28.418 UTC [1096381] ERROR:  21000: more than one row returned by a subquery used as an expression
2023-11-25 04:50:28.418 UTC [1096381] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:28.418 UTC [1096381] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:28.418 UTC [1096381] STATEMENT:  DELETE FROM users_test_table
	WHERE  users_test_table.user_id = (SELECT user_id
	                                   FROM   events_test_table);
2023-11-25 04:50:28.425 UTC [1096381] ERROR:  0A000: cannot run DML queries with cursors
2023-11-25 04:50:28.425 UTC [1096381] LOCATION:  TargetlistAndFunctionsSupported, multi_router_planner.c:655
2023-11-25 04:50:28.425 UTC [1096381] STATEMENT:  UPDATE users_test_table SET value_2 = 5 WHERE CURRENT OF test_cursor;
2023-11-25 04:50:28.466 UTC [1096381] ERROR:  0A000: functions used in UPDATE queries on distributed tables must not be VOLATILE
2023-11-25 04:50:28.466 UTC [1096381] LOCATION:  TargetlistAndFunctionsSupported, multi_router_planner.c:575
2023-11-25 04:50:28.466 UTC [1096381] STATEMENT:  UPDATE test_table_2 SET double_col = random();
2023-11-25 04:50:28.695 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.695 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.695 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.695 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.695 UTC [1096741] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 04:50:28.695 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.695 UTC [1096741] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 04:50:28.695 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.695 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.695 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.695 UTC [1096741] DEBUG:  00000: push down of limit count: 5
2023-11-25 04:50:28.695 UTC [1096741] LOCATION:  WorkerLimitCount, multi_logical_optimizer.c:4778
2023-11-25 04:50:28.695 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.695 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.695 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.695 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.695 UTC [1096741] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 04:50:28.695 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.695 UTC [1096741] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 04:50:28.695 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.695 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum, worker_column_2 AS worker_column_3 FROM (SELECT us.user_id AS worker_column_1, us.value_1 AS worker_column_2 FROM (public.users_table_1400256 us JOIN public.events_table_1400260 ev ON ((us.user_id OPERATOR(pg_catalog.=) ev.user_id)))) worker_subquery GROUP BY worker_column_1, worker_column_2 ORDER BY worker_column_1, (sum(worker_column_2) OVER (PARTITION BY worker_column_1)) LIMIT '5'::bigint
2023-11-25 04:50:28.695 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.695 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum, worker_column_2 AS worker_column_3 FROM (SELECT us.user_id AS worker_column_1, us.value_1 AS worker_column_2 FROM (public.users_table_1400257 us JOIN public.events_table_1400261 ev ON ((us.user_id OPERATOR(pg_catalog.=) ev.user_id)))) worker_subquery GROUP BY worker_column_1, worker_column_2 ORDER BY worker_column_1, (sum(worker_column_2) OVER (PARTITION BY worker_column_1)) LIMIT '5'::bigint
2023-11-25 04:50:28.695 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.695 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum, worker_column_2 AS worker_column_3 FROM (SELECT us.user_id AS worker_column_1, us.value_1 AS worker_column_2 FROM (public.users_table_1400258 us JOIN public.events_table_1400262 ev ON ((us.user_id OPERATOR(pg_catalog.=) ev.user_id)))) worker_subquery GROUP BY worker_column_1, worker_column_2 ORDER BY worker_column_1, (sum(worker_column_2) OVER (PARTITION BY worker_column_1)) LIMIT '5'::bigint
2023-11-25 04:50:28.695 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.695 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum, worker_column_2 AS worker_column_3 FROM (SELECT us.user_id AS worker_column_1, us.value_1 AS worker_column_2 FROM (public.users_table_1400259 us JOIN public.events_table_1400263 ev ON ((us.user_id OPERATOR(pg_catalog.=) ev.user_id)))) worker_subquery GROUP BY worker_column_1, worker_column_2 ORDER BY worker_column_1, (sum(worker_column_2) OVER (PARTITION BY worker_column_1)) LIMIT '5'::bigint
2023-11-25 04:50:28.695 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.695 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.695 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.695 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.695 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.695 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.695 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.695 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.695 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.695 UTC [1096741] DEBUG:  00000: combine query: SELECT user_id, sum FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, sum bigint, worker_column_3 integer) ORDER BY user_id, sum LIMIT '5'::bigint
2023-11-25 04:50:28.695 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.695 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.695 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.696 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 13 in 3764 microseconds
2023-11-25 04:50:28.696 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.696 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.696 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.697 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 15 in 3465 microseconds
2023-11-25 04:50:28.697 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.698 UTC [1096741] DEBUG:  00000: task execution (1) for placement (832) on anchor shard (1400260) finished in 2316 microseconds on worker node localhost:57637
2023-11-25 04:50:28.698 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.698 UTC [1096741] DEBUG:  00000: task execution (2) for placement (833) on anchor shard (1400261) finished in 1253 microseconds on worker node localhost:57638
2023-11-25 04:50:28.698 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.699 UTC [1096741] DEBUG:  00000: task execution (3) for placement (834) on anchor shard (1400262) finished in 852 microseconds on worker node localhost:57637
2023-11-25 04:50:28.699 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.699 UTC [1096741] DEBUG:  00000: task execution (4) for placement (835) on anchor shard (1400263) finished in 1069 microseconds on worker node localhost:57638
2023-11-25 04:50:28.699 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.699 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 16 in 2685 microseconds
2023-11-25 04:50:28.699 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.702 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 14 in 5968 microseconds
2023-11-25 04:50:28.702 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.702 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 13: 2 to node localhost:57637
2023-11-25 04:50:28.702 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.702 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 14: 0 to node localhost:57637
2023-11-25 04:50:28.702 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.702 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 15: 2 to node localhost:57638
2023-11-25 04:50:28.702 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.702 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 16: 0 to node localhost:57638
2023-11-25 04:50:28.702 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.702 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.702 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.702 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.702 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.702 UTC [1096741] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 04:50:28.702 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.702 UTC [1096741] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 04:50:28.702 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.702 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.702 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.702 UTC [1096741] DEBUG:  00000: push down of limit count: 5
2023-11-25 04:50:28.702 UTC [1096741] LOCATION:  WorkerLimitCount, multi_logical_optimizer.c:4778
2023-11-25 04:50:28.702 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.702 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.702 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.702 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.702 UTC [1096741] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 04:50:28.702 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.702 UTC [1096741] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 04:50:28.702 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.702 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS value_1, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum FROM (SELECT j.user_id AS worker_column_1, j.value_1 AS worker_column_2 FROM (public.users_table_1400256 us(user_id, "time", value_1, value_2, value_3, value_4) JOIN public.events_table_1400260 ev(user_id, "time", event_type, value_2, value_3, value_4) USING (user_id)) j(user_id, "time", value_1, value_2, value_3, value_4, time_1, event_type, value_2_1, value_3_1, value_4_1)) worker_subquery GROUP BY worker_column_1, worker_column_2 ORDER BY (sum(worker_column_2) OVER (PARTITION BY worker_column_1)) DESC, worker_column_2 DESC, worker_column_1 DESC LIMIT '5'::bigint
2023-11-25 04:50:28.702 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.703 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS value_1, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum FROM (SELECT j.user_id AS worker_column_1, j.value_1 AS worker_column_2 FROM (public.users_table_1400257 us(user_id, "time", value_1, value_2, value_3, value_4) JOIN public.events_table_1400261 ev(user_id, "time", event_type, value_2, value_3, value_4) USING (user_id)) j(user_id, "time", value_1, value_2, value_3, value_4, time_1, event_type, value_2_1, value_3_1, value_4_1)) worker_subquery GROUP BY worker_column_1, worker_column_2 ORDER BY (sum(worker_column_2) OVER (PARTITION BY worker_column_1)) DESC, worker_column_2 DESC, worker_column_1 DESC LIMIT '5'::bigint
2023-11-25 04:50:28.703 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.703 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS value_1, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum FROM (SELECT j.user_id AS worker_column_1, j.value_1 AS worker_column_2 FROM (public.users_table_1400258 us(user_id, "time", value_1, value_2, value_3, value_4) JOIN public.events_table_1400262 ev(user_id, "time", event_type, value_2, value_3, value_4) USING (user_id)) j(user_id, "time", value_1, value_2, value_3, value_4, time_1, event_type, value_2_1, value_3_1, value_4_1)) worker_subquery GROUP BY worker_column_1, worker_column_2 ORDER BY (sum(worker_column_2) OVER (PARTITION BY worker_column_1)) DESC, worker_column_2 DESC, worker_column_1 DESC LIMIT '5'::bigint
2023-11-25 04:50:28.703 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.703 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS value_1, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum FROM (SELECT j.user_id AS worker_column_1, j.value_1 AS worker_column_2 FROM (public.users_table_1400259 us(user_id, "time", value_1, value_2, value_3, value_4) JOIN public.events_table_1400263 ev(user_id, "time", event_type, value_2, value_3, value_4) USING (user_id)) j(user_id, "time", value_1, value_2, value_3, value_4, time_1, event_type, value_2_1, value_3_1, value_4_1)) worker_subquery GROUP BY worker_column_1, worker_column_2 ORDER BY (sum(worker_column_2) OVER (PARTITION BY worker_column_1)) DESC, worker_column_2 DESC, worker_column_1 DESC LIMIT '5'::bigint
2023-11-25 04:50:28.703 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.703 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.703 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.703 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.703 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.703 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.703 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.703 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.703 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.703 UTC [1096741] DEBUG:  00000: combine query: SELECT user_id, value_1, sum FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, value_1 integer, sum bigint) ORDER BY sum DESC, value_1 DESC, user_id DESC LIMIT '5'::bigint
2023-11-25 04:50:28.703 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.703 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.703 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.703 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 17 in 3764 microseconds
2023-11-25 04:50:28.703 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.703 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.703 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.703 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 19 in 3465 microseconds
2023-11-25 04:50:28.703 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.704 UTC [1096741] DEBUG:  00000: task execution (1) for placement (832) on anchor shard (1400260) finished in 944 microseconds on worker node localhost:57637
2023-11-25 04:50:28.704 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.704 UTC [1096741] DEBUG:  00000: task execution (2) for placement (833) on anchor shard (1400261) finished in 950 microseconds on worker node localhost:57638
2023-11-25 04:50:28.704 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.705 UTC [1096741] DEBUG:  00000: task execution (3) for placement (834) on anchor shard (1400262) finished in 634 microseconds on worker node localhost:57637
2023-11-25 04:50:28.705 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.705 UTC [1096741] DEBUG:  00000: task execution (4) for placement (835) on anchor shard (1400263) finished in 794 microseconds on worker node localhost:57638
2023-11-25 04:50:28.705 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.706 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 20 in 2711 microseconds
2023-11-25 04:50:28.706 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.706 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 18 in 2916 microseconds
2023-11-25 04:50:28.706 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.706 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 17: 2 to node localhost:57637
2023-11-25 04:50:28.706 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.706 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 18: 0 to node localhost:57637
2023-11-25 04:50:28.706 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.706 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 19: 2 to node localhost:57638
2023-11-25 04:50:28.706 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.706 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 20: 0 to node localhost:57638
2023-11-25 04:50:28.706 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.715 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.715 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.715 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.715 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.715 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.715 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.715 UTC [1096741] DEBUG:  00000: push down of limit count: 10
2023-11-25 04:50:28.715 UTC [1096741] LOCATION:  WorkerLimitCount, multi_logical_optimizer.c:4778
2023-11-25 04:50:28.715 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.715 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.715 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.715 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.715 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS rank FROM (SELECT window_view.user_id AS worker_column_1, window_view.rank AS worker_column_2 FROM (SELECT DISTINCT users_table.user_id, rank() OVER (PARTITION BY users_table.user_id ORDER BY users_table.value_1) AS rank FROM public.users_table_1400256 users_table GROUP BY users_table.user_id, users_table.value_1 HAVING (count(*) OPERATOR(pg_catalog.>) 1)) window_view) worker_subquery ORDER BY worker_column_2 DESC, worker_column_1 LIMIT '10'::bigint
2023-11-25 04:50:28.715 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.715 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS rank FROM (SELECT window_view.user_id AS worker_column_1, window_view.rank AS worker_column_2 FROM (SELECT DISTINCT users_table.user_id, rank() OVER (PARTITION BY users_table.user_id ORDER BY users_table.value_1) AS rank FROM public.users_table_1400257 users_table GROUP BY users_table.user_id, users_table.value_1 HAVING (count(*) OPERATOR(pg_catalog.>) 1)) window_view) worker_subquery ORDER BY worker_column_2 DESC, worker_column_1 LIMIT '10'::bigint
2023-11-25 04:50:28.715 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.715 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS rank FROM (SELECT window_view.user_id AS worker_column_1, window_view.rank AS worker_column_2 FROM (SELECT DISTINCT users_table.user_id, rank() OVER (PARTITION BY users_table.user_id ORDER BY users_table.value_1) AS rank FROM public.users_table_1400258 users_table GROUP BY users_table.user_id, users_table.value_1 HAVING (count(*) OPERATOR(pg_catalog.>) 1)) window_view) worker_subquery ORDER BY worker_column_2 DESC, worker_column_1 LIMIT '10'::bigint
2023-11-25 04:50:28.715 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.715 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS rank FROM (SELECT window_view.user_id AS worker_column_1, window_view.rank AS worker_column_2 FROM (SELECT DISTINCT users_table.user_id, rank() OVER (PARTITION BY users_table.user_id ORDER BY users_table.value_1) AS rank FROM public.users_table_1400259 users_table GROUP BY users_table.user_id, users_table.value_1 HAVING (count(*) OPERATOR(pg_catalog.>) 1)) window_view) worker_subquery ORDER BY worker_column_2 DESC, worker_column_1 LIMIT '10'::bigint
2023-11-25 04:50:28.715 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.715 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.715 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.715 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.715 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.715 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.715 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.715 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.715 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.716 UTC [1096741] DEBUG:  00000: combine query: SELECT user_id, rank FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, rank bigint) ORDER BY rank DESC, user_id LIMIT '10'::bigint
2023-11-25 04:50:28.716 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.716 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.716 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.716 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 21 in 3764 microseconds
2023-11-25 04:50:28.716 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.716 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.716 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.717 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 23 in 3465 microseconds
2023-11-25 04:50:28.717 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.718 UTC [1096741] DEBUG:  00000: task execution (1) for placement (820) on anchor shard (1400256) finished in 1728 microseconds on worker node localhost:57637
2023-11-25 04:50:28.718 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.718 UTC [1096741] DEBUG:  00000: task execution (2) for placement (821) on anchor shard (1400257) finished in 652 microseconds on worker node localhost:57638
2023-11-25 04:50:28.718 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.718 UTC [1096741] DEBUG:  00000: task execution (4) for placement (823) on anchor shard (1400259) finished in 456 microseconds on worker node localhost:57638
2023-11-25 04:50:28.718 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.718 UTC [1096741] DEBUG:  00000: task execution (3) for placement (822) on anchor shard (1400258) finished in 559 microseconds on worker node localhost:57637
2023-11-25 04:50:28.718 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.720 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 22 in 4482 microseconds
2023-11-25 04:50:28.720 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.720 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 24 in 3604 microseconds
2023-11-25 04:50:28.720 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.720 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 21: 2 to node localhost:57637
2023-11-25 04:50:28.720 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.720 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 22: 0 to node localhost:57637
2023-11-25 04:50:28.720 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.721 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 23: 2 to node localhost:57638
2023-11-25 04:50:28.721 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.721 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 24: 0 to node localhost:57638
2023-11-25 04:50:28.721 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.727 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.727 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.727 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.727 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.727 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.727 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.727 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.727 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.727 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.727 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.727 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, rank() OVER (PARTITION BY worker_column_1 ORDER BY worker_column_2) AS rank, worker_column_2 AS worker_column_3 FROM (SELECT users_view.user_id AS worker_column_1, users_view.value_1 AS worker_column_2 FROM (SELECT users_table.user_id, users_table."time", users_table.value_1, users_table.value_2, users_table.value_3, users_table.value_4 FROM public.users_table_1400256 users_table) users_view) worker_subquery GROUP BY worker_column_1, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 4)
2023-11-25 04:50:28.727 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.727 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, rank() OVER (PARTITION BY worker_column_1 ORDER BY worker_column_2) AS rank, worker_column_2 AS worker_column_3 FROM (SELECT users_view.user_id AS worker_column_1, users_view.value_1 AS worker_column_2 FROM (SELECT users_table.user_id, users_table."time", users_table.value_1, users_table.value_2, users_table.value_3, users_table.value_4 FROM public.users_table_1400257 users_table) users_view) worker_subquery GROUP BY worker_column_1, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 4)
2023-11-25 04:50:28.727 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.727 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, rank() OVER (PARTITION BY worker_column_1 ORDER BY worker_column_2) AS rank, worker_column_2 AS worker_column_3 FROM (SELECT users_view.user_id AS worker_column_1, users_view.value_1 AS worker_column_2 FROM (SELECT users_table.user_id, users_table."time", users_table.value_1, users_table.value_2, users_table.value_3, users_table.value_4 FROM public.users_table_1400258 users_table) users_view) worker_subquery GROUP BY worker_column_1, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 4)
2023-11-25 04:50:28.727 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.727 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, rank() OVER (PARTITION BY worker_column_1 ORDER BY worker_column_2) AS rank, worker_column_2 AS worker_column_3 FROM (SELECT users_view.user_id AS worker_column_1, users_view.value_1 AS worker_column_2 FROM (SELECT users_table.user_id, users_table."time", users_table.value_1, users_table.value_2, users_table.value_3, users_table.value_4 FROM public.users_table_1400259 users_table) users_view) worker_subquery GROUP BY worker_column_1, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 4)
2023-11-25 04:50:28.727 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.727 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.727 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.727 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.727 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.727 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.727 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.727 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.727 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.727 UTC [1096741] DEBUG:  00000: combine query: SELECT DISTINCT user_id, rank FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, rank bigint, worker_column_3 integer) ORDER BY rank DESC, user_id
2023-11-25 04:50:28.727 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.727 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.727 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.728 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 25 in 3764 microseconds
2023-11-25 04:50:28.728 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.728 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.728 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.728 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 27 in 3465 microseconds
2023-11-25 04:50:28.728 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.728 UTC [1096741] DEBUG:  00000: task execution (1) for placement (820) on anchor shard (1400256) finished in 392 microseconds on worker node localhost:57637
2023-11-25 04:50:28.728 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.728 UTC [1096741] DEBUG:  00000: task execution (2) for placement (821) on anchor shard (1400257) finished in 416 microseconds on worker node localhost:57638
2023-11-25 04:50:28.728 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.729 UTC [1096741] DEBUG:  00000: task execution (4) for placement (823) on anchor shard (1400259) finished in 337 microseconds on worker node localhost:57638
2023-11-25 04:50:28.729 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.729 UTC [1096741] DEBUG:  00000: task execution (3) for placement (822) on anchor shard (1400258) finished in 606 microseconds on worker node localhost:57637
2023-11-25 04:50:28.729 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.733 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 26 in 5158 microseconds
2023-11-25 04:50:28.733 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.734 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 28 in 5902 microseconds
2023-11-25 04:50:28.734 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.734 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 25: 2 to node localhost:57637
2023-11-25 04:50:28.734 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.734 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 26: 0 to node localhost:57637
2023-11-25 04:50:28.734 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.734 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 27: 2 to node localhost:57638
2023-11-25 04:50:28.734 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.734 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 28: 0 to node localhost:57638
2023-11-25 04:50:28.734 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.734 UTC [1096741] DEBUG:  00000: switching to sequential query execution mode
2023-11-25 04:50:28.734 UTC [1096741] DETAIL:  A command for a distributed view is run. To make sure subsequent commands see the view correctly we need to make sure to use only one connection for all future commands
2023-11-25 04:50:28.734 UTC [1096741] LOCATION:  EnsureSequentialMode, multi_executor.c:745
2023-11-25 04:50:28.734 UTC [1096741] DEBUG:  00000: drop auto-cascades to type window_view
2023-11-25 04:50:28.734 UTC [1096741] LOCATION:  reportDependentObjects, dependency.c:1122
2023-11-25 04:50:28.734 UTC [1096741] DEBUG:  00000: drop auto-cascades to type window_view[]
2023-11-25 04:50:28.734 UTC [1096741] LOCATION:  reportDependentObjects, dependency.c:1122
2023-11-25 04:50:28.734 UTC [1096741] DEBUG:  00000: drop auto-cascades to rule _RETURN on view window_view
2023-11-25 04:50:28.734 UTC [1096741] LOCATION:  reportDependentObjects, dependency.c:1122
2023-11-25 04:50:28.734 UTC [1096741] DEBUG:  00000: drop auto-cascades to type users_view
2023-11-25 04:50:28.734 UTC [1096741] LOCATION:  reportDependentObjects, dependency.c:1122
2023-11-25 04:50:28.734 UTC [1096741] DEBUG:  00000: drop auto-cascades to type users_view[]
2023-11-25 04:50:28.734 UTC [1096741] LOCATION:  reportDependentObjects, dependency.c:1122
2023-11-25 04:50:28.734 UTC [1096741] DEBUG:  00000: drop auto-cascades to rule _RETURN on view users_view
2023-11-25 04:50:28.734 UTC [1096741] LOCATION:  reportDependentObjects, dependency.c:1122
2023-11-25 04:50:28.735 UTC [1096741] DEBUG:  00000: EventTriggerInvoke 16675
2023-11-25 04:50:28.735 UTC [1096741] LOCATION:  EventTriggerInvoke, event_trigger.c:900
2023-11-25 04:50:28.738 UTC [1096741] DEBUG:  00000: opening 1 new connections to localhost:57637
2023-11-25 04:50:28.738 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.738 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 29 in 3764 microseconds
2023-11-25 04:50:28.738 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.739 UTC [1096741] DEBUG:  00000: opening 1 new connections to localhost:57638
2023-11-25 04:50:28.739 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.739 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 30 in 3465 microseconds
2023-11-25 04:50:28.739 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.743 UTC [1096741] DEBUG:  00000: task execution (0) for placement (0) on anchor shard (0) finished in 3835 microseconds on worker node localhost:57637
2023-11-25 04:50:28.743 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.743 UTC [1096741] DEBUG:  00000: task execution (0) for placement (0) on anchor shard (0) finished in 3822 microseconds on worker node localhost:57638
2023-11-25 04:50:28.743 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.743 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 29: 1 to node localhost:57637
2023-11-25 04:50:28.743 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.743 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 30: 1 to node localhost:57638
2023-11-25 04:50:28.743 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.744 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.744 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.744 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.744 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.744 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.744 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.744 UTC [1096741] DEBUG:  00000: Distributed planning for a fast-path router query
2023-11-25 04:50:28.744 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2322
2023-11-25 04:50:28.744 UTC [1096741] DEBUG:  00000: Creating router plan
2023-11-25 04:50:28.744 UTC [1096741] LOCATION:  CreateSingleTaskRouterSelectPlan, multi_router_planner.c:284
2023-11-25 04:50:28.744 UTC [1096741] DEBUG:  00000: generating subplan 8_1 for subquery SELECT min(k_no) AS min FROM public.users_ref_test_table
2023-11-25 04:50:28.744 UTC [1096741] LOCATION:  RecursivelyPlanSubquery, recursive_planning.c:1551
2023-11-25 04:50:28.744 UTC [1096741] DEBUG:  00000: Plan 8 query after replacing subqueries and CTEs: SELECT user_id, count(user_id) OVER (PARTITION BY user_id) AS count FROM public.users_table GROUP BY user_id HAVING (avg(value_1) OPERATOR(pg_catalog.<) ((SELECT intermediate_result.min FROM read_intermediate_result('8_1'::text, 'binary'::citus_copy_format) intermediate_result(min integer)))::numeric) ORDER BY user_id DESC, (count(user_id) OVER (PARTITION BY user_id)) DESC LIMIT 1
2023-11-25 04:50:28.744 UTC [1096741] LOCATION:  GenerateSubplansForSubqueriesAndCTEs, recursive_planning.c:248
2023-11-25 04:50:28.744 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.744 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.744 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.744 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.744 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.744 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.744 UTC [1096741] DEBUG:  00000: push down of limit count: 1
2023-11-25 04:50:28.744 UTC [1096741] LOCATION:  WorkerLimitCount, multi_logical_optimizer.c:4778
2023-11-25 04:50:28.744 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.744 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.744 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.744 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.744 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, count(worker_column_1) OVER (PARTITION BY worker_column_1) AS count FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1 HAVING (avg(worker_column_2) OPERATOR(pg_catalog.<) ((SELECT intermediate_result.min FROM read_intermediate_result('8_1'::text, 'binary'::citus_copy_format) intermediate_result(min integer)))::numeric) ORDER BY worker_column_1 DESC, (count(worker_column_1) OVER (PARTITION BY worker_column_1)) DESC LIMIT '1'::bigint
2023-11-25 04:50:28.744 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.744 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, count(worker_column_1) OVER (PARTITION BY worker_column_1) AS count FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1 HAVING (avg(worker_column_2) OPERATOR(pg_catalog.<) ((SELECT intermediate_result.min FROM read_intermediate_result('8_1'::text, 'binary'::citus_copy_format) intermediate_result(min integer)))::numeric) ORDER BY worker_column_1 DESC, (count(worker_column_1) OVER (PARTITION BY worker_column_1)) DESC LIMIT '1'::bigint
2023-11-25 04:50:28.744 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.745 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, count(worker_column_1) OVER (PARTITION BY worker_column_1) AS count FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1 HAVING (avg(worker_column_2) OPERATOR(pg_catalog.<) ((SELECT intermediate_result.min FROM read_intermediate_result('8_1'::text, 'binary'::citus_copy_format) intermediate_result(min integer)))::numeric) ORDER BY worker_column_1 DESC, (count(worker_column_1) OVER (PARTITION BY worker_column_1)) DESC LIMIT '1'::bigint
2023-11-25 04:50:28.745 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.745 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, count(worker_column_1) OVER (PARTITION BY worker_column_1) AS count FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1 HAVING (avg(worker_column_2) OPERATOR(pg_catalog.<) ((SELECT intermediate_result.min FROM read_intermediate_result('8_1'::text, 'binary'::citus_copy_format) intermediate_result(min integer)))::numeric) ORDER BY worker_column_1 DESC, (count(worker_column_1) OVER (PARTITION BY worker_column_1)) DESC LIMIT '1'::bigint
2023-11-25 04:50:28.745 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.745 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.745 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.745 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.745 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.745 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.745 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.745 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.745 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.745 UTC [1096741] DEBUG:  00000: combine query: SELECT user_id, count FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, count bigint) ORDER BY user_id DESC, count DESC LIMIT '1'::bigint
2023-11-25 04:50:28.745 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.745 UTC [1096741] DEBUG:  00000: Subplan 8_1 is used in 8
2023-11-25 04:50:28.745 UTC [1096741] LOCATION:  RecordSubplanExecutionsOnNodes, intermediate_result_pruning.c:193
2023-11-25 04:50:28.745 UTC [1096741] DEBUG:  00000: Subplan 8_1 will be sent to localhost:57637
2023-11-25 04:50:28.745 UTC [1096741] LOCATION:  LogIntermediateResultMulticastSummary, intermediate_result_pruning.c:422
2023-11-25 04:50:28.745 UTC [1096741] DEBUG:  00000: Subplan 8_1 will be sent to localhost:57638
2023-11-25 04:50:28.745 UTC [1096741] LOCATION:  LogIntermediateResultMulticastSummary, intermediate_result_pruning.c:422
2023-11-25 04:50:28.745 UTC [1096741] DEBUG:  00000: opening 1 new connections to localhost:57637
2023-11-25 04:50:28.745 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.745 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 31 in 3764 microseconds
2023-11-25 04:50:28.745 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.745 UTC [1096741] DEBUG:  00000: task execution (0) for placement (872) on anchor shard (1400284) finished in 206 microseconds on worker node localhost:57637
2023-11-25 04:50:28.745 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.745 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 31: 1 to node localhost:57637
2023-11-25 04:50:28.745 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.746 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.746 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.746 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 32 in 3764 microseconds
2023-11-25 04:50:28.746 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.746 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.746 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.746 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 34 in 3465 microseconds
2023-11-25 04:50:28.746 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.747 UTC [1096741] DEBUG:  00000: task execution (1) for placement (820) on anchor shard (1400256) finished in 683 microseconds on worker node localhost:57637
2023-11-25 04:50:28.747 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.747 UTC [1096741] DEBUG:  00000: task execution (2) for placement (821) on anchor shard (1400257) finished in 643 microseconds on worker node localhost:57638
2023-11-25 04:50:28.747 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.747 UTC [1096741] DEBUG:  00000: task execution (3) for placement (822) on anchor shard (1400258) finished in 463 microseconds on worker node localhost:57637
2023-11-25 04:50:28.747 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.747 UTC [1096741] DEBUG:  00000: task execution (4) for placement (823) on anchor shard (1400259) finished in 622 microseconds on worker node localhost:57638
2023-11-25 04:50:28.747 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.749 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 33 in 3184 microseconds
2023-11-25 04:50:28.749 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.749 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 35 in 3170 microseconds
2023-11-25 04:50:28.749 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.749 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 32: 2 to node localhost:57637
2023-11-25 04:50:28.749 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.749 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 33: 0 to node localhost:57637
2023-11-25 04:50:28.749 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.749 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 34: 2 to node localhost:57638
2023-11-25 04:50:28.749 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.749 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 35: 0 to node localhost:57638
2023-11-25 04:50:28.749 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.750 UTC [1096741] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 04:50:28.750 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.750 UTC [1096741] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 04:50:28.750 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.750 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.750 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.750 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.750 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.750 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.750 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.750 UTC [1096741] DEBUG:  00000: push down of limit count: 10
2023-11-25 04:50:28.750 UTC [1096741] LOCATION:  WorkerLimitCount, multi_logical_optimizer.c:4778
2023-11-25 04:50:28.750 UTC [1096741] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 04:50:28.750 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.751 UTC [1096741] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 04:50:28.751 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.751 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.751 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.751 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.751 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.751 UTC [1096741] DEBUG:  00000: distributed statement: SELECT DISTINCT ON ((rank() OVER my_win), worker_column_1) worker_column_1 AS user_id, rank() OVER my_win AS rnk, worker_column_2 AS worker_column_3, worker_column_3 AS worker_column_4 FROM (SELECT events_table.user_id AS worker_column_1, events_table."time" AS worker_column_2, users_table.value_1 AS worker_column_3 FROM public.events_table_1400260 events_table, public.users_table_1400256 users_table WHERE (users_table.user_id OPERATOR(pg_catalog.=) events_table.user_id)) worker_subquery WINDOW my_win AS (PARTITION BY worker_column_1, worker_column_3 ORDER BY worker_column_2 DESC) ORDER BY (rank() OVER my_win) DESC, worker_column_1 DESC LIMIT '10'::bigint
2023-11-25 04:50:28.751 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.751 UTC [1096741] DEBUG:  00000: distributed statement: SELECT DISTINCT ON ((rank() OVER my_win), worker_column_1) worker_column_1 AS user_id, rank() OVER my_win AS rnk, worker_column_2 AS worker_column_3, worker_column_3 AS worker_column_4 FROM (SELECT events_table.user_id AS worker_column_1, events_table."time" AS worker_column_2, users_table.value_1 AS worker_column_3 FROM public.events_table_1400261 events_table, public.users_table_1400257 users_table WHERE (users_table.user_id OPERATOR(pg_catalog.=) events_table.user_id)) worker_subquery WINDOW my_win AS (PARTITION BY worker_column_1, worker_column_3 ORDER BY worker_column_2 DESC) ORDER BY (rank() OVER my_win) DESC, worker_column_1 DESC LIMIT '10'::bigint
2023-11-25 04:50:28.751 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.751 UTC [1096741] DEBUG:  00000: distributed statement: SELECT DISTINCT ON ((rank() OVER my_win), worker_column_1) worker_column_1 AS user_id, rank() OVER my_win AS rnk, worker_column_2 AS worker_column_3, worker_column_3 AS worker_column_4 FROM (SELECT events_table.user_id AS worker_column_1, events_table."time" AS worker_column_2, users_table.value_1 AS worker_column_3 FROM public.events_table_1400262 events_table, public.users_table_1400258 users_table WHERE (users_table.user_id OPERATOR(pg_catalog.=) events_table.user_id)) worker_subquery WINDOW my_win AS (PARTITION BY worker_column_1, worker_column_3 ORDER BY worker_column_2 DESC) ORDER BY (rank() OVER my_win) DESC, worker_column_1 DESC LIMIT '10'::bigint
2023-11-25 04:50:28.751 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.751 UTC [1096741] DEBUG:  00000: distributed statement: SELECT DISTINCT ON ((rank() OVER my_win), worker_column_1) worker_column_1 AS user_id, rank() OVER my_win AS rnk, worker_column_2 AS worker_column_3, worker_column_3 AS worker_column_4 FROM (SELECT events_table.user_id AS worker_column_1, events_table."time" AS worker_column_2, users_table.value_1 AS worker_column_3 FROM public.events_table_1400263 events_table, public.users_table_1400259 users_table WHERE (users_table.user_id OPERATOR(pg_catalog.=) events_table.user_id)) worker_subquery WINDOW my_win AS (PARTITION BY worker_column_1, worker_column_3 ORDER BY worker_column_2 DESC) ORDER BY (rank() OVER my_win) DESC, worker_column_1 DESC LIMIT '10'::bigint
2023-11-25 04:50:28.751 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.751 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.751 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.751 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.751 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.751 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.751 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.751 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.751 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.751 UTC [1096741] DEBUG:  00000: combine query: SELECT DISTINCT ON (rnk, user_id) user_id, rnk FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, rnk bigint, worker_column_3 timestamp without time zone, worker_column_4 integer) ORDER BY rnk DESC, user_id DESC LIMIT '10'::bigint
2023-11-25 04:50:28.751 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.751 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.751 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.751 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 36 in 3764 microseconds
2023-11-25 04:50:28.751 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.751 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.751 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.751 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 38 in 3465 microseconds
2023-11-25 04:50:28.751 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.753 UTC [1096741] DEBUG:  00000: task execution (1) for placement (820) on anchor shard (1400256) finished in 2215 microseconds on worker node localhost:57637
2023-11-25 04:50:28.753 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.754 UTC [1096741] DEBUG:  00000: task execution (2) for placement (821) on anchor shard (1400257) finished in 2374 microseconds on worker node localhost:57638
2023-11-25 04:50:28.754 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.754 UTC [1096741] DEBUG:  00000: task execution (3) for placement (822) on anchor shard (1400258) finished in 709 microseconds on worker node localhost:57637
2023-11-25 04:50:28.754 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.755 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 39 in 3151 microseconds
2023-11-25 04:50:28.755 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.755 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 37 in 3748 microseconds
2023-11-25 04:50:28.755 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.755 UTC [1096741] DEBUG:  00000: task execution (4) for placement (823) on anchor shard (1400259) finished in 1580 microseconds on worker node localhost:57638
2023-11-25 04:50:28.755 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.755 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 36: 2 to node localhost:57637
2023-11-25 04:50:28.755 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.755 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 37: 0 to node localhost:57637
2023-11-25 04:50:28.755 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.755 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 38: 2 to node localhost:57638
2023-11-25 04:50:28.755 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.755 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 39: 0 to node localhost:57638
2023-11-25 04:50:28.755 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.756 UTC [1096741] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 04:50:28.756 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.756 UTC [1096741] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 04:50:28.756 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.756 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.756 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.756 UTC [1096741] DEBUG:  00000: push down of limit count: 10
2023-11-25 04:50:28.756 UTC [1096741] LOCATION:  WorkerLimitCount, multi_logical_optimizer.c:4778
2023-11-25 04:50:28.756 UTC [1096741] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 04:50:28.756 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.756 UTC [1096741] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 04:50:28.756 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.756 UTC [1096741] DEBUG:  00000: distributed statement: SELECT DISTINCT ON ((rank() OVER my_win), worker_column_1) worker_column_1 AS user_id, rank() OVER my_win AS rnk, worker_column_2 AS worker_column_3, worker_column_3 AS worker_column_4 FROM (SELECT events_table.user_id AS worker_column_1, events_table."time" AS worker_column_2, uref.k_no AS worker_column_3 FROM public.events_table_1400260 events_table, public.users_ref_test_table_1400284 uref WHERE (uref.id OPERATOR(pg_catalog.=) events_table.user_id)) worker_subquery WINDOW my_win AS (PARTITION BY worker_column_1, worker_column_3 ORDER BY worker_column_2 DESC) ORDER BY (rank() OVER my_win) DESC, worker_column_1 DESC LIMIT '10'::bigint
2023-11-25 04:50:28.756 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.756 UTC [1096741] DEBUG:  00000: distributed statement: SELECT DISTINCT ON ((rank() OVER my_win), worker_column_1) worker_column_1 AS user_id, rank() OVER my_win AS rnk, worker_column_2 AS worker_column_3, worker_column_3 AS worker_column_4 FROM (SELECT events_table.user_id AS worker_column_1, events_table."time" AS worker_column_2, uref.k_no AS worker_column_3 FROM public.events_table_1400261 events_table, public.users_ref_test_table_1400284 uref WHERE (uref.id OPERATOR(pg_catalog.=) events_table.user_id)) worker_subquery WINDOW my_win AS (PARTITION BY worker_column_1, worker_column_3 ORDER BY worker_column_2 DESC) ORDER BY (rank() OVER my_win) DESC, worker_column_1 DESC LIMIT '10'::bigint
2023-11-25 04:50:28.756 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.756 UTC [1096741] DEBUG:  00000: distributed statement: SELECT DISTINCT ON ((rank() OVER my_win), worker_column_1) worker_column_1 AS user_id, rank() OVER my_win AS rnk, worker_column_2 AS worker_column_3, worker_column_3 AS worker_column_4 FROM (SELECT events_table.user_id AS worker_column_1, events_table."time" AS worker_column_2, uref.k_no AS worker_column_3 FROM public.events_table_1400262 events_table, public.users_ref_test_table_1400284 uref WHERE (uref.id OPERATOR(pg_catalog.=) events_table.user_id)) worker_subquery WINDOW my_win AS (PARTITION BY worker_column_1, worker_column_3 ORDER BY worker_column_2 DESC) ORDER BY (rank() OVER my_win) DESC, worker_column_1 DESC LIMIT '10'::bigint
2023-11-25 04:50:28.756 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.756 UTC [1096741] DEBUG:  00000: distributed statement: SELECT DISTINCT ON ((rank() OVER my_win), worker_column_1) worker_column_1 AS user_id, rank() OVER my_win AS rnk, worker_column_2 AS worker_column_3, worker_column_3 AS worker_column_4 FROM (SELECT events_table.user_id AS worker_column_1, events_table."time" AS worker_column_2, uref.k_no AS worker_column_3 FROM public.events_table_1400263 events_table, public.users_ref_test_table_1400284 uref WHERE (uref.id OPERATOR(pg_catalog.=) events_table.user_id)) worker_subquery WINDOW my_win AS (PARTITION BY worker_column_1, worker_column_3 ORDER BY worker_column_2 DESC) ORDER BY (rank() OVER my_win) DESC, worker_column_1 DESC LIMIT '10'::bigint
2023-11-25 04:50:28.756 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.756 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.756 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.756 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.756 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.756 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.756 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.756 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.756 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.756 UTC [1096741] DEBUG:  00000: combine query: SELECT DISTINCT ON (rnk, user_id) user_id, rnk FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, rnk bigint, worker_column_3 timestamp without time zone, worker_column_4 integer) ORDER BY rnk DESC, user_id DESC LIMIT '10'::bigint
2023-11-25 04:50:28.756 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.757 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.757 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.757 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 40 in 3764 microseconds
2023-11-25 04:50:28.757 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.757 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.757 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.757 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 42 in 3465 microseconds
2023-11-25 04:50:28.757 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.757 UTC [1096741] DEBUG:  00000: task execution (1) for placement (832) on anchor shard (1400260) finished in 593 microseconds on worker node localhost:57637
2023-11-25 04:50:28.757 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.758 UTC [1096741] DEBUG:  00000: task execution (3) for placement (834) on anchor shard (1400262) finished in 446 microseconds on worker node localhost:57637
2023-11-25 04:50:28.758 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.758 UTC [1096741] DEBUG:  00000: task execution (2) for placement (833) on anchor shard (1400261) finished in 1082 microseconds on worker node localhost:57638
2023-11-25 04:50:28.758 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.759 UTC [1096741] DEBUG:  00000: task execution (4) for placement (835) on anchor shard (1400263) finished in 462 microseconds on worker node localhost:57638
2023-11-25 04:50:28.759 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.760 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 41 in 3326 microseconds
2023-11-25 04:50:28.760 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.761 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 43 in 4579 microseconds
2023-11-25 04:50:28.761 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.762 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 40: 2 to node localhost:57637
2023-11-25 04:50:28.762 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.762 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 41: 0 to node localhost:57637
2023-11-25 04:50:28.762 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.762 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 42: 2 to node localhost:57638
2023-11-25 04:50:28.762 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.762 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 43: 0 to node localhost:57638
2023-11-25 04:50:28.762 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.762 UTC [1096741] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 04:50:28.762 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.762 UTC [1096741] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 04:50:28.762 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.762 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.762 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.762 UTC [1096741] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 04:50:28.762 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.762 UTC [1096741] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 04:50:28.762 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.762 UTC [1096741] DEBUG:  00000: generated sql query for task 1
2023-11-25 04:50:28.762 UTC [1096741] DETAIL:  query string: "SELECT events_table.user_id, events_table."time" AS worker_column_2, events_table.value_2 AS worker_column_3, uref.k_no AS worker_column_4 FROM (public.events_table_1400260 events_table JOIN public.users_ref_test_table_1400284 uref ON ((uref.id OPERATOR(pg_catalog.=) events_table.user_id))) WHERE true"
2023-11-25 04:50:28.762 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.762 UTC [1096741] DEBUG:  00000: generated sql query for task 2
2023-11-25 04:50:28.762 UTC [1096741] DETAIL:  query string: "SELECT events_table.user_id, events_table."time" AS worker_column_2, events_table.value_2 AS worker_column_3, uref.k_no AS worker_column_4 FROM (public.events_table_1400261 events_table JOIN public.users_ref_test_table_1400284 uref ON ((uref.id OPERATOR(pg_catalog.=) events_table.user_id))) WHERE true"
2023-11-25 04:50:28.762 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.762 UTC [1096741] DEBUG:  00000: generated sql query for task 3
2023-11-25 04:50:28.762 UTC [1096741] DETAIL:  query string: "SELECT events_table.user_id, events_table."time" AS worker_column_2, events_table.value_2 AS worker_column_3, uref.k_no AS worker_column_4 FROM (public.events_table_1400262 events_table JOIN public.users_ref_test_table_1400284 uref ON ((uref.id OPERATOR(pg_catalog.=) events_table.user_id))) WHERE true"
2023-11-25 04:50:28.762 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.762 UTC [1096741] DEBUG:  00000: generated sql query for task 4
2023-11-25 04:50:28.762 UTC [1096741] DETAIL:  query string: "SELECT events_table.user_id, events_table."time" AS worker_column_2, events_table.value_2 AS worker_column_3, uref.k_no AS worker_column_4 FROM (public.events_table_1400263 events_table JOIN public.users_ref_test_table_1400284 uref ON ((uref.id OPERATOR(pg_catalog.=) events_table.user_id))) WHERE true"
2023-11-25 04:50:28.762 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.762 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.762 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.762 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.762 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.762 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.762 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.762 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.762 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.762 UTC [1096741] DEBUG:  00000: combine query: SELECT DISTINCT ON ((rank() OVER my_win), user_id) user_id, rank() OVER my_win AS rnk FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), '(i 1 2)'::cstring(0)) remote_scan(user_id integer, worker_column_2 timestamp without time zone, worker_column_3 integer, worker_column_4 integer) WINDOW my_win AS (PARTITION BY worker_column_3, worker_column_4 ORDER BY worker_column_2 DESC) ORDER BY (rank() OVER my_win) DESC, user_id DESC LIMIT '10'::bigint
2023-11-25 04:50:28.762 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.763 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.763 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.763 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 44 in 3764 microseconds
2023-11-25 04:50:28.763 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.763 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.763 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.763 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 46 in 3465 microseconds
2023-11-25 04:50:28.763 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.763 UTC [1096741] DEBUG:  00000: task execution (1) for placement (832) on anchor shard (1400260) finished in 506 microseconds on worker node localhost:57637
2023-11-25 04:50:28.763 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.764 UTC [1096741] DEBUG:  00000: task execution (3) for placement (834) on anchor shard (1400262) finished in 351 microseconds on worker node localhost:57637
2023-11-25 04:50:28.764 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.767 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 45 in 4502 microseconds
2023-11-25 04:50:28.767 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.768 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 47 in 4935 microseconds
2023-11-25 04:50:28.768 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.769 UTC [1096741] DEBUG:  00000: task execution (2) for placement (833) on anchor shard (1400261) finished in 6429 microseconds on worker node localhost:57638
2023-11-25 04:50:28.769 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.770 UTC [1096741] DEBUG:  00000: task execution (4) for placement (835) on anchor shard (1400263) finished in 2392 microseconds on worker node localhost:57638
2023-11-25 04:50:28.770 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.770 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 44: 2 to node localhost:57637
2023-11-25 04:50:28.770 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.770 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 45: 0 to node localhost:57637
2023-11-25 04:50:28.770 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.770 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 46: 1 to node localhost:57638
2023-11-25 04:50:28.770 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.770 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 47: 1 to node localhost:57638
2023-11-25 04:50:28.770 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.771 UTC [1096741] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 04:50:28.771 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.771 UTC [1096741] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 04:50:28.771 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.771 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.771 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.771 UTC [1096741] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 04:50:28.771 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.771 UTC [1096741] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 04:50:28.771 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.771 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, rank() OVER my_win AS rnk, avg(worker_column_2) AS avg_val_2, date_trunc('day'::text, worker_column_3) AS worker_column_4, avg(worker_column_4) AS worker_column_5 FROM (SELECT events_table.user_id AS worker_column_1, events_table.value_2 AS worker_column_2, events_table."time" AS worker_column_3, events_table.event_type AS worker_column_4 FROM public.events_table_1400260 events_table) worker_subquery GROUP BY worker_column_1, (date_trunc('day'::text, worker_column_3)) WINDOW my_win AS (PARTITION BY worker_column_1 ORDER BY (avg(worker_column_4)) DESC)
2023-11-25 04:50:28.771 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.771 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, rank() OVER my_win AS rnk, avg(worker_column_2) AS avg_val_2, date_trunc('day'::text, worker_column_3) AS worker_column_4, avg(worker_column_4) AS worker_column_5 FROM (SELECT events_table.user_id AS worker_column_1, events_table.value_2 AS worker_column_2, events_table."time" AS worker_column_3, events_table.event_type AS worker_column_4 FROM public.events_table_1400261 events_table) worker_subquery GROUP BY worker_column_1, (date_trunc('day'::text, worker_column_3)) WINDOW my_win AS (PARTITION BY worker_column_1 ORDER BY (avg(worker_column_4)) DESC)
2023-11-25 04:50:28.771 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.771 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, rank() OVER my_win AS rnk, avg(worker_column_2) AS avg_val_2, date_trunc('day'::text, worker_column_3) AS worker_column_4, avg(worker_column_4) AS worker_column_5 FROM (SELECT events_table.user_id AS worker_column_1, events_table.value_2 AS worker_column_2, events_table."time" AS worker_column_3, events_table.event_type AS worker_column_4 FROM public.events_table_1400262 events_table) worker_subquery GROUP BY worker_column_1, (date_trunc('day'::text, worker_column_3)) WINDOW my_win AS (PARTITION BY worker_column_1 ORDER BY (avg(worker_column_4)) DESC)
2023-11-25 04:50:28.771 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.771 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, rank() OVER my_win AS rnk, avg(worker_column_2) AS avg_val_2, date_trunc('day'::text, worker_column_3) AS worker_column_4, avg(worker_column_4) AS worker_column_5 FROM (SELECT events_table.user_id AS worker_column_1, events_table.value_2 AS worker_column_2, events_table."time" AS worker_column_3, events_table.event_type AS worker_column_4 FROM public.events_table_1400263 events_table) worker_subquery GROUP BY worker_column_1, (date_trunc('day'::text, worker_column_3)) WINDOW my_win AS (PARTITION BY worker_column_1 ORDER BY (avg(worker_column_4)) DESC)
2023-11-25 04:50:28.771 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.771 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.771 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.771 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.771 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.771 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.771 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.771 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.771 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.771 UTC [1096741] DEBUG:  00000: combine query: SELECT user_id, rnk, avg_val_2 FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, rnk bigint, avg_val_2 numeric, worker_column_4 timestamp without time zone, worker_column_5 numeric) ORDER BY avg_val_2 DESC, rnk DESC, user_id DESC
2023-11-25 04:50:28.771 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.772 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.772 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.772 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 48 in 3764 microseconds
2023-11-25 04:50:28.772 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.772 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.772 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.772 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 50 in 3465 microseconds
2023-11-25 04:50:28.772 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.773 UTC [1096741] DEBUG:  00000: task execution (1) for placement (832) on anchor shard (1400260) finished in 947 microseconds on worker node localhost:57637
2023-11-25 04:50:28.773 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.773 UTC [1096741] DEBUG:  00000: task execution (2) for placement (833) on anchor shard (1400261) finished in 1219 microseconds on worker node localhost:57638
2023-11-25 04:50:28.773 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.773 UTC [1096741] DEBUG:  00000: task execution (3) for placement (834) on anchor shard (1400262) finished in 450 microseconds on worker node localhost:57637
2023-11-25 04:50:28.773 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.774 UTC [1096741] DEBUG:  00000: task execution (4) for placement (835) on anchor shard (1400263) finished in 357 microseconds on worker node localhost:57638
2023-11-25 04:50:28.774 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.774 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 51 in 2403 microseconds
2023-11-25 04:50:28.774 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.774 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 49 in 2752 microseconds
2023-11-25 04:50:28.774 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.775 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 48: 2 to node localhost:57637
2023-11-25 04:50:28.775 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.775 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 49: 0 to node localhost:57637
2023-11-25 04:50:28.775 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.775 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 50: 2 to node localhost:57638
2023-11-25 04:50:28.775 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.775 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 51: 0 to node localhost:57638
2023-11-25 04:50:28.775 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.776 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.776 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.776 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.776 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.776 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.776 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.776 UTC [1096741] DEBUG:  00000: push down of limit count: 5
2023-11-25 04:50:28.776 UTC [1096741] LOCATION:  WorkerLimitCount, multi_logical_optimizer.c:4778
2023-11-25 04:50:28.776 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.776 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.776 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.776 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.777 UTC [1096741] DEBUG:  00000: distributed statement: SELECT count(*) OVER (PARTITION BY worker_column_2, (worker_column_2 OPERATOR(pg_catalog.+) 1)) AS count, rank() OVER (PARTITION BY worker_column_2) AS cnt1, count(*) OVER (PARTITION BY worker_column_2, (abs((worker_column_3 OPERATOR(pg_catalog.-) worker_column_4)))) AS cnt2, date_trunc('min'::text, lag(worker_column_1) OVER (PARTITION BY worker_column_2 ORDER BY worker_column_1)) AS datee, rank() OVER my_win AS rnnk, avg(CASE WHEN (worker_column_2 OPERATOR(pg_catalog.>) 4) THEN worker_column_3 ELSE worker_column_4 END) FILTER (WHERE (worker_column_2 OPERATOR(pg_catalog.>) 2)) OVER my_win_2 AS filtered_count, sum(((((worker_column_2)::numeric OPERATOR(pg_catalog.*) (5.0 OPERATOR(pg_catalog./) (((worker_column_3 OPERATOR(pg_catalog.+) worker_column_4))::numeric OPERATOR(pg_catalog.+) 0.1))))::double precision OPERATOR(pg_catalog.*) worker_column_5)) FILTER (WHERE ((worker_column_3)::text OPERATOR(pg_catalog.~~) '%1%'::text)) OVER my_win_4 AS cnt_with_filter_2, worker_column_2 AS worker_column_8, worker_column_1 AS worker_column_9, (worker_column_3 OPERATOR(pg_catalog.%) 3) AS worker_column_10, worker_column_3 AS worker_column_11, date_trunc('min'::text, worker_column_1) AS worker_column_12, worker_column_4 AS worker_column_13, worker_column_5 AS worker_column_14, (worker_column_2 OPERATOR(pg_catalog.+) 1) AS worker_column_15, abs((worker_column_3 OPERATOR(pg_catalog.-) worker_column_4)) AS worker_column_16 FROM (SELECT users_table."time" AS worker_column_1, users_table.user_id AS worker_column_2, users_table.value_1 AS worker_column_3, users_table.value_2 AS worker_column_4, users_table.value_3 AS worker_column_5 FROM public.users_table_1400256 users_table) worker_subquery WINDOW my_win AS (PARTITION BY worker_column_2, (worker_column_3 OPERATOR(pg_catalog.%) 3) ORDER BY worker_column_1 DESC), my_win_2 AS (PARTITION BY worker_column_2, worker_column_3 ORDER BY worker_column_1 DESC), my_win_3 AS (PARTITION BY worker_column_2, (date_trunc('min'::text, worker_column_1))), my_win_4 AS (my_win_3 ORDER BY worker_column_4, worker_column_5) ORDER BY (sum(((((worker_column_2)::numeric OPERATOR(pg_catalog.*) (5.0 OPERATOR(pg_catalog./) (((worker_column_3 OPERATOR(pg_catalog.+) worker_column_4))::numeric OPERATOR(pg_catalog.+) 0.1))))::double precision OPERATOR(pg_catalog.*) worker_column_5)) FILTER (WHERE ((worker_column_3)::text OPERATOR(pg_catalog.~~) '%1%'::text)) OVER my_win_4) DESC NULLS LAST, (avg(CASE WHEN (worker_column_2 OPERATOR(pg_catalog.>) 4) THEN worker_column_3 ELSE worker_column_4 END) FILTER (WHERE (worker_column_2 OPERATOR(pg_catalog.>) 2)) OVER my_win_2) DESC NULLS LAST, (date_trunc('min'::text, lag(worker_column_1) OVER (PARTITION BY worker_column_2 ORDER BY worker_column_1))) DESC NULLS LAST, (rank() OVER my_win) DESC, (count(*) OVER (PARTITION BY worker_column_2, (abs((worker_column_3 OPERATOR(pg_catalog.-) worker_column_4))))) DESC, (rank() OVER (PARTITION BY worker_column_2)) DESC, worker_column_2 DESC LIMIT '5'::bigint
2023-11-25 04:50:28.777 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.777 UTC [1096741] DEBUG:  00000: distributed statement: SELECT count(*) OVER (PARTITION BY worker_column_2, (worker_column_2 OPERATOR(pg_catalog.+) 1)) AS count, rank() OVER (PARTITION BY worker_column_2) AS cnt1, count(*) OVER (PARTITION BY worker_column_2, (abs((worker_column_3 OPERATOR(pg_catalog.-) worker_column_4)))) AS cnt2, date_trunc('min'::text, lag(worker_column_1) OVER (PARTITION BY worker_column_2 ORDER BY worker_column_1)) AS datee, rank() OVER my_win AS rnnk, avg(CASE WHEN (worker_column_2 OPERATOR(pg_catalog.>) 4) THEN worker_column_3 ELSE worker_column_4 END) FILTER (WHERE (worker_column_2 OPERATOR(pg_catalog.>) 2)) OVER my_win_2 AS filtered_count, sum(((((worker_column_2)::numeric OPERATOR(pg_catalog.*) (5.0 OPERATOR(pg_catalog./) (((worker_column_3 OPERATOR(pg_catalog.+) worker_column_4))::numeric OPERATOR(pg_catalog.+) 0.1))))::double precision OPERATOR(pg_catalog.*) worker_column_5)) FILTER (WHERE ((worker_column_3)::text OPERATOR(pg_catalog.~~) '%1%'::text)) OVER my_win_4 AS cnt_with_filter_2, worker_column_2 AS worker_column_8, worker_column_1 AS worker_column_9, (worker_column_3 OPERATOR(pg_catalog.%) 3) AS worker_column_10, worker_column_3 AS worker_column_11, date_trunc('min'::text, worker_column_1) AS worker_column_12, worker_column_4 AS worker_column_13, worker_column_5 AS worker_column_14, (worker_column_2 OPERATOR(pg_catalog.+) 1) AS worker_column_15, abs((worker_column_3 OPERATOR(pg_catalog.-) worker_column_4)) AS worker_column_16 FROM (SELECT users_table."time" AS worker_column_1, users_table.user_id AS worker_column_2, users_table.value_1 AS worker_column_3, users_table.value_2 AS worker_column_4, users_table.value_3 AS worker_column_5 FROM public.users_table_1400257 users_table) worker_subquery WINDOW my_win AS (PARTITION BY worker_column_2, (worker_column_3 OPERATOR(pg_catalog.%) 3) ORDER BY worker_column_1 DESC), my_win_2 AS (PARTITION BY worker_column_2, worker_column_3 ORDER BY worker_column_1 DESC), my_win_3 AS (PARTITION BY worker_column_2, (date_trunc('min'::text, worker_column_1))), my_win_4 AS (my_win_3 ORDER BY worker_column_4, worker_column_5) ORDER BY (sum(((((worker_column_2)::numeric OPERATOR(pg_catalog.*) (5.0 OPERATOR(pg_catalog./) (((worker_column_3 OPERATOR(pg_catalog.+) worker_column_4))::numeric OPERATOR(pg_catalog.+) 0.1))))::double precision OPERATOR(pg_catalog.*) worker_column_5)) FILTER (WHERE ((worker_column_3)::text OPERATOR(pg_catalog.~~) '%1%'::text)) OVER my_win_4) DESC NULLS LAST, (avg(CASE WHEN (worker_column_2 OPERATOR(pg_catalog.>) 4) THEN worker_column_3 ELSE worker_column_4 END) FILTER (WHERE (worker_column_2 OPERATOR(pg_catalog.>) 2)) OVER my_win_2) DESC NULLS LAST, (date_trunc('min'::text, lag(worker_column_1) OVER (PARTITION BY worker_column_2 ORDER BY worker_column_1))) DESC NULLS LAST, (rank() OVER my_win) DESC, (count(*) OVER (PARTITION BY worker_column_2, (abs((worker_column_3 OPERATOR(pg_catalog.-) worker_column_4))))) DESC, (rank() OVER (PARTITION BY worker_column_2)) DESC, worker_column_2 DESC LIMIT '5'::bigint
2023-11-25 04:50:28.777 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.777 UTC [1096741] DEBUG:  00000: distributed statement: SELECT count(*) OVER (PARTITION BY worker_column_2, (worker_column_2 OPERATOR(pg_catalog.+) 1)) AS count, rank() OVER (PARTITION BY worker_column_2) AS cnt1, count(*) OVER (PARTITION BY worker_column_2, (abs((worker_column_3 OPERATOR(pg_catalog.-) worker_column_4)))) AS cnt2, date_trunc('min'::text, lag(worker_column_1) OVER (PARTITION BY worker_column_2 ORDER BY worker_column_1)) AS datee, rank() OVER my_win AS rnnk, avg(CASE WHEN (worker_column_2 OPERATOR(pg_catalog.>) 4) THEN worker_column_3 ELSE worker_column_4 END) FILTER (WHERE (worker_column_2 OPERATOR(pg_catalog.>) 2)) OVER my_win_2 AS filtered_count, sum(((((worker_column_2)::numeric OPERATOR(pg_catalog.*) (5.0 OPERATOR(pg_catalog./) (((worker_column_3 OPERATOR(pg_catalog.+) worker_column_4))::numeric OPERATOR(pg_catalog.+) 0.1))))::double precision OPERATOR(pg_catalog.*) worker_column_5)) FILTER (WHERE ((worker_column_3)::text OPERATOR(pg_catalog.~~) '%1%'::text)) OVER my_win_4 AS cnt_with_filter_2, worker_column_2 AS worker_column_8, worker_column_1 AS worker_column_9, (worker_column_3 OPERATOR(pg_catalog.%) 3) AS worker_column_10, worker_column_3 AS worker_column_11, date_trunc('min'::text, worker_column_1) AS worker_column_12, worker_column_4 AS worker_column_13, worker_column_5 AS worker_column_14, (worker_column_2 OPERATOR(pg_catalog.+) 1) AS worker_column_15, abs((worker_column_3 OPERATOR(pg_catalog.-) worker_column_4)) AS worker_column_16 FROM (SELECT users_table."time" AS worker_column_1, users_table.user_id AS worker_column_2, users_table.value_1 AS worker_column_3, users_table.value_2 AS worker_column_4, users_table.value_3 AS worker_column_5 FROM public.users_table_1400258 users_table) worker_subquery WINDOW my_win AS (PARTITION BY worker_column_2, (worker_column_3 OPERATOR(pg_catalog.%) 3) ORDER BY worker_column_1 DESC), my_win_2 AS (PARTITION BY worker_column_2, worker_column_3 ORDER BY worker_column_1 DESC), my_win_3 AS (PARTITION BY worker_column_2, (date_trunc('min'::text, worker_column_1))), my_win_4 AS (my_win_3 ORDER BY worker_column_4, worker_column_5) ORDER BY (sum(((((worker_column_2)::numeric OPERATOR(pg_catalog.*) (5.0 OPERATOR(pg_catalog./) (((worker_column_3 OPERATOR(pg_catalog.+) worker_column_4))::numeric OPERATOR(pg_catalog.+) 0.1))))::double precision OPERATOR(pg_catalog.*) worker_column_5)) FILTER (WHERE ((worker_column_3)::text OPERATOR(pg_catalog.~~) '%1%'::text)) OVER my_win_4) DESC NULLS LAST, (avg(CASE WHEN (worker_column_2 OPERATOR(pg_catalog.>) 4) THEN worker_column_3 ELSE worker_column_4 END) FILTER (WHERE (worker_column_2 OPERATOR(pg_catalog.>) 2)) OVER my_win_2) DESC NULLS LAST, (date_trunc('min'::text, lag(worker_column_1) OVER (PARTITION BY worker_column_2 ORDER BY worker_column_1))) DESC NULLS LAST, (rank() OVER my_win) DESC, (count(*) OVER (PARTITION BY worker_column_2, (abs((worker_column_3 OPERATOR(pg_catalog.-) worker_column_4))))) DESC, (rank() OVER (PARTITION BY worker_column_2)) DESC, worker_column_2 DESC LIMIT '5'::bigint
2023-11-25 04:50:28.777 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.777 UTC [1096741] DEBUG:  00000: distributed statement: SELECT count(*) OVER (PARTITION BY worker_column_2, (worker_column_2 OPERATOR(pg_catalog.+) 1)) AS count, rank() OVER (PARTITION BY worker_column_2) AS cnt1, count(*) OVER (PARTITION BY worker_column_2, (abs((worker_column_3 OPERATOR(pg_catalog.-) worker_column_4)))) AS cnt2, date_trunc('min'::text, lag(worker_column_1) OVER (PARTITION BY worker_column_2 ORDER BY worker_column_1)) AS datee, rank() OVER my_win AS rnnk, avg(CASE WHEN (worker_column_2 OPERATOR(pg_catalog.>) 4) THEN worker_column_3 ELSE worker_column_4 END) FILTER (WHERE (worker_column_2 OPERATOR(pg_catalog.>) 2)) OVER my_win_2 AS filtered_count, sum(((((worker_column_2)::numeric OPERATOR(pg_catalog.*) (5.0 OPERATOR(pg_catalog./) (((worker_column_3 OPERATOR(pg_catalog.+) worker_column_4))::numeric OPERATOR(pg_catalog.+) 0.1))))::double precision OPERATOR(pg_catalog.*) worker_column_5)) FILTER (WHERE ((worker_column_3)::text OPERATOR(pg_catalog.~~) '%1%'::text)) OVER my_win_4 AS cnt_with_filter_2, worker_column_2 AS worker_column_8, worker_column_1 AS worker_column_9, (worker_column_3 OPERATOR(pg_catalog.%) 3) AS worker_column_10, worker_column_3 AS worker_column_11, date_trunc('min'::text, worker_column_1) AS worker_column_12, worker_column_4 AS worker_column_13, worker_column_5 AS worker_column_14, (worker_column_2 OPERATOR(pg_catalog.+) 1) AS worker_column_15, abs((worker_column_3 OPERATOR(pg_catalog.-) worker_column_4)) AS worker_column_16 FROM (SELECT users_table."time" AS worker_column_1, users_table.user_id AS worker_column_2, users_table.value_1 AS worker_column_3, users_table.value_2 AS worker_column_4, users_table.value_3 AS worker_column_5 FROM public.users_table_1400259 users_table) worker_subquery WINDOW my_win AS (PARTITION BY worker_column_2, (worker_column_3 OPERATOR(pg_catalog.%) 3) ORDER BY worker_column_1 DESC), my_win_2 AS (PARTITION BY worker_column_2, worker_column_3 ORDER BY worker_column_1 DESC), my_win_3 AS (PARTITION BY worker_column_2, (date_trunc('min'::text, worker_column_1))), my_win_4 AS (my_win_3 ORDER BY worker_column_4, worker_column_5) ORDER BY (sum(((((worker_column_2)::numeric OPERATOR(pg_catalog.*) (5.0 OPERATOR(pg_catalog./) (((worker_column_3 OPERATOR(pg_catalog.+) worker_column_4))::numeric OPERATOR(pg_catalog.+) 0.1))))::double precision OPERATOR(pg_catalog.*) worker_column_5)) FILTER (WHERE ((worker_column_3)::text OPERATOR(pg_catalog.~~) '%1%'::text)) OVER my_win_4) DESC NULLS LAST, (avg(CASE WHEN (worker_column_2 OPERATOR(pg_catalog.>) 4) THEN worker_column_3 ELSE worker_column_4 END) FILTER (WHERE (worker_column_2 OPERATOR(pg_catalog.>) 2)) OVER my_win_2) DESC NULLS LAST, (date_trunc('min'::text, lag(worker_column_1) OVER (PARTITION BY worker_column_2 ORDER BY worker_column_1))) DESC NULLS LAST, (rank() OVER my_win) DESC, (count(*) OVER (PARTITION BY worker_column_2, (abs((worker_column_3 OPERATOR(pg_catalog.-) worker_column_4))))) DESC, (rank() OVER (PARTITION BY worker_column_2)) DESC, worker_column_2 DESC LIMIT '5'::bigint
2023-11-25 04:50:28.777 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.777 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.777 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.777 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.777 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.777 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.777 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.777 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.777 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.777 UTC [1096741] DEBUG:  00000: combine query: SELECT count, cnt1, cnt2, datee, rnnk, filtered_count, cnt_with_filter_2 FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(count bigint, cnt1 bigint, cnt2 bigint, datee timestamp without time zone, rnnk bigint, filtered_count numeric, cnt_with_filter_2 double precision, worker_column_8 integer, worker_column_9 timestamp without time zone, worker_column_10 integer, worker_column_11 integer, worker_column_12 timestamp without time zone, worker_column_13 integer, worker_column_14 double precision, worker_column_15 integer, worker_column_16 integer) ORDER BY cnt_with_filter_2 DESC NULLS LAST, filtered_count DESC NULLS LAST, datee DESC NULLS LAST, rnnk DESC, cnt2 DESC, cnt1 DESC, worker_column_8 DESC LIMIT '5'::bigint
2023-11-25 04:50:28.777 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.777 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.777 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.778 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 52 in 3764 microseconds
2023-11-25 04:50:28.778 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.778 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.778 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.778 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 54 in 3465 microseconds
2023-11-25 04:50:28.778 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.779 UTC [1096741] DEBUG:  00000: task execution (1) for placement (820) on anchor shard (1400256) finished in 1717 microseconds on worker node localhost:57637
2023-11-25 04:50:28.779 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.780 UTC [1096741] DEBUG:  00000: task execution (3) for placement (822) on anchor shard (1400258) finished in 1090 microseconds on worker node localhost:57637
2023-11-25 04:50:28.780 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.781 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 53 in 3644 microseconds
2023-11-25 04:50:28.781 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.783 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 55 in 4885 microseconds
2023-11-25 04:50:28.783 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.783 UTC [1096741] DEBUG:  00000: task execution (2) for placement (821) on anchor shard (1400257) finished in 5341 microseconds on worker node localhost:57638
2023-11-25 04:50:28.783 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.788 UTC [1096741] DEBUG:  00000: task execution (4) for placement (823) on anchor shard (1400259) finished in 5587 microseconds on worker node localhost:57638
2023-11-25 04:50:28.788 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.788 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 52: 2 to node localhost:57637
2023-11-25 04:50:28.788 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.788 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 53: 0 to node localhost:57637
2023-11-25 04:50:28.788 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.788 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 54: 1 to node localhost:57638
2023-11-25 04:50:28.788 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.788 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 55: 1 to node localhost:57638
2023-11-25 04:50:28.788 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.789 UTC [1096741] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 04:50:28.789 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.789 UTC [1096741] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 04:50:28.789 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.789 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.789 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.789 UTC [1096741] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 04:50:28.789 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.789 UTC [1096741] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 04:50:28.789 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.789 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, rank() OVER my_win AS my_rank, avg(avg(worker_column_2)) OVER my_win_2 AS avg, max(worker_column_3) AS mx_time, worker_column_4 AS worker_column_5, count(*) AS worker_column_6, max(worker_column_2) AS worker_column_7, avg(worker_column_1) AS worker_column_8 FROM (SELECT events_table.user_id AS worker_column_1, events_table.event_type AS worker_column_2, events_table."time" AS worker_column_3, events_table.value_2 AS worker_column_4 FROM public.events_table_1400260 events_table) worker_subquery GROUP BY worker_column_1, worker_column_4 WINDOW my_win AS (PARTITION BY worker_column_1, (max(worker_column_2)) ORDER BY (count(*)) DESC), my_win_2 AS (PARTITION BY worker_column_1, (avg(worker_column_1)) ORDER BY (count(*)) DESC)
2023-11-25 04:50:28.789 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.789 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, rank() OVER my_win AS my_rank, avg(avg(worker_column_2)) OVER my_win_2 AS avg, max(worker_column_3) AS mx_time, worker_column_4 AS worker_column_5, count(*) AS worker_column_6, max(worker_column_2) AS worker_column_7, avg(worker_column_1) AS worker_column_8 FROM (SELECT events_table.user_id AS worker_column_1, events_table.event_type AS worker_column_2, events_table."time" AS worker_column_3, events_table.value_2 AS worker_column_4 FROM public.events_table_1400261 events_table) worker_subquery GROUP BY worker_column_1, worker_column_4 WINDOW my_win AS (PARTITION BY worker_column_1, (max(worker_column_2)) ORDER BY (count(*)) DESC), my_win_2 AS (PARTITION BY worker_column_1, (avg(worker_column_1)) ORDER BY (count(*)) DESC)
2023-11-25 04:50:28.789 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.789 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, rank() OVER my_win AS my_rank, avg(avg(worker_column_2)) OVER my_win_2 AS avg, max(worker_column_3) AS mx_time, worker_column_4 AS worker_column_5, count(*) AS worker_column_6, max(worker_column_2) AS worker_column_7, avg(worker_column_1) AS worker_column_8 FROM (SELECT events_table.user_id AS worker_column_1, events_table.event_type AS worker_column_2, events_table."time" AS worker_column_3, events_table.value_2 AS worker_column_4 FROM public.events_table_1400262 events_table) worker_subquery GROUP BY worker_column_1, worker_column_4 WINDOW my_win AS (PARTITION BY worker_column_1, (max(worker_column_2)) ORDER BY (count(*)) DESC), my_win_2 AS (PARTITION BY worker_column_1, (avg(worker_column_1)) ORDER BY (count(*)) DESC)
2023-11-25 04:50:28.789 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.789 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, rank() OVER my_win AS my_rank, avg(avg(worker_column_2)) OVER my_win_2 AS avg, max(worker_column_3) AS mx_time, worker_column_4 AS worker_column_5, count(*) AS worker_column_6, max(worker_column_2) AS worker_column_7, avg(worker_column_1) AS worker_column_8 FROM (SELECT events_table.user_id AS worker_column_1, events_table.event_type AS worker_column_2, events_table."time" AS worker_column_3, events_table.value_2 AS worker_column_4 FROM public.events_table_1400263 events_table) worker_subquery GROUP BY worker_column_1, worker_column_4 WINDOW my_win AS (PARTITION BY worker_column_1, (max(worker_column_2)) ORDER BY (count(*)) DESC), my_win_2 AS (PARTITION BY worker_column_1, (avg(worker_column_1)) ORDER BY (count(*)) DESC)
2023-11-25 04:50:28.789 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.789 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.789 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.789 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.789 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.789 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.789 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.789 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.789 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.789 UTC [1096741] DEBUG:  00000: combine query: SELECT user_id, my_rank, avg, mx_time FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, my_rank bigint, avg numeric, mx_time timestamp without time zone, worker_column_5 integer, worker_column_6 bigint, worker_column_7 integer, worker_column_8 numeric) ORDER BY avg DESC, mx_time DESC, my_rank DESC, user_id DESC
2023-11-25 04:50:28.789 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.789 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.789 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.790 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 56 in 3764 microseconds
2023-11-25 04:50:28.790 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.790 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.790 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.790 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 58 in 3465 microseconds
2023-11-25 04:50:28.790 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.791 UTC [1096741] DEBUG:  00000: task execution (2) for placement (833) on anchor shard (1400261) finished in 889 microseconds on worker node localhost:57638
2023-11-25 04:50:28.791 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.791 UTC [1096741] DEBUG:  00000: task execution (4) for placement (835) on anchor shard (1400263) finished in 515 microseconds on worker node localhost:57638
2023-11-25 04:50:28.791 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.792 UTC [1096741] DEBUG:  00000: task execution (1) for placement (832) on anchor shard (1400260) finished in 2089 microseconds on worker node localhost:57637
2023-11-25 04:50:28.792 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.792 UTC [1096741] DEBUG:  00000: task execution (3) for placement (834) on anchor shard (1400262) finished in 444 microseconds on worker node localhost:57637
2023-11-25 04:50:28.792 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.793 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 57 in 3392 microseconds
2023-11-25 04:50:28.793 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.795 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 59 in 5201 microseconds
2023-11-25 04:50:28.795 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.795 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 56: 2 to node localhost:57637
2023-11-25 04:50:28.795 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.795 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 57: 0 to node localhost:57637
2023-11-25 04:50:28.795 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.795 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 58: 2 to node localhost:57638
2023-11-25 04:50:28.795 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.795 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 59: 0 to node localhost:57638
2023-11-25 04:50:28.795 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.796 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.796 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.796 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.796 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.796 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.796 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.796 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.796 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.796 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.796 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.796 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, rank() OVER (PARTITION BY worker_column_1 ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS rank, dense_rank() OVER (PARTITION BY worker_column_1 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS dense_rank, cume_dist() OVER (PARTITION BY worker_column_1 RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS cume_dist, percent_rank() OVER (PARTITION BY worker_column_1 ORDER BY (avg(worker_column_2)) RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS percent_rank, avg(worker_column_2) AS worker_column_6 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1
2023-11-25 04:50:28.796 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.796 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, rank() OVER (PARTITION BY worker_column_1 ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS rank, dense_rank() OVER (PARTITION BY worker_column_1 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS dense_rank, cume_dist() OVER (PARTITION BY worker_column_1 RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS cume_dist, percent_rank() OVER (PARTITION BY worker_column_1 ORDER BY (avg(worker_column_2)) RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS percent_rank, avg(worker_column_2) AS worker_column_6 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1
2023-11-25 04:50:28.796 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.796 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, rank() OVER (PARTITION BY worker_column_1 ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS rank, dense_rank() OVER (PARTITION BY worker_column_1 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS dense_rank, cume_dist() OVER (PARTITION BY worker_column_1 RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS cume_dist, percent_rank() OVER (PARTITION BY worker_column_1 ORDER BY (avg(worker_column_2)) RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS percent_rank, avg(worker_column_2) AS worker_column_6 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1
2023-11-25 04:50:28.796 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.797 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, rank() OVER (PARTITION BY worker_column_1 ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS rank, dense_rank() OVER (PARTITION BY worker_column_1 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS dense_rank, cume_dist() OVER (PARTITION BY worker_column_1 RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS cume_dist, percent_rank() OVER (PARTITION BY worker_column_1 ORDER BY (avg(worker_column_2)) RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS percent_rank, avg(worker_column_2) AS worker_column_6 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1
2023-11-25 04:50:28.797 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.797 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.797 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.797 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.797 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.797 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.797 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.797 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.797 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.797 UTC [1096741] DEBUG:  00000: combine query: SELECT user_id, rank, dense_rank, cume_dist, percent_rank FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, rank bigint, dense_rank bigint, cume_dist double precision, percent_rank double precision, worker_column_6 numeric) ORDER BY cume_dist DESC, dense_rank DESC, rank DESC, user_id DESC
2023-11-25 04:50:28.797 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.797 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.797 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.797 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 60 in 3764 microseconds
2023-11-25 04:50:28.797 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.797 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.797 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.797 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 62 in 3465 microseconds
2023-11-25 04:50:28.797 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.798 UTC [1096741] DEBUG:  00000: task execution (1) for placement (820) on anchor shard (1400256) finished in 606 microseconds on worker node localhost:57637
2023-11-25 04:50:28.798 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.798 UTC [1096741] DEBUG:  00000: task execution (2) for placement (821) on anchor shard (1400257) finished in 486 microseconds on worker node localhost:57638
2023-11-25 04:50:28.798 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.798 UTC [1096741] DEBUG:  00000: task execution (4) for placement (823) on anchor shard (1400259) finished in 344 microseconds on worker node localhost:57638
2023-11-25 04:50:28.798 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.798 UTC [1096741] DEBUG:  00000: task execution (3) for placement (822) on anchor shard (1400258) finished in 409 microseconds on worker node localhost:57637
2023-11-25 04:50:28.798 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.801 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 61 in 3665 microseconds
2023-11-25 04:50:28.801 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.802 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 63 in 4272 microseconds
2023-11-25 04:50:28.802 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.802 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 60: 2 to node localhost:57637
2023-11-25 04:50:28.802 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.802 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 61: 0 to node localhost:57637
2023-11-25 04:50:28.802 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.802 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 62: 2 to node localhost:57638
2023-11-25 04:50:28.802 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.802 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 63: 0 to node localhost:57638
2023-11-25 04:50:28.802 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.802 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.802 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.802 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.802 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.803 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.803 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.803 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.803 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.803 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.803 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.803 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS value_1, array_agg(worker_column_2) OVER (PARTITION BY worker_column_1 ORDER BY worker_column_2 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS array_agg, array_agg(worker_column_2) OVER (PARTITION BY worker_column_1 ORDER BY worker_column_2 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW EXCLUDE CURRENT ROW) AS array_agg FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400256 users_table WHERE ((users_table.user_id OPERATOR(pg_catalog.>) 2) AND (users_table.user_id OPERATOR(pg_catalog.<) 6))) worker_subquery
2023-11-25 04:50:28.803 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.803 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS value_1, array_agg(worker_column_2) OVER (PARTITION BY worker_column_1 ORDER BY worker_column_2 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS array_agg, array_agg(worker_column_2) OVER (PARTITION BY worker_column_1 ORDER BY worker_column_2 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW EXCLUDE CURRENT ROW) AS array_agg FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400257 users_table WHERE ((users_table.user_id OPERATOR(pg_catalog.>) 2) AND (users_table.user_id OPERATOR(pg_catalog.<) 6))) worker_subquery
2023-11-25 04:50:28.803 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.803 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS value_1, array_agg(worker_column_2) OVER (PARTITION BY worker_column_1 ORDER BY worker_column_2 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS array_agg, array_agg(worker_column_2) OVER (PARTITION BY worker_column_1 ORDER BY worker_column_2 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW EXCLUDE CURRENT ROW) AS array_agg FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400258 users_table WHERE ((users_table.user_id OPERATOR(pg_catalog.>) 2) AND (users_table.user_id OPERATOR(pg_catalog.<) 6))) worker_subquery
2023-11-25 04:50:28.803 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.803 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS value_1, array_agg(worker_column_2) OVER (PARTITION BY worker_column_1 ORDER BY worker_column_2 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS array_agg, array_agg(worker_column_2) OVER (PARTITION BY worker_column_1 ORDER BY worker_column_2 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW EXCLUDE CURRENT ROW) AS array_agg FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400259 users_table WHERE ((users_table.user_id OPERATOR(pg_catalog.>) 2) AND (users_table.user_id OPERATOR(pg_catalog.<) 6))) worker_subquery
2023-11-25 04:50:28.803 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.803 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.803 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.803 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.803 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.803 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.803 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.803 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.803 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.803 UTC [1096741] DEBUG:  00000: combine query: SELECT user_id, value_1, array_agg, array_agg_1 AS array_agg FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, value_1 integer, array_agg integer[], array_agg_1 integer[]) ORDER BY user_id, value_1, array_agg, array_agg_1
2023-11-25 04:50:28.803 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.803 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.803 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.803 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 64 in 3764 microseconds
2023-11-25 04:50:28.803 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.803 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.803 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.803 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 66 in 3465 microseconds
2023-11-25 04:50:28.803 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.804 UTC [1096741] DEBUG:  00000: task execution (2) for placement (821) on anchor shard (1400257) finished in 1047 microseconds on worker node localhost:57638
2023-11-25 04:50:28.804 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.805 UTC [1096741] DEBUG:  00000: task execution (1) for placement (820) on anchor shard (1400256) finished in 1277 microseconds on worker node localhost:57637
2023-11-25 04:50:28.805 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.805 UTC [1096741] DEBUG:  00000: task execution (4) for placement (823) on anchor shard (1400259) finished in 351 microseconds on worker node localhost:57638
2023-11-25 04:50:28.805 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.805 UTC [1096741] DEBUG:  00000: task execution (3) for placement (822) on anchor shard (1400258) finished in 374 microseconds on worker node localhost:57637
2023-11-25 04:50:28.805 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.807 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 65 in 3295 microseconds
2023-11-25 04:50:28.807 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.807 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 67 in 3166 microseconds
2023-11-25 04:50:28.807 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.807 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 64: 2 to node localhost:57637
2023-11-25 04:50:28.807 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.807 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 65: 0 to node localhost:57637
2023-11-25 04:50:28.807 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.807 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 66: 2 to node localhost:57638
2023-11-25 04:50:28.807 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.807 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 67: 0 to node localhost:57638
2023-11-25 04:50:28.807 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.808 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.808 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.808 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.808 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.808 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.808 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.808 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.808 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.808 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.808 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.808 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS value_1, array_agg(worker_column_2) OVER range_window AS array_agg, array_agg(worker_column_2) OVER range_window_exclude AS array_agg FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400256 users_table WHERE ((users_table.user_id OPERATOR(pg_catalog.>) 2) AND (users_table.user_id OPERATOR(pg_catalog.<) 6))) worker_subquery WINDOW range_window AS (PARTITION BY worker_column_1 ORDER BY worker_column_2 RANGE BETWEEN 1 PRECEDING AND 1 FOLLOWING), range_window_exclude AS (PARTITION BY worker_column_1 ORDER BY worker_column_2 RANGE BETWEEN 1 PRECEDING AND 1 FOLLOWING EXCLUDE CURRENT ROW)
2023-11-25 04:50:28.808 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.808 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS value_1, array_agg(worker_column_2) OVER range_window AS array_agg, array_agg(worker_column_2) OVER range_window_exclude AS array_agg FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400257 users_table WHERE ((users_table.user_id OPERATOR(pg_catalog.>) 2) AND (users_table.user_id OPERATOR(pg_catalog.<) 6))) worker_subquery WINDOW range_window AS (PARTITION BY worker_column_1 ORDER BY worker_column_2 RANGE BETWEEN 1 PRECEDING AND 1 FOLLOWING), range_window_exclude AS (PARTITION BY worker_column_1 ORDER BY worker_column_2 RANGE BETWEEN 1 PRECEDING AND 1 FOLLOWING EXCLUDE CURRENT ROW)
2023-11-25 04:50:28.808 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.808 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS value_1, array_agg(worker_column_2) OVER range_window AS array_agg, array_agg(worker_column_2) OVER range_window_exclude AS array_agg FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400258 users_table WHERE ((users_table.user_id OPERATOR(pg_catalog.>) 2) AND (users_table.user_id OPERATOR(pg_catalog.<) 6))) worker_subquery WINDOW range_window AS (PARTITION BY worker_column_1 ORDER BY worker_column_2 RANGE BETWEEN 1 PRECEDING AND 1 FOLLOWING), range_window_exclude AS (PARTITION BY worker_column_1 ORDER BY worker_column_2 RANGE BETWEEN 1 PRECEDING AND 1 FOLLOWING EXCLUDE CURRENT ROW)
2023-11-25 04:50:28.808 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.808 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS value_1, array_agg(worker_column_2) OVER range_window AS array_agg, array_agg(worker_column_2) OVER range_window_exclude AS array_agg FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400259 users_table WHERE ((users_table.user_id OPERATOR(pg_catalog.>) 2) AND (users_table.user_id OPERATOR(pg_catalog.<) 6))) worker_subquery WINDOW range_window AS (PARTITION BY worker_column_1 ORDER BY worker_column_2 RANGE BETWEEN 1 PRECEDING AND 1 FOLLOWING), range_window_exclude AS (PARTITION BY worker_column_1 ORDER BY worker_column_2 RANGE BETWEEN 1 PRECEDING AND 1 FOLLOWING EXCLUDE CURRENT ROW)
2023-11-25 04:50:28.808 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.808 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.808 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.808 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.808 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.808 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.808 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.808 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.808 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.808 UTC [1096741] DEBUG:  00000: combine query: SELECT user_id, value_1, array_agg, array_agg_1 AS array_agg FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, value_1 integer, array_agg integer[], array_agg_1 integer[]) ORDER BY user_id, value_1, array_agg, array_agg_1
2023-11-25 04:50:28.808 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.809 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.809 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.809 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 68 in 3764 microseconds
2023-11-25 04:50:28.809 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.809 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.809 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.809 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 70 in 3465 microseconds
2023-11-25 04:50:28.809 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.810 UTC [1096741] DEBUG:  00000: task execution (1) for placement (820) on anchor shard (1400256) finished in 1390 microseconds on worker node localhost:57637
2023-11-25 04:50:28.810 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.810 UTC [1096741] DEBUG:  00000: task execution (2) for placement (821) on anchor shard (1400257) finished in 1354 microseconds on worker node localhost:57638
2023-11-25 04:50:28.810 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.810 UTC [1096741] DEBUG:  00000: task execution (3) for placement (822) on anchor shard (1400258) finished in 327 microseconds on worker node localhost:57637
2023-11-25 04:50:28.810 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.811 UTC [1096741] DEBUG:  00000: task execution (4) for placement (823) on anchor shard (1400259) finished in 287 microseconds on worker node localhost:57638
2023-11-25 04:50:28.811 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.812 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 71 in 3473 microseconds
2023-11-25 04:50:28.812 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.813 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 69 in 4543 microseconds
2023-11-25 04:50:28.813 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.813 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 68: 2 to node localhost:57637
2023-11-25 04:50:28.813 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.813 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 69: 0 to node localhost:57637
2023-11-25 04:50:28.813 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.813 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 70: 2 to node localhost:57638
2023-11-25 04:50:28.813 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.813 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 71: 0 to node localhost:57638
2023-11-25 04:50:28.813 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.815 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.815 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.815 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.815 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.815 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.815 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.815 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.815 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.815 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.815 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.815 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS value_1, array_agg(worker_column_2) OVER row_window AS array_agg, array_agg(worker_column_2) OVER row_window_exclude AS array_agg FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400256 users_table WHERE ((users_table.user_id OPERATOR(pg_catalog.>) 2) AND (users_table.user_id OPERATOR(pg_catalog.<) 6))) worker_subquery WINDOW row_window AS (PARTITION BY worker_column_1 ORDER BY worker_column_2 ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING), row_window_exclude AS (PARTITION BY worker_column_1 ORDER BY worker_column_2 ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING EXCLUDE CURRENT ROW)
2023-11-25 04:50:28.815 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.815 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS value_1, array_agg(worker_column_2) OVER row_window AS array_agg, array_agg(worker_column_2) OVER row_window_exclude AS array_agg FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400257 users_table WHERE ((users_table.user_id OPERATOR(pg_catalog.>) 2) AND (users_table.user_id OPERATOR(pg_catalog.<) 6))) worker_subquery WINDOW row_window AS (PARTITION BY worker_column_1 ORDER BY worker_column_2 ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING), row_window_exclude AS (PARTITION BY worker_column_1 ORDER BY worker_column_2 ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING EXCLUDE CURRENT ROW)
2023-11-25 04:50:28.815 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.815 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS value_1, array_agg(worker_column_2) OVER row_window AS array_agg, array_agg(worker_column_2) OVER row_window_exclude AS array_agg FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400258 users_table WHERE ((users_table.user_id OPERATOR(pg_catalog.>) 2) AND (users_table.user_id OPERATOR(pg_catalog.<) 6))) worker_subquery WINDOW row_window AS (PARTITION BY worker_column_1 ORDER BY worker_column_2 ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING), row_window_exclude AS (PARTITION BY worker_column_1 ORDER BY worker_column_2 ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING EXCLUDE CURRENT ROW)
2023-11-25 04:50:28.815 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.815 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS value_1, array_agg(worker_column_2) OVER row_window AS array_agg, array_agg(worker_column_2) OVER row_window_exclude AS array_agg FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400259 users_table WHERE ((users_table.user_id OPERATOR(pg_catalog.>) 2) AND (users_table.user_id OPERATOR(pg_catalog.<) 6))) worker_subquery WINDOW row_window AS (PARTITION BY worker_column_1 ORDER BY worker_column_2 ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING), row_window_exclude AS (PARTITION BY worker_column_1 ORDER BY worker_column_2 ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING EXCLUDE CURRENT ROW)
2023-11-25 04:50:28.815 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.815 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.815 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.815 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.815 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.815 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.815 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.815 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.815 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.815 UTC [1096741] DEBUG:  00000: combine query: SELECT user_id, value_1, array_agg, array_agg_1 AS array_agg FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, value_1 integer, array_agg integer[], array_agg_1 integer[]) ORDER BY user_id, value_1, array_agg, array_agg_1
2023-11-25 04:50:28.815 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.815 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.815 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.815 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 72 in 3764 microseconds
2023-11-25 04:50:28.815 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.815 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.815 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.815 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 74 in 3465 microseconds
2023-11-25 04:50:28.815 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.816 UTC [1096741] DEBUG:  00000: task execution (1) for placement (820) on anchor shard (1400256) finished in 858 microseconds on worker node localhost:57637
2023-11-25 04:50:28.816 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.817 UTC [1096741] DEBUG:  00000: task execution (2) for placement (821) on anchor shard (1400257) finished in 1252 microseconds on worker node localhost:57638
2023-11-25 04:50:28.817 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.817 UTC [1096741] DEBUG:  00000: task execution (3) for placement (822) on anchor shard (1400258) finished in 594 microseconds on worker node localhost:57637
2023-11-25 04:50:28.817 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.817 UTC [1096741] DEBUG:  00000: task execution (4) for placement (823) on anchor shard (1400259) finished in 634 microseconds on worker node localhost:57638
2023-11-25 04:50:28.817 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.818 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 73 in 3198 microseconds
2023-11-25 04:50:28.818 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.819 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 75 in 3225 microseconds
2023-11-25 04:50:28.819 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.819 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 72: 2 to node localhost:57637
2023-11-25 04:50:28.819 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.819 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 73: 0 to node localhost:57637
2023-11-25 04:50:28.819 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.819 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 74: 2 to node localhost:57638
2023-11-25 04:50:28.819 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.819 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 75: 0 to node localhost:57638
2023-11-25 04:50:28.819 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.819 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.819 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.819 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.819 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.819 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.819 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.820 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.820 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.820 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.820 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.820 UTC [1096741] DEBUG:  00000: generated sql query for task 1
2023-11-25 04:50:28.820 UTC [1096741] DETAIL:  query string: "SELECT value_2, sum(value_1) AS worker_column_2, count(value_1) AS worker_column_3 FROM public.users_table_1400256 users_table WHERE true GROUP BY value_2"
2023-11-25 04:50:28.820 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.820 UTC [1096741] DEBUG:  00000: generated sql query for task 2
2023-11-25 04:50:28.820 UTC [1096741] DETAIL:  query string: "SELECT value_2, sum(value_1) AS worker_column_2, count(value_1) AS worker_column_3 FROM public.users_table_1400257 users_table WHERE true GROUP BY value_2"
2023-11-25 04:50:28.820 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.820 UTC [1096741] DEBUG:  00000: generated sql query for task 3
2023-11-25 04:50:28.820 UTC [1096741] DETAIL:  query string: "SELECT value_2, sum(value_1) AS worker_column_2, count(value_1) AS worker_column_3 FROM public.users_table_1400258 users_table WHERE true GROUP BY value_2"
2023-11-25 04:50:28.820 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.820 UTC [1096741] DEBUG:  00000: generated sql query for task 4
2023-11-25 04:50:28.820 UTC [1096741] DETAIL:  query string: "SELECT value_2, sum(value_1) AS worker_column_2, count(value_1) AS worker_column_3 FROM public.users_table_1400259 users_table WHERE true GROUP BY value_2"
2023-11-25 04:50:28.820 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.820 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.820 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.820 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.820 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.820 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.820 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.820 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.820 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.820 UTC [1096741] DEBUG:  00000: combine query: SELECT value_2, rank() OVER (PARTITION BY value_2 ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS rank, dense_rank() OVER (PARTITION BY value_2 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS dense_rank, cume_dist() OVER (PARTITION BY value_2 RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS cume_dist, percent_rank() OVER (PARTITION BY value_2 ORDER BY (pg_catalog.sum(worker_column_2) OPERATOR(pg_catalog./) pg_catalog.sum(worker_column_3)) RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS percent_rank FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), '(i 1)'::cstring(0)) remote_scan(value_2 integer, worker_column_2 bigint, worker_column_3 bigint) GROUP BY value_2 ORDER BY (cume_dist() OVER (PARTITION BY value_2 RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)) DESC, (dense_rank() OVER (PARTITION BY value_2 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)) DESC, (rank() OVER (PARTITION BY value_2 ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)) DESC, value_2 DESC
2023-11-25 04:50:28.820 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.820 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.820 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.820 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 76 in 3764 microseconds
2023-11-25 04:50:28.820 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.820 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.820 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.821 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 78 in 3465 microseconds
2023-11-25 04:50:28.821 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.821 UTC [1096741] DEBUG:  00000: task execution (2) for placement (821) on anchor shard (1400257) finished in 589 microseconds on worker node localhost:57638
2023-11-25 04:50:28.821 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.821 UTC [1096741] DEBUG:  00000: task execution (1) for placement (820) on anchor shard (1400256) finished in 1086 microseconds on worker node localhost:57637
2023-11-25 04:50:28.821 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.822 UTC [1096741] DEBUG:  00000: task execution (4) for placement (823) on anchor shard (1400259) finished in 327 microseconds on worker node localhost:57638
2023-11-25 04:50:28.822 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.822 UTC [1096741] DEBUG:  00000: task execution (3) for placement (822) on anchor shard (1400258) finished in 211 microseconds on worker node localhost:57637
2023-11-25 04:50:28.822 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.823 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 77 in 2959 microseconds
2023-11-25 04:50:28.823 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.824 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 79 in 3117 microseconds
2023-11-25 04:50:28.824 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.824 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 76: 2 to node localhost:57637
2023-11-25 04:50:28.824 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.824 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 77: 0 to node localhost:57637
2023-11-25 04:50:28.824 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.824 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 78: 2 to node localhost:57638
2023-11-25 04:50:28.824 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.824 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 79: 0 to node localhost:57638
2023-11-25 04:50:28.824 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.824 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.824 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.824 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.824 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.824 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.824 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.824 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.824 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.824 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.824 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.824 UTC [1096741] DEBUG:  00000: generated sql query for task 1
2023-11-25 04:50:28.824 UTC [1096741] DETAIL:  query string: "SELECT value_2, value_1, value_1 AS array_agg, value_1 AS array_agg FROM public.users_table_1400256 users_table WHERE ((value_2 OPERATOR(pg_catalog.>) 2) AND (value_2 OPERATOR(pg_catalog.<) 6))"
2023-11-25 04:50:28.824 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.824 UTC [1096741] DEBUG:  00000: generated sql query for task 2
2023-11-25 04:50:28.824 UTC [1096741] DETAIL:  query string: "SELECT value_2, value_1, value_1 AS array_agg, value_1 AS array_agg FROM public.users_table_1400257 users_table WHERE ((value_2 OPERATOR(pg_catalog.>) 2) AND (value_2 OPERATOR(pg_catalog.<) 6))"
2023-11-25 04:50:28.824 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.824 UTC [1096741] DEBUG:  00000: generated sql query for task 3
2023-11-25 04:50:28.824 UTC [1096741] DETAIL:  query string: "SELECT value_2, value_1, value_1 AS array_agg, value_1 AS array_agg FROM public.users_table_1400258 users_table WHERE ((value_2 OPERATOR(pg_catalog.>) 2) AND (value_2 OPERATOR(pg_catalog.<) 6))"
2023-11-25 04:50:28.824 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.824 UTC [1096741] DEBUG:  00000: generated sql query for task 4
2023-11-25 04:50:28.824 UTC [1096741] DETAIL:  query string: "SELECT value_2, value_1, value_1 AS array_agg, value_1 AS array_agg FROM public.users_table_1400259 users_table WHERE ((value_2 OPERATOR(pg_catalog.>) 2) AND (value_2 OPERATOR(pg_catalog.<) 6))"
2023-11-25 04:50:28.824 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.824 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.824 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.824 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.824 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.824 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.824 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.824 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.824 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.825 UTC [1096741] DEBUG:  00000: combine query: SELECT value_2, value_1, array_agg(array_agg) OVER (PARTITION BY value_2 ORDER BY value_1 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS array_agg, array_agg(array_agg_1) OVER (PARTITION BY value_2 ORDER BY value_1 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW EXCLUDE CURRENT ROW) AS array_agg FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), '(i 1)'::cstring(0)) remote_scan(value_2 integer, value_1 integer, array_agg integer, array_agg_1 integer) ORDER BY value_2, value_1, (array_agg(array_agg) OVER (PARTITION BY value_2 ORDER BY value_1 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)), (array_agg(array_agg_1) OVER (PARTITION BY value_2 ORDER BY value_1 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW EXCLUDE CURRENT ROW))
2023-11-25 04:50:28.825 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.825 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.825 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.825 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 80 in 3764 microseconds
2023-11-25 04:50:28.825 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.825 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.825 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.825 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 82 in 3465 microseconds
2023-11-25 04:50:28.825 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.825 UTC [1096741] DEBUG:  00000: task execution (1) for placement (820) on anchor shard (1400256) finished in 431 microseconds on worker node localhost:57637
2023-11-25 04:50:28.825 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.825 UTC [1096741] DEBUG:  00000: task execution (2) for placement (821) on anchor shard (1400257) finished in 388 microseconds on worker node localhost:57638
2023-11-25 04:50:28.825 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.826 UTC [1096741] DEBUG:  00000: task execution (3) for placement (822) on anchor shard (1400258) finished in 198 microseconds on worker node localhost:57637
2023-11-25 04:50:28.826 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.826 UTC [1096741] DEBUG:  00000: task execution (4) for placement (823) on anchor shard (1400259) finished in 370 microseconds on worker node localhost:57638
2023-11-25 04:50:28.826 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.828 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 81 in 3228 microseconds
2023-11-25 04:50:28.828 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.829 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 83 in 3676 microseconds
2023-11-25 04:50:28.829 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.829 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 80: 2 to node localhost:57637
2023-11-25 04:50:28.829 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.829 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 81: 0 to node localhost:57637
2023-11-25 04:50:28.829 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.829 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 82: 2 to node localhost:57638
2023-11-25 04:50:28.829 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.829 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 83: 0 to node localhost:57638
2023-11-25 04:50:28.829 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.830 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.830 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.830 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.830 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.830 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.830 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.831 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.831 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.831 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.831 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.831 UTC [1096741] DEBUG:  00000: generated sql query for task 1
2023-11-25 04:50:28.831 UTC [1096741] DETAIL:  query string: "SELECT value_2, value_1, value_1 AS array_agg, value_1 AS array_agg FROM public.users_table_1400256 users_table WHERE ((value_2 OPERATOR(pg_catalog.>) 2) AND (value_2 OPERATOR(pg_catalog.<) 6))"
2023-11-25 04:50:28.831 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.831 UTC [1096741] DEBUG:  00000: generated sql query for task 2
2023-11-25 04:50:28.831 UTC [1096741] DETAIL:  query string: "SELECT value_2, value_1, value_1 AS array_agg, value_1 AS array_agg FROM public.users_table_1400257 users_table WHERE ((value_2 OPERATOR(pg_catalog.>) 2) AND (value_2 OPERATOR(pg_catalog.<) 6))"
2023-11-25 04:50:28.831 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.831 UTC [1096741] DEBUG:  00000: generated sql query for task 3
2023-11-25 04:50:28.831 UTC [1096741] DETAIL:  query string: "SELECT value_2, value_1, value_1 AS array_agg, value_1 AS array_agg FROM public.users_table_1400258 users_table WHERE ((value_2 OPERATOR(pg_catalog.>) 2) AND (value_2 OPERATOR(pg_catalog.<) 6))"
2023-11-25 04:50:28.831 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.831 UTC [1096741] DEBUG:  00000: generated sql query for task 4
2023-11-25 04:50:28.831 UTC [1096741] DETAIL:  query string: "SELECT value_2, value_1, value_1 AS array_agg, value_1 AS array_agg FROM public.users_table_1400259 users_table WHERE ((value_2 OPERATOR(pg_catalog.>) 2) AND (value_2 OPERATOR(pg_catalog.<) 6))"
2023-11-25 04:50:28.831 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.831 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.831 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.831 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.831 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.831 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.831 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.831 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.831 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.831 UTC [1096741] DEBUG:  00000: combine query: SELECT value_2, value_1, array_agg(array_agg) OVER range_window AS array_agg, array_agg(array_agg_1) OVER range_window_exclude AS array_agg FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), '(i 1)'::cstring(0)) remote_scan(value_2 integer, value_1 integer, array_agg integer, array_agg_1 integer) WINDOW range_window AS (PARTITION BY value_2 ORDER BY value_1 RANGE BETWEEN 1 PRECEDING AND 1 FOLLOWING), range_window_exclude AS (PARTITION BY value_2 ORDER BY value_1 RANGE BETWEEN 1 PRECEDING AND 1 FOLLOWING EXCLUDE CURRENT ROW) ORDER BY value_2, value_1, (array_agg(array_agg) OVER range_window), (array_agg(array_agg_1) OVER range_window_exclude)
2023-11-25 04:50:28.831 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.831 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.831 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.831 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 84 in 3764 microseconds
2023-11-25 04:50:28.831 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.831 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.831 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.831 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 86 in 3465 microseconds
2023-11-25 04:50:28.831 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.832 UTC [1096741] DEBUG:  00000: task execution (1) for placement (820) on anchor shard (1400256) finished in 345 microseconds on worker node localhost:57637
2023-11-25 04:50:28.832 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.832 UTC [1096741] DEBUG:  00000: task execution (3) for placement (822) on anchor shard (1400258) finished in 186 microseconds on worker node localhost:57637
2023-11-25 04:50:28.832 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.832 UTC [1096741] DEBUG:  00000: task execution (2) for placement (821) on anchor shard (1400257) finished in 437 microseconds on worker node localhost:57638
2023-11-25 04:50:28.832 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.832 UTC [1096741] DEBUG:  00000: task execution (4) for placement (823) on anchor shard (1400259) finished in 417 microseconds on worker node localhost:57638
2023-11-25 04:50:28.832 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.835 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 87 in 3258 microseconds
2023-11-25 04:50:28.835 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.836 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 85 in 4817 microseconds
2023-11-25 04:50:28.836 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.836 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 84: 2 to node localhost:57637
2023-11-25 04:50:28.836 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.836 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 85: 0 to node localhost:57637
2023-11-25 04:50:28.836 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.836 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 86: 2 to node localhost:57638
2023-11-25 04:50:28.836 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.836 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 87: 0 to node localhost:57638
2023-11-25 04:50:28.836 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.838 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.838 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.838 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.838 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.838 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.838 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.838 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.838 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.838 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.838 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.838 UTC [1096741] DEBUG:  00000: generated sql query for task 1
2023-11-25 04:50:28.838 UTC [1096741] DETAIL:  query string: "SELECT value_2, value_1, value_1 AS array_agg, value_1 AS array_agg FROM public.users_table_1400256 users_table WHERE ((value_2 OPERATOR(pg_catalog.>) 2) AND (value_2 OPERATOR(pg_catalog.<) 6))"
2023-11-25 04:50:28.838 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.838 UTC [1096741] DEBUG:  00000: generated sql query for task 2
2023-11-25 04:50:28.838 UTC [1096741] DETAIL:  query string: "SELECT value_2, value_1, value_1 AS array_agg, value_1 AS array_agg FROM public.users_table_1400257 users_table WHERE ((value_2 OPERATOR(pg_catalog.>) 2) AND (value_2 OPERATOR(pg_catalog.<) 6))"
2023-11-25 04:50:28.838 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.838 UTC [1096741] DEBUG:  00000: generated sql query for task 3
2023-11-25 04:50:28.838 UTC [1096741] DETAIL:  query string: "SELECT value_2, value_1, value_1 AS array_agg, value_1 AS array_agg FROM public.users_table_1400258 users_table WHERE ((value_2 OPERATOR(pg_catalog.>) 2) AND (value_2 OPERATOR(pg_catalog.<) 6))"
2023-11-25 04:50:28.838 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.838 UTC [1096741] DEBUG:  00000: generated sql query for task 4
2023-11-25 04:50:28.838 UTC [1096741] DETAIL:  query string: "SELECT value_2, value_1, value_1 AS array_agg, value_1 AS array_agg FROM public.users_table_1400259 users_table WHERE ((value_2 OPERATOR(pg_catalog.>) 2) AND (value_2 OPERATOR(pg_catalog.<) 6))"
2023-11-25 04:50:28.838 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.838 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.838 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.838 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.838 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.838 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.838 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.838 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.838 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.838 UTC [1096741] DEBUG:  00000: combine query: SELECT value_2, value_1, array_agg(array_agg) OVER row_window AS array_agg, array_agg(array_agg_1) OVER row_window_exclude AS array_agg FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), '(i 1)'::cstring(0)) remote_scan(value_2 integer, value_1 integer, array_agg integer, array_agg_1 integer) WINDOW row_window AS (PARTITION BY value_2 ORDER BY value_1 ROWS BETWEEN '1'::bigint PRECEDING AND '1'::bigint FOLLOWING), row_window_exclude AS (PARTITION BY value_2 ORDER BY value_1 ROWS BETWEEN '1'::bigint PRECEDING AND '1'::bigint FOLLOWING EXCLUDE CURRENT ROW) ORDER BY value_2, value_1, (array_agg(array_agg) OVER row_window), (array_agg(array_agg_1) OVER row_window_exclude)
2023-11-25 04:50:28.838 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.838 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.838 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.838 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 88 in 3764 microseconds
2023-11-25 04:50:28.838 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.838 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.838 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.839 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 90 in 3465 microseconds
2023-11-25 04:50:28.839 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.839 UTC [1096741] DEBUG:  00000: task execution (1) for placement (820) on anchor shard (1400256) finished in 572 microseconds on worker node localhost:57637
2023-11-25 04:50:28.839 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.840 UTC [1096741] DEBUG:  00000: task execution (2) for placement (821) on anchor shard (1400257) finished in 1077 microseconds on worker node localhost:57638
2023-11-25 04:50:28.840 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.840 UTC [1096741] DEBUG:  00000: task execution (3) for placement (822) on anchor shard (1400258) finished in 727 microseconds on worker node localhost:57637
2023-11-25 04:50:28.840 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.840 UTC [1096741] DEBUG:  00000: task execution (4) for placement (823) on anchor shard (1400259) finished in 332 microseconds on worker node localhost:57638
2023-11-25 04:50:28.840 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.842 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 89 in 3607 microseconds
2023-11-25 04:50:28.842 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.843 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 91 in 4387 microseconds
2023-11-25 04:50:28.843 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.843 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 88: 2 to node localhost:57637
2023-11-25 04:50:28.843 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.843 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 89: 0 to node localhost:57637
2023-11-25 04:50:28.843 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.843 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 90: 2 to node localhost:57638
2023-11-25 04:50:28.843 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.843 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 91: 0 to node localhost:57638
2023-11-25 04:50:28.843 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.844 UTC [1096741] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 04:50:28.844 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.844 UTC [1096741] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 04:50:28.844 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.844 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.844 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.844 UTC [1096741] DEBUG:  00000: push down of limit count: 5
2023-11-25 04:50:28.844 UTC [1096741] LOCATION:  WorkerLimitCount, multi_logical_optimizer.c:4778
2023-11-25 04:50:28.844 UTC [1096741] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 04:50:28.844 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.844 UTC [1096741] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 04:50:28.844 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.844 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER my_win AS sum, worker_column_2 AS event_type, count(*) AS worker_column_4, max(worker_column_2) AS worker_column_5 FROM (SELECT events_table.user_id AS worker_column_1, events_table.event_type AS worker_column_2 FROM public.events_table_1400260 events_table) worker_subquery GROUP BY worker_column_1, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 2) WINDOW my_win AS (PARTITION BY worker_column_1, (max(worker_column_2)) ORDER BY (count(*)) DESC) ORDER BY (sum(worker_column_2) OVER my_win) DESC, worker_column_2 DESC, worker_column_1 DESC LIMIT '5'::bigint
2023-11-25 04:50:28.844 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.844 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER my_win AS sum, worker_column_2 AS event_type, count(*) AS worker_column_4, max(worker_column_2) AS worker_column_5 FROM (SELECT events_table.user_id AS worker_column_1, events_table.event_type AS worker_column_2 FROM public.events_table_1400261 events_table) worker_subquery GROUP BY worker_column_1, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 2) WINDOW my_win AS (PARTITION BY worker_column_1, (max(worker_column_2)) ORDER BY (count(*)) DESC) ORDER BY (sum(worker_column_2) OVER my_win) DESC, worker_column_2 DESC, worker_column_1 DESC LIMIT '5'::bigint
2023-11-25 04:50:28.844 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.844 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER my_win AS sum, worker_column_2 AS event_type, count(*) AS worker_column_4, max(worker_column_2) AS worker_column_5 FROM (SELECT events_table.user_id AS worker_column_1, events_table.event_type AS worker_column_2 FROM public.events_table_1400262 events_table) worker_subquery GROUP BY worker_column_1, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 2) WINDOW my_win AS (PARTITION BY worker_column_1, (max(worker_column_2)) ORDER BY (count(*)) DESC) ORDER BY (sum(worker_column_2) OVER my_win) DESC, worker_column_2 DESC, worker_column_1 DESC LIMIT '5'::bigint
2023-11-25 04:50:28.844 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.844 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER my_win AS sum, worker_column_2 AS event_type, count(*) AS worker_column_4, max(worker_column_2) AS worker_column_5 FROM (SELECT events_table.user_id AS worker_column_1, events_table.event_type AS worker_column_2 FROM public.events_table_1400263 events_table) worker_subquery GROUP BY worker_column_1, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 2) WINDOW my_win AS (PARTITION BY worker_column_1, (max(worker_column_2)) ORDER BY (count(*)) DESC) ORDER BY (sum(worker_column_2) OVER my_win) DESC, worker_column_2 DESC, worker_column_1 DESC LIMIT '5'::bigint
2023-11-25 04:50:28.844 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.844 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.844 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.844 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.844 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.844 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.844 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.844 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.844 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.844 UTC [1096741] DEBUG:  00000: combine query: SELECT user_id, sum, event_type FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, sum bigint, event_type integer, worker_column_4 bigint, worker_column_5 integer) ORDER BY sum DESC, event_type DESC, user_id DESC LIMIT '5'::bigint
2023-11-25 04:50:28.844 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.844 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.844 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.845 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 92 in 3764 microseconds
2023-11-25 04:50:28.845 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.845 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.845 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.845 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 94 in 3465 microseconds
2023-11-25 04:50:28.845 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.845 UTC [1096741] DEBUG:  00000: task execution (1) for placement (832) on anchor shard (1400260) finished in 862 microseconds on worker node localhost:57637
2023-11-25 04:50:28.845 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.846 UTC [1096741] DEBUG:  00000: task execution (3) for placement (834) on anchor shard (1400262) finished in 357 microseconds on worker node localhost:57637
2023-11-25 04:50:28.846 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.846 UTC [1096741] DEBUG:  00000: task execution (2) for placement (833) on anchor shard (1400261) finished in 1459 microseconds on worker node localhost:57638
2023-11-25 04:50:28.846 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.847 UTC [1096741] DEBUG:  00000: task execution (4) for placement (835) on anchor shard (1400263) finished in 939 microseconds on worker node localhost:57638
2023-11-25 04:50:28.847 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.848 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 93 in 3909 microseconds
2023-11-25 04:50:28.848 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.851 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 95 in 6211 microseconds
2023-11-25 04:50:28.851 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.851 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 92: 2 to node localhost:57637
2023-11-25 04:50:28.851 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.851 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 93: 0 to node localhost:57637
2023-11-25 04:50:28.851 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.851 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 94: 2 to node localhost:57638
2023-11-25 04:50:28.851 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.851 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 95: 0 to node localhost:57638
2023-11-25 04:50:28.851 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.852 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.852 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.852 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.852 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.852 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.852 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.852 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.852 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.852 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.852 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.852 UTC [1096741] DEBUG:  00000: generated sql query for task 1
2023-11-25 04:50:28.852 UTC [1096741] DETAIL:  query string: "SELECT value_1, sum(value_3) AS avg, count(value_3) AS avg, sum(value_2) AS worker_column_4, count(value_2) AS worker_column_5 FROM public.users_table_1400256 users_table WHERE true GROUP BY value_1"
2023-11-25 04:50:28.852 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.852 UTC [1096741] DEBUG:  00000: generated sql query for task 2
2023-11-25 04:50:28.852 UTC [1096741] DETAIL:  query string: "SELECT value_1, sum(value_3) AS avg, count(value_3) AS avg, sum(value_2) AS worker_column_4, count(value_2) AS worker_column_5 FROM public.users_table_1400257 users_table WHERE true GROUP BY value_1"
2023-11-25 04:50:28.852 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.852 UTC [1096741] DEBUG:  00000: generated sql query for task 3
2023-11-25 04:50:28.852 UTC [1096741] DETAIL:  query string: "SELECT value_1, sum(value_3) AS avg, count(value_3) AS avg, sum(value_2) AS worker_column_4, count(value_2) AS worker_column_5 FROM public.users_table_1400258 users_table WHERE true GROUP BY value_1"
2023-11-25 04:50:28.852 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.852 UTC [1096741] DEBUG:  00000: generated sql query for task 4
2023-11-25 04:50:28.852 UTC [1096741] DETAIL:  query string: "SELECT value_1, sum(value_3) AS avg, count(value_3) AS avg, sum(value_2) AS worker_column_4, count(value_2) AS worker_column_5 FROM public.users_table_1400259 users_table WHERE true GROUP BY value_1"
2023-11-25 04:50:28.852 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.852 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.852 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.852 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.852 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.852 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.852 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.852 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.852 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.852 UTC [1096741] DEBUG:  00000: combine query: SELECT value_1, (sum(avg) OPERATOR(pg_catalog./) (pg_catalog.sum(avg_1))::double precision) AS avg, dense_rank() OVER (PARTITION BY (sum(avg) OPERATOR(pg_catalog./) (pg_catalog.sum(avg_1))::double precision) ORDER BY (pg_catalog.sum(worker_column_4) OPERATOR(pg_catalog./) pg_catalog.sum(worker_column_5))) AS dense_rank FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), '(i 1)'::cstring(0)) remote_scan(value_1 integer, avg double precision, avg_1 bigint, worker_column_4 bigint, worker_column_5 bigint) GROUP BY value_1 ORDER BY value_1
2023-11-25 04:50:28.852 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.852 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.852 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.852 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 96 in 3764 microseconds
2023-11-25 04:50:28.852 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.853 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.853 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.853 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 98 in 3465 microseconds
2023-11-25 04:50:28.853 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.853 UTC [1096741] DEBUG:  00000: task execution (1) for placement (820) on anchor shard (1400256) finished in 737 microseconds on worker node localhost:57637
2023-11-25 04:50:28.853 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.853 UTC [1096741] DEBUG:  00000: task execution (2) for placement (821) on anchor shard (1400257) finished in 588 microseconds on worker node localhost:57638
2023-11-25 04:50:28.853 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.854 UTC [1096741] DEBUG:  00000: task execution (3) for placement (822) on anchor shard (1400258) finished in 597 microseconds on worker node localhost:57637
2023-11-25 04:50:28.854 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.854 UTC [1096741] DEBUG:  00000: task execution (4) for placement (823) on anchor shard (1400259) finished in 608 microseconds on worker node localhost:57638
2023-11-25 04:50:28.854 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.856 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 97 in 3269 microseconds
2023-11-25 04:50:28.856 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.856 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 99 in 3127 microseconds
2023-11-25 04:50:28.856 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.856 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 96: 2 to node localhost:57637
2023-11-25 04:50:28.856 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.856 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 97: 0 to node localhost:57637
2023-11-25 04:50:28.856 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.856 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 98: 2 to node localhost:57638
2023-11-25 04:50:28.856 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.856 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 99: 0 to node localhost:57638
2023-11-25 04:50:28.856 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.856 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.856 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.857 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.857 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.857 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.857 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.857 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.857 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.857 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.857 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.857 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum, worker_column_3, worker_column_2 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_2 AS worker_column_2, users_table.value_1 AS worker_column_3 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 2)
2023-11-25 04:50:28.857 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.857 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum, worker_column_3, worker_column_2 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_2 AS worker_column_2, users_table.value_1 AS worker_column_3 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 2)
2023-11-25 04:50:28.857 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.857 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum, worker_column_3, worker_column_2 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_2 AS worker_column_2, users_table.value_1 AS worker_column_3 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 2)
2023-11-25 04:50:28.857 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.857 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum, worker_column_3, worker_column_2 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_2 AS worker_column_2, users_table.value_1 AS worker_column_3 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 2)
2023-11-25 04:50:28.857 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.857 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.857 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.857 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.857 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.857 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.857 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.857 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.857 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.857 UTC [1096741] DEBUG:  00000: combine query: SELECT DISTINCT user_id, sum FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, sum bigint, worker_column_3 integer, worker_column_4 integer) ORDER BY sum DESC, user_id LIMIT '10'::bigint
2023-11-25 04:50:28.857 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.857 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.857 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.857 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 100 in 3764 microseconds
2023-11-25 04:50:28.857 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.857 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.857 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.857 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 102 in 3465 microseconds
2023-11-25 04:50:28.857 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.858 UTC [1096741] DEBUG:  00000: task execution (1) for placement (820) on anchor shard (1400256) finished in 644 microseconds on worker node localhost:57637
2023-11-25 04:50:28.858 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.858 UTC [1096741] DEBUG:  00000: task execution (2) for placement (821) on anchor shard (1400257) finished in 724 microseconds on worker node localhost:57638
2023-11-25 04:50:28.858 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.859 UTC [1096741] DEBUG:  00000: task execution (3) for placement (822) on anchor shard (1400258) finished in 569 microseconds on worker node localhost:57637
2023-11-25 04:50:28.859 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.859 UTC [1096741] DEBUG:  00000: task execution (4) for placement (823) on anchor shard (1400259) finished in 390 microseconds on worker node localhost:57638
2023-11-25 04:50:28.859 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.860 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 101 in 3173 microseconds
2023-11-25 04:50:28.860 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.863 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 103 in 5129 microseconds
2023-11-25 04:50:28.863 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.863 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 100: 2 to node localhost:57637
2023-11-25 04:50:28.863 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.863 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 101: 0 to node localhost:57637
2023-11-25 04:50:28.863 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.863 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 102: 2 to node localhost:57638
2023-11-25 04:50:28.863 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.863 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 103: 0 to node localhost:57638
2023-11-25 04:50:28.863 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.863 UTC [1096742] ERROR:  22004: the partition column of table insert_select_repartition.target_table should have a value
2023-11-25 04:50:28.863 UTC [1096742] LOCATION:  NonPushableInsertSelectExecScan, insert_select_executor.c:153
2023-11-25 04:50:28.863 UTC [1096742] STATEMENT:  INSERT INTO target_table(value) SELECT value FROM source_table;
2023-11-25 04:50:28.863 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.863 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.863 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.863 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.863 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.863 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.863 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.863 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.863 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.863 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.863 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum, worker_column_3, worker_column_2 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_2 AS worker_column_2, users_table.value_1 AS worker_column_3 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 2)
2023-11-25 04:50:28.863 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.863 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum, worker_column_3, worker_column_2 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_2 AS worker_column_2, users_table.value_1 AS worker_column_3 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 2)
2023-11-25 04:50:28.863 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.863 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum, worker_column_3, worker_column_2 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_2 AS worker_column_2, users_table.value_1 AS worker_column_3 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 2)
2023-11-25 04:50:28.863 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.863 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum, worker_column_3, worker_column_2 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_2 AS worker_column_2, users_table.value_1 AS worker_column_3 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 2)
2023-11-25 04:50:28.863 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.863 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.863 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.863 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.863 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.863 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.863 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.863 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.863 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.863 UTC [1096741] DEBUG:  00000: combine query: SELECT DISTINCT ON (user_id) user_id, sum FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, sum bigint, worker_column_3 integer, worker_column_4 integer) ORDER BY user_id, sum DESC LIMIT '10'::bigint
2023-11-25 04:50:28.863 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.864 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.864 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.864 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 104 in 3764 microseconds
2023-11-25 04:50:28.864 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.864 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.864 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.864 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 106 in 3465 microseconds
2023-11-25 04:50:28.864 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.864 UTC [1096741] DEBUG:  00000: task execution (1) for placement (820) on anchor shard (1400256) finished in 453 microseconds on worker node localhost:57637
2023-11-25 04:50:28.864 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.865 UTC [1096741] DEBUG:  00000: task execution (3) for placement (822) on anchor shard (1400258) finished in 499 microseconds on worker node localhost:57637
2023-11-25 04:50:28.865 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.865 UTC [1096741] DEBUG:  00000: task execution (2) for placement (821) on anchor shard (1400257) finished in 1341 microseconds on worker node localhost:57638
2023-11-25 04:50:28.865 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.866 UTC [1096741] DEBUG:  00000: task execution (4) for placement (823) on anchor shard (1400259) finished in 412 microseconds on worker node localhost:57638
2023-11-25 04:50:28.866 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.868 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 107 in 4176 microseconds
2023-11-25 04:50:28.868 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.869 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 105 in 5015 microseconds
2023-11-25 04:50:28.869 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.869 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 104: 2 to node localhost:57637
2023-11-25 04:50:28.869 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.869 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 105: 0 to node localhost:57637
2023-11-25 04:50:28.869 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.869 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 106: 2 to node localhost:57638
2023-11-25 04:50:28.869 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.869 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 107: 0 to node localhost:57638
2023-11-25 04:50:28.869 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.869 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.869 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.869 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.869 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.869 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.869 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.869 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.869 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.869 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.869 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.870 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum, sum(worker_column_3) OVER (PARTITION BY worker_column_1) AS worker_column_3, worker_column_3 AS worker_column_4, worker_column_2 AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_2 AS worker_column_2, users_table.value_1 AS worker_column_3 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 2)
2023-11-25 04:50:28.870 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.870 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum, sum(worker_column_3) OVER (PARTITION BY worker_column_1) AS worker_column_3, worker_column_3 AS worker_column_4, worker_column_2 AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_2 AS worker_column_2, users_table.value_1 AS worker_column_3 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 2)
2023-11-25 04:50:28.870 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.870 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum, sum(worker_column_3) OVER (PARTITION BY worker_column_1) AS worker_column_3, worker_column_3 AS worker_column_4, worker_column_2 AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_2 AS worker_column_2, users_table.value_1 AS worker_column_3 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 2)
2023-11-25 04:50:28.870 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.870 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum, sum(worker_column_3) OVER (PARTITION BY worker_column_1) AS worker_column_3, worker_column_3 AS worker_column_4, worker_column_2 AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_2 AS worker_column_2, users_table.value_1 AS worker_column_3 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 2)
2023-11-25 04:50:28.870 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.870 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.870 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.870 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.870 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.870 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.870 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.870 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.870 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.870 UTC [1096741] DEBUG:  00000: combine query: SELECT DISTINCT ON (worker_column_3) user_id, sum FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, sum bigint, worker_column_3 bigint, worker_column_4 integer, worker_column_5 integer) ORDER BY worker_column_3, sum DESC, user_id LIMIT '10'::bigint
2023-11-25 04:50:28.870 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.870 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.870 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.870 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 108 in 3764 microseconds
2023-11-25 04:50:28.870 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.870 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.870 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.870 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 110 in 3465 microseconds
2023-11-25 04:50:28.870 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.871 UTC [1096741] DEBUG:  00000: task execution (1) for placement (820) on anchor shard (1400256) finished in 508 microseconds on worker node localhost:57637
2023-11-25 04:50:28.871 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.871 UTC [1096741] DEBUG:  00000: task execution (2) for placement (821) on anchor shard (1400257) finished in 698 microseconds on worker node localhost:57638
2023-11-25 04:50:28.871 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.871 UTC [1096741] DEBUG:  00000: task execution (4) for placement (823) on anchor shard (1400259) finished in 415 microseconds on worker node localhost:57638
2023-11-25 04:50:28.871 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.872 UTC [1096741] DEBUG:  00000: task execution (3) for placement (822) on anchor shard (1400258) finished in 1319 microseconds on worker node localhost:57637
2023-11-25 04:50:28.872 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.873 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 109 in 3298 microseconds
2023-11-25 04:50:28.873 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.874 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 111 in 3965 microseconds
2023-11-25 04:50:28.874 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.874 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 108: 2 to node localhost:57637
2023-11-25 04:50:28.874 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.874 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 109: 0 to node localhost:57637
2023-11-25 04:50:28.874 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.874 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 110: 2 to node localhost:57638
2023-11-25 04:50:28.874 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.874 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 111: 0 to node localhost:57638
2023-11-25 04:50:28.874 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.875 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.875 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.875 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.875 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.875 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.875 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.875 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.875 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.875 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.875 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.875 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(avg(worker_column_2)) OVER (PARTITION BY worker_column_1, (max(worker_column_1)), (min(worker_column_3))) AS avg, avg(avg(worker_column_1)) OVER (PARTITION BY worker_column_1, (min(worker_column_1)), (avg(worker_column_2))) AS avg, max(worker_column_1) AS worker_column_4, min(worker_column_3) AS worker_column_5, min(worker_column_1) AS worker_column_6, avg(worker_column_2) AS worker_column_7 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1
2023-11-25 04:50:28.875 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.875 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(avg(worker_column_2)) OVER (PARTITION BY worker_column_1, (max(worker_column_1)), (min(worker_column_3))) AS avg, avg(avg(worker_column_1)) OVER (PARTITION BY worker_column_1, (min(worker_column_1)), (avg(worker_column_2))) AS avg, max(worker_column_1) AS worker_column_4, min(worker_column_3) AS worker_column_5, min(worker_column_1) AS worker_column_6, avg(worker_column_2) AS worker_column_7 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1
2023-11-25 04:50:28.875 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.875 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(avg(worker_column_2)) OVER (PARTITION BY worker_column_1, (max(worker_column_1)), (min(worker_column_3))) AS avg, avg(avg(worker_column_1)) OVER (PARTITION BY worker_column_1, (min(worker_column_1)), (avg(worker_column_2))) AS avg, max(worker_column_1) AS worker_column_4, min(worker_column_3) AS worker_column_5, min(worker_column_1) AS worker_column_6, avg(worker_column_2) AS worker_column_7 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1
2023-11-25 04:50:28.875 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.875 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(avg(worker_column_2)) OVER (PARTITION BY worker_column_1, (max(worker_column_1)), (min(worker_column_3))) AS avg, avg(avg(worker_column_1)) OVER (PARTITION BY worker_column_1, (min(worker_column_1)), (avg(worker_column_2))) AS avg, max(worker_column_1) AS worker_column_4, min(worker_column_3) AS worker_column_5, min(worker_column_1) AS worker_column_6, avg(worker_column_2) AS worker_column_7 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1
2023-11-25 04:50:28.875 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.875 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.875 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.875 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.875 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.875 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.875 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.875 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.875 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.875 UTC [1096741] DEBUG:  00000: combine query: SELECT user_id, avg, avg_1 AS avg FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, avg numeric, avg_1 numeric, worker_column_4 integer, worker_column_5 integer, worker_column_6 integer, worker_column_7 numeric) ORDER BY avg_1 DESC, avg DESC, user_id DESC
2023-11-25 04:50:28.875 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.875 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.875 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.875 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 112 in 3764 microseconds
2023-11-25 04:50:28.875 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.876 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.876 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.876 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 114 in 3465 microseconds
2023-11-25 04:50:28.876 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.877 UTC [1096741] DEBUG:  00000: task execution (2) for placement (821) on anchor shard (1400257) finished in 1027 microseconds on worker node localhost:57638
2023-11-25 04:50:28.877 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.877 UTC [1096741] DEBUG:  00000: task execution (4) for placement (823) on anchor shard (1400259) finished in 793 microseconds on worker node localhost:57638
2023-11-25 04:50:28.877 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.878 UTC [1096741] DEBUG:  00000: task execution (1) for placement (820) on anchor shard (1400256) finished in 2533 microseconds on worker node localhost:57637
2023-11-25 04:50:28.878 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.880 UTC [1096741] DEBUG:  00000: task execution (3) for placement (822) on anchor shard (1400258) finished in 1604 microseconds on worker node localhost:57637
2023-11-25 04:50:28.880 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.880 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 115 in 4315 microseconds
2023-11-25 04:50:28.880 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.882 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 113 in 6434 microseconds
2023-11-25 04:50:28.882 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.882 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 112: 2 to node localhost:57637
2023-11-25 04:50:28.882 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.882 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 113: 0 to node localhost:57637
2023-11-25 04:50:28.882 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.882 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 114: 2 to node localhost:57638
2023-11-25 04:50:28.882 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.882 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 115: 0 to node localhost:57638
2023-11-25 04:50:28.882 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.883 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.883 UTC [1096741] CONTEXT:  PL/pgSQL function coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 04:50:28.883 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.883 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.883 UTC [1096741] CONTEXT:  PL/pgSQL function coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 04:50:28.883 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.883 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.883 UTC [1096741] CONTEXT:  PL/pgSQL function coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 04:50:28.883 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.883 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.883 UTC [1096741] CONTEXT:  PL/pgSQL function coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 04:50:28.883 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.883 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.883 UTC [1096741] CONTEXT:  PL/pgSQL function coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 04:50:28.883 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.883 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(avg(worker_column_2)) OVER (PARTITION BY worker_column_1, (max(worker_column_1)), (min(worker_column_3))) AS avg, avg(avg(worker_column_1)) OVER (PARTITION BY worker_column_1, (min(worker_column_1)), (avg(worker_column_2))) AS avg, max(worker_column_1) AS worker_column_4, min(worker_column_3) AS worker_column_5, min(worker_column_1) AS worker_column_6, avg(worker_column_2) AS worker_column_7 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1
2023-11-25 04:50:28.883 UTC [1096741] CONTEXT:  PL/pgSQL function coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 04:50:28.883 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.883 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(avg(worker_column_2)) OVER (PARTITION BY worker_column_1, (max(worker_column_1)), (min(worker_column_3))) AS avg, avg(avg(worker_column_1)) OVER (PARTITION BY worker_column_1, (min(worker_column_1)), (avg(worker_column_2))) AS avg, max(worker_column_1) AS worker_column_4, min(worker_column_3) AS worker_column_5, min(worker_column_1) AS worker_column_6, avg(worker_column_2) AS worker_column_7 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1
2023-11-25 04:50:28.883 UTC [1096741] CONTEXT:  PL/pgSQL function coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 04:50:28.883 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.883 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(avg(worker_column_2)) OVER (PARTITION BY worker_column_1, (max(worker_column_1)), (min(worker_column_3))) AS avg, avg(avg(worker_column_1)) OVER (PARTITION BY worker_column_1, (min(worker_column_1)), (avg(worker_column_2))) AS avg, max(worker_column_1) AS worker_column_4, min(worker_column_3) AS worker_column_5, min(worker_column_1) AS worker_column_6, avg(worker_column_2) AS worker_column_7 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1
2023-11-25 04:50:28.883 UTC [1096741] CONTEXT:  PL/pgSQL function coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 04:50:28.883 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.883 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(avg(worker_column_2)) OVER (PARTITION BY worker_column_1, (max(worker_column_1)), (min(worker_column_3))) AS avg, avg(avg(worker_column_1)) OVER (PARTITION BY worker_column_1, (min(worker_column_1)), (avg(worker_column_2))) AS avg, max(worker_column_1) AS worker_column_4, min(worker_column_3) AS worker_column_5, min(worker_column_1) AS worker_column_6, avg(worker_column_2) AS worker_column_7 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1
2023-11-25 04:50:28.883 UTC [1096741] CONTEXT:  PL/pgSQL function coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 04:50:28.883 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.883 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.883 UTC [1096741] CONTEXT:  PL/pgSQL function coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 04:50:28.883 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.883 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.883 UTC [1096741] CONTEXT:  PL/pgSQL function coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 04:50:28.883 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.883 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.883 UTC [1096741] CONTEXT:  PL/pgSQL function coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 04:50:28.883 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.883 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.883 UTC [1096741] CONTEXT:  PL/pgSQL function coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 04:50:28.883 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.883 UTC [1096741] DEBUG:  00000: combine query: SELECT user_id, avg, avg_1 AS avg FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, avg numeric, avg_1 numeric, worker_column_4 integer, worker_column_5 integer, worker_column_6 integer, worker_column_7 numeric) ORDER BY avg_1 DESC, avg DESC, user_id DESC
2023-11-25 04:50:28.883 UTC [1096741] CONTEXT:  PL/pgSQL function coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 04:50:28.883 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.885 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.885 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.885 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.885 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.885 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.885 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.885 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.885 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.885 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.885 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.885 UTC [1096741] DEBUG:  00000: generated sql query for task 1
2023-11-25 04:50:28.885 UTC [1096741] DETAIL:  query string: "SELECT value_2, sum(value_1) AS avg, count(value_1) AS avg, sum(value_2) AS avg, count(value_2) AS avg, max(value_2) AS worker_column_6, min(value_2) AS worker_column_7, sum(value_1) AS worker_column_8, count(value_1) AS worker_column_9 FROM public.users_table_1400256 users_table WHERE true GROUP BY value_2"
2023-11-25 04:50:28.885 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.885 UTC [1096741] DEBUG:  00000: generated sql query for task 2
2023-11-25 04:50:28.885 UTC [1096741] DETAIL:  query string: "SELECT value_2, sum(value_1) AS avg, count(value_1) AS avg, sum(value_2) AS avg, count(value_2) AS avg, max(value_2) AS worker_column_6, min(value_2) AS worker_column_7, sum(value_1) AS worker_column_8, count(value_1) AS worker_column_9 FROM public.users_table_1400257 users_table WHERE true GROUP BY value_2"
2023-11-25 04:50:28.885 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.885 UTC [1096741] DEBUG:  00000: generated sql query for task 3
2023-11-25 04:50:28.885 UTC [1096741] DETAIL:  query string: "SELECT value_2, sum(value_1) AS avg, count(value_1) AS avg, sum(value_2) AS avg, count(value_2) AS avg, max(value_2) AS worker_column_6, min(value_2) AS worker_column_7, sum(value_1) AS worker_column_8, count(value_1) AS worker_column_9 FROM public.users_table_1400258 users_table WHERE true GROUP BY value_2"
2023-11-25 04:50:28.885 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.885 UTC [1096741] DEBUG:  00000: generated sql query for task 4
2023-11-25 04:50:28.885 UTC [1096741] DETAIL:  query string: "SELECT value_2, sum(value_1) AS avg, count(value_1) AS avg, sum(value_2) AS avg, count(value_2) AS avg, max(value_2) AS worker_column_6, min(value_2) AS worker_column_7, sum(value_1) AS worker_column_8, count(value_1) AS worker_column_9 FROM public.users_table_1400259 users_table WHERE true GROUP BY value_2"
2023-11-25 04:50:28.885 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.885 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.885 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.885 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.885 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.885 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.885 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.885 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.885 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.886 UTC [1096741] DEBUG:  00000: combine query: SELECT value_2, avg((pg_catalog.sum(avg) OPERATOR(pg_catalog./) pg_catalog.sum(avg_1))) OVER (PARTITION BY value_2, (max(worker_column_6)), (min(worker_column_7))) AS avg, avg((pg_catalog.sum(avg_2) OPERATOR(pg_catalog./) pg_catalog.sum(avg_3))) OVER (PARTITION BY value_2, (min(worker_column_7)), (pg_catalog.sum(worker_column_8) OPERATOR(pg_catalog./) pg_catalog.sum(worker_column_9))) AS avg FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), '(i 1)'::cstring(0)) remote_scan(value_2 integer, avg bigint, avg_1 bigint, avg_2 bigint, avg_3 bigint, worker_column_6 integer, worker_column_7 integer, worker_column_8 bigint, worker_column_9 bigint) GROUP BY value_2 ORDER BY (avg((pg_catalog.sum(avg_2) OPERATOR(pg_catalog./) pg_catalog.sum(avg_3))) OVER (PARTITION BY value_2, (min(worker_column_7)), (pg_catalog.sum(worker_column_8) OPERATOR(pg_catalog./) pg_catalog.sum(worker_column_9)))) DESC, (avg((pg_catalog.sum(avg) OPERATOR(pg_catalog./) pg_catalog.sum(avg_1))) OVER (PARTITION BY value_2, (max(worker_column_6)), (min(worker_column_7)))) DESC, value_2 DESC
2023-11-25 04:50:28.886 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.886 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.886 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.886 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 116 in 3764 microseconds
2023-11-25 04:50:28.886 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.886 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.886 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.886 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 118 in 3465 microseconds
2023-11-25 04:50:28.886 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.887 UTC [1096741] DEBUG:  00000: task execution (1) for placement (820) on anchor shard (1400256) finished in 790 microseconds on worker node localhost:57637
2023-11-25 04:50:28.887 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.887 UTC [1096741] DEBUG:  00000: task execution (2) for placement (821) on anchor shard (1400257) finished in 687 microseconds on worker node localhost:57638
2023-11-25 04:50:28.887 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.887 UTC [1096741] DEBUG:  00000: task execution (3) for placement (822) on anchor shard (1400258) finished in 407 microseconds on worker node localhost:57637
2023-11-25 04:50:28.887 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.887 UTC [1096741] DEBUG:  00000: task execution (4) for placement (823) on anchor shard (1400259) finished in 400 microseconds on worker node localhost:57638
2023-11-25 04:50:28.887 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.890 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 119 in 3786 microseconds
2023-11-25 04:50:28.890 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.890 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 117 in 4295 microseconds
2023-11-25 04:50:28.890 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.890 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 116: 2 to node localhost:57637
2023-11-25 04:50:28.890 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.890 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 117: 0 to node localhost:57637
2023-11-25 04:50:28.890 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.890 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 118: 2 to node localhost:57638
2023-11-25 04:50:28.890 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.890 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 119: 0 to node localhost:57638
2023-11-25 04:50:28.890 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.891 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.891 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.891 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.891 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.891 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.891 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.891 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.891 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.891 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.891 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.891 UTC [1096741] DEBUG:  00000: generated sql query for task 1
2023-11-25 04:50:28.891 UTC [1096741] DETAIL:  query string: "SELECT value_2, user_id, avg(value_1) AS avg, avg(value_2) AS avg, max(value_2) AS worker_column_5, min(value_2) AS worker_column_6, avg(value_1) AS worker_column_7 FROM public.users_table_1400256 users_table WHERE true GROUP BY value_2, user_id"
2023-11-25 04:50:28.891 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.891 UTC [1096741] DEBUG:  00000: generated sql query for task 2
2023-11-25 04:50:28.891 UTC [1096741] DETAIL:  query string: "SELECT value_2, user_id, avg(value_1) AS avg, avg(value_2) AS avg, max(value_2) AS worker_column_5, min(value_2) AS worker_column_6, avg(value_1) AS worker_column_7 FROM public.users_table_1400257 users_table WHERE true GROUP BY value_2, user_id"
2023-11-25 04:50:28.891 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.891 UTC [1096741] DEBUG:  00000: generated sql query for task 3
2023-11-25 04:50:28.891 UTC [1096741] DETAIL:  query string: "SELECT value_2, user_id, avg(value_1) AS avg, avg(value_2) AS avg, max(value_2) AS worker_column_5, min(value_2) AS worker_column_6, avg(value_1) AS worker_column_7 FROM public.users_table_1400258 users_table WHERE true GROUP BY value_2, user_id"
2023-11-25 04:50:28.891 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.891 UTC [1096741] DEBUG:  00000: generated sql query for task 4
2023-11-25 04:50:28.891 UTC [1096741] DETAIL:  query string: "SELECT value_2, user_id, avg(value_1) AS avg, avg(value_2) AS avg, max(value_2) AS worker_column_5, min(value_2) AS worker_column_6, avg(value_1) AS worker_column_7 FROM public.users_table_1400259 users_table WHERE true GROUP BY value_2, user_id"
2023-11-25 04:50:28.891 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.891 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.891 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.891 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.891 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.891 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.891 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.891 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.891 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.891 UTC [1096741] DEBUG:  00000: combine query: SELECT value_2, user_id, avg(avg) OVER (PARTITION BY value_2, worker_column_5, worker_column_6) AS avg, avg(avg_1) OVER (PARTITION BY user_id, worker_column_6, worker_column_7) AS avg FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), '(i 1)'::cstring(0)) remote_scan(value_2 integer, user_id integer, avg numeric, avg_1 numeric, worker_column_5 integer, worker_column_6 integer, worker_column_7 numeric) ORDER BY (avg(avg) OVER (PARTITION BY value_2, worker_column_5, worker_column_6)) DESC, user_id DESC, value_2 DESC
2023-11-25 04:50:28.891 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.891 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.891 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.892 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 120 in 3764 microseconds
2023-11-25 04:50:28.892 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.892 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.892 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.892 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 122 in 3465 microseconds
2023-11-25 04:50:28.892 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.892 UTC [1096741] DEBUG:  00000: task execution (1) for placement (820) on anchor shard (1400256) finished in 427 microseconds on worker node localhost:57637
2023-11-25 04:50:28.892 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.892 UTC [1096741] DEBUG:  00000: task execution (2) for placement (821) on anchor shard (1400257) finished in 525 microseconds on worker node localhost:57638
2023-11-25 04:50:28.892 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.892 UTC [1096741] DEBUG:  00000: task execution (3) for placement (822) on anchor shard (1400258) finished in 348 microseconds on worker node localhost:57637
2023-11-25 04:50:28.892 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.893 UTC [1096741] DEBUG:  00000: task execution (4) for placement (823) on anchor shard (1400259) finished in 360 microseconds on worker node localhost:57638
2023-11-25 04:50:28.893 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.895 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 123 in 3608 microseconds
2023-11-25 04:50:28.895 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.895 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 121 in 3808 microseconds
2023-11-25 04:50:28.895 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.895 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 120: 2 to node localhost:57637
2023-11-25 04:50:28.895 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.895 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 121: 0 to node localhost:57637
2023-11-25 04:50:28.895 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.895 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 122: 2 to node localhost:57638
2023-11-25 04:50:28.895 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.895 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 123: 0 to node localhost:57638
2023-11-25 04:50:28.895 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.897 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.897 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.897 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.897 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.897 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.897 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.897 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.897 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.897 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.897 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.897 UTC [1096741] DEBUG:  00000: generated sql query for task 1
2023-11-25 04:50:28.897 UTC [1096741] DETAIL:  query string: "SELECT user_id, avg(user_id) AS sum FROM public.users_table_1400256 users_table WHERE true GROUP BY user_id"
2023-11-25 04:50:28.897 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.897 UTC [1096741] DEBUG:  00000: generated sql query for task 2
2023-11-25 04:50:28.897 UTC [1096741] DETAIL:  query string: "SELECT user_id, avg(user_id) AS sum FROM public.users_table_1400257 users_table WHERE true GROUP BY user_id"
2023-11-25 04:50:28.897 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.897 UTC [1096741] DEBUG:  00000: generated sql query for task 3
2023-11-25 04:50:28.897 UTC [1096741] DETAIL:  query string: "SELECT user_id, avg(user_id) AS sum FROM public.users_table_1400258 users_table WHERE true GROUP BY user_id"
2023-11-25 04:50:28.897 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.897 UTC [1096741] DEBUG:  00000: generated sql query for task 4
2023-11-25 04:50:28.897 UTC [1096741] DETAIL:  query string: "SELECT user_id, avg(user_id) AS sum FROM public.users_table_1400259 users_table WHERE true GROUP BY user_id"
2023-11-25 04:50:28.897 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.897 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.897 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.897 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.897 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.897 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.897 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.897 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.897 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.897 UTC [1096741] DEBUG:  00000: combine query: SELECT user_id, sum(sum) OVER () AS sum FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), '(i 1)'::cstring(0)) remote_scan(user_id integer, sum numeric) ORDER BY user_id LIMIT '10'::bigint
2023-11-25 04:50:28.897 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.897 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.897 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.897 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 124 in 3764 microseconds
2023-11-25 04:50:28.897 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.897 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.897 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.898 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 126 in 3465 microseconds
2023-11-25 04:50:28.898 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.898 UTC [1096741] DEBUG:  00000: task execution (2) for placement (821) on anchor shard (1400257) finished in 798 microseconds on worker node localhost:57638
2023-11-25 04:50:28.898 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.898 UTC [1096741] DEBUG:  00000: task execution (1) for placement (820) on anchor shard (1400256) finished in 1032 microseconds on worker node localhost:57637
2023-11-25 04:50:28.898 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.899 UTC [1096741] DEBUG:  00000: task execution (3) for placement (822) on anchor shard (1400258) finished in 238 microseconds on worker node localhost:57637
2023-11-25 04:50:28.899 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.900 UTC [1096741] DEBUG:  00000: task execution (4) for placement (823) on anchor shard (1400259) finished in 1964 microseconds on worker node localhost:57638
2023-11-25 04:50:28.900 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.901 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 127 in 3686 microseconds
2023-11-25 04:50:28.901 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.902 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 125 in 5054 microseconds
2023-11-25 04:50:28.902 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.902 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 124: 2 to node localhost:57637
2023-11-25 04:50:28.902 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.902 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 125: 0 to node localhost:57637
2023-11-25 04:50:28.902 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.902 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 126: 2 to node localhost:57638
2023-11-25 04:50:28.902 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.902 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 127: 0 to node localhost:57638
2023-11-25 04:50:28.902 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.903 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.903 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.903 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.903 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.903 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.903 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.903 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.903 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.903 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.903 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.903 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, (1 OPERATOR(pg_catalog.+) sum(worker_column_2)), ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_3) OVER (PARTITION BY worker_column_1)), worker_column_3 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 04:50:28.903 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.903 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, (1 OPERATOR(pg_catalog.+) sum(worker_column_2)), ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_3) OVER (PARTITION BY worker_column_1)), worker_column_3 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 04:50:28.903 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.903 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, (1 OPERATOR(pg_catalog.+) sum(worker_column_2)), ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_3) OVER (PARTITION BY worker_column_1)), worker_column_3 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 04:50:28.903 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.903 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, (1 OPERATOR(pg_catalog.+) sum(worker_column_2)), ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_3) OVER (PARTITION BY worker_column_1)), worker_column_3 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 04:50:28.903 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.903 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.903 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.903 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.903 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.903 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.903 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.903 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.903 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.903 UTC [1096741] DEBUG:  00000: combine query: SELECT user_id, "?column?", "?column?_1" AS "?column?" FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, "?column?" bigint, "?column?_1" numeric, worker_column_4 integer) ORDER BY user_id, worker_column_4
2023-11-25 04:50:28.903 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.903 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.903 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.903 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 128 in 3764 microseconds
2023-11-25 04:50:28.903 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.904 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.904 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.904 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 130 in 3465 microseconds
2023-11-25 04:50:28.904 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.904 UTC [1096741] DEBUG:  00000: task execution (1) for placement (820) on anchor shard (1400256) finished in 489 microseconds on worker node localhost:57637
2023-11-25 04:50:28.904 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.904 UTC [1096741] DEBUG:  00000: task execution (3) for placement (822) on anchor shard (1400258) finished in 356 microseconds on worker node localhost:57637
2023-11-25 04:50:28.904 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.904 UTC [1096741] DEBUG:  00000: task execution (2) for placement (821) on anchor shard (1400257) finished in 841 microseconds on worker node localhost:57638
2023-11-25 04:50:28.904 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.905 UTC [1096741] DEBUG:  00000: task execution (4) for placement (823) on anchor shard (1400259) finished in 375 microseconds on worker node localhost:57638
2023-11-25 04:50:28.905 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.907 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 129 in 3686 microseconds
2023-11-25 04:50:28.907 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.907 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 131 in 3873 microseconds
2023-11-25 04:50:28.907 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.907 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 128: 2 to node localhost:57637
2023-11-25 04:50:28.907 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.908 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 129: 0 to node localhost:57637
2023-11-25 04:50:28.908 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.908 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 130: 2 to node localhost:57638
2023-11-25 04:50:28.908 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.908 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 131: 0 to node localhost:57638
2023-11-25 04:50:28.908 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.908 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.908 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.908 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.908 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.908 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.908 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.908 UTC [1096741] DEBUG:  00000: push down of limit count: 5
2023-11-25 04:50:28.908 UTC [1096741] LOCATION:  WorkerLimitCount, multi_logical_optimizer.c:4778
2023-11-25 04:50:28.908 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.908 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.908 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.908 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.908 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, (1 OPERATOR(pg_catalog.+) sum(worker_column_2)), ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_3) OVER (PARTITION BY worker_column_1)), worker_column_3 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY (1 OPERATOR(pg_catalog.+) sum(worker_column_2)) DESC, worker_column_1 LIMIT '5'::bigint
2023-11-25 04:50:28.908 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.909 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, (1 OPERATOR(pg_catalog.+) sum(worker_column_2)), ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_3) OVER (PARTITION BY worker_column_1)), worker_column_3 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY (1 OPERATOR(pg_catalog.+) sum(worker_column_2)) DESC, worker_column_1 LIMIT '5'::bigint
2023-11-25 04:50:28.909 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.909 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, (1 OPERATOR(pg_catalog.+) sum(worker_column_2)), ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_3) OVER (PARTITION BY worker_column_1)), worker_column_3 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY (1 OPERATOR(pg_catalog.+) sum(worker_column_2)) DESC, worker_column_1 LIMIT '5'::bigint
2023-11-25 04:50:28.909 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.909 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, (1 OPERATOR(pg_catalog.+) sum(worker_column_2)), ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_3) OVER (PARTITION BY worker_column_1)), worker_column_3 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY (1 OPERATOR(pg_catalog.+) sum(worker_column_2)) DESC, worker_column_1 LIMIT '5'::bigint
2023-11-25 04:50:28.909 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.909 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.909 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.909 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.909 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.909 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.909 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.909 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.909 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.909 UTC [1096741] DEBUG:  00000: combine query: SELECT user_id, "?column?", "?column?_1" AS "?column?" FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, "?column?" bigint, "?column?_1" numeric, worker_column_4 integer) ORDER BY "?column?" DESC, user_id LIMIT '5'::bigint
2023-11-25 04:50:28.909 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.909 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.909 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.909 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 132 in 3764 microseconds
2023-11-25 04:50:28.909 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.909 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.909 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.909 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 134 in 3465 microseconds
2023-11-25 04:50:28.909 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.910 UTC [1096741] DEBUG:  00000: task execution (1) for placement (820) on anchor shard (1400256) finished in 963 microseconds on worker node localhost:57637
2023-11-25 04:50:28.910 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.910 UTC [1096741] DEBUG:  00000: task execution (2) for placement (821) on anchor shard (1400257) finished in 852 microseconds on worker node localhost:57638
2023-11-25 04:50:28.910 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.910 UTC [1096741] DEBUG:  00000: task execution (4) for placement (823) on anchor shard (1400259) finished in 418 microseconds on worker node localhost:57638
2023-11-25 04:50:28.910 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.911 UTC [1096741] DEBUG:  00000: task execution (3) for placement (822) on anchor shard (1400258) finished in 647 microseconds on worker node localhost:57637
2023-11-25 04:50:28.911 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.915 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 133 in 5578 microseconds
2023-11-25 04:50:28.915 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.919 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 135 in 9601 microseconds
2023-11-25 04:50:28.919 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.919 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 132: 2 to node localhost:57637
2023-11-25 04:50:28.919 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.919 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 133: 0 to node localhost:57637
2023-11-25 04:50:28.919 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.919 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 134: 2 to node localhost:57638
2023-11-25 04:50:28.919 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.919 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 135: 0 to node localhost:57638
2023-11-25 04:50:28.919 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.919 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.919 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.919 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.919 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.919 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.919 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.919 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.919 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.919 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.919 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.920 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY worker_column_3) AS rank, worker_column_3 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 04:50:28.920 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.920 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY worker_column_3) AS rank, worker_column_3 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 04:50:28.920 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.920 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY worker_column_3) AS rank, worker_column_3 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 04:50:28.920 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.920 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY worker_column_3) AS rank, worker_column_3 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 04:50:28.920 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.920 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.920 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.920 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.920 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.920 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.920 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.920 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.920 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.920 UTC [1096741] DEBUG:  00000: combine query: SELECT user_id, avg, rank FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, avg numeric, rank bigint, worker_column_4 integer) ORDER BY user_id, worker_column_4 DESC
2023-11-25 04:50:28.920 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.920 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.920 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.920 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 136 in 3764 microseconds
2023-11-25 04:50:28.920 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.920 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.920 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.920 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 138 in 3465 microseconds
2023-11-25 04:50:28.920 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.921 UTC [1096741] DEBUG:  00000: task execution (1) for placement (820) on anchor shard (1400256) finished in 473 microseconds on worker node localhost:57637
2023-11-25 04:50:28.921 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.921 UTC [1096741] DEBUG:  00000: task execution (2) for placement (821) on anchor shard (1400257) finished in 436 microseconds on worker node localhost:57638
2023-11-25 04:50:28.921 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.921 UTC [1096741] DEBUG:  00000: task execution (3) for placement (822) on anchor shard (1400258) finished in 408 microseconds on worker node localhost:57637
2023-11-25 04:50:28.921 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.921 UTC [1096741] DEBUG:  00000: task execution (4) for placement (823) on anchor shard (1400259) finished in 322 microseconds on worker node localhost:57638
2023-11-25 04:50:28.921 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.925 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 137 in 4601 microseconds
2023-11-25 04:50:28.925 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.926 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 139 in 5909 microseconds
2023-11-25 04:50:28.926 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.926 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 136: 2 to node localhost:57637
2023-11-25 04:50:28.926 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.926 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 137: 0 to node localhost:57637
2023-11-25 04:50:28.926 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.926 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 138: 2 to node localhost:57638
2023-11-25 04:50:28.926 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.926 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 139: 0 to node localhost:57638
2023-11-25 04:50:28.926 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.927 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.927 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.927 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.927 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.927 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.927 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.927 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.927 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.927 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.927 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.927 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 04:50:28.927 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.927 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 04:50:28.927 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.927 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 04:50:28.927 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.927 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 04:50:28.927 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.927 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.927 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.927 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.927 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.927 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.927 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.927 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.927 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.927 UTC [1096741] DEBUG:  00000: combine query: SELECT user_id, avg, rank FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, avg numeric, rank bigint, worker_column_4 integer, worker_column_5 numeric) ORDER BY user_id, avg DESC
2023-11-25 04:50:28.927 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.927 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.927 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.928 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 140 in 3764 microseconds
2023-11-25 04:50:28.928 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.928 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.928 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.928 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 142 in 3465 microseconds
2023-11-25 04:50:28.928 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.928 UTC [1096741] DEBUG:  00000: task execution (1) for placement (820) on anchor shard (1400256) finished in 720 microseconds on worker node localhost:57637
2023-11-25 04:50:28.928 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.928 UTC [1096741] DEBUG:  00000: task execution (2) for placement (821) on anchor shard (1400257) finished in 610 microseconds on worker node localhost:57638
2023-11-25 04:50:28.928 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.929 UTC [1096741] DEBUG:  00000: task execution (3) for placement (822) on anchor shard (1400258) finished in 491 microseconds on worker node localhost:57637
2023-11-25 04:50:28.929 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.929 UTC [1096741] DEBUG:  00000: task execution (4) for placement (823) on anchor shard (1400259) finished in 404 microseconds on worker node localhost:57638
2023-11-25 04:50:28.929 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.932 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 143 in 4516 microseconds
2023-11-25 04:50:28.932 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.936 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 141 in 8162 microseconds
2023-11-25 04:50:28.936 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.936 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 140: 2 to node localhost:57637
2023-11-25 04:50:28.936 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.936 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 141: 0 to node localhost:57637
2023-11-25 04:50:28.936 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.936 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 142: 2 to node localhost:57638
2023-11-25 04:50:28.936 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.936 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 143: 0 to node localhost:57638
2023-11-25 04:50:28.936 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.937 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.937 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.937 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.937 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.937 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.937 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.937 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.937 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.937 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.937 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.937 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 04:50:28.937 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.937 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 04:50:28.937 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.937 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 04:50:28.937 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.937 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 04:50:28.937 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.937 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.937 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.937 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.937 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.937 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.937 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.937 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.937 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.937 UTC [1096741] DEBUG:  00000: combine query: SELECT user_id, avg, rank FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, avg numeric, rank bigint, worker_column_4 integer, worker_column_5 numeric) ORDER BY user_id, avg DESC
2023-11-25 04:50:28.937 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.940 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.940 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.940 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.940 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.940 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.940 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.940 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.940 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.940 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.940 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.940 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 04:50:28.940 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.940 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 04:50:28.940 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.940 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 04:50:28.940 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.940 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 04:50:28.940 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.940 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.940 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.940 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.940 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.940 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.940 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.940 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.940 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.940 UTC [1096741] DEBUG:  00000: combine query: SELECT user_id, avg, rank FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, avg numeric, rank bigint, worker_column_4 integer, worker_column_5 numeric) ORDER BY user_id, avg DESC
2023-11-25 04:50:28.940 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.940 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.940 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.940 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 144 in 3764 microseconds
2023-11-25 04:50:28.940 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.940 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.940 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.940 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 146 in 3465 microseconds
2023-11-25 04:50:28.940 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.941 UTC [1096741] DEBUG:  00000: task execution (1) for placement (820) on anchor shard (1400256) finished in 747 microseconds on worker node localhost:57637
2023-11-25 04:50:28.941 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.942 UTC [1096741] DEBUG:  00000: task execution (2) for placement (821) on anchor shard (1400257) finished in 1159 microseconds on worker node localhost:57638
2023-11-25 04:50:28.942 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.942 UTC [1096741] DEBUG:  00000: task execution (3) for placement (822) on anchor shard (1400258) finished in 608 microseconds on worker node localhost:57637
2023-11-25 04:50:28.942 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.942 UTC [1096741] DEBUG:  00000: task execution (4) for placement (823) on anchor shard (1400259) finished in 534 microseconds on worker node localhost:57638
2023-11-25 04:50:28.942 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.944 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 145 in 4154 microseconds
2023-11-25 04:50:28.944 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.945 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 147 in 4864 microseconds
2023-11-25 04:50:28.945 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.945 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 144: 2 to node localhost:57637
2023-11-25 04:50:28.945 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.945 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 145: 0 to node localhost:57637
2023-11-25 04:50:28.945 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.945 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 146: 2 to node localhost:57638
2023-11-25 04:50:28.945 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.945 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 147: 0 to node localhost:57638
2023-11-25 04:50:28.945 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.946 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.946 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.946 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.946 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.946 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.946 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.946 UTC [1096741] DEBUG:  00000: push down of limit count: 5
2023-11-25 04:50:28.946 UTC [1096741] LOCATION:  WorkerLimitCount, multi_logical_optimizer.c:4778
2023-11-25 04:50:28.946 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.946 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.946 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.946 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.946 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY worker_column_1, (avg(worker_column_2)) DESC LIMIT '5'::bigint
2023-11-25 04:50:28.946 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.946 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY worker_column_1, (avg(worker_column_2)) DESC LIMIT '5'::bigint
2023-11-25 04:50:28.946 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.946 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY worker_column_1, (avg(worker_column_2)) DESC LIMIT '5'::bigint
2023-11-25 04:50:28.946 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.947 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY worker_column_1, (avg(worker_column_2)) DESC LIMIT '5'::bigint
2023-11-25 04:50:28.947 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.947 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.947 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.947 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.947 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.947 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.947 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.947 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.947 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.947 UTC [1096741] DEBUG:  00000: combine query: SELECT user_id, avg, rank FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, avg numeric, rank bigint, worker_column_4 integer, worker_column_5 numeric) ORDER BY user_id, avg DESC LIMIT '5'::bigint
2023-11-25 04:50:28.947 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.948 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.948 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.948 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.948 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.948 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.948 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.948 UTC [1096741] DEBUG:  00000: push down of limit count: 5
2023-11-25 04:50:28.948 UTC [1096741] LOCATION:  WorkerLimitCount, multi_logical_optimizer.c:4778
2023-11-25 04:50:28.948 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.948 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.948 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.948 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.948 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY worker_column_1, (avg(worker_column_2)) DESC LIMIT '5'::bigint
2023-11-25 04:50:28.948 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.948 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY worker_column_1, (avg(worker_column_2)) DESC LIMIT '5'::bigint
2023-11-25 04:50:28.948 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.948 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY worker_column_1, (avg(worker_column_2)) DESC LIMIT '5'::bigint
2023-11-25 04:50:28.948 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.948 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY worker_column_1, (avg(worker_column_2)) DESC LIMIT '5'::bigint
2023-11-25 04:50:28.948 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.948 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.948 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.948 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.948 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.948 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.948 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.948 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.948 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.948 UTC [1096741] DEBUG:  00000: combine query: SELECT user_id, avg, rank FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, avg numeric, rank bigint, worker_column_4 integer, worker_column_5 numeric) ORDER BY user_id, avg DESC LIMIT '5'::bigint
2023-11-25 04:50:28.948 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.950 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.950 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.950 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.950 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.950 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.950 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.950 UTC [1096741] DEBUG:  00000: push down of limit count: 5
2023-11-25 04:50:28.950 UTC [1096741] LOCATION:  WorkerLimitCount, multi_logical_optimizer.c:4778
2023-11-25 04:50:28.950 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.950 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.950 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.950 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.950 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY (1 OPERATOR(pg_catalog./) (1 OPERATOR(pg_catalog.+) sum(worker_column_3)))) AS rank, worker_column_3 AS worker_column_4, (1 OPERATOR(pg_catalog./) (1 OPERATOR(pg_catalog.+) sum(worker_column_3))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY worker_column_1, (avg(worker_column_2)) DESC LIMIT '5'::bigint
2023-11-25 04:50:28.950 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.950 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY (1 OPERATOR(pg_catalog./) (1 OPERATOR(pg_catalog.+) sum(worker_column_3)))) AS rank, worker_column_3 AS worker_column_4, (1 OPERATOR(pg_catalog./) (1 OPERATOR(pg_catalog.+) sum(worker_column_3))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY worker_column_1, (avg(worker_column_2)) DESC LIMIT '5'::bigint
2023-11-25 04:50:28.950 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.950 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY (1 OPERATOR(pg_catalog./) (1 OPERATOR(pg_catalog.+) sum(worker_column_3)))) AS rank, worker_column_3 AS worker_column_4, (1 OPERATOR(pg_catalog./) (1 OPERATOR(pg_catalog.+) sum(worker_column_3))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY worker_column_1, (avg(worker_column_2)) DESC LIMIT '5'::bigint
2023-11-25 04:50:28.950 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.950 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY (1 OPERATOR(pg_catalog./) (1 OPERATOR(pg_catalog.+) sum(worker_column_3)))) AS rank, worker_column_3 AS worker_column_4, (1 OPERATOR(pg_catalog./) (1 OPERATOR(pg_catalog.+) sum(worker_column_3))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY worker_column_1, (avg(worker_column_2)) DESC LIMIT '5'::bigint
2023-11-25 04:50:28.950 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.950 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.950 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.950 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.950 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.950 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.950 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.950 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.950 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.950 UTC [1096741] DEBUG:  00000: combine query: SELECT user_id, avg, rank FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, avg numeric, rank bigint, worker_column_4 integer, worker_column_5 bigint) ORDER BY user_id, avg DESC LIMIT '5'::bigint
2023-11-25 04:50:28.950 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.953 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.953 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.953 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.953 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.953 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.953 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.953 UTC [1096741] DEBUG:  00000: push down of limit count: 5
2023-11-25 04:50:28.953 UTC [1096741] LOCATION:  WorkerLimitCount, multi_logical_optimizer.c:4778
2023-11-25 04:50:28.953 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.953 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.953 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.953 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.953 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY (sum(worker_column_3))) AS rank, worker_column_3 AS worker_column_4, sum(worker_column_3) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY worker_column_1, (avg(worker_column_2)) DESC LIMIT '5'::bigint
2023-11-25 04:50:28.953 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.953 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY (sum(worker_column_3))) AS rank, worker_column_3 AS worker_column_4, sum(worker_column_3) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY worker_column_1, (avg(worker_column_2)) DESC LIMIT '5'::bigint
2023-11-25 04:50:28.953 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.953 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY (sum(worker_column_3))) AS rank, worker_column_3 AS worker_column_4, sum(worker_column_3) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY worker_column_1, (avg(worker_column_2)) DESC LIMIT '5'::bigint
2023-11-25 04:50:28.953 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.953 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY (sum(worker_column_3))) AS rank, worker_column_3 AS worker_column_4, sum(worker_column_3) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY worker_column_1, (avg(worker_column_2)) DESC LIMIT '5'::bigint
2023-11-25 04:50:28.953 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.953 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.953 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.953 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.953 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.953 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.953 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.953 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.953 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.953 UTC [1096741] DEBUG:  00000: combine query: SELECT user_id, avg, rank FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, avg numeric, rank bigint, worker_column_4 integer, worker_column_5 bigint) ORDER BY user_id, avg DESC LIMIT '5'::bigint
2023-11-25 04:50:28.953 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.955 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.955 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.955 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.955 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.955 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.955 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.955 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:28.955 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.955 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:28.955 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.955 UTC [1096741] DEBUG:  00000: generated sql query for task 1
2023-11-25 04:50:28.955 UTC [1096741] DETAIL:  query string: "SELECT user_id, count(value_1) AS count, stddev(value_1) AS stddev, user_id AS count, random() AS worker_column_5 FROM public.users_table_1400256 users_table WHERE true GROUP BY user_id HAVING (avg(value_1) OPERATOR(pg_catalog.>) '2'::numeric)"
2023-11-25 04:50:28.955 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.955 UTC [1096741] DEBUG:  00000: generated sql query for task 2
2023-11-25 04:50:28.955 UTC [1096741] DETAIL:  query string: "SELECT user_id, count(value_1) AS count, stddev(value_1) AS stddev, user_id AS count, random() AS worker_column_5 FROM public.users_table_1400257 users_table WHERE true GROUP BY user_id HAVING (avg(value_1) OPERATOR(pg_catalog.>) '2'::numeric)"
2023-11-25 04:50:28.955 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.955 UTC [1096741] DEBUG:  00000: generated sql query for task 3
2023-11-25 04:50:28.955 UTC [1096741] DETAIL:  query string: "SELECT user_id, count(value_1) AS count, stddev(value_1) AS stddev, user_id AS count, random() AS worker_column_5 FROM public.users_table_1400258 users_table WHERE true GROUP BY user_id HAVING (avg(value_1) OPERATOR(pg_catalog.>) '2'::numeric)"
2023-11-25 04:50:28.955 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.955 UTC [1096741] DEBUG:  00000: generated sql query for task 4
2023-11-25 04:50:28.955 UTC [1096741] DETAIL:  query string: "SELECT user_id, count(value_1) AS count, stddev(value_1) AS stddev, user_id AS count, random() AS worker_column_5 FROM public.users_table_1400259 users_table WHERE true GROUP BY user_id HAVING (avg(value_1) OPERATOR(pg_catalog.>) '2'::numeric)"
2023-11-25 04:50:28.955 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.955 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.955 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.955 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.955 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.955 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.955 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.955 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.955 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.955 UTC [1096741] DEBUG:  00000: combine query: SELECT user_id, count, stddev, count(count_1) OVER (PARTITION BY worker_column_5) AS count FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), '(i 1)'::cstring(0)) remote_scan(user_id integer, count bigint, stddev numeric, count_1 integer, worker_column_5 double precision) LIMIT '1'::bigint
2023-11-25 04:50:28.955 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.958 UTC [1096741] DEBUG:  00000: CTE cte is going to be inlined via distributed planning
2023-11-25 04:50:28.958 UTC [1096741] LOCATION:  InlineCTEsInQueryTree, cte_inline.c:117
2023-11-25 04:50:28.958 UTC [1096741] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 04:50:28.958 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.958 UTC [1096741] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 04:50:28.958 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.958 UTC [1096741] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 04:50:28.958 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.958 UTC [1096741] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 04:50:28.958 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.958 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.958 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.958 UTC [1096741] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 04:50:28.958 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.958 UTC [1096741] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 04:50:28.958 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.958 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.958 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.958 UTC [1096741] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 04:50:28.958 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.958 UTC [1096741] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 04:50:28.958 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.958 UTC [1096741] DEBUG:  00000: generated sql query for task 1
2023-11-25 04:50:28.958 UTC [1096741] DETAIL:  query string: "SELECT uref.id AS user_id, events_table.value_2, count(*) AS c FROM (public.events_table_1400260 events_table JOIN public.users_ref_test_table_1400284 uref ON ((uref.id OPERATOR(pg_catalog.=) events_table.user_id))) WHERE true GROUP BY uref.id, events_table.value_2"
2023-11-25 04:50:28.958 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.958 UTC [1096741] DEBUG:  00000: generated sql query for task 2
2023-11-25 04:50:28.958 UTC [1096741] DETAIL:  query string: "SELECT uref.id AS user_id, events_table.value_2, count(*) AS c FROM (public.events_table_1400261 events_table JOIN public.users_ref_test_table_1400284 uref ON ((uref.id OPERATOR(pg_catalog.=) events_table.user_id))) WHERE true GROUP BY uref.id, events_table.value_2"
2023-11-25 04:50:28.958 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.958 UTC [1096741] DEBUG:  00000: generated sql query for task 3
2023-11-25 04:50:28.958 UTC [1096741] DETAIL:  query string: "SELECT uref.id AS user_id, events_table.value_2, count(*) AS c FROM (public.events_table_1400262 events_table JOIN public.users_ref_test_table_1400284 uref ON ((uref.id OPERATOR(pg_catalog.=) events_table.user_id))) WHERE true GROUP BY uref.id, events_table.value_2"
2023-11-25 04:50:28.958 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.958 UTC [1096741] DEBUG:  00000: generated sql query for task 4
2023-11-25 04:50:28.958 UTC [1096741] DETAIL:  query string: "SELECT uref.id AS user_id, events_table.value_2, count(*) AS c FROM (public.events_table_1400263 events_table JOIN public.users_ref_test_table_1400284 uref ON ((uref.id OPERATOR(pg_catalog.=) events_table.user_id))) WHERE true GROUP BY uref.id, events_table.value_2"
2023-11-25 04:50:28.958 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:28.958 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.958 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.958 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.958 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.958 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.958 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.958 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.958 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.959 UTC [1096741] DEBUG:  00000: combine query: SELECT user_id, value_2, COALESCE((pg_catalog.sum(c))::bigint, '0'::bigint) AS c FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), '(i 1 2)'::cstring(0)) remote_scan(user_id integer, value_2 integer, c bigint) GROUP BY user_id, value_2
2023-11-25 04:50:28.959 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.959 UTC [1096741] DEBUG:  00000: generating subplan 46_1 for subquery SELECT uref.id AS user_id, events_table.value_2, count(*) AS c FROM (public.events_table JOIN public.users_ref_test_table uref ON ((uref.id OPERATOR(pg_catalog.=) events_table.user_id))) GROUP BY uref.id, events_table.value_2
2023-11-25 04:50:28.959 UTC [1096741] LOCATION:  RecursivelyPlanSubquery, recursive_planning.c:1551
2023-11-25 04:50:28.959 UTC [1096741] DEBUG:  00000: Plan 46 query after replacing subqueries and CTEs: SELECT DISTINCT cte.value_2, cte.c, sum(cte.value_2) OVER (PARTITION BY cte.c) AS sum FROM ((SELECT intermediate_result.user_id, intermediate_result.value_2, intermediate_result.c FROM read_intermediate_result('46_1'::text, 'binary'::citus_copy_format) intermediate_result(user_id integer, value_2 integer, c bigint)) cte JOIN public.events_table et ON (((et.value_2 OPERATOR(pg_catalog.=) cte.value_2) AND (et.value_2 OPERATOR(pg_catalog.=) cte.c)))) ORDER BY cte.value_2
2023-11-25 04:50:28.959 UTC [1096741] LOCATION:  GenerateSubplansForSubqueriesAndCTEs, recursive_planning.c:248
2023-11-25 04:50:28.959 UTC [1096741] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 04:50:28.959 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.959 UTC [1096741] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 04:50:28.959 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.959 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:28.959 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:28.959 UTC [1096741] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 04:50:28.959 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:28.959 UTC [1096741] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 04:50:28.959 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:28.959 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS value_2, worker_column_2 AS c, worker_column_1 AS sum FROM (SELECT cte.value_2 AS worker_column_1, cte.c AS worker_column_2 FROM ((SELECT intermediate_result.user_id, intermediate_result.value_2, intermediate_result.c FROM read_intermediate_result('46_1'::text, 'binary'::citus_copy_format) intermediate_result(user_id integer, value_2 integer, c bigint)) cte JOIN public.events_table_1400260 et ON (((et.value_2 OPERATOR(pg_catalog.=) cte.value_2) AND (et.value_2 OPERATOR(pg_catalog.=) cte.c))))) worker_subquery
2023-11-25 04:50:28.959 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.959 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS value_2, worker_column_2 AS c, worker_column_1 AS sum FROM (SELECT cte.value_2 AS worker_column_1, cte.c AS worker_column_2 FROM ((SELECT intermediate_result.user_id, intermediate_result.value_2, intermediate_result.c FROM read_intermediate_result('46_1'::text, 'binary'::citus_copy_format) intermediate_result(user_id integer, value_2 integer, c bigint)) cte JOIN public.events_table_1400261 et ON (((et.value_2 OPERATOR(pg_catalog.=) cte.value_2) AND (et.value_2 OPERATOR(pg_catalog.=) cte.c))))) worker_subquery
2023-11-25 04:50:28.959 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.959 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS value_2, worker_column_2 AS c, worker_column_1 AS sum FROM (SELECT cte.value_2 AS worker_column_1, cte.c AS worker_column_2 FROM ((SELECT intermediate_result.user_id, intermediate_result.value_2, intermediate_result.c FROM read_intermediate_result('46_1'::text, 'binary'::citus_copy_format) intermediate_result(user_id integer, value_2 integer, c bigint)) cte JOIN public.events_table_1400262 et ON (((et.value_2 OPERATOR(pg_catalog.=) cte.value_2) AND (et.value_2 OPERATOR(pg_catalog.=) cte.c))))) worker_subquery
2023-11-25 04:50:28.959 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.959 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS value_2, worker_column_2 AS c, worker_column_1 AS sum FROM (SELECT cte.value_2 AS worker_column_1, cte.c AS worker_column_2 FROM ((SELECT intermediate_result.user_id, intermediate_result.value_2, intermediate_result.c FROM read_intermediate_result('46_1'::text, 'binary'::citus_copy_format) intermediate_result(user_id integer, value_2 integer, c bigint)) cte JOIN public.events_table_1400263 et ON (((et.value_2 OPERATOR(pg_catalog.=) cte.value_2) AND (et.value_2 OPERATOR(pg_catalog.=) cte.c))))) worker_subquery
2023-11-25 04:50:28.959 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:28.959 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:28.959 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.959 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:28.959 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.959 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:28.959 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.959 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:28.959 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:28.960 UTC [1096741] DEBUG:  00000: combine query: SELECT DISTINCT value_2, c, sum(sum) OVER (PARTITION BY c) AS sum FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(value_2 integer, c bigint, sum integer) ORDER BY value_2
2023-11-25 04:50:28.960 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:28.960 UTC [1096741] DEBUG:  00000: Subplan 46_1 is used in 46
2023-11-25 04:50:28.960 UTC [1096741] LOCATION:  RecordSubplanExecutionsOnNodes, intermediate_result_pruning.c:193
2023-11-25 04:50:28.960 UTC [1096741] DEBUG:  00000: Subplan 46_1 will be sent to localhost:57637
2023-11-25 04:50:28.960 UTC [1096741] LOCATION:  LogIntermediateResultMulticastSummary, intermediate_result_pruning.c:422
2023-11-25 04:50:28.960 UTC [1096741] DEBUG:  00000: Subplan 46_1 will be sent to localhost:57638
2023-11-25 04:50:28.960 UTC [1096741] LOCATION:  LogIntermediateResultMulticastSummary, intermediate_result_pruning.c:422
2023-11-25 04:50:28.960 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:28.960 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.960 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 148 in 3764 microseconds
2023-11-25 04:50:28.960 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.960 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:28.960 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.960 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 150 in 3465 microseconds
2023-11-25 04:50:28.960 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.961 UTC [1096741] DEBUG:  00000: task execution (1) for placement (832) on anchor shard (1400260) finished in 302 microseconds on worker node localhost:57637
2023-11-25 04:50:28.961 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.961 UTC [1096741] DEBUG:  00000: task execution (2) for placement (833) on anchor shard (1400261) finished in 310 microseconds on worker node localhost:57638
2023-11-25 04:50:28.961 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.961 UTC [1096741] DEBUG:  00000: task execution (3) for placement (834) on anchor shard (1400262) finished in 394 microseconds on worker node localhost:57637
2023-11-25 04:50:28.961 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.961 UTC [1096741] DEBUG:  00000: task execution (4) for placement (835) on anchor shard (1400263) finished in 335 microseconds on worker node localhost:57638
2023-11-25 04:50:28.961 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.964 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 149 in 3641 microseconds
2023-11-25 04:50:28.964 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.972 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 151 in 12000 microseconds
2023-11-25 04:50:28.972 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.972 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 148: 2 to node localhost:57637
2023-11-25 04:50:28.972 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.972 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 149: 0 to node localhost:57637
2023-11-25 04:50:28.972 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.972 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 150: 2 to node localhost:57638
2023-11-25 04:50:28.972 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.972 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 151: 0 to node localhost:57638
2023-11-25 04:50:28.972 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.973 UTC [1096741] DEBUG:  00000: Session 152 (localhost:57637) has an assigned task
2023-11-25 04:50:28.973 UTC [1096741] LOCATION:  AssignTasksToConnectionsOrWorkerPool, adaptive_executor.c:1468
2023-11-25 04:50:28.973 UTC [1096741] DEBUG:  00000: Session 153 (localhost:57638) has an assigned task
2023-11-25 04:50:28.973 UTC [1096741] LOCATION:  AssignTasksToConnectionsOrWorkerPool, adaptive_executor.c:1468
2023-11-25 04:50:28.973 UTC [1096741] DEBUG:  00000: Session 152 (localhost:57637) has an assigned task
2023-11-25 04:50:28.973 UTC [1096741] LOCATION:  AssignTasksToConnectionsOrWorkerPool, adaptive_executor.c:1468
2023-11-25 04:50:28.973 UTC [1096741] DEBUG:  00000: Session 153 (localhost:57638) has an assigned task
2023-11-25 04:50:28.973 UTC [1096741] LOCATION:  AssignTasksToConnectionsOrWorkerPool, adaptive_executor.c:1468
2023-11-25 04:50:28.974 UTC [1096741] DEBUG:  00000: task execution (1) for placement (832) on anchor shard (1400260) finished in 846 microseconds on worker node localhost:57637
2023-11-25 04:50:28.974 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.974 UTC [1096741] DEBUG:  00000: task execution (2) for placement (833) on anchor shard (1400261) finished in 915 microseconds on worker node localhost:57638
2023-11-25 04:50:28.974 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.974 UTC [1096741] DEBUG:  00000: task execution (3) for placement (834) on anchor shard (1400262) finished in 471 microseconds on worker node localhost:57637
2023-11-25 04:50:28.974 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.974 UTC [1096741] DEBUG:  00000: task execution (4) for placement (835) on anchor shard (1400263) finished in 417 microseconds on worker node localhost:57638
2023-11-25 04:50:28.974 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.974 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 152: 2 to node localhost:57637
2023-11-25 04:50:28.974 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.974 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 153: 2 to node localhost:57638
2023-11-25 04:50:28.974 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.976 UTC [1096741] DEBUG:  00000: sending sinval catchup signal to PID 1094389
2023-11-25 04:50:28.976 UTC [1096741] LOCATION:  SICleanupQueue, sinvaladt.c:747
2023-11-25 04:50:28.981 UTC [1096741] DEBUG:  00000: FromList item is not empty
2023-11-25 04:50:28.981 UTC [1096741] CONTEXT:  SQL statement "SELECT TRUE FROM public.daily_uniques LIMIT 1"
2023-11-25 04:50:28.981 UTC [1096741] LOCATION:  TryToDelegateFunctionCall, function_call_delegation.c:196
2023-11-25 04:50:28.983 UTC [1096741] DEBUG:  00000: opening 1 new connections to localhost:57637
2023-11-25 04:50:28.983 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.983 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 154 in 3764 microseconds
2023-11-25 04:50:28.983 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.983 UTC [1096741] DEBUG:  00000: opening 1 new connections to localhost:57638
2023-11-25 04:50:28.983 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:28.983 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 155 in 3465 microseconds
2023-11-25 04:50:28.983 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:28.984 UTC [1096741] DEBUG:  00000: task execution (1) for placement (1067) on anchor shard (360164) finished in 1018 microseconds on worker node localhost:57637
2023-11-25 04:50:28.984 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.985 UTC [1096741] DEBUG:  00000: task execution (3) for placement (1069) on anchor shard (360165) finished in 504 microseconds on worker node localhost:57637
2023-11-25 04:50:28.985 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.985 UTC [1096741] DEBUG:  00000: task execution (2) for placement (1068) on anchor shard (360164) finished in 920 microseconds on worker node localhost:57638
2023-11-25 04:50:28.985 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.986 UTC [1096741] DEBUG:  00000: task execution (5) for placement (1071) on anchor shard (360166) finished in 996 microseconds on worker node localhost:57637
2023-11-25 04:50:28.986 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.986 UTC [1096741] DEBUG:  00000: task execution (7) for placement (1073) on anchor shard (360167) finished in 529 microseconds on worker node localhost:57637
2023-11-25 04:50:28.986 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.988 UTC [1096741] DEBUG:  00000: task execution (4) for placement (1070) on anchor shard (360165) finished in 2700 microseconds on worker node localhost:57638
2023-11-25 04:50:28.988 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.990 UTC [1096741] DEBUG:  00000: task execution (6) for placement (1072) on anchor shard (360166) finished in 2556 microseconds on worker node localhost:57638
2023-11-25 04:50:28.990 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.996 UTC [1096741] DEBUG:  00000: task execution (8) for placement (1074) on anchor shard (360167) finished in 5724 microseconds on worker node localhost:57638
2023-11-25 04:50:28.996 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:28.996 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 154: 4 to node localhost:57637
2023-11-25 04:50:28.996 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:28.996 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 155: 4 to node localhost:57638
2023-11-25 04:50:28.996 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:29.007 UTC [1096741] DEBUG:  00000: no shard pruning constraints on daily_uniques found
2023-11-25 04:50:29.007 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:29.007 UTC [1096741] DEBUG:  00000: shard count after pruning for daily_uniques: 4
2023-11-25 04:50:29.007 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:29.007 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:29.007 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:29.007 UTC [1096741] DEBUG:  00000: push down of limit count: 10
2023-11-25 04:50:29.007 UTC [1096741] LOCATION:  WorkerLimitCount, multi_logical_optimizer.c:4778
2023-11-25 04:50:29.007 UTC [1096741] DEBUG:  00000: no shard pruning constraints on daily_uniques found
2023-11-25 04:50:29.007 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:29.007 UTC [1096741] DEBUG:  00000: shard count after pruning for daily_uniques: 4
2023-11-25 04:50:29.007 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:29.007 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) AS commits, rank() OVER (PARTITION BY worker_column_1 ORDER BY (sum(worker_column_2)) DESC) AS rank FROM (SELECT daily_uniques.user_id AS worker_column_1, daily_uniques.value_2 AS worker_column_2 FROM public.daily_uniques_360164 daily_uniques) worker_subquery GROUP BY worker_column_1 HAVING (sum(worker_column_2) OPERATOR(pg_catalog.>) (0)::double precision) ORDER BY (sum(worker_column_2)) DESC LIMIT '10'::bigint
2023-11-25 04:50:29.007 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:29.007 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) AS commits, rank() OVER (PARTITION BY worker_column_1 ORDER BY (sum(worker_column_2)) DESC) AS rank FROM (SELECT daily_uniques.user_id AS worker_column_1, daily_uniques.value_2 AS worker_column_2 FROM public.daily_uniques_360165 daily_uniques) worker_subquery GROUP BY worker_column_1 HAVING (sum(worker_column_2) OPERATOR(pg_catalog.>) (0)::double precision) ORDER BY (sum(worker_column_2)) DESC LIMIT '10'::bigint
2023-11-25 04:50:29.007 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:29.007 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) AS commits, rank() OVER (PARTITION BY worker_column_1 ORDER BY (sum(worker_column_2)) DESC) AS rank FROM (SELECT daily_uniques.user_id AS worker_column_1, daily_uniques.value_2 AS worker_column_2 FROM public.daily_uniques_360166 daily_uniques) worker_subquery GROUP BY worker_column_1 HAVING (sum(worker_column_2) OPERATOR(pg_catalog.>) (0)::double precision) ORDER BY (sum(worker_column_2)) DESC LIMIT '10'::bigint
2023-11-25 04:50:29.007 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:29.007 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) AS commits, rank() OVER (PARTITION BY worker_column_1 ORDER BY (sum(worker_column_2)) DESC) AS rank FROM (SELECT daily_uniques.user_id AS worker_column_1, daily_uniques.value_2 AS worker_column_2 FROM public.daily_uniques_360167 daily_uniques) worker_subquery GROUP BY worker_column_1 HAVING (sum(worker_column_2) OPERATOR(pg_catalog.>) (0)::double precision) ORDER BY (sum(worker_column_2)) DESC LIMIT '10'::bigint
2023-11-25 04:50:29.007 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:29.007 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:29.007 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:29.007 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:29.007 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:29.007 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:29.007 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:29.007 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:29.007 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:29.008 UTC [1096741] DEBUG:  00000: combine query: SELECT user_id, commits, rank FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id bigint, commits double precision, rank bigint) ORDER BY commits DESC LIMIT '10'::bigint
2023-11-25 04:50:29.008 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:29.010 UTC [1096741] DEBUG:  00000: drop auto-cascades to type daily_uniques
2023-11-25 04:50:29.010 UTC [1096741] LOCATION:  reportDependentObjects, dependency.c:1122
2023-11-25 04:50:29.010 UTC [1096741] DEBUG:  00000: drop auto-cascades to type daily_uniques[]
2023-11-25 04:50:29.010 UTC [1096741] LOCATION:  reportDependentObjects, dependency.c:1122
2023-11-25 04:50:29.011 UTC [1096741] DEBUG:  00000: drop auto-cascades to trigger truncate_trigger_18430 on table daily_uniques
2023-11-25 04:50:29.011 UTC [1096741] LOCATION:  reportDependentObjects, dependency.c:1122
2023-11-25 04:50:29.011 UTC [1096741] DEBUG:  00000: EventTriggerInvoke 16675
2023-11-25 04:50:29.011 UTC [1096741] LOCATION:  EventTriggerInvoke, event_trigger.c:900
2023-11-25 04:50:29.031 UTC [1096741] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 04:50:29.031 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:29.031 UTC [1096741] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 04:50:29.031 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:29.031 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:29.031 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:29.031 UTC [1096741] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 04:50:29.031 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:29.031 UTC [1096741] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 04:50:29.031 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:29.031 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:29.031 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:29.031 UTC [1096741] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 04:50:29.031 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:29.031 UTC [1096741] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 04:50:29.031 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:29.031 UTC [1096741] DEBUG:  00000: generated sql query for task 1
2023-11-25 04:50:29.031 UTC [1096741] DETAIL:  query string: "SELECT events_table.value_2, uref.k_no AS rnk, uref.id AS worker_column_3 FROM (public.events_table_1400260 events_table JOIN public.users_ref_test_table_1400284 uref ON ((uref.id OPERATOR(pg_catalog.=) events_table.user_id))) WHERE true"
2023-11-25 04:50:29.031 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:29.031 UTC [1096741] DEBUG:  00000: generated sql query for task 2
2023-11-25 04:50:29.031 UTC [1096741] DETAIL:  query string: "SELECT events_table.value_2, uref.k_no AS rnk, uref.id AS worker_column_3 FROM (public.events_table_1400261 events_table JOIN public.users_ref_test_table_1400284 uref ON ((uref.id OPERATOR(pg_catalog.=) events_table.user_id))) WHERE true"
2023-11-25 04:50:29.031 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:29.031 UTC [1096741] DEBUG:  00000: generated sql query for task 3
2023-11-25 04:50:29.031 UTC [1096741] DETAIL:  query string: "SELECT events_table.value_2, uref.k_no AS rnk, uref.id AS worker_column_3 FROM (public.events_table_1400262 events_table JOIN public.users_ref_test_table_1400284 uref ON ((uref.id OPERATOR(pg_catalog.=) events_table.user_id))) WHERE true"
2023-11-25 04:50:29.031 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:29.032 UTC [1096741] DEBUG:  00000: generated sql query for task 4
2023-11-25 04:50:29.032 UTC [1096741] DETAIL:  query string: "SELECT events_table.value_2, uref.k_no AS rnk, uref.id AS worker_column_3 FROM (public.events_table_1400263 events_table JOIN public.users_ref_test_table_1400284 uref ON ((uref.id OPERATOR(pg_catalog.=) events_table.user_id))) WHERE true"
2023-11-25 04:50:29.032 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:29.032 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:29.032 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:29.032 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:29.032 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:29.032 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:29.032 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:29.032 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:29.032 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:29.032 UTC [1096741] DEBUG:  00000: combine query: SELECT value_2, sum(rnk) OVER (PARTITION BY worker_column_3) AS rnk FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), '(i 1 2)'::cstring(0)) remote_scan(value_2 integer, rnk integer, worker_column_3 integer)
2023-11-25 04:50:29.032 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:29.032 UTC [1096741] DEBUG:  00000: generating subplan 49_1 for subquery SELECT events_table.value_2, sum(uref.k_no) OVER (PARTITION BY uref.id) AS rnk FROM (public.events_table JOIN public.users_ref_test_table uref ON ((uref.id OPERATOR(pg_catalog.=) events_table.user_id)))
2023-11-25 04:50:29.032 UTC [1096741] LOCATION:  RecursivelyPlanSubquery, recursive_planning.c:1551
2023-11-25 04:50:29.032 UTC [1096741] DEBUG:  00000: Plan 49 query after replacing subqueries and CTEs: SELECT DISTINCT value_2, array_agg(rnk ORDER BY rnk) AS array_agg FROM (SELECT intermediate_result.value_2, intermediate_result.rnk FROM read_intermediate_result('49_1'::text, 'binary'::citus_copy_format) intermediate_result(value_2 integer, rnk bigint)) sq GROUP BY value_2 ORDER BY value_2
2023-11-25 04:50:29.032 UTC [1096741] LOCATION:  GenerateSubplansForSubqueriesAndCTEs, recursive_planning.c:248
2023-11-25 04:50:29.032 UTC [1096741] DEBUG:  00000: Creating router plan
2023-11-25 04:50:29.032 UTC [1096741] LOCATION:  CreateSingleTaskRouterSelectPlan, multi_router_planner.c:284
2023-11-25 04:50:29.032 UTC [1096741] DEBUG:  00000: Subplan 49_1 is used in 49
2023-11-25 04:50:29.032 UTC [1096741] LOCATION:  RecordSubplanExecutionsOnNodes, intermediate_result_pruning.c:193
2023-11-25 04:50:29.032 UTC [1096741] DEBUG:  00000: Subplan 49_1 will be written to local file
2023-11-25 04:50:29.032 UTC [1096741] LOCATION:  LogIntermediateResultMulticastSummary, intermediate_result_pruning.c:416
2023-11-25 04:50:29.032 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:29.032 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:29.032 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 156 in 3764 microseconds
2023-11-25 04:50:29.032 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:29.032 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:29.032 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:29.032 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 158 in 3465 microseconds
2023-11-25 04:50:29.032 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:29.035 UTC [1096741] DEBUG:  00000: task execution (1) for placement (832) on anchor shard (1400260) finished in 2375 microseconds on worker node localhost:57637
2023-11-25 04:50:29.035 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:29.035 UTC [1096741] DEBUG:  00000: task execution (2) for placement (833) on anchor shard (1400261) finished in 2011 microseconds on worker node localhost:57638
2023-11-25 04:50:29.035 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:29.035 UTC [1096741] DEBUG:  00000: task execution (3) for placement (834) on anchor shard (1400262) finished in 400 microseconds on worker node localhost:57637
2023-11-25 04:50:29.035 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:29.036 UTC [1096741] DEBUG:  00000: task execution (4) for placement (835) on anchor shard (1400263) finished in 637 microseconds on worker node localhost:57638
2023-11-25 04:50:29.036 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:29.044 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 159 in 11434 microseconds
2023-11-25 04:50:29.044 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:29.049 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 157 in 16468 microseconds
2023-11-25 04:50:29.049 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:29.049 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 156: 2 to node localhost:57637
2023-11-25 04:50:29.049 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:29.049 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 157: 0 to node localhost:57637
2023-11-25 04:50:29.049 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:29.049 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 158: 2 to node localhost:57638
2023-11-25 04:50:29.049 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:29.049 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 159: 0 to node localhost:57638
2023-11-25 04:50:29.049 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:29.051 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:29.051 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:29.051 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:29.051 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:29.051 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:29.051 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:29.051 UTC [1096741] DEBUG:  00000: push down of limit count: 1
2023-11-25 04:50:29.051 UTC [1096741] LOCATION:  WorkerLimitCount, multi_logical_optimizer.c:4778
2023-11-25 04:50:29.051 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:29.051 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:29.051 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:29.051 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:29.051 UTC [1096741] DEBUG:  00000: generated sql query for task 1
2023-11-25 04:50:29.051 UTC [1096741] DETAIL:  query string: "SELECT NULL::boolean FROM public.users_table_1400256 ut WHERE true LIMIT '1'::bigint"
2023-11-25 04:50:29.051 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:29.051 UTC [1096741] DEBUG:  00000: generated sql query for task 2
2023-11-25 04:50:29.051 UTC [1096741] DETAIL:  query string: "SELECT NULL::boolean FROM public.users_table_1400257 ut WHERE true LIMIT '1'::bigint"
2023-11-25 04:50:29.051 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:29.051 UTC [1096741] DEBUG:  00000: generated sql query for task 3
2023-11-25 04:50:29.051 UTC [1096741] DETAIL:  query string: "SELECT NULL::boolean FROM public.users_table_1400258 ut WHERE true LIMIT '1'::bigint"
2023-11-25 04:50:29.051 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:29.051 UTC [1096741] DEBUG:  00000: generated sql query for task 4
2023-11-25 04:50:29.051 UTC [1096741] DETAIL:  query string: "SELECT NULL::boolean FROM public.users_table_1400259 ut WHERE true LIMIT '1'::bigint"
2023-11-25 04:50:29.051 UTC [1096741] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 04:50:29.051 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:29.051 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:29.051 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:29.051 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:29.051 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:29.051 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:29.051 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:29.051 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:29.051 UTC [1096741] DEBUG:  00000: combine query: SELECT "?column?" FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), '(i 1)'::cstring(0)) remote_scan("?column?" boolean) LIMIT '1'::bigint
2023-11-25 04:50:29.051 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:29.051 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:29.051 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:29.051 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 160 in 3764 microseconds
2023-11-25 04:50:29.051 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:29.051 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:29.051 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:29.052 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 162 in 3465 microseconds
2023-11-25 04:50:29.052 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:29.052 UTC [1096741] DEBUG:  00000: task execution (2) for placement (821) on anchor shard (1400257) finished in 609 microseconds on worker node localhost:57638
2023-11-25 04:50:29.052 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:29.052 UTC [1096741] DEBUG:  00000: task execution (1) for placement (820) on anchor shard (1400256) finished in 865 microseconds on worker node localhost:57637
2023-11-25 04:50:29.052 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:29.053 UTC [1096741] DEBUG:  00000: task execution (3) for placement (822) on anchor shard (1400258) finished in 301 microseconds on worker node localhost:57637
2023-11-25 04:50:29.053 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:29.053 UTC [1096741] DEBUG:  00000: task execution (4) for placement (823) on anchor shard (1400259) finished in 353 microseconds on worker node localhost:57638
2023-11-25 04:50:29.053 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:29.057 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 161 in 5299 microseconds
2023-11-25 04:50:29.057 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:29.064 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 163 in 11941 microseconds
2023-11-25 04:50:29.064 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:29.064 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 160: 2 to node localhost:57637
2023-11-25 04:50:29.064 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:29.064 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 161: 0 to node localhost:57637
2023-11-25 04:50:29.064 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:29.064 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 162: 2 to node localhost:57638
2023-11-25 04:50:29.064 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:29.064 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 163: 0 to node localhost:57638
2023-11-25 04:50:29.064 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:29.064 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:29.064 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:29.064 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:29.064 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:29.064 UTC [1096741] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 04:50:29.064 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:29.064 UTC [1096741] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 04:50:29.064 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:29.064 UTC [1096741] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 04:50:29.064 UTC [1096741] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 04:50:29.064 UTC [1096741] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 04:50:29.064 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:29.064 UTC [1096741] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 04:50:29.064 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:29.064 UTC [1096741] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 04:50:29.064 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 04:50:29.064 UTC [1096741] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 04:50:29.064 UTC [1096741] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 04:50:29.065 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, max(worker_column_2) OVER (PARTITION BY worker_column_1, (min(worker_column_3))) AS max, worker_column_2 AS worker_column_3, min(worker_column_3) AS worker_column_4 FROM (SELECT s.user_id AS worker_column_1, s.value_1 AS worker_column_2, s.value_2 AS worker_column_3 FROM (SELECT DISTINCT us.user_id, us.value_2, us.value_1, random() AS r1 FROM public.users_table_1400256 us, public.events_table_1400260 events_table WHERE ((us.user_id OPERATOR(pg_catalog.=) events_table.user_id) AND (events_table.event_type OPERATOR(pg_catalog.=) ANY (ARRAY[1, 2]))) ORDER BY us.user_id, us.value_2) s) worker_subquery GROUP BY worker_column_1, worker_column_2
2023-11-25 04:50:29.065 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:29.065 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, max(worker_column_2) OVER (PARTITION BY worker_column_1, (min(worker_column_3))) AS max, worker_column_2 AS worker_column_3, min(worker_column_3) AS worker_column_4 FROM (SELECT s.user_id AS worker_column_1, s.value_1 AS worker_column_2, s.value_2 AS worker_column_3 FROM (SELECT DISTINCT us.user_id, us.value_2, us.value_1, random() AS r1 FROM public.users_table_1400257 us, public.events_table_1400261 events_table WHERE ((us.user_id OPERATOR(pg_catalog.=) events_table.user_id) AND (events_table.event_type OPERATOR(pg_catalog.=) ANY (ARRAY[1, 2]))) ORDER BY us.user_id, us.value_2) s) worker_subquery GROUP BY worker_column_1, worker_column_2
2023-11-25 04:50:29.065 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:29.065 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, max(worker_column_2) OVER (PARTITION BY worker_column_1, (min(worker_column_3))) AS max, worker_column_2 AS worker_column_3, min(worker_column_3) AS worker_column_4 FROM (SELECT s.user_id AS worker_column_1, s.value_1 AS worker_column_2, s.value_2 AS worker_column_3 FROM (SELECT DISTINCT us.user_id, us.value_2, us.value_1, random() AS r1 FROM public.users_table_1400258 us, public.events_table_1400262 events_table WHERE ((us.user_id OPERATOR(pg_catalog.=) events_table.user_id) AND (events_table.event_type OPERATOR(pg_catalog.=) ANY (ARRAY[1, 2]))) ORDER BY us.user_id, us.value_2) s) worker_subquery GROUP BY worker_column_1, worker_column_2
2023-11-25 04:50:29.065 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:29.065 UTC [1096741] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, max(worker_column_2) OVER (PARTITION BY worker_column_1, (min(worker_column_3))) AS max, worker_column_2 AS worker_column_3, min(worker_column_3) AS worker_column_4 FROM (SELECT s.user_id AS worker_column_1, s.value_1 AS worker_column_2, s.value_2 AS worker_column_3 FROM (SELECT DISTINCT us.user_id, us.value_2, us.value_1, random() AS r1 FROM public.users_table_1400259 us, public.events_table_1400263 events_table WHERE ((us.user_id OPERATOR(pg_catalog.=) events_table.user_id) AND (events_table.event_type OPERATOR(pg_catalog.=) ANY (ARRAY[1, 2]))) ORDER BY us.user_id, us.value_2) s) worker_subquery GROUP BY worker_column_1, worker_column_2
2023-11-25 04:50:29.065 UTC [1096741] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 04:50:29.065 UTC [1096741] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 04:50:29.065 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:29.065 UTC [1096741] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 04:50:29.065 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:29.065 UTC [1096741] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 04:50:29.065 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:29.065 UTC [1096741] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 04:50:29.065 UTC [1096741] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 04:50:29.065 UTC [1096741] DEBUG:  00000: combine query: SELECT user_id, max FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, max integer, worker_column_3 integer, worker_column_4 integer) ORDER BY max DESC, user_id
2023-11-25 04:50:29.065 UTC [1096741] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 04:50:29.065 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 04:50:29.065 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:29.065 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 164 in 3764 microseconds
2023-11-25 04:50:29.065 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:29.065 UTC [1096741] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 04:50:29.065 UTC [1096741] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 04:50:29.065 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 166 in 3465 microseconds
2023-11-25 04:50:29.065 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:29.068 UTC [1096741] DEBUG:  00000: task execution (1) for placement (832) on anchor shard (1400260) finished in 3200 microseconds on worker node localhost:57637
2023-11-25 04:50:29.068 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:29.069 UTC [1096741] DEBUG:  00000: task execution (2) for placement (833) on anchor shard (1400261) finished in 3107 microseconds on worker node localhost:57638
2023-11-25 04:50:29.069 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:29.070 UTC [1096741] DEBUG:  00000: task execution (3) for placement (834) on anchor shard (1400262) finished in 1557 microseconds on worker node localhost:57637
2023-11-25 04:50:29.070 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:29.070 UTC [1096741] DEBUG:  00000: task execution (4) for placement (835) on anchor shard (1400263) finished in 1523 microseconds on worker node localhost:57638
2023-11-25 04:50:29.070 UTC [1096741] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 04:50:29.074 UTC [1096741] DEBUG:  00000: established connection to localhost:57637 for session 165 in 8301 microseconds
2023-11-25 04:50:29.074 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:29.076 UTC [1096741] DEBUG:  00000: established connection to localhost:57638 for session 167 in 10806 microseconds
2023-11-25 04:50:29.076 UTC [1096741] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 04:50:29.076 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 164: 2 to node localhost:57637
2023-11-25 04:50:29.076 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:29.076 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 165: 0 to node localhost:57637
2023-11-25 04:50:29.076 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:29.076 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 166: 2 to node localhost:57638
2023-11-25 04:50:29.076 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:29.076 UTC [1096741] DEBUG:  00000: Total number of commands sent over the session 167: 0 to node localhost:57638
2023-11-25 04:50:29.076 UTC [1096741] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 04:50:29.077 UTC [1096741] DEBUG:  00000: shmem_exit(0): 6 before_shmem_exit callbacks to make
2023-11-25 04:50:29.077 UTC [1096741] LOCATION:  shmem_exit, ipc.c:236
2023-11-25 04:50:29.082 UTC [1096741] DEBUG:  00000: shmem_exit(0): 6 on_shmem_exit callbacks to make
2023-11-25 04:50:29.082 UTC [1096741] LOCATION:  shmem_exit, ipc.c:269
2023-11-25 04:50:29.082 UTC [1096741] DEBUG:  00000: proc_exit(0): 2 callbacks to make
2023-11-25 04:50:29.082 UTC [1096741] LOCATION:  proc_exit_prepare, ipc.c:196
2023-11-25 04:50:29.082 UTC [1096741] DEBUG:  00000: exit(0)
2023-11-25 04:50:29.082 UTC [1096741] LOCATION:  proc_exit, ipc.c:150
2023-11-25 04:50:29.082 UTC [1096741] DEBUG:  00000: shmem_exit(-1): 0 before_shmem_exit callbacks to make
2023-11-25 04:50:29.082 UTC [1096741] LOCATION:  shmem_exit, ipc.c:236
2023-11-25 04:50:29.082 UTC [1096741] DEBUG:  00000: shmem_exit(-1): 0 on_shmem_exit callbacks to make
2023-11-25 04:50:29.082 UTC [1096741] LOCATION:  shmem_exit, ipc.c:269
2023-11-25 04:50:29.082 UTC [1096741] DEBUG:  00000: proc_exit(-1): 0 callbacks to make
2023-11-25 04:50:29.082 UTC [1096741] LOCATION:  proc_exit_prepare, ipc.c:196
2023-11-25 04:50:29.179 UTC [1096743] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:29.179 UTC [1096743] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:29.179 UTC [1096743] STATEMENT:  UPDATE
		second_distributed_table
	SET
		dept = foo.tenant_id::int / 4
	FROM
	(
		SELECT DISTINCT foo_inner_1.tenant_id FROM
		(
			SELECT
				second_distributed_table.dept, second_distributed_table.tenant_id
			FROM
				second_distributed_table, distributed_table
			WHERE
				distributed_table.tenant_id = second_distributed_table.tenant_id
			AND
				second_distributed_table.dept IN (select dept from second_distributed_table))
		foo_inner_1 JOIN LATERAL
		(
			SELECT
				second_distributed_table.tenant_id
			FROM
				second_distributed_table, distributed_table
			WHERE
				distributed_table.tenant_id = second_distributed_table.tenant_id
				AND foo_inner_1.dept = second_distributed_table.dept
			AND
				second_distributed_table.dept IN (4,5)
		) foo_inner_2
		ON (foo_inner_2.tenant_id != foo_inner_1.tenant_id)
		) as foo
	RETURNING *;
2023-11-25 04:50:29.180 UTC [1096743] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:29.180 UTC [1096743] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:29.180 UTC [1096743] STATEMENT:  UPDATE
		second_distributed_table
	SET
		dept = foo.tenant_id::int / 4
	FROM
	(
		SELECT baz.tenant_id FROM
		(
			SELECT
				second_distributed_table.dept, second_distributed_table.tenant_id
			FROM
				second_distributed_table, distributed_table as d1
			WHERE
				d1.tenant_id = second_distributed_table.tenant_id
			AND
				second_distributed_table.dept IN (3,4)
				AND
				second_distributed_table.tenant_id IN
				(
						SELECT s2.tenant_id
						FROM second_distributed_table as s2
						GROUP BY d1.tenant_id, s2.tenant_id
				)
		) as baz
		) as foo WHERE second_distributed_table.tenant_id = foo.tenant_id
	RETURNING *;
2023-11-25 04:50:29.181 UTC [1096743] ERROR:  0A000: subqueries are not supported within INSERT queries
2023-11-25 04:50:29.181 UTC [1096743] HINT:  Try rewriting your queries with 'INSERT INTO ... SELECT' syntax.
2023-11-25 04:50:29.181 UTC [1096743] LOCATION:  ModifyPartialQuerySupported, multi_router_planner.c:696
2023-11-25 04:50:29.181 UTC [1096743] STATEMENT:  INSERT INTO
		second_distributed_table (tenant_id, dept)
	VALUES ('3', (WITH  vals AS (SELECT 3) select * from vals));
2023-11-25 04:50:29.181 UTC [1096743] ERROR:  0A000: subqueries are not supported within INSERT queries
2023-11-25 04:50:29.181 UTC [1096743] HINT:  Try rewriting your queries with 'INSERT INTO ... SELECT' syntax.
2023-11-25 04:50:29.181 UTC [1096743] LOCATION:  ModifyPartialQuerySupported, multi_router_planner.c:696
2023-11-25 04:50:29.181 UTC [1096743] STATEMENT:  INSERT INTO
		second_distributed_table (tenant_id, dept)
	VALUES ('3', (SELECT 3));
2023-11-25 04:50:29.362 UTC [1096742] ERROR:  XX000: EXPLAIN ANALYZE is currently not supported for INSERT ... SELECT commands with repartitioning
2023-11-25 04:50:29.362 UTC [1096742] LOCATION:  NonPushableInsertSelectExplainScan, multi_explain.c:252
2023-11-25 04:50:29.362 UTC [1096742] STATEMENT:  EXPLAIN ANALYZE INSERT INTO target_table SELECT a, max(b) FROM source_table GROUP BY a;
2023-11-25 04:50:29.621 UTC [1096742] ERROR:  23502: null value in column "b" of relation "target_table_4213617" violates not-null constraint
2023-11-25 04:50:29.621 UTC [1096742] DETAIL:  Failing row contains (2, null).
2023-11-25 04:50:29.621 UTC [1096742] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:29.621 UTC [1096742] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:29.621 UTC [1096742] STATEMENT:  INSERT INTO target_table SELECT * FROM source_table;
2023-11-25 04:50:29.690 UTC [1096742] ERROR:  55000: could not find shard for partition column value
2023-11-25 04:50:29.690 UTC [1096742] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:29.690 UTC [1096742] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:29.690 UTC [1096742] STATEMENT:  INSERT INTO target_table SELECT a * 10, b FROM source_table WHERE b IS NOT NULL;
2023-11-25 04:50:30.351 UTC [1097545] ERROR:  XX000: triggers are not supported on distributed tables
2023-11-25 04:50:30.351 UTC [1097545] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:726
2023-11-25 04:50:30.351 UTC [1097545] STATEMENT:  CREATE TRIGGER update_value_dist
	AFTER INSERT ON distributed_table
	FOR EACH ROW EXECUTE FUNCTION update_value();
2023-11-25 04:50:30.351 UTC [1097545] ERROR:  XX000: triggers are not supported on reference tables
2023-11-25 04:50:30.351 UTC [1097545] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:722
2023-11-25 04:50:30.351 UTC [1097545] STATEMENT:  CREATE TRIGGER update_value_ref
	AFTER INSERT ON reference_table
	FOR EACH ROW EXECUTE FUNCTION update_value();
2023-11-25 04:50:30.376 UTC [1097545] ERROR:  XX000: triggers are not supported on distributed tables
2023-11-25 04:50:30.376 UTC [1097545] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:726
2023-11-25 04:50:30.376 UTC [1097545] STATEMENT:  ALTER TRIGGER update_value_dist ON distributed_table RENAME TO update_value_dist1;
2023-11-25 04:50:30.377 UTC [1097545] ERROR:  XX000: trigger "update_value_dist" depends on an extension and this is not supported for distributed tables and local tables added to metadata
2023-11-25 04:50:30.377 UTC [1097545] DETAIL:  Triggers from extensions are expected to be created on the workers by the extension they depend on.
2023-11-25 04:50:30.377 UTC [1097545] LOCATION:  PreprocessAlterTriggerDependsStmt, trigger.c:552
2023-11-25 04:50:30.377 UTC [1097545] STATEMENT:  ALTER TRIGGER update_value_dist ON distributed_table DEPENDS ON EXTENSION seg;
2023-11-25 04:50:30.377 UTC [1097545] ERROR:  XX000: triggers are not supported on distributed tables
2023-11-25 04:50:30.377 UTC [1097545] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:726
2023-11-25 04:50:30.377 UTC [1097545] STATEMENT:  DROP TRIGGER update_value_dist ON distributed_table;
2023-11-25 04:50:30.377 UTC [1097545] ERROR:  XX000: triggers are not supported on distributed tables
2023-11-25 04:50:30.377 UTC [1097545] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:726
2023-11-25 04:50:30.377 UTC [1097545] STATEMENT:  ALTER TABLE distributed_table DISABLE TRIGGER ALL;
2023-11-25 04:50:30.377 UTC [1097545] ERROR:  XX000: triggers are not supported on distributed tables
2023-11-25 04:50:30.377 UTC [1097545] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:726
2023-11-25 04:50:30.377 UTC [1097545] STATEMENT:  ALTER TABLE distributed_table DISABLE TRIGGER USER;
2023-11-25 04:50:30.377 UTC [1097545] ERROR:  XX000: triggers are not supported on distributed tables
2023-11-25 04:50:30.377 UTC [1097545] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:726
2023-11-25 04:50:30.377 UTC [1097545] STATEMENT:  ALTER TABLE distributed_table DISABLE TRIGGER update_value_dist;
2023-11-25 04:50:30.377 UTC [1097545] ERROR:  XX000: triggers are not supported on distributed tables
2023-11-25 04:50:30.377 UTC [1097545] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:726
2023-11-25 04:50:30.377 UTC [1097545] STATEMENT:  ALTER TABLE distributed_table ENABLE TRIGGER ALL;
2023-11-25 04:50:30.377 UTC [1097545] ERROR:  XX000: triggers are not supported on distributed tables
2023-11-25 04:50:30.377 UTC [1097545] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:726
2023-11-25 04:50:30.377 UTC [1097545] STATEMENT:  ALTER TABLE distributed_table ENABLE TRIGGER USER;
2023-11-25 04:50:30.377 UTC [1097545] ERROR:  XX000: triggers are not supported on distributed tables
2023-11-25 04:50:30.377 UTC [1097545] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:726
2023-11-25 04:50:30.377 UTC [1097545] STATEMENT:  ALTER TABLE distributed_table ENABLE TRIGGER update_value_dist;
2023-11-25 04:50:30.377 UTC [1097545] ERROR:  XX000: triggers are not supported on reference tables
2023-11-25 04:50:30.377 UTC [1097545] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:722
2023-11-25 04:50:30.377 UTC [1097545] STATEMENT:  ALTER TRIGGER update_value_ref ON reference_table RENAME TO update_value_ref1;
2023-11-25 04:50:30.377 UTC [1097545] ERROR:  XX000: trigger "update_value_ref" depends on an extension and this is not supported for distributed tables and local tables added to metadata
2023-11-25 04:50:30.377 UTC [1097545] DETAIL:  Triggers from extensions are expected to be created on the workers by the extension they depend on.
2023-11-25 04:50:30.377 UTC [1097545] LOCATION:  PreprocessAlterTriggerDependsStmt, trigger.c:552
2023-11-25 04:50:30.377 UTC [1097545] STATEMENT:  ALTER TRIGGER update_value_ref ON reference_table DEPENDS ON EXTENSION seg;
2023-11-25 04:50:30.378 UTC [1097545] ERROR:  XX000: triggers are not supported on reference tables
2023-11-25 04:50:30.378 UTC [1097545] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:722
2023-11-25 04:50:30.378 UTC [1097545] STATEMENT:  DROP TRIGGER update_value_ref ON reference_table;
2023-11-25 04:50:30.378 UTC [1097545] ERROR:  XX000: triggers are not supported on reference tables
2023-11-25 04:50:30.378 UTC [1097545] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:722
2023-11-25 04:50:30.378 UTC [1097545] STATEMENT:  ALTER TABLE reference_table DISABLE TRIGGER ALL;
2023-11-25 04:50:30.378 UTC [1097545] ERROR:  XX000: triggers are not supported on reference tables
2023-11-25 04:50:30.378 UTC [1097545] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:722
2023-11-25 04:50:30.378 UTC [1097545] STATEMENT:  ALTER TABLE reference_table DISABLE TRIGGER USER;
2023-11-25 04:50:30.378 UTC [1097545] ERROR:  XX000: triggers are not supported on reference tables
2023-11-25 04:50:30.378 UTC [1097545] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:722
2023-11-25 04:50:30.378 UTC [1097545] STATEMENT:  ALTER TABLE reference_table DISABLE TRIGGER update_value_ref;
2023-11-25 04:50:30.378 UTC [1097545] ERROR:  XX000: triggers are not supported on reference tables
2023-11-25 04:50:30.378 UTC [1097545] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:722
2023-11-25 04:50:30.378 UTC [1097545] STATEMENT:  ALTER TABLE reference_table ENABLE TRIGGER ALL;
2023-11-25 04:50:30.378 UTC [1097545] ERROR:  XX000: triggers are not supported on reference tables
2023-11-25 04:50:30.378 UTC [1097545] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:722
2023-11-25 04:50:30.378 UTC [1097545] STATEMENT:  ALTER TABLE reference_table ENABLE TRIGGER USER;
2023-11-25 04:50:30.378 UTC [1097545] ERROR:  XX000: triggers are not supported on reference tables
2023-11-25 04:50:30.378 UTC [1097545] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:722
2023-11-25 04:50:30.378 UTC [1097545] STATEMENT:  ALTER TABLE reference_table ENABLE TRIGGER update_value_ref;
2023-11-25 04:50:30.380 UTC [1097545] ERROR:  XX000: cannot distribute relation "distributed_table_1" because it has triggers
2023-11-25 04:50:30.380 UTC [1097545] HINT:  Consider dropping all the triggers on "distributed_table_1" and retry.
2023-11-25 04:50:30.380 UTC [1097545] LOCATION:  EnsureRelationHasNoTriggers, create_distributed_table.c:2095
2023-11-25 04:50:30.380 UTC [1097545] STATEMENT:  SELECT create_distributed_table('distributed_table_1', 'value');
2023-11-25 04:50:30.381 UTC [1097545] ERROR:  XX000: cannot distribute relation "reference_table_1" because it has triggers
2023-11-25 04:50:30.381 UTC [1097545] HINT:  Consider dropping all the triggers on "reference_table_1" and retry.
2023-11-25 04:50:30.381 UTC [1097545] LOCATION:  EnsureRelationHasNoTriggers, create_distributed_table.c:2095
2023-11-25 04:50:30.381 UTC [1097545] STATEMENT:  SELECT create_reference_table('reference_table_1');
2023-11-25 04:50:30.464 UTC [1097544] ERROR:  23503: insert or update on table "target_table_1900000" violates foreign key constraint "fkey_1900000"
2023-11-25 04:50:30.464 UTC [1097544] DETAIL:  Key (col_1)=(1) is not present in table "test_ref_table_1900012".
2023-11-25 04:50:30.464 UTC [1097544] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:30.464 UTC [1097544] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:30.464 UTC [1097544] STATEMENT:  INSERT INTO
			target_table
		SELECT
			col_2,
			col_1
		FROM source_table_1 ON CONFLICT (col_1) DO UPDATE SET col_2 = 55 RETURNING *;
2023-11-25 04:50:31.161 UTC [1097664] ERROR:  XX000: cannot undistribute table because the table is not distributed
2023-11-25 04:50:31.161 UTC [1097664] LOCATION:  UndistributeTable, alter_table.c:379
2023-11-25 04:50:31.161 UTC [1097664] STATEMENT:  SELECT undistribute_table('local_source_table_1');
2023-11-25 04:50:31.313 UTC [1097819] ERROR:  42883: function hll_hash_bigint(bigint) does not exist at character 49
2023-11-25 04:50:31.313 UTC [1097819] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:50:31.313 UTC [1097819] QUERY:  
	 EXPLAIN SELECT symbol_id,
	        HLL_ADD_AGG(HLL_HASH_BIGINT(event_id)) AS event_hll_hash,
	        HLL_CARDINALITY(HLL_ADD_AGG(HLL_HASH_BIGINT(event_id))) AS event_n_users
	 FROM (
	    SELECT event_time, composite_id, event_id, 4640476 symbol_id FROM "events"
	 UNION ALL
	    SELECT event_time, composite_id, event_id, 4640477 symbol_id FROM "events"
	 ) pushdown_events
	 GROUP BY symbol_id;
	 
2023-11-25 04:50:31.313 UTC [1097819] CONTEXT:  PL/pgSQL function explain_has_distributed_subplan(text) line 5 at FOR over EXECUTE statement
2023-11-25 04:50:31.313 UTC [1097819] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 04:50:31.313 UTC [1097819] STATEMENT:  SELECT public.explain_has_distributed_subplan($$
	 EXPLAIN SELECT symbol_id,
	        HLL_ADD_AGG(HLL_HASH_BIGINT(event_id)) AS event_hll_hash,
	        HLL_CARDINALITY(HLL_ADD_AGG(HLL_HASH_BIGINT(event_id))) AS event_n_users
	 FROM (
	    SELECT event_time, composite_id, event_id, 4640476 symbol_id FROM "events"
	 UNION ALL
	    SELECT event_time, composite_id, event_id, 4640477 symbol_id FROM "events"
	 ) pushdown_events
	 GROUP BY symbol_id;
	 $$);
2023-11-25 04:50:31.320 UTC [1097819] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 04:50:31.320 UTC [1097819] HINT:  You need to have the hll extension loaded.
2023-11-25 04:50:31.320 UTC [1097819] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 04:50:31.320 UTC [1097819] STATEMENT:  SELECT count(distinct l_orderkey) FROM lineitem;
2023-11-25 04:50:31.320 UTC [1097819] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 04:50:31.320 UTC [1097819] HINT:  You need to have the hll extension loaded.
2023-11-25 04:50:31.320 UTC [1097819] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 04:50:31.320 UTC [1097819] STATEMENT:  SELECT count(distinct l_orderkey) FROM lineitem;
2023-11-25 04:50:31.320 UTC [1097819] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 04:50:31.320 UTC [1097819] HINT:  You need to have the hll extension loaded.
2023-11-25 04:50:31.320 UTC [1097819] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 04:50:31.320 UTC [1097819] STATEMENT:  SELECT count(distinct l_partkey) FROM lineitem;
2023-11-25 04:50:31.320 UTC [1097819] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 04:50:31.320 UTC [1097819] HINT:  You need to have the hll extension loaded.
2023-11-25 04:50:31.320 UTC [1097819] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 04:50:31.320 UTC [1097819] STATEMENT:  SELECT count(distinct l_extendedprice) FROM lineitem;
2023-11-25 04:50:31.321 UTC [1097819] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 04:50:31.321 UTC [1097819] HINT:  You need to have the hll extension loaded.
2023-11-25 04:50:31.321 UTC [1097819] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 04:50:31.321 UTC [1097819] STATEMENT:  SELECT count(distinct l_shipdate) FROM lineitem;
2023-11-25 04:50:31.321 UTC [1097819] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 04:50:31.321 UTC [1097819] HINT:  You need to have the hll extension loaded.
2023-11-25 04:50:31.321 UTC [1097819] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 04:50:31.321 UTC [1097819] STATEMENT:  SELECT count(distinct l_comment) FROM lineitem;
2023-11-25 04:50:31.321 UTC [1097819] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 04:50:31.321 UTC [1097819] HINT:  You need to have the hll extension loaded.
2023-11-25 04:50:31.321 UTC [1097819] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 04:50:31.321 UTC [1097819] STATEMENT:  SELECT count(distinct (l_orderkey * 2 + 1)) FROM lineitem;
2023-11-25 04:50:31.321 UTC [1097819] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 04:50:31.321 UTC [1097819] HINT:  You need to have the hll extension loaded.
2023-11-25 04:50:31.321 UTC [1097819] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 04:50:31.321 UTC [1097819] STATEMENT:  SELECT count(distinct extract(month from l_shipdate)) AS my_month FROM lineitem;
2023-11-25 04:50:31.321 UTC [1097819] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 04:50:31.321 UTC [1097819] HINT:  You need to have the hll extension loaded.
2023-11-25 04:50:31.321 UTC [1097819] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 04:50:31.321 UTC [1097819] STATEMENT:  SELECT count(distinct l_partkey) / count(distinct l_orderkey) FROM lineitem;
2023-11-25 04:50:31.321 UTC [1097819] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 04:50:31.321 UTC [1097819] HINT:  You need to have the hll extension loaded.
2023-11-25 04:50:31.321 UTC [1097819] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 04:50:31.321 UTC [1097819] STATEMENT:  SELECT count(distinct l_orderkey) FROM lineitem
		WHERE octet_length(l_comment) + octet_length('randomtext'::text) > 40;
2023-11-25 04:50:31.322 UTC [1097819] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 04:50:31.322 UTC [1097819] HINT:  You need to have the hll extension loaded.
2023-11-25 04:50:31.322 UTC [1097819] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 04:50:31.322 UTC [1097819] STATEMENT:  SELECT count(DISTINCT l_orderkey) FROM lineitem, orders
		WHERE l_orderkey = o_orderkey AND l_quantity < 5;
2023-11-25 04:50:31.322 UTC [1097819] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 04:50:31.322 UTC [1097819] HINT:  You need to have the hll extension loaded.
2023-11-25 04:50:31.322 UTC [1097819] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 04:50:31.322 UTC [1097819] STATEMENT:  SELECT count(DISTINCT l_orderkey) as distinct_order_count, l_quantity FROM lineitem
		WHERE l_quantity < 32.0
		GROUP BY l_quantity
		ORDER BY distinct_order_count ASC, l_quantity ASC
		LIMIT 10;
2023-11-25 04:50:31.348 UTC [1097819] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 04:50:31.348 UTC [1097819] HINT:  You need to have the hll extension loaded.
2023-11-25 04:50:31.348 UTC [1097819] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 04:50:31.348 UTC [1097819] STATEMENT:  SELECT COUNT (DISTINCT n_regionkey) FROM test_count_distinct_schema.nation_hash;
2023-11-25 04:50:31.348 UTC [1097819] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 04:50:31.348 UTC [1097819] HINT:  You need to have the hll extension loaded.
2023-11-25 04:50:31.348 UTC [1097819] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 04:50:31.348 UTC [1097819] STATEMENT:  SELECT COUNT (DISTINCT n_regionkey) FROM nation_hash;
2023-11-25 04:50:31.349 UTC [1097819] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 04:50:31.349 UTC [1097819] HINT:  You need to have the hll extension loaded.
2023-11-25 04:50:31.349 UTC [1097819] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 04:50:31.349 UTC [1097819] STATEMENT:  SELECT l_returnflag, count(DISTINCT l_shipdate) as count_distinct, count(*) as total
		FROM lineitem
		GROUP BY l_returnflag
		ORDER BY count_distinct
		LIMIT 10;
2023-11-25 04:50:31.349 UTC [1097819] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 04:50:31.349 UTC [1097819] HINT:  You need to have the hll extension loaded.
2023-11-25 04:50:31.349 UTC [1097819] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 04:50:31.349 UTC [1097819] STATEMENT:  SELECT l_returnflag, count(DISTINCT l_shipdate) as count_distinct, count(*) as total
		FROM lineitem
		GROUP BY l_returnflag
		ORDER BY total
		LIMIT 10;
2023-11-25 04:50:31.349 UTC [1097819] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 04:50:31.349 UTC [1097819] HINT:  You need to have the hll extension loaded.
2023-11-25 04:50:31.349 UTC [1097819] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 04:50:31.349 UTC [1097819] STATEMENT:  SELECT
		l_partkey,
		count(l_partkey) FILTER (WHERE l_shipmode = 'AIR'),
		count(DISTINCT l_partkey) FILTER (WHERE l_shipmode = 'AIR'),
		count(DISTINCT CASE WHEN l_shipmode = 'AIR' THEN l_partkey ELSE NULL END)
		FROM lineitem
		GROUP BY l_partkey
		ORDER BY 2 DESC, 1 DESC
		LIMIT 10;
2023-11-25 04:50:32.058 UTC [1097868] ERROR:  23502: null value in column "b" of relation "target_table_4213646" violates not-null constraint
2023-11-25 04:50:32.058 UTC [1097868] DETAIL:  Failing row contains (84, null).
2023-11-25 04:50:32.058 UTC [1097868] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:32.058 UTC [1097868] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:32.058 UTC [1097868] STATEMENT:  INSERT INTO target_table SELECT a, CASE WHEN a < 50 THEN b ELSE null END  FROM source_table;
2023-11-25 04:50:32.461 UTC [1097994] ERROR:  XX000: "parent_table" is not a partition
2023-11-25 04:50:32.461 UTC [1097994] LOCATION:  GenerateAlterTableAttachPartitionCommand, multi_partitioning_utils.c:1221
2023-11-25 04:50:32.461 UTC [1097994] STATEMENT:  SELECT public.generate_alter_table_attach_partition_command('parent_table');
2023-11-25 04:50:32.461 UTC [1097994] ERROR:  XX000: "child_1" is not a parent table
2023-11-25 04:50:32.461 UTC [1097994] LOCATION:  GeneratePartitioningInformation, multi_partitioning_utils.c:1155
2023-11-25 04:50:32.461 UTC [1097994] STATEMENT:  SELECT public.generate_partition_information('partition_child_1_schema.child_1');
2023-11-25 04:50:32.461 UTC [1097994] ERROR:  XX000: "child_1" is not a parent table
2023-11-25 04:50:32.461 UTC [1097994] LOCATION:  PartitionList, multi_partitioning_utils.c:1075
2023-11-25 04:50:32.461 UTC [1097994] STATEMENT:  SELECT public.print_partitions('partition_child_1_schema.child_1');
2023-11-25 04:50:32.485 UTC [1097994] ERROR:  42809: capitals is not a regular, foreign or partitioned table
2023-11-25 04:50:32.485 UTC [1097994] LOCATION:  EnsureRelationKindSupported, citus_ruleutils.c:635
2023-11-25 04:50:32.485 UTC [1097994] STATEMENT:  SELECT master_get_table_ddl_events('capitals');
2023-11-25 04:50:32.486 UTC [1097994] ERROR:  42809: cities is not a regular, foreign or partitioned table
2023-11-25 04:50:32.486 UTC [1097994] LOCATION:  EnsureRelationKindSupported, citus_ruleutils.c:635
2023-11-25 04:50:32.486 UTC [1097994] STATEMENT:  SELECT master_get_table_ddl_events('cities');
2023-11-25 04:50:32.544 UTC [1097992] ERROR:  23514: no partition of relation "partitioning_hash_test_1660012" found for row
2023-11-25 04:50:32.544 UTC [1097992] DETAIL:  Partition key of the failing row contains (subid) = (5).
2023-11-25 04:50:32.544 UTC [1097992] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:32.544 UTC [1097992] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:32.544 UTC [1097992] STATEMENT:  INSERT INTO partitioning_hash_test VALUES (8, 5);
2023-11-25 04:50:32.547 UTC [1097992] ERROR:  23514: no partition of relation "partitioning_hash_test_1660015" found for row
2023-11-25 04:50:32.547 UTC [1097992] DETAIL:  Partition key of the failing row contains (subid) = (12).
2023-11-25 04:50:32.547 UTC [1097992] CONTEXT:  while executing command on localhost:57638
2023-11-25 04:50:32.547 UTC [1097992] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:32.547 UTC [1097992] STATEMENT:  INSERT INTO partitioning_hash_test VALUES (9, 12);
2023-11-25 04:50:32.560 UTC [1097993] ERROR:  0A000: modifications on partitions when replication factor is greater than 1 is not supported
2023-11-25 04:50:32.560 UTC [1097993] HINT:  Run the query on the parent table "collections" instead.
2023-11-25 04:50:32.560 UTC [1097993] LOCATION:  DeferErrorIfPartitionTableNotSingleReplicated, distributed_planner.c:1224
2023-11-25 04:50:32.560 UTC [1097993] STATEMENT:  INSERT INTO collections_4 (key, ts, collection_id, value) VALUES (4, '2009-01-01', 2, 2);
2023-11-25 04:50:32.560 UTC [1097993] ERROR:  0A000: modifications on partitions when replication factor is greater than 1 is not supported
2023-11-25 04:50:32.560 UTC [1097993] HINT:  Run the query on the parent table "collections" instead.
2023-11-25 04:50:32.560 UTC [1097993] LOCATION:  DeferErrorIfPartitionTableNotSingleReplicated, distributed_planner.c:1224
2023-11-25 04:50:32.560 UTC [1097993] STATEMENT:  UPDATE collections_1 SET ts = now() WHERE key = 1;
2023-11-25 04:50:32.560 UTC [1097993] ERROR:  0A000: modifications on partitions when replication factor is greater than 1 is not supported
2023-11-25 04:50:32.560 UTC [1097993] HINT:  Run the query on the parent table "collections" instead.
2023-11-25 04:50:32.560 UTC [1097993] LOCATION:  DeferErrorIfPartitionTableNotSingleReplicated, distributed_planner.c:1224
2023-11-25 04:50:32.560 UTC [1097993] STATEMENT:  DELETE FROM collections_1 WHERE ts = now() AND key = 1;
2023-11-25 04:50:32.560 UTC [1097993] ERROR:  0A000: modifications on partitions when replication factor is greater than 1 is not supported
2023-11-25 04:50:32.560 UTC [1097993] HINT:  Run the query on the parent table "collections" instead.
2023-11-25 04:50:32.560 UTC [1097993] LOCATION:  DeferErrorIfPartitionTableNotSingleReplicated, distributed_planner.c:1224
2023-11-25 04:50:32.560 UTC [1097993] STATEMENT:  UPDATE collections_1 SET ts = now();
2023-11-25 04:50:32.560 UTC [1097993] ERROR:  0A000: modifications on partitions when replication factor is greater than 1 is not supported
2023-11-25 04:50:32.560 UTC [1097993] HINT:  Run the query on the parent table "collections" instead.
2023-11-25 04:50:32.560 UTC [1097993] LOCATION:  DeferErrorIfPartitionTableNotSingleReplicated, distributed_planner.c:1224
2023-11-25 04:50:32.560 UTC [1097993] STATEMENT:  DELETE FROM collections_1 WHERE ts = now();
2023-11-25 04:50:32.560 UTC [1097993] ERROR:  0A000: modifications on partitions when replication factor is greater than 1 is not supported
2023-11-25 04:50:32.560 UTC [1097993] HINT:  Run the query on the parent table "collections" instead.
2023-11-25 04:50:32.560 UTC [1097993] LOCATION:  DeferErrorIfPartitionTableNotSingleReplicated, distributed_planner.c:1224
2023-11-25 04:50:32.560 UTC [1097993] STATEMENT:  INSERT INTO collections_1 SELECT * FROM collections_1;
2023-11-25 04:50:32.561 UTC [1097993] ERROR:  0A000: modifications on partitions when replication factor is greater than 1 is not supported
2023-11-25 04:50:32.561 UTC [1097993] HINT:  Run the query on the parent table "collections" instead.
2023-11-25 04:50:32.561 UTC [1097993] LOCATION:  DeferErrorIfPartitionTableNotSingleReplicated, distributed_planner.c:1224
2023-11-25 04:50:32.561 UTC [1097993] STATEMENT:  INSERT INTO collections_1 SELECT * FROM collections_1 OFFSET 0;
2023-11-25 04:50:32.561 UTC [1097993] ERROR:  0A000: modifications on partitions when replication factor is greater than 1 is not supported
2023-11-25 04:50:32.561 UTC [1097993] HINT:  Run the query on the parent table "collections" instead.
2023-11-25 04:50:32.561 UTC [1097993] LOCATION:  DeferErrorIfPartitionTableNotSingleReplicated, distributed_planner.c:1224
2023-11-25 04:50:32.561 UTC [1097993] STATEMENT:  COPY collections_1 FROM STDIN;
2023-11-25 04:50:32.561 UTC [1097993] ERROR:  0A000: modifications on partitions when replication factor is greater than 1 is not supported
2023-11-25 04:50:32.561 UTC [1097993] HINT:  Run the query on the parent table "collections" instead.
2023-11-25 04:50:32.561 UTC [1097993] LOCATION:  DeferErrorIfPartitionTableNotSingleReplicated, distributed_planner.c:1224
2023-11-25 04:50:32.561 UTC [1097993] STATEMENT:  CREATE INDEX index_on_partition ON collections_1(key);
2023-11-25 04:50:32.562 UTC [1097993] ERROR:  0A000: modifications on partitions when replication factor is greater than 1 is not supported
2023-11-25 04:50:32.562 UTC [1097993] HINT:  Run the query on the parent table "collections" instead.
2023-11-25 04:50:32.562 UTC [1097993] LOCATION:  DeferErrorIfPartitionTableNotSingleReplicated, distributed_planner.c:1224
2023-11-25 04:50:32.562 UTC [1097993] STATEMENT:  UPDATE collections_1 SET ts = now() WHERE key = 1;
2023-11-25 04:50:32.562 UTC [1097993] ERROR:  0A000: modifications on partitions when replication factor is greater than 1 is not supported
2023-11-25 04:50:32.562 UTC [1097993] HINT:  Run the query on the parent table "collections" instead.
2023-11-25 04:50:32.562 UTC [1097993] LOCATION:  DeferErrorIfPartitionTableNotSingleReplicated, distributed_planner.c:1224
2023-11-25 04:50:32.562 UTC [1097993] STATEMENT:  TRUNCATE collections_1;
2023-11-25 04:50:32.562 UTC [1097993] ERROR:  0A000: modifications on partitions when replication factor is greater than 1 is not supported
2023-11-25 04:50:32.562 UTC [1097993] HINT:  Run the query on the parent table "collections" instead.
2023-11-25 04:50:32.562 UTC [1097993] LOCATION:  DeferErrorIfPartitionTableNotSingleReplicated, distributed_planner.c:1224
2023-11-25 04:50:32.562 UTC [1097993] STATEMENT:  TRUNCATE collections, collections_1;
2023-11-25 04:50:32.563 UTC [1097993] ERROR:  0A000: modifications on partitions when replication factor is greater than 1 is not supported
2023-11-25 04:50:32.563 UTC [1097993] HINT:  Run the query on the parent table "collections" instead.
2023-11-25 04:50:32.563 UTC [1097993] LOCATION:  DeferErrorIfPartitionTableNotSingleReplicated, distributed_planner.c:1224
2023-11-25 04:50:32.563 UTC [1097993] STATEMENT:  WITH collections_5_cte AS
	(
		DELETE FROM collections_5 RETURNING *
	)
	SELECT * FROM collections_5_cte;
2023-11-25 04:50:32.574 UTC [1097993] ERROR:  0A000: cannot create foreign key constraint
2023-11-25 04:50:32.574 UTC [1097993] DETAIL:  Citus currently supports foreign key constraints only for "citus.shard_replication_factor = 1".
2023-11-25 04:50:32.574 UTC [1097993] HINT:  Please change "citus.shard_replication_factor to 1". To learn more about using foreign keys with other replication factors, please contact us at https://citusdata.com/about/contact_us.
2023-11-25 04:50:32.574 UTC [1097993] LOCATION:  EnsureReferencingTableNotReplicated, foreign_constraint.c:599
2023-11-25 04:50:32.574 UTC [1097993] STATEMENT:  ALTER TABLE
		collections_5
	ADD CONSTRAINT
		fkey_delete FOREIGN KEY(key)
	REFERENCES
		fkey_test(key) ON DELETE CASCADE;
2023-11-25 04:50:32.593 UTC [1097992] ERROR:  XX000: cannot distribute relation "partitioning_test_failure_2009" which is partition of "partitioning_test_failure"
2023-11-25 04:50:32.593 UTC [1097992] DETAIL:  Citus does not support distributing partitions if their parent is not distributed table.
2023-11-25 04:50:32.593 UTC [1097992] HINT:  Distribute the partitioned table "partitioning_test_failure" instead.
2023-11-25 04:50:32.593 UTC [1097992] LOCATION:  EnsureRelationCanBeDistributed, create_distributed_table.c:1895
2023-11-25 04:50:32.593 UTC [1097992] STATEMENT:  SELECT create_distributed_table('partitioning_test_failure_2009', 'id');
2023-11-25 04:50:32.593 UTC [1097992] ERROR:  0A000: distributing partitioned tables in only supported for hash-distributed tables
2023-11-25 04:50:32.593 UTC [1097992] LOCATION:  EnsureRelationCanBeDistributed, create_distributed_table.c:1914
2023-11-25 04:50:32.593 UTC [1097992] STATEMENT:  SELECT create_distributed_table('partitioning_test_failure', 'id', 'append');
2023-11-25 04:50:32.593 UTC [1097992] ERROR:  0A000: distributing partitioned tables in only supported for hash-distributed tables
2023-11-25 04:50:32.593 UTC [1097992] LOCATION:  EnsureRelationCanBeDistributed, create_distributed_table.c:1914
2023-11-25 04:50:32.593 UTC [1097992] STATEMENT:  SELECT create_distributed_table('partitioning_test_failure', 'id', 'range');
2023-11-25 04:50:32.594 UTC [1097992] ERROR:  0A000: distributing partitioned tables in only supported for hash-distributed tables
2023-11-25 04:50:32.594 UTC [1097992] LOCATION:  EnsureRelationCanBeDistributed, create_distributed_table.c:1914
2023-11-25 04:50:32.594 UTC [1097992] STATEMENT:  SELECT create_reference_table('partitioning_test_failure');
2023-11-25 04:50:32.604 UTC [1097992] ERROR:  XX000: non-citus partitioned tables cannot have citus table partitions
2023-11-25 04:50:32.604 UTC [1097992] HINT:  Distribute the partitioned table "partitioning_test_failure" instead, or add it to metadata
2023-11-25 04:50:32.604 UTC [1097992] LOCATION:  ErrorIfAttachCitusTableToPgLocalTable, table.c:651
2023-11-25 04:50:32.604 UTC [1097992] STATEMENT:  ALTER TABLE partitioning_test_failure ATTACH PARTITION partitioning_test_failure_2009 FOR VALUES FROM ('2009-01-01') TO ('2010-01-01');
2023-11-25 04:50:32.618 UTC [1097992] ERROR:  0A000: distributing multi-level partitioned tables is not supported
2023-11-25 04:50:32.618 UTC [1097992] DETAIL:  Relation "partitioning_test_failure_2009" is partitioned table itself and it is also partition of relation "partitioning_test_failure".
2023-11-25 04:50:32.618 UTC [1097992] LOCATION:  EnsureRelationCanBeDistributed, create_distributed_table.c:1924
2023-11-25 04:50:32.618 UTC [1097992] STATEMENT:  SELECT create_distributed_table('partitioning_test_failure', 'id');
2023-11-25 04:50:32.626 UTC [1097992] ERROR:  0A000: distributing multi-level partitioned tables is not supported
2023-11-25 04:50:32.626 UTC [1097992] DETAIL:  Relation "partitioning_test_failure_2009" is partitioned table itself and it is also partition of relation "partitioning_test_failure".
2023-11-25 04:50:32.626 UTC [1097992] LOCATION:  EnsureRelationCanBeDistributed, create_distributed_table.c:1924
2023-11-25 04:50:32.626 UTC [1097992] STATEMENT:  CREATE TABLE partitioning_test_failure_2009 PARTITION OF partitioning_test_failure FOR VALUES FROM ('2009-01-01') TO ('2010-01-01') PARTITION BY RANGE (time);
2023-11-25 04:50:32.652 UTC [1097992] ERROR:  23514: no partition of relation "partitioning_test_1660001" found for row
2023-11-25 04:50:32.652 UTC [1097992] DETAIL:  Partition key of the failing row contains ("time") = (2020-07-07).
2023-11-25 04:50:32.652 UTC [1097992] CONTEXT:  while executing command on localhost:57638
2023-11-25 04:50:32.652 UTC [1097992] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:32.652 UTC [1097992] STATEMENT:  UPDATE partitioning_test SET time = '2020-07-07' WHERE id = 7;
2023-11-25 04:50:32.672 UTC [1097993] ERROR:  25001: cannot open new connections after the first modification command within a transaction
2023-11-25 04:50:32.672 UTC [1097993] LOCATION:  EnsureNoModificationsHaveBeenDone, worker_transaction.c:320
2023-11-25 04:50:32.672 UTC [1097993] STATEMENT:  SELECT citus_copy_shard_placement(1760036, 'localhost', 57637, 'localhost', 57638, transfer_mode := 'block_writes');
2023-11-25 04:50:32.683 UTC [1098123] LOG:  00000: deferred drop of orphaned resource public.shard_split_table_360047 on localhost:57638 completed
2023-11-25 04:50:32.683 UTC [1098123] LOCATION:  DropOrphanedResourcesForCleanup, shard_cleaner.c:298
2023-11-25 04:50:32.683 UTC [1098123] STATEMENT:  CALL citus_cleanup_orphaned_resources()
2023-11-25 04:50:32.687 UTC [1097992] ERROR:  23514: updated partition constraint for default partition "partitioning_test_default_1660054" would be violated by some row
2023-11-25 04:50:32.687 UTC [1097992] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:32.687 UTC [1097992] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:32.687 UTC [1097992] STATEMENT:  CREATE TABLE partitioning_test_2014 PARTITION OF partitioning_test FOR VALUES FROM ('2014-01-01') TO ('2015-01-01');
2023-11-25 04:50:32.733 UTC [1097992] ERROR:  23514: new row for relation "partitioning_test_2009_1660005" violates partition constraint
2023-11-25 04:50:32.733 UTC [1097992] DETAIL:  Failing row contains (3, 2010-03-11).
2023-11-25 04:50:32.733 UTC [1097992] CONTEXT:  while executing command on localhost:57638
2023-11-25 04:50:32.733 UTC [1097992] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:32.733 UTC [1097992] STATEMENT:  UPDATE partitioning_test_2009 SET time = time + INTERVAL '6 month';
2023-11-25 04:50:32.783 UTC [1098169] LOG:  00000: cleaned up orphaned resource partitioned_table_replicated.customer_engagements_1760036 on localhost:57638 which was left behind after a failed operation
2023-11-25 04:50:32.783 UTC [1098169] LOCATION:  DropOrphanedResourcesForCleanup, shard_cleaner.c:305
2023-11-25 04:50:32.783 UTC [1098169] STATEMENT:  CALL citus_cleanup_orphaned_resources()
2023-11-25 04:50:32.784 UTC [1098169] LOG:  00000: cleaned up orphaned resource partitioned_table_replicated.customer_engagements_1_1760037 on localhost:57638 which was left behind after a failed operation
2023-11-25 04:50:32.784 UTC [1098169] LOCATION:  DropOrphanedResourcesForCleanup, shard_cleaner.c:305
2023-11-25 04:50:32.784 UTC [1098169] STATEMENT:  CALL citus_cleanup_orphaned_resources()
2023-11-25 04:50:32.784 UTC [1098169] LOG:  00000: cleaned up orphaned resource partitioned_table_replicated.customer_engagements_2_1760038 on localhost:57638 which was left behind after a failed operation
2023-11-25 04:50:32.784 UTC [1098169] LOCATION:  DropOrphanedResourcesForCleanup, shard_cleaner.c:305
2023-11-25 04:50:32.784 UTC [1098169] STATEMENT:  CALL citus_cleanup_orphaned_resources()
2023-11-25 04:50:32.875 UTC [1097992] ERROR:  2BP01: cannot drop index partitioning_test_2009_id_idx because index partitioning_index requires it
2023-11-25 04:50:32.875 UTC [1097992] HINT:  You can drop index partitioning_index instead.
2023-11-25 04:50:32.875 UTC [1097992] LOCATION:  reportDependentObjects, dependency.c:1055
2023-11-25 04:50:32.875 UTC [1097992] STATEMENT:  DROP INDEX partitioning_test_2009_id_idx;
2023-11-25 04:50:32.898 UTC [1097992] ERROR:  42809: cannot add column to a partition
2023-11-25 04:50:32.898 UTC [1097992] LOCATION:  ATExecAddColumn, tablecmds.c:6753
2023-11-25 04:50:32.898 UTC [1097992] STATEMENT:  ALTER TABLE partitioning_test_2010 ADD new_column_2 int;
2023-11-25 04:50:32.906 UTC [1097992] ERROR:  0A000: unique constraint on partitioned table must include all partitioning columns
2023-11-25 04:50:32.906 UTC [1097992] DETAIL:  PRIMARY KEY constraint on table "partitioning_test" lacks column "time" which is part of the partition key.
2023-11-25 04:50:32.906 UTC [1097992] LOCATION:  DefineIndex, indexcmds.c:1035
2023-11-25 04:50:32.906 UTC [1097992] STATEMENT:  ALTER TABLE partitioning_test ADD CONSTRAINT partitioning_primary PRIMARY KEY (id);
2023-11-25 04:50:32.927 UTC [1097992] ERROR:  55006: cannot ALTER TABLE "partitioning_test_2009" because it is being used by active queries in this session
2023-11-25 04:50:32.927 UTC [1097992] LOCATION:  CheckTableNotInUse, tablecmds.c:4005
2023-11-25 04:50:32.927 UTC [1097992] STATEMENT:  ALTER TABLE partitioning_test ADD CONSTRAINT partitioning_foreign FOREIGN KEY (id) REFERENCES partitioning_test_2009 (id);
2023-11-25 04:50:33.276 UTC [1097992] ERROR:  23514: no partition of relation "multi_column_partitioning_1660133" found for row
2023-11-25 04:50:33.276 UTC [1097992] DETAIL:  Partition key of the failing row contains (c1, c2) = (10, 1).
2023-11-25 04:50:33.276 UTC [1097992] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:33.276 UTC [1097992] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:33.276 UTC [1097992] STATEMENT:  INSERT INTO multi_column_partitioning VALUES(10, 1);
2023-11-25 04:50:33.290 UTC [1097992] ERROR:  23514: no partition of relation "multi_column_partitioning_1660133" found for row
2023-11-25 04:50:33.290 UTC [1097992] DETAIL:  Partition key of the failing row contains (c1, c2) = (20, -20).
2023-11-25 04:50:33.290 UTC [1097992] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:33.290 UTC [1097992] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:33.290 UTC [1097992] STATEMENT:  INSERT INTO multi_column_partitioning VALUES(20, -20);
2023-11-25 04:50:33.560 UTC [1097992] ERROR:  23503: insert or update on table "partitioning_test_2010_1660314" violates foreign key constraint "partitioning_reference_fkey_1660302"
2023-11-25 04:50:33.560 UTC [1097992] DETAIL:  Key (id)=(1) is not present in table "reference_table_1660300".
2023-11-25 04:50:33.560 UTC [1097992] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:33.560 UTC [1097992] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:33.560 UTC [1097992] STATEMENT:  ALTER TABLE partitioning_test ATTACH PARTITION partitioning_test_2010
	      FOR VALUES FROM ('2010-01-01') TO ('2011-01-01');
2023-11-25 04:50:33.586 UTC [1097992] ERROR:  42P07: relation "partitioning_test_2011" already exists
2023-11-25 04:50:33.586 UTC [1097992] LOCATION:  heap_create_with_catalog, heap.c:1146
2023-11-25 04:50:33.586 UTC [1097992] STATEMENT:  CREATE TABLE partitioning_test_2011 PARTITION OF partitioning_test FOR VALUES FROM ('2011-01-01') TO ('2012-01-01');
2023-11-25 04:50:33.594 UTC [1097992] ERROR:  42P07: relation "not_partition" already exists
2023-11-25 04:50:33.594 UTC [1097992] LOCATION:  heap_create_with_catalog, heap.c:1146
2023-11-25 04:50:33.594 UTC [1097992] STATEMENT:  CREATE TABLE not_partition PARTITION OF partitioning_test FOR VALUES FROM ('2011-01-01') TO ('2012-01-01');
2023-11-25 04:50:33.595 UTC [1097992] ERROR:  42P07: relation "partition_of_other_table" already exists
2023-11-25 04:50:33.595 UTC [1097992] LOCATION:  heap_create_with_catalog, heap.c:1146
2023-11-25 04:50:33.595 UTC [1097992] STATEMENT:  CREATE TABLE partition_of_other_table PARTITION OF partitioning_test FOR VALUES FROM ('2014-01-01') TO ('2015-01-01');
2023-11-25 04:50:33.597 UTC [1097992] ERROR:  XX000: fix_pre_citus10_partitioned_table_constraint_names can only be called for distributed partitioned tables
2023-11-25 04:50:33.597 UTC [1097992] LOCATION:  fix_pre_citus10_partitioned_table_constraint_names, multi_partitioning_utils.c:105
2023-11-25 04:50:33.597 UTC [1097992] STATEMENT:  SELECT fix_pre_citus10_partitioned_table_constraint_names('public.non_distributed_partitioned_table');
2023-11-25 04:50:33.598 UTC [1097992] ERROR:  XX000: could not fix partition constraints: relation does not exist or is not partitioned
2023-11-25 04:50:33.598 UTC [1097992] LOCATION:  fix_pre_citus10_partitioned_table_constraint_names, multi_partitioning_utils.c:100
2023-11-25 04:50:33.598 UTC [1097992] STATEMENT:  SELECT fix_pre_citus10_partitioned_table_constraint_names('reference_table');
2023-11-25 04:50:33.646 UTC [1097992] ERROR:  XX000: relation "multi_column_partitioned_p1" is a partition with multiple partition columns
2023-11-25 04:50:33.646 UTC [1097992] DETAIL:  time_partition_range can only be used for partitions of range-partitioned tables with a single partition column
2023-11-25 04:50:33.646 UTC [1097992] LOCATION:  time_partition_range, partitioning.c:102
2023-11-25 04:50:33.646 UTC [1097992] STATEMENT:  SELECT * FROM time_partition_range('multi_column_partitioned_p1');
2023-11-25 04:50:33.648 UTC [1097992] ERROR:  XX000: relation "list_partitioned_p1" is not a range partition
2023-11-25 04:50:33.648 UTC [1097992] DETAIL:  time_partition_range can only be used for partitions of range-partitioned tables with a single partition column
2023-11-25 04:50:33.648 UTC [1097992] LOCATION:  time_partition_range, partitioning.c:78
2023-11-25 04:50:33.648 UTC [1097992] STATEMENT:  SELECT * FROM time_partition_range('list_partitioned_p1');
2023-11-25 04:50:33.654 UTC [1097992] ERROR:  XX000: non-distributed tables cannot inherit distributed tables
2023-11-25 04:50:33.654 UTC [1097992] LOCATION:  PostprocessCreateTableStmt, table.c:253
2023-11-25 04:50:33.654 UTC [1097992] STATEMENT:  CREATE TABLE local_inheritance (k int) INHERITS (test_inheritance);
2023-11-25 04:50:33.762 UTC [1098325] ERROR:  P0001: partition interval of date partitioned table must be day or multiple days
2023-11-25 04:50:33.762 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 53 at RAISE
2023-11-25 04:50:33.762 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:33.762 UTC [1098325] STATEMENT:  SELECT * FROM get_missing_time_partition_ranges('date_partitioned_table', INTERVAL '6 hours', '2022-01-01', '2021-01-01');
2023-11-25 04:50:33.763 UTC [1098325] ERROR:  P0001: partition interval of date partitioned table must be day or multiple days
2023-11-25 04:50:33.763 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 53 at RAISE
2023-11-25 04:50:33.763 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:33.763 UTC [1098325] STATEMENT:  SELECT * FROM get_missing_time_partition_ranges('date_partitioned_table', INTERVAL '1 week 1 day 1 hour', '2022-01-01', '2021-01-01');
2023-11-25 04:50:33.804 UTC [1098325] ERROR:  P0001: partition date_partitioned_table_2021_01_01 with the range from 01-01-2021 to 01-02-2021 does not align with the initial partition given the partition interval
2023-11-25 04:50:33.804 UTC [1098325] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 04:50:33.804 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
2023-11-25 04:50:33.804 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:33.804 UTC [1098325] STATEMENT:  SELECT * FROM get_missing_time_partition_ranges('date_partitioned_table', INTERVAL '2 days', '2021-01-05', '2020-12-30');
2023-11-25 04:50:33.825 UTC [1098325] ERROR:  P0001: partition date_partitioned_table_2021_01_01 with the range from 01-01-2021 to 01-02-2021 does not align with the initial partition given the partition interval
2023-11-25 04:50:33.825 UTC [1098325] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 04:50:33.825 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
2023-11-25 04:50:33.825 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:33.825 UTC [1098325] STATEMENT:  SELECT * FROM get_missing_time_partition_ranges('date_partitioned_table', INTERVAL '2 days', '2021-01-05', '2020-12-30');
2023-11-25 04:50:33.864 UTC [1098325] ERROR:  P0001: partition date_partitioned_table_2021_01_01 with the range from 01-01-2021 to 01-02-2021 does not align with the initial partition given the partition interval
2023-11-25 04:50:33.864 UTC [1098325] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 04:50:33.864 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
2023-11-25 04:50:33.864 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:33.864 UTC [1098325] STATEMENT:  SELECT * FROM get_missing_time_partition_ranges('date_partitioned_table', INTERVAL '2 days', '2021-01-05', '2020-12-30');
2023-11-25 04:50:33.883 UTC [1098325] ERROR:  P0001: partition date_partitioned_table_2021_01_02 with the range from 01-04-2021 to 01-06-2021 does not align with the initial partition given the partition interval
2023-11-25 04:50:33.883 UTC [1098325] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 04:50:33.883 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
2023-11-25 04:50:33.883 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:33.883 UTC [1098325] STATEMENT:  SELECT * FROM get_missing_time_partition_ranges('date_partitioned_table', INTERVAL '2 days', '2021-01-15', '2020-12-30');
2023-11-25 04:50:33.914 UTC [1098325] ERROR:  P0001: partition tstz_partitioned_table_2021_01_01 with the range from Fri Jan 01 00:00:00 2021 PST to Sat Jan 02 00:00:00 2021 PST does not align with the initial partition given the partition interval
2023-11-25 04:50:33.914 UTC [1098325] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 04:50:33.914 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
2023-11-25 04:50:33.914 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:33.914 UTC [1098325] STATEMENT:  SELECT * FROM get_missing_time_partition_ranges('tstz_partitioned_table', INTERVAL '2 days', '2021-01-05 00:00:00', '2020-12-30 00:00:00');
2023-11-25 04:50:33.935 UTC [1098325] ERROR:  P0001: partition tstz_partitioned_table_2021_01_01 with the range from Fri Jan 01 00:00:00 2021 PST to Sat Jan 02 00:00:00 2021 PST does not align with the initial partition given the partition interval
2023-11-25 04:50:33.935 UTC [1098325] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 04:50:33.935 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
2023-11-25 04:50:33.935 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:33.935 UTC [1098325] STATEMENT:  SELECT * FROM get_missing_time_partition_ranges('tstz_partitioned_table', INTERVAL '2 days', '2021-01-05 00:00:00', '2020-12-30 00:00:00');
2023-11-25 04:50:33.975 UTC [1098325] ERROR:  P0001: partition tstz_partitioned_table_2021_01_01 with the range from Fri Jan 01 00:00:00 2021 PST to Sat Jan 02 00:00:00 2021 PST does not align with the initial partition given the partition interval
2023-11-25 04:50:33.975 UTC [1098325] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 04:50:33.975 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
2023-11-25 04:50:33.975 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:33.975 UTC [1098325] STATEMENT:  SELECT * FROM get_missing_time_partition_ranges('tstz_partitioned_table', INTERVAL '2 days', '2021-01-05 00:00:00', '2020-12-30 00:00:00');
2023-11-25 04:50:33.993 UTC [1098325] ERROR:  P0001: partition tstz_partitioned_table_2021_01_02 with the range from Mon Jan 04 00:00:00 2021 PST to Wed Jan 06 00:00:00 2021 PST does not align with the initial partition given the partition interval
2023-11-25 04:50:33.993 UTC [1098325] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 04:50:33.993 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
2023-11-25 04:50:33.993 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:33.993 UTC [1098325] STATEMENT:  SELECT * FROM get_missing_time_partition_ranges('tstz_partitioned_table', INTERVAL '2 days', '2021-01-15 00:00:00', '2020-12-30 00:00:00');
2023-11-25 04:50:34.034 UTC [1098325] ERROR:  P0001: partition tswtz_partitioned_table_2021_01_01 with the range from Fri Jan 01 00:00:00 2021 to Sat Jan 02 00:00:00 2021 does not align with the initial partition given the partition interval
2023-11-25 04:50:34.034 UTC [1098325] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 04:50:34.034 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
2023-11-25 04:50:34.034 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:34.034 UTC [1098325] STATEMENT:  SELECT * FROM get_missing_time_partition_ranges('tswtz_partitioned_table', INTERVAL '2 days', '2021-01-05 00:00:00', '2020-12-30 00:00:00');
2023-11-25 04:50:34.054 UTC [1098325] ERROR:  P0001: partition tswtz_partitioned_table_2021_01_01 with the range from Fri Jan 01 00:00:00 2021 to Sat Jan 02 00:00:00 2021 does not align with the initial partition given the partition interval
2023-11-25 04:50:34.054 UTC [1098325] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 04:50:34.054 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
2023-11-25 04:50:34.054 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:34.054 UTC [1098325] STATEMENT:  SELECT * FROM get_missing_time_partition_ranges('tswtz_partitioned_table', INTERVAL '2 days', '2021-01-05 00:00:00', '2020-12-30 00:00:00');
2023-11-25 04:50:34.093 UTC [1098325] ERROR:  P0001: partition tswtz_partitioned_table_2021_01_01 with the range from Fri Jan 01 00:00:00 2021 to Sat Jan 02 00:00:00 2021 does not align with the initial partition given the partition interval
2023-11-25 04:50:34.093 UTC [1098325] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 04:50:34.093 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
2023-11-25 04:50:34.093 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:34.093 UTC [1098325] STATEMENT:  SELECT * FROM get_missing_time_partition_ranges('tswtz_partitioned_table', INTERVAL '2 days', '2021-01-05 00:00:00', '2020-12-30 00:00:00');
2023-11-25 04:50:34.111 UTC [1098325] ERROR:  P0001: partition tswtz_partitioned_table_2021_01_02 with the range from Mon Jan 04 00:00:00 2021 to Wed Jan 06 00:00:00 2021 does not align with the initial partition given the partition interval
2023-11-25 04:50:34.111 UTC [1098325] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 04:50:34.111 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
2023-11-25 04:50:34.111 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:34.111 UTC [1098325] STATEMENT:  SELECT * FROM get_missing_time_partition_ranges('tswtz_partitioned_table', INTERVAL '2 days', '2021-01-15 00:00:00', '2020-12-30 00:00:00');
2023-11-25 04:50:34.134 UTC [1098325] ERROR:  P0001: partition interval of date partitioned table must be day or multiple days
2023-11-25 04:50:34.134 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 53 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 04:50:34.134 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:34.134 UTC [1098325] STATEMENT:  SELECT create_time_partitions('date_partitioned_table', INTERVAL '6 hours', '2022-01-01', '2021-01-01');
2023-11-25 04:50:34.134 UTC [1098325] ERROR:  P0001: partition interval of date partitioned table must be day or multiple days
2023-11-25 04:50:34.134 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 53 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 04:50:34.134 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:34.134 UTC [1098325] STATEMENT:  SELECT create_time_partitions('date_partitioned_table', INTERVAL '1 week 1 day 1 hour', '2022-01-01', '2021-01-01');
2023-11-25 04:50:34.216 UTC [1098325] ERROR:  P0001: start_from (Mon Feb 01 00:00:00 2021 PST) must be older than end_at (Fri Jan 01 00:00:00 2021 PST)
2023-11-25 04:50:34.216 UTC [1098325] CONTEXT:  PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 12 at RAISE
2023-11-25 04:50:34.216 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:34.216 UTC [1098325] STATEMENT:  SELECT * FROM create_time_partitions('date_partitioned_table', INTERVAL '1 day', '2021-01-01', '2021-02-01');
2023-11-25 04:50:34.222 UTC [1098325] ERROR:  P0001: partition date_partitioned_table_p2020_12_30 with the range from 12-30-2020 to 12-31-2020 does not align with the initial partition given the partition interval
2023-11-25 04:50:34.222 UTC [1098325] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 04:50:34.222 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 04:50:34.222 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:34.222 UTC [1098325] STATEMENT:  SELECT create_time_partitions('date_partitioned_table', INTERVAL '2 days', '2021-01-15', '2020-12-25');
2023-11-25 04:50:34.229 UTC [1098325] ERROR:  P0001: partition date_partitioned_table_p2020_12_30 with the range from 12-30-2020 to 12-31-2020 does not align with the initial partition given the partition interval
2023-11-25 04:50:34.229 UTC [1098325] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 04:50:34.229 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 04:50:34.229 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:34.229 UTC [1098325] STATEMENT:  SELECT create_time_partitions('date_partitioned_table', INTERVAL '2 days', '2021-01-05', '2020-12-30');
2023-11-25 04:50:34.240 UTC [1098325] ERROR:  P0001: partition date_partitioned_table_2020_01_02 with the range from 01-02-2021 to 01-04-2021 does not align with the initial partition given the partition interval
2023-11-25 04:50:34.240 UTC [1098325] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 04:50:34.240 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 04:50:34.240 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:34.240 UTC [1098325] STATEMENT:  SELECT create_time_partitions('date_partitioned_table', INTERVAL '1 day', '2021-01-05', '2020-12-30');
2023-11-25 04:50:34.243 UTC [1098325] ERROR:  P0001: partition date_partitioned_table_2021_01_02 with the range from 01-04-2021 to 01-06-2021 does not align with the initial partition given the partition interval
2023-11-25 04:50:34.243 UTC [1098325] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 04:50:34.243 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 04:50:34.243 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:34.243 UTC [1098325] STATEMENT:  SELECT create_time_partitions('date_partitioned_table', INTERVAL '2 days', '2021-01-15', '2020-12-30');
2023-11-25 04:50:34.297 UTC [1098325] ERROR:  P0001: start_from (Tue Jan 05 00:00:00 2021 PST) must be older than end_at (Fri Jan 01 00:00:00 2021 PST)
2023-11-25 04:50:34.297 UTC [1098325] CONTEXT:  PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 12 at RAISE
2023-11-25 04:50:34.297 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:34.297 UTC [1098325] STATEMENT:  SELECT create_time_partitions('tstz_partitioned_table', INTERVAL '1 day', '2021-01-01 00:00:00', '2021-01-05 00:00:00');
2023-11-25 04:50:34.303 UTC [1098325] ERROR:  P0001: partition tstz_partitioned_table_p2020_12_30 with the range from Wed Dec 30 00:00:00 2020 PST to Thu Dec 31 00:00:00 2020 PST does not align with the initial partition given the partition interval
2023-11-25 04:50:34.303 UTC [1098325] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 04:50:34.303 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
2023-11-25 04:50:34.303 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:34.303 UTC [1098325] STATEMENT:  SELECT * FROM get_missing_time_partition_ranges('tstz_partitioned_table', INTERVAL '2 days', '2021-01-05 00:00:00', '2020-12-30 00:00:00');
2023-11-25 04:50:34.310 UTC [1098325] ERROR:  P0001: partition tstz_partitioned_table_p2020_12_30 with the range from Wed Dec 30 00:00:00 2020 PST to Thu Dec 31 00:00:00 2020 PST does not align with the initial partition given the partition interval
2023-11-25 04:50:34.310 UTC [1098325] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 04:50:34.310 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 04:50:34.310 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:34.310 UTC [1098325] STATEMENT:  SELECT create_time_partitions('tstz_partitioned_table', INTERVAL '2 days', '2021-01-05 00:00:00', '2020-12-30 00:00:00');
2023-11-25 04:50:34.320 UTC [1098325] ERROR:  P0001: partition tstz_partitioned_table_2021_01_01 with the range from Fri Jan 01 00:00:00 2021 PST to Sat Jan 02 00:00:00 2021 PST does not align with the initial partition given the partition interval
2023-11-25 04:50:34.320 UTC [1098325] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 04:50:34.320 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 04:50:34.320 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:34.320 UTC [1098325] STATEMENT:  SELECT create_time_partitions('tstz_partitioned_table', INTERVAL '2 days', '2021-01-05 00:00:00', '2020-12-30 00:00:00');
2023-11-25 04:50:34.323 UTC [1098325] ERROR:  P0001: partition tstz_partitioned_table_2021_01_02 with the range from Mon Jan 04 00:00:00 2021 PST to Wed Jan 06 00:00:00 2021 PST does not align with the initial partition given the partition interval
2023-11-25 04:50:34.323 UTC [1098325] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 04:50:34.323 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 04:50:34.323 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:34.323 UTC [1098325] STATEMENT:  SELECT create_time_partitions('tstz_partitioned_table', INTERVAL '2 days', '2021-01-15 00:00:00', '2020-12-30 00:00:00');
2023-11-25 04:50:34.784 UTC [1098325] ERROR:  P0001: start_from (Tue Jan 05 00:00:00 2021 PST) must be older than end_at (Fri Jan 01 00:00:00 2021 PST)
2023-11-25 04:50:34.784 UTC [1098325] CONTEXT:  PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 12 at RAISE
2023-11-25 04:50:34.784 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:34.784 UTC [1098325] STATEMENT:  SELECT create_time_partitions('tswtz_partitioned_table', INTERVAL '1 day', '2021-01-01 00:00:00', '2021-01-05 00:00:00');
2023-11-25 04:50:34.831 UTC [1098325] ERROR:  P0001: partition tswtz_partitioned_table_p2020_12_30 with the range from Wed Dec 30 00:00:00 2020 to Thu Dec 31 00:00:00 2020 does not align with the initial partition given the partition interval
2023-11-25 04:50:34.831 UTC [1098325] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 04:50:34.831 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 04:50:34.831 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:34.831 UTC [1098325] STATEMENT:  SELECT create_time_partitions('tswtz_partitioned_table', INTERVAL '2 days', '2021-01-05 00:00:00', '2020-12-30 00:00:00');
2023-11-25 04:50:34.882 UTC [1098325] ERROR:  P0001: partition tswtz_partitioned_table_p2020_12_30 with the range from Wed Dec 30 00:00:00 2020 to Thu Dec 31 00:00:00 2020 does not align with the initial partition given the partition interval
2023-11-25 04:50:34.882 UTC [1098325] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 04:50:34.882 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 04:50:34.882 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:34.882 UTC [1098325] STATEMENT:  SELECT create_time_partitions('tswtz_partitioned_table', INTERVAL '2 days', '2021-01-05 00:00:00', '2020-12-30 00:00:00');
2023-11-25 04:50:34.960 UTC [1098325] ERROR:  P0001: partition tswtz_partitioned_table_2020_01_01 with the range from Fri Jan 01 00:00:00 2021 to Sat Jan 02 00:00:00 2021 does not align with the initial partition given the partition interval
2023-11-25 04:50:34.960 UTC [1098325] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 04:50:34.960 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 04:50:34.960 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:34.960 UTC [1098325] STATEMENT:  SELECT create_time_partitions('tswtz_partitioned_table', INTERVAL '2 days', '2021-01-05 00:00:00', '2020-12-30 00:00:00');
2023-11-25 04:50:34.978 UTC [1098325] ERROR:  P0001: partition tswtz_partitioned_table_2020_01_02 with the range from Mon Jan 04 00:00:00 2021 to Wed Jan 06 00:00:00 2021 does not align with the initial partition given the partition interval
2023-11-25 04:50:34.978 UTC [1098325] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 04:50:34.978 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 04:50:34.978 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:34.978 UTC [1098325] STATEMENT:  SELECT create_time_partitions('tswtz_partitioned_table', INTERVAL '2 days', '2021-01-15 00:00:00', '2020-12-30 00:00:00');
2023-11-25 04:50:35.206 UTC [1098325] ERROR:  P0001: partition interval of date partitioned table must be day or multiple days
2023-11-25 04:50:35.206 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 53 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 04:50:35.206 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:35.206 UTC [1098325] STATEMENT:  SELECT create_time_partitions('date_distributed_partitioned_table', INTERVAL '6 hours', '2022-01-01', '2021-01-01');
2023-11-25 04:50:35.206 UTC [1098325] ERROR:  P0001: partition interval of date partitioned table must be day or multiple days
2023-11-25 04:50:35.206 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 53 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 04:50:35.206 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:35.206 UTC [1098325] STATEMENT:  SELECT create_time_partitions('date_distributed_partitioned_table', INTERVAL '1 week 1 day 1 hour', '2022-01-01', '2021-01-01');
2023-11-25 04:50:35.860 UTC [1098395] LOG:  00000: cleaned up orphaned resource partitioned_table_replicated.customer_engagements_1760036 on localhost:57638 which was left behind after a failed operation
2023-11-25 04:50:35.860 UTC [1098395] LOCATION:  DropOrphanedResourcesForCleanup, shard_cleaner.c:305
2023-11-25 04:50:35.860 UTC [1098395] STATEMENT:  CALL citus_cleanup_orphaned_resources()
2023-11-25 04:50:35.861 UTC [1098395] LOG:  00000: cleaned up orphaned resource partitioned_table_replicated.customer_engagements_1_1760037 on localhost:57638 which was left behind after a failed operation
2023-11-25 04:50:35.861 UTC [1098395] LOCATION:  DropOrphanedResourcesForCleanup, shard_cleaner.c:305
2023-11-25 04:50:35.861 UTC [1098395] STATEMENT:  CALL citus_cleanup_orphaned_resources()
2023-11-25 04:50:35.861 UTC [1098395] LOG:  00000: cleaned up orphaned resource partitioned_table_replicated.customer_engagements_2_1760038 on localhost:57638 which was left behind after a failed operation
2023-11-25 04:50:35.861 UTC [1098395] LOCATION:  DropOrphanedResourcesForCleanup, shard_cleaner.c:305
2023-11-25 04:50:35.861 UTC [1098395] STATEMENT:  CALL citus_cleanup_orphaned_resources()
2023-11-25 04:50:35.994 UTC [1098325] ERROR:  P0001: partition interval of date partitioned table must be day or multiple days
2023-11-25 04:50:35.994 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 53 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 04:50:35.994 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:35.994 UTC [1098325] STATEMENT:  SELECT create_time_partitions('date_partitioned_citus_local_table', INTERVAL '6 hours', '2022-01-01', '2021-01-01');
2023-11-25 04:50:35.995 UTC [1098325] ERROR:  P0001: partition interval of date partitioned table must be day or multiple days
2023-11-25 04:50:35.995 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 53 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 04:50:35.995 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:35.995 UTC [1098325] STATEMENT:  SELECT create_time_partitions('date_partitioned_citus_local_table', INTERVAL '1 week 1 day 1 hour', '2022-01-01', '2021-01-01');
2023-11-25 04:50:38.147 UTC [1098325] ERROR:  P0001: partitioned tables with multiple partition columns are not supported
2023-11-25 04:50:38.147 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 33 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 04:50:38.147 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:38.147 UTC [1098325] STATEMENT:  SELECT create_time_partitions('multiple_partition_column_table', INTERVAL '1 month', now() + INTERVAL '1 year');
2023-11-25 04:50:38.147 UTC [1098325] ERROR:  P0001: partitioned tables with multiple partition columns are not supported
2023-11-25 04:50:38.147 UTC [1098325] CONTEXT:  PL/pgSQL function drop_old_time_partitions(regclass,timestamp with time zone) line 17 at RAISE
2023-11-25 04:50:38.147 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:38.147 UTC [1098325] STATEMENT:  CALL drop_old_time_partitions('multiple_partition_column_table', now());
2023-11-25 04:50:38.150 UTC [1098325] ERROR:  P0001: type of the partition column of the table invalid_partition_column_table must be date, timestamp or timestamptz
2023-11-25 04:50:38.150 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 47 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 04:50:38.150 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:38.150 UTC [1098325] STATEMENT:  SELECT create_time_partitions('invalid_partition_column_table', INTERVAL '1 month', now() + INTERVAL '1 year');
2023-11-25 04:50:38.150 UTC [1098325] ERROR:  P0001: type of the partition column of the table invalid_partition_column_table must be date, timestamp or timestamptz
2023-11-25 04:50:38.150 UTC [1098325] CONTEXT:  PL/pgSQL function drop_old_time_partitions(regclass,timestamp with time zone) line 29 at RAISE
2023-11-25 04:50:38.150 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:38.150 UTC [1098325] STATEMENT:  CALL drop_old_time_partitions('invalid_partition_column_table', now());
2023-11-25 04:50:38.152 UTC [1098325] ERROR:  P0001: non_partitioned_table is not partitioned
2023-11-25 04:50:38.152 UTC [1098325] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 31 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 04:50:38.152 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:38.152 UTC [1098325] STATEMENT:  SELECT create_time_partitions('non_partitioned_table', INTERVAL '1 month', now() + INTERVAL '1 year');
2023-11-25 04:50:38.152 UTC [1098325] ERROR:  P0001: non_partitioned_table is not partitioned
2023-11-25 04:50:38.152 UTC [1098325] CONTEXT:  PL/pgSQL function drop_old_time_partitions(regclass,timestamp with time zone) line 15 at RAISE
2023-11-25 04:50:38.152 UTC [1098325] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:38.152 UTC [1098325] STATEMENT:  CALL drop_old_time_partitions('non_partitioned_table', now());
2023-11-25 04:50:38.908 UTC [1098492] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a reference table
2023-11-25 04:50:38.908 UTC [1098492] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:671
2023-11-25 04:50:38.908 UTC [1098492] STATEMENT:  SELECT (SELECT id FROM dist WHERE dist.id > d1.id GROUP BY id) FROM ref FULL JOIN dist d1 USING (id);
2023-11-25 04:50:39.979 UTC [1098759] ERROR:  0A000: recursive CTEs are not supported in distributed queries
2023-11-25 04:50:39.979 UTC [1098759] LOCATION:  RecursivelyPlanCTEs, recursive_planning.c:1089
2023-11-25 04:50:39.979 UTC [1098759] STATEMENT:  SELECT
	   bar.user_id
	FROM
	    (
		     WITH RECURSIVE cte AS MATERIALIZED (
		    SELECT
		    	DISTINCT users_table.user_id
		     FROM
		     	users_table, events_table
		     WHERE
		     	users_table.user_id = events_table.user_id AND
		     event_type IN (1,2,3,4)
		     ) SELECT * FROM cte ORDER BY 1 DESC
	     ) as foo,
	    (
		    SELECT
		    	DISTINCT users_table.user_id
		     FROM
		     	users_table, events_table
		     WHERE
		     	users_table.user_id = events_table.user_id AND
		     event_type IN (1,2,3,4)
	     ) as bar
	WHERE foo.user_id = bar.user_id
	ORDER BY 1 DESC;
2023-11-25 04:50:40.075 UTC [1098759] ERROR:  P0001: (3/3) failed to execute one of the tasks
2023-11-25 04:50:40.075 UTC [1098759] CONTEXT:  PL/pgSQL function inline_code_block line 31 at RAISE
2023-11-25 04:50:40.075 UTC [1098759] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:50:40.075 UTC [1098759] STATEMENT:  DO $$
	DECLARE
		errors_received INTEGER;
	BEGIN
	errors_received := 0;
	FOR i IN 1..3 LOOP
		BEGIN
			WITH cte as (
				SELECT
					user_id, value_2
				from
					events_table
			)
			SELECT * FROM users_table where value_2 < (
				SELECT
					min(cte.value_2)
				FROM
					cte
				WHERE
					users_table.user_id=cte.user_id
				GROUP BY
					user_id, cte.value_2);
		EXCEPTION WHEN OTHERS THEN
			IF SQLERRM LIKE 'more than one row returned by a subquery%%' THEN
				errors_received := errors_received + 1;
			ELSIF SQLERRM LIKE 'failed to execute task%' THEN
				errors_received := errors_received + 1;
			END IF;
		END;
	END LOOP;
	RAISE '(%/3) failed to execute one of the tasks', errors_received;
	END;
	$$;
2023-11-25 04:50:40.507 UTC [1098888] ERROR:  0A000: cannot pushdown the subquery since not all subqueries in the UNION have the partition column in the same position
2023-11-25 04:50:40.507 UTC [1098888] DETAIL:  Each leaf query of the UNION should return the partition column in the same position and all joins must be on the partition column
2023-11-25 04:50:40.507 UTC [1098888] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:582
2023-11-25 04:50:40.507 UTC [1098888] STATEMENT:  SELECT * FROM ((SELECT * FROM test) UNION (SELECT * FROM test)) foo WHERE x IN (SELECT y FROM test);
2023-11-25 04:50:40.561 UTC [1098888] ERROR:  0A000: cannot push down this subquery
2023-11-25 04:50:40.561 UTC [1098888] DETAIL:  Complex subqueries and CTEs are not supported within a UNION
2023-11-25 04:50:40.561 UTC [1098888] LOCATION:  DeferErrorIfUnsupportedUnionQuery, query_pushdown_planning.c:1362
2023-11-25 04:50:40.561 UTC [1098888] STATEMENT:  SELECT * FROM test a WHERE x IN (SELECT x FROM test b UNION SELECT y FROM test c WHERE a.x = c.x) ORDER BY 1,2;
2023-11-25 04:50:40.581 UTC [1098888] ERROR:  0A000: cannot compute aggregate (distinct)
2023-11-25 04:50:40.581 UTC [1098888] DETAIL:  table partitioning is unsuitable for aggregate (distinct)
2023-11-25 04:50:40.581 UTC [1098888] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4205
2023-11-25 04:50:40.581 UTC [1098888] STATEMENT:  select count(DISTINCT t.x) FROM ((SELECT avg(DISTINCT y) FROM test GROUP BY y) UNION (SELECT avg(DISTINCT y) FROM test GROUP BY y)) as t(x) ORDER BY 1;
2023-11-25 04:50:41.197 UTC [1099093] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:41.197 UTC [1099093] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:41.197 UTC [1099093] STATEMENT:  EXPLAIN
	   SELECT
	    (SELECT user_id FROM users_table_part WHERE user_id = e.value_1
	        UNION ALL
	     SELECT user_id FROM users_table_part WHERE user_id = e.value_1)
	  FROM
	    (SELECT * FROM users_table_part) as e;
2023-11-25 04:50:41.229 UTC [1099093] WARNING:  0A000: "view v2" has dependency to "table range_dist_table_2" that is not in Citus' metadata
2023-11-25 04:50:41.229 UTC [1099093] DETAIL:  "view v2" will be created only locally
2023-11-25 04:50:41.229 UTC [1099093] HINT:  Distribute "table range_dist_table_2" first to distribute "view v2"
2023-11-25 04:50:41.229 UTC [1099093] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:950
2023-11-25 04:50:41.316 UTC [1099093] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:41.316 UTC [1099093] CONTEXT:  PL/pgSQL function public.explain_has_distributed_subplan(text) line 5 at FOR over EXECUTE statement
2023-11-25 04:50:41.316 UTC [1099093] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:41.316 UTC [1099093] STATEMENT:  SELECT public.explain_has_distributed_subplan($$
	EXPLAIN SELECT * FROM users_table_part u1 WHERE (value_1, user_id) IN
	(
	SELECT u1.user_id, user_id FROM users_table_part
	UNION
	SELECT u1.user_id, user_id FROM users_table_part
	);
	$$);
2023-11-25 04:50:41.511 UTC [1099156] ERROR:  22012: division by zero
2023-11-25 04:50:41.511 UTC [1099156] LOCATION:  int4div, int.c:840
2023-11-25 04:50:41.511 UTC [1099156] STATEMENT:  (SELECT x FROM test) INTERSECT (SELECT i/0 FROM generate_series(0, 100) i) ORDER BY 1 DESC;
2023-11-25 04:50:41.742 UTC [1099287] ERROR:  42P01: relation "events_table_local" does not exist at character 130
2023-11-25 04:50:41.742 UTC [1099287] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 04:50:41.742 UTC [1099287] STATEMENT:  SELECT COUNT(user_id) FROM users_table WHERE user_id IN
		(SELECT
			user_id
		 FROM
		 	users_table_local JOIN (SELECT user_id FROM events_table_local) as foo
		 USING (user_id)
		 );
2023-11-25 04:50:41.744 UTC [1099287] ERROR:  0A000: cannot compute aggregate (distinct)
2023-11-25 04:50:41.744 UTC [1099287] DETAIL:  table partitioning is unsuitable for aggregate (distinct)
2023-11-25 04:50:41.744 UTC [1099287] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4205
2023-11-25 04:50:41.744 UTC [1099287] STATEMENT:  SELECT
		*
	FROM
	(
		SELECT avg(DISTINCT value_1), random() FROM users_table GROUP BY user_id OFFSET 3
	) as baz,
	(
		SELECT count(DISTINCT value_1), random() FROM users_table GROUP BY value_2 OFFSET 3
	) as bar,
	(
		SELECT avg(DISTINCT value_1), random() FROM users_table GROUP BY value_2 OFFSET 3
	) as foo;
2023-11-25 04:50:41.747 UTC [1099287] ERROR:  0A000: array_agg with order by is unsupported
2023-11-25 04:50:41.747 UTC [1099287] LOCATION:  DeferErrorIfUnsupportedArrayAggregate, multi_logical_optimizer.c:4008
2023-11-25 04:50:41.747 UTC [1099287] STATEMENT:  SELECT
		*
	FROM
		(
			SELECT
				array_agg(users_table.value_2 ORDER BY users_table.time)
			FROM
				users_table, (SELECT user_id FROM events_table) as evs
			WHERE users_table.user_id = evs.user_id
			GROUP BY users_table.value_2
			LIMIT 5
		) as foo;
2023-11-25 04:50:41.748 UTC [1099287] ERROR:  0A000: cannot handle complex subqueries when the router executor is disabled
2023-11-25 04:50:41.748 UTC [1099287] LOCATION:  QueryPushdownSqlTaskList, multi_physical_planner.c:2182
2023-11-25 04:50:41.748 UTC [1099287] STATEMENT:  SELECT
	   user_id
	FROM
	    (SELECT
	    	DISTINCT users_table.user_id
	     FROM
	     	users_table, events_table
	     WHERE
	     	users_table.user_id = events_table.user_id AND
	     event_type IN (1,2,3,4)
	     ORDER BY 1 DESC LIMIT 5
	     ) as foo
	    ORDER BY 1 DESC;
2023-11-25 04:50:41.762 UTC [1099287] ERROR:  0A000: could not run distributed query with GROUPING SETS, CUBE, or ROLLUP
2023-11-25 04:50:41.762 UTC [1099287] HINT:  Consider using an equality filter on the distributed table's partition column.
2023-11-25 04:50:41.762 UTC [1099287] LOCATION:  DeferErrorIfQueryNotSupported, multi_logical_planner.c:945
2023-11-25 04:50:41.762 UTC [1099287] STATEMENT:  SELECT * FROM (SELECT user_id, value_1 FROM users_table GROUP BY GROUPING SETS ((user_id), (value_1))) s;
2023-11-25 04:50:41.762 UTC [1099287] ERROR:  0A000: could not run distributed query with GROUPING SETS, CUBE, or ROLLUP
2023-11-25 04:50:41.762 UTC [1099287] HINT:  Consider using an equality filter on the distributed table's partition column.
2023-11-25 04:50:41.762 UTC [1099287] LOCATION:  DeferErrorIfQueryNotSupported, multi_logical_planner.c:945
2023-11-25 04:50:41.762 UTC [1099287] STATEMENT:  SELECT * FROM (SELECT user_id, value_1 FROM users_table GROUP BY ROLLUP (user_id, value_1)) s;
2023-11-25 04:50:41.762 UTC [1099287] ERROR:  0A000: could not run distributed query with GROUPING SETS, CUBE, or ROLLUP
2023-11-25 04:50:41.762 UTC [1099287] HINT:  Consider using an equality filter on the distributed table's partition column.
2023-11-25 04:50:41.762 UTC [1099287] LOCATION:  DeferErrorIfQueryNotSupported, multi_logical_planner.c:945
2023-11-25 04:50:41.762 UTC [1099287] STATEMENT:  SELECT * FROM (SELECT user_id, value_1 FROM users_table GROUP BY CUBE (user_id, value_1)) s;
2023-11-25 04:50:41.863 UTC [1099285] WARNING:  0A000: "view subquery_from_from_where_local_table" has dependency to "table events_table_local" that is not in Citus' metadata
2023-11-25 04:50:41.863 UTC [1099285] DETAIL:  "view subquery_from_from_where_local_table" will be created only locally
2023-11-25 04:50:41.863 UTC [1099285] HINT:  Distribute "table events_table_local" first to distribute "view subquery_from_from_where_local_table"
2023-11-25 04:50:41.863 UTC [1099285] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:950
2023-11-25 04:50:41.893 UTC [1099286] WARNING:  0A000: "view subquery_and_ctes" has dependency to "table users_table_local" that is not in Citus' metadata
2023-11-25 04:50:41.893 UTC [1099286] DETAIL:  "view subquery_and_ctes" will be created only locally
2023-11-25 04:50:41.893 UTC [1099286] HINT:  Distribute "table users_table_local" first to distribute "view subquery_and_ctes"
2023-11-25 04:50:41.893 UTC [1099286] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:950
2023-11-25 04:50:41.984 UTC [1099285] WARNING:  0A000: "view all_executors_view" has dependency to "table users_table_local" that is not in Citus' metadata
2023-11-25 04:50:41.984 UTC [1099285] DETAIL:  "view all_executors_view" will be created only locally
2023-11-25 04:50:41.984 UTC [1099285] HINT:  Distribute "table users_table_local" first to distribute "view all_executors_view"
2023-11-25 04:50:41.984 UTC [1099285] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:950
2023-11-25 04:50:42.069 UTC [1099285] WARNING:  0A000: "view subquery_and_ctes" has dependency to "table users_table_local" that is not in Citus' metadata
2023-11-25 04:50:42.069 UTC [1099285] DETAIL:  "view subquery_and_ctes" will be created only locally
2023-11-25 04:50:42.069 UTC [1099285] HINT:  Distribute "table users_table_local" first to distribute "view subquery_and_ctes"
2023-11-25 04:50:42.069 UTC [1099285] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:950
2023-11-25 04:50:42.088 UTC [1099285] WARNING:  0A000: "view subquery_and_ctes_second" has dependency to "table users_table_local" that is not in Citus' metadata
2023-11-25 04:50:42.088 UTC [1099285] DETAIL:  "view subquery_and_ctes_second" will be created only locally
2023-11-25 04:50:42.088 UTC [1099285] HINT:  Distribute "table users_table_local" first to distribute "view subquery_and_ctes_second"
2023-11-25 04:50:42.088 UTC [1099285] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:950
2023-11-25 04:50:42.414 UTC [1099598] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a CTE or subquery
2023-11-25 04:50:42.414 UTC [1099598] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:684
2023-11-25 04:50:42.414 UTC [1099598] STATEMENT:  WITH event_id
	     AS(SELECT user_id AS events_user_id,
	                time    AS events_time,
	                event_type
	         FROM   events_table)
	SELECT Count(*)
	FROM   event_id
	WHERE  events_user_id IN (SELECT user_id
	                          FROM   users_table
	                          WHERE  users_table.time = events_time);
2023-11-25 04:50:42.496 UTC [1099595] ERROR:  XX000: the query contains a join that requires repartitioning
2023-11-25 04:50:42.496 UTC [1099595] HINT:  Set citus.enable_repartition_joins to on to enable repartitioning
2023-11-25 04:50:42.496 UTC [1099595] LOCATION:  JobExecutorType, multi_server_executor.c:68
2023-11-25 04:50:42.496 UTC [1099595] STATEMENT:  SELECT key, count(*) FROM (SELECT *, random() FROM append_table a JOIN append_table b USING (key)) u GROUP BY key ORDER BY 1,2 LIMIT 3;
2023-11-25 04:50:42.506 UTC [1099596] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:42.506 UTC [1099596] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:42.506 UTC [1099596] STATEMENT:  SELECT event_type, (SELECT max(time) FROM users_table WHERE user_id = e.user_id GROUP BY user_id)
	FROM users_table u JOIN events_table e USING (value_2)
	ORDER BY 1,2 LIMIT 1;
2023-11-25 04:50:42.506 UTC [1099596] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:42.506 UTC [1099596] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:42.506 UTC [1099596] STATEMENT:  SELECT event_type, (SELECT max(time) FROM users_table WHERE user_id = e.value_2)
	FROM events_table e
	ORDER BY 1,2 LIMIT 1;
2023-11-25 04:50:42.507 UTC [1099595] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:42.507 UTC [1099595] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:42.507 UTC [1099595] STATEMENT:  SELECT key, value FROM append_table a WHERE key IN (SELECT key FROM append_table WHERE value > 100) ORDER BY 1,2;
2023-11-25 04:50:42.507 UTC [1099596] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:42.507 UTC [1099596] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:42.507 UTC [1099596] STATEMENT:  SELECT event_type, (SELECT max(time) FROM users_table WHERE user_id = e.value_2 GROUP BY user_id)
	FROM events_table e
	ORDER BY 1,2 LIMIT 1;
2023-11-25 04:50:42.536 UTC [1099596] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a reference table
2023-11-25 04:50:42.536 UTC [1099596] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:671
2023-11-25 04:50:42.536 UTC [1099596] STATEMENT:  SELECT (SELECT max(u1.time) FROM users_table u1 JOIN users_reference_table u2 USING (user_id) WHERE u2.user_id = e.user_id GROUP BY user_id), 5
	FROM events_reference_table e
	GROUP BY 1
	ORDER BY 1,2 LIMIT 1;
2023-11-25 04:50:42.536 UTC [1099596] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:42.536 UTC [1099596] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:42.536 UTC [1099596] STATEMENT:  SELECT event_type, (SELECT max(time) FROM users_table WHERE user_id = e.user_id GROUP BY user_id)
	FROM users_table u JOIN events_table e USING (value_2)
	ORDER BY 1,2 LIMIT 1;
2023-11-25 04:50:42.541 UTC [1099595] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:42.541 UTC [1099595] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:42.541 UTC [1099595] STATEMENT:  DELETE FROM append_table a USING append_table b WHERE a.key = b.key;
2023-11-25 04:50:42.557 UTC [1099595] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:42.557 UTC [1099595] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:42.557 UTC [1099595] STATEMENT:  UPDATE append_table a sET extra = 1 FROM append_table b WHERE a.key = b.key;
2023-11-25 04:50:42.570 UTC [1099596] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a CTE or subquery
2023-11-25 04:50:42.570 UTC [1099596] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:684
2023-11-25 04:50:42.570 UTC [1099596] STATEMENT:  WITH cte_1 AS (SELECT min(user_id) u, max(time) m FROM users_table)
	SELECT count(*), (SELECT max(time) FROM users_table WHERE user_id = cte_1.u GROUP BY user_id)
	FROM cte_1
	GROUP BY 2
	ORDER BY 1,2 LIMIT 1;
2023-11-25 04:50:42.584 UTC [1099596] ERROR:  0A000: cannot push down subquery on the target list
2023-11-25 04:50:42.584 UTC [1099596] DETAIL:  Subqueries in the SELECT part of the query can only be pushed down if they happen before aggregates and window functions
2023-11-25 04:50:42.584 UTC [1099596] LOCATION:  MultiLogicalPlanOptimize, multi_logical_optimizer.c:479
2023-11-25 04:50:42.584 UTC [1099596] STATEMENT:  SELECT sum(e.user_id) + (SELECT max(value_3) FROM users_reference_table WHERE value_2 = e.value_2 GROUP BY user_id)
	FROM events_table e
	GROUP BY e.value_2
	ORDER BY 1 LIMIT 3;
2023-11-25 04:50:42.584 UTC [1099596] ERROR:  0A000: cannot push down subquery on the target list
2023-11-25 04:50:42.584 UTC [1099596] DETAIL:  Subqueries in the SELECT part of the query can only be pushed down if they happen before aggregates and window functions
2023-11-25 04:50:42.584 UTC [1099596] LOCATION:  MultiLogicalPlanOptimize, multi_logical_optimizer.c:479
2023-11-25 04:50:42.584 UTC [1099596] STATEMENT:  SELECT sum(e.user_id) + (SELECT user_id FROM users_reference_table WHERE user_id = 1 AND value_1 = 1)
	FROM events_table e;
2023-11-25 04:50:42.591 UTC [1099596] ERROR:  0A000: cannot push down subquery on the target list
2023-11-25 04:50:42.591 UTC [1099596] DETAIL:  Subqueries in the SELECT part of the query can only be pushed down if they happen before aggregates and window functions
2023-11-25 04:50:42.591 UTC [1099596] LOCATION:  MultiLogicalPlanOptimize, multi_logical_optimizer.c:479
2023-11-25 04:50:42.591 UTC [1099596] STATEMENT:  SELECT e.value_2, sum((SELECT any_value(value_3) FROM users_reference_table WHERE user_id = e.user_id GROUP BY user_id)) OVER (PARTITION BY e.value_2)
	FROM events_table e
	ORDER BY 1, 2 LIMIT 3;
2023-11-25 04:50:42.614 UTC [1099596] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a reference table
2023-11-25 04:50:42.614 UTC [1099596] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:671
2023-11-25 04:50:42.614 UTC [1099596] STATEMENT:  SELECT (SELECT (SELECT e.user_id + user_id) FROM users_reference_table WHERE user_id = e.user_id GROUP BY user_id)
	FROM events_table e
	GROUP BY 1
	ORDER BY 1 LIMIT 3;
2023-11-25 04:50:42.615 UTC [1099596] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a CTE or subquery
2023-11-25 04:50:42.615 UTC [1099596] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:684
2023-11-25 04:50:42.615 UTC [1099596] STATEMENT:  WITH cte_1 AS (SELECT user_id FROM users_table ORDER BY 1 LIMIT 1)
	SELECT (SELECT (SELECT e.user_id + user_id) FROM cte_1 WHERE user_id = e.user_id GROUP BY user_id)
	FROM events_table e
	GROUP BY 1
	ORDER BY 1 LIMIT 3;
2023-11-25 04:50:42.616 UTC [1099596] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a subquery without FROM
2023-11-25 04:50:42.616 UTC [1099596] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:690
2023-11-25 04:50:42.616 UTC [1099596] STATEMENT:  SELECT (SELECT (SELECT e.user_id + user_id) FROM (SELECT 1 AS user_id) s WHERE user_id = e.user_id GROUP BY user_id)
	FROM events_table e
	GROUP BY 1
	ORDER BY 1 LIMIT 3;
2023-11-25 04:50:42.617 UTC [1099596] WARNING:  0A000: "view view_1" has dependency on unsupported object "schema pg_temp_7"
2023-11-25 04:50:42.617 UTC [1099596] DETAIL:  "view view_1" will be created only locally
2023-11-25 04:50:42.617 UTC [1099596] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:958
2023-11-25 04:50:42.642 UTC [1099596] WARNING:  0A000: "view view_2" has dependency on unsupported object "schema pg_temp_7"
2023-11-25 04:50:42.642 UTC [1099596] DETAIL:  "view view_2" will be created only locally
2023-11-25 04:50:42.642 UTC [1099596] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:958
2023-11-25 04:50:42.650 UTC [1099596] ERROR:  42704: type "view_1" does not exist
2023-11-25 04:50:42.650 UTC [1099596] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:42.650 UTC [1099596] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:42.650 UTC [1099596] STATEMENT:  SELECT (SELECT view_1)
	FROM view_1
	ORDER BY 1 LIMIT 1;
2023-11-25 04:50:42.652 UTC [1099596] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a subquery without FROM
2023-11-25 04:50:42.652 UTC [1099596] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:690
2023-11-25 04:50:42.652 UTC [1099596] STATEMENT:  SELECT (SELECT (SELECT user_id))
	FROM events_table e
	ORDER BY 1 LIMIT 1;
2023-11-25 04:50:42.666 UTC [1099596] ERROR:  0A000: input of anonymous composite types is not implemented
2023-11-25 04:50:42.666 UTC [1099596] LOCATION:  record_in, rowtypes.c:103
2023-11-25 04:50:42.666 UTC [1099596] STATEMENT:  SELECT (SELECT (user_id,value_1) FROM users_table u WHERE u.user_id = e.user_id AND time = 'Thu Nov 23 09:26:42.145043 2017')
	FROM events_table e
	WHERE user_id < 3
	GROUP BY 1
	ORDER BY 1 LIMIT 3;
2023-11-25 04:50:42.681 UTC [1099598] ERROR:  0A000: Subqueries in HAVING cannot refer to outer query
2023-11-25 04:50:42.681 UTC [1099598] LOCATION:  RecursivelyPlanSubqueriesAndCTEs, recursive_planning.c:323
2023-11-25 04:50:42.681 UTC [1099598] STATEMENT:  SELECT
	  count(*)
	FROM
	  events_table e
	WHERE
	  value_3 IN (SELECT min(value_3) v FROM users_table WHERE user_id = e.user_id GROUP BY e.value_2 HAVING min(value_3) > (SELECT e.value_3));
2023-11-25 04:50:42.696 UTC [1099598] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:42.696 UTC [1099598] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:42.696 UTC [1099598] STATEMENT:  SELECT
	  count(*)
	FROM
	  events_table e
	WHERE
	  value_3 IN (
	    SELECT min(r.value_3) v FROM users_reference_table r JOIN (SELECT * FROM users_table WHERE value_2 = e.user_id) u USING (user_id)
	    WHERE u.value_2 > 3
	    GROUP BY e.value_2 HAVING min(r.value_3) > e.value_3);
2023-11-25 04:50:42.697 UTC [1099598] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:42.697 UTC [1099598] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:42.697 UTC [1099598] STATEMENT:  SELECT
	  count(*)
	FROM
	  events_table e
	WHERE
	  value_3 IN (
	    SELECT min(r.value_3) v FROM users_reference_table r JOIN users_table u USING (user_id)
	    WHERE u.value_2 > 3
	    GROUP BY e.value_2 HAVING min(r.value_3) > e.value_3);
2023-11-25 04:50:42.697 UTC [1099598] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:42.697 UTC [1099598] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:42.697 UTC [1099598] STATEMENT:  SELECT
	  count(*)
	FROM
	  events_table e
	WHERE
	  value_3 IN (
	    SELECT min(r.value_3) v FROM users_reference_table r JOIN users_table u USING (user_id)
	    WHERE u.value_2 > 3
	    GROUP BY u.value_2 HAVING min(r.value_3) > e.value_3);
2023-11-25 04:50:42.699 UTC [1099597] ERROR:  0A000: input of anonymous composite types is not implemented
2023-11-25 04:50:42.699 UTC [1099597] LOCATION:  record_in, rowtypes.c:103
2023-11-25 04:50:42.699 UTC [1099597] STATEMENT:  SELECT b FROM (SELECT a FROM items a GROUP BY key) b ORDER BY b;
2023-11-25 04:50:42.727 UTC [1099596] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a subquery without FROM
2023-11-25 04:50:42.727 UTC [1099596] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:690
2023-11-25 04:50:42.727 UTC [1099596] STATEMENT:  SELECT (SELECT DISTINCT user_id FROM users_table WHERE user_id = (SELECT max(user_id) FROM users_table) AND value_2 = a)
	FROM (SELECT 1 AS a) r;
2023-11-25 04:50:42.728 UTC [1099596] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a reference table
2023-11-25 04:50:42.728 UTC [1099596] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:671
2023-11-25 04:50:42.728 UTC [1099596] STATEMENT:  SELECT (SELECT DISTINCT user_id FROM users_table WHERE user_id = (SELECT max(user_id) FROM users_table) AND value_2 = r.user_id)
	FROM users_reference_table r;
2023-11-25 04:50:42.728 UTC [1099596] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:42.728 UTC [1099596] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:42.728 UTC [1099596] STATEMENT:  SELECT (SELECT DISTINCT user_id FROM users_table WHERE user_id = (SELECT max(user_id) FROM users_table WHERE user_id = a))
	FROM (SELECT 1 AS a) r;
2023-11-25 04:50:42.738 UTC [1099598] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:42.738 UTC [1099598] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:42.738 UTC [1099598] STATEMENT:  SELECT
		u1.user_id, u2.user_id
	FROM
		users_table u1, users_table u2
	WHERE
		u1.value_1 < u2.value_1 AND
		(SELECT
			count(*)
		FROM
			events_table e1
		WHERE
			e1.user_id = u2.user_id AND
			u1.user_id = u2.user_id) > 10
	ORDER BY 1,2;
2023-11-25 04:50:42.740 UTC [1099598] WARNING:  0A000: "view correlated_subquery_view" has dependency on unsupported object "schema pg_temp_9"
2023-11-25 04:50:42.740 UTC [1099598] DETAIL:  "view correlated_subquery_view" will be created only locally
2023-11-25 04:50:42.740 UTC [1099598] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:958
2023-11-25 04:50:42.741 UTC [1099596] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a reference table
2023-11-25 04:50:42.741 UTC [1099596] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:671
2023-11-25 04:50:42.741 UTC [1099596] STATEMENT:  SELECT (SELECT DISTINCT user_id FROM users_table WHERE user_id = (SELECT max(user_id) FROM users_table ))
	FROM (SELECT * FROM users_reference_table WHERE value_2 IN (SELECT value_2 FROM events_table WHERE events_table.user_id = users_reference_table.user_id)) r
	ORDER BY 1 LIMIT 3;
2023-11-25 04:50:42.758 UTC [1099596] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a reference table
2023-11-25 04:50:42.758 UTC [1099596] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:671
2023-11-25 04:50:42.758 UTC [1099596] STATEMENT:  SELECT (SELECT (SELECT user_id FROM users_table WHERE user_id = users_reference_table.user_id GROUP BY user_id)
	        FROM users_reference_table WHERE user_id < 2 GROUP BY user_id)
	FROM users_reference_table r
	ORDER BY 1 LIMIT 3;
2023-11-25 04:50:42.759 UTC [1099598] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:42.759 UTC [1099598] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:42.759 UTC [1099598] STATEMENT:  SELECT sum(value_1)
	FROM users_table u1
	WHERE (SELECT COUNT(DISTINCT e1.value_2)
	     FROM events_table e1
	     WHERE e1.user_id = u1.user_id AND false
	          ) > 115;
2023-11-25 04:50:42.762 UTC [1099596] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:42.762 UTC [1099596] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:42.762 UTC [1099596] STATEMENT:  SELECT (SELECT DISTINCT user_id FROM users_table WHERE user_id = (SELECT max(user_id) FROM users_table WHERE user_id = r.user_id))
	FROM (SELECT user_id FROM users_table ORDER BY 1 LIMIT 3) r;
2023-11-25 04:50:42.762 UTC [1099596] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a CTE or subquery
2023-11-25 04:50:42.762 UTC [1099596] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:684
2023-11-25 04:50:42.762 UTC [1099596] STATEMENT:  SELECT (SELECT (SELECT max(user_id) FROM users_table) FROM users_table WHERE user_id = r.user_id)
	FROM (SELECT user_id FROM users_table ORDER BY 1 LIMIT 3) r;
2023-11-25 04:50:42.763 UTC [1099596] ERROR:  0A000: cannot push down this subquery
2023-11-25 04:50:42.763 UTC [1099596] DETAIL:  For Update/Share commands are currently unsupported
2023-11-25 04:50:42.763 UTC [1099596] LOCATION:  DeferErrorIfCannotPushdownSubquery, query_pushdown_planning.c:1048
2023-11-25 04:50:42.763 UTC [1099596] STATEMENT:  SELECT count(*) FROM (SELECT
	  (SELECT user_id FROM users_table WHERE user_id = u1.user_id FOR UPDATE)
	FROM users_table u1
	GROUP BY user_id) as foo;
2023-11-25 04:50:43.774 UTC [1099967] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:43.774 UTC [1099967] CONTEXT:  SQL statement "EXPLAIN (FORMAT JSON) 
	
	    SELECT
	        count(*)
	    FROM
	        (users_table u1 JOIN users_table u2 using(value_1)) a JOIN (SELECT value_1, random() FROM users_table) as u3 USING (value_1);
	"
	PL/pgSQL function explain_json_2(text) line 5 at EXECUTE
2023-11-25 04:50:43.774 UTC [1099967] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:43.774 UTC [1099967] STATEMENT:  SELECT true AS valid FROM explain_json_2($$
	
	    SELECT
	        count(*)
	    FROM
	        (users_table u1 JOIN users_table u2 using(value_1)) a JOIN (SELECT value_1, random() FROM users_table) as u3 USING (value_1);
	$$);
2023-11-25 04:50:43.804 UTC [1099967] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:43.804 UTC [1099967] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:43.804 UTC [1099967] STATEMENT:  SELECT *
	FROM
	  (SELECT *
	   FROM users_table
	   OFFSET 0) AS users_table
	JOIN LATERAL
	  (SELECT *
	   FROM
	     (SELECT *
	      FROM events_table
	      WHERE user_id = users_table.user_id) AS bar
	   LEFT JOIN users_table u2 ON u2.user_id = bar.value_2) AS foo ON TRUE;
2023-11-25 04:50:43.804 UTC [1099967] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:43.804 UTC [1099967] CONTEXT:  SQL statement "EXPLAIN (FORMAT JSON) 
	SELECT *
	FROM
	  (SELECT 1 AS user_id) AS users_table
	JOIN LATERAL
	  (SELECT *
	   FROM
	     (SELECT *
	      FROM events_table
	      WHERE user_id = users_table.user_id) AS bar
	   LEFT JOIN users_table u2 ON u2.user_id = bar.value_2) AS foo ON TRUE
	"
	PL/pgSQL function explain_json_2(text) line 5 at EXECUTE
2023-11-25 04:50:43.804 UTC [1099967] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:43.804 UTC [1099967] STATEMENT:  SELECT true AS valid FROM explain_json_2($$
	SELECT *
	FROM
	  (SELECT 1 AS user_id) AS users_table
	JOIN LATERAL
	  (SELECT *
	   FROM
	     (SELECT *
	      FROM events_table
	      WHERE user_id = users_table.user_id) AS bar
	   LEFT JOIN users_table u2 ON u2.user_id = bar.value_2) AS foo ON TRUE
	$$);
2023-11-25 04:50:44.023 UTC [1100020] WARNING:  0A000: "view recursive_defined_non_recursive_view" has dependency to "table local_table" that is not in Citus' metadata
2023-11-25 04:50:44.023 UTC [1100020] DETAIL:  "view recursive_defined_non_recursive_view" will be created only locally
2023-11-25 04:50:44.023 UTC [1100020] HINT:  Distribute "table local_table" first to distribute "view recursive_defined_non_recursive_view"
2023-11-25 04:50:44.023 UTC [1100020] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:950
2023-11-25 04:50:44.054 UTC [1100020] ERROR:  0A000: direct joins between distributed and local tables are not supported
2023-11-25 04:50:44.054 UTC [1100020] HINT:  Use CTE's or subqueries to select from local tables and use them in joins
2023-11-25 04:50:44.054 UTC [1100020] LOCATION:  DeferErrorIfQueryNotSupported, multi_logical_planner.c:945
2023-11-25 04:50:44.054 UTC [1100020] STATEMENT:  SELECT ref_table.* FROM ref_table WHERE EXISTS (SELECT * FROM local_table l WHERE l.a = ref_table.a);
2023-11-25 04:50:44.092 UTC [1100022] ERROR:  0A000: recursive complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:44.092 UTC [1100022] LOCATION:  CreateDistributedPlan, distributed_planner.c:1092
2023-11-25 04:50:44.092 UTC [1100022] STATEMENT:  SELECT t1.id
	FROM (
	    SELECT t2.id
	    FROM (
	        SELECT t0.id
	        FROM tbl_dist1 t0
	        LIMIT 5
	    ) AS t2
	    INNER JOIN tbl_dist1 AS t3 USING (id)
	) AS t1
	FULL JOIN tbl_dist1 t4 USING (id);
2023-11-25 04:50:44.115 UTC [1100019] ERROR:  0A000: could not run distributed query with FOR UPDATE/SHARE commands
2023-11-25 04:50:44.115 UTC [1100019] HINT:  Consider using an equality filter on the distributed table's partition column.
2023-11-25 04:50:44.115 UTC [1100019] LOCATION:  DeferErrorIfQueryNotSupported, multi_logical_planner.c:945
2023-11-25 04:50:44.115 UTC [1100019] STATEMENT:  WITH cte_1 AS (SELECT * FROM test_table)
	SELECT
		count(*)
	FROM
		cte_1
	WHERE
		key IN (
				SELECT
					key
				FROM
					test_table
	  			 	FOR UPDATE
				);
2023-11-25 04:50:44.221 UTC [1100019] ERROR:  0A000: CTEs that refer to other subqueries are not supported in multi-shard queries
2023-11-25 04:50:44.221 UTC [1100019] LOCATION:  RecursivelyPlanCTEs, recursive_planning.c:1110
2023-11-25 04:50:44.221 UTC [1100019] STATEMENT:  SELECT count(*)
		FROM
		  (SELECT *
		   FROM test_table) AS test_table_cte
		JOIN LATERAL
		  (WITH bar AS  (SELECT *
		      FROM test_table
		      WHERE key = test_table_cte.key)
		  	SELECT *
		   FROM
		      bar
		   LEFT JOIN test_table u2 ON u2.key = bar.value::int) AS foo ON TRUE;
2023-11-25 04:50:44.250 UTC [1100023] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains VALUES
2023-11-25 04:50:44.250 UTC [1100023] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:697
2023-11-25 04:50:44.250 UTC [1100023] STATEMENT:  SELECT
		*
	FROM
		test_values as t1
			JOIN LATERAL (
				SELECT
					t1.key
				FROM
					(VALUES (1, 'one'), (2, 'two'), (3, 'three')) as t(num, v)
					  WHERE num > (SELECT max(key) FROM test_values)) as foo
		ON (true);
2023-11-25 04:50:44.266 UTC [1100023] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains VALUES
2023-11-25 04:50:44.266 UTC [1100023] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:697
2023-11-25 04:50:44.266 UTC [1100023] STATEMENT:  SELECT
	  count(*)
	FROM
	 (SELECT a, b FROM (VALUES (1, 'one'), (2, 'two'), (3, 'three')) as t(a,b)) as values_data(a,b)
	WHERE
	  NOT EXISTS
	      (SELECT
	          value
	       FROM
	          test_values
	       WHERE
	          test_values.key = values_data.a
	      );
2023-11-25 04:50:44.267 UTC [1100019] ERROR:  21000: more than one row returned by a subquery used as an expression
2023-11-25 04:50:44.267 UTC [1100019] CONTEXT:  while executing command on localhost:57638
2023-11-25 04:50:44.267 UTC [1100019] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:44.267 UTC [1100019] STATEMENT:  WITH cte_1 AS (SELECT * FROM test_table),
		 cte_2 AS (SELECT * FROM test_table)
	(SELECT *, (SELECT key FROM cte_1) FROM test_table)
	UNION
	(SELECT *, 1 FROM cte_2);
2023-11-25 04:50:44.411 UTC [1100022] ERROR:  0A000: recursive complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:44.411 UTC [1100022] LOCATION:  CreateDistributedPlan, distributed_planner.c:1092
2023-11-25 04:50:44.411 UTC [1100022] STATEMENT:  SELECT avg(avgsub.id) FROM (
	    SELECT table_0.id FROM (
	        SELECT table_1.id FROM (
	            SELECT table_2.id FROM (
	                SELECT table_3.id FROM (
	                    SELECT table_4.id FROM dist0 AS table_4
	                    LEFT JOIN dist1 AS table_5 USING (id)
	                ) AS table_3 INNER JOIN dist0 AS table_6 USING (id)
	            ) AS table_2 WHERE table_2.id < 10 ORDER BY id LIMIT 47
	        ) AS table_1 RIGHT JOIN dist0 AS table_7 USING (id)
	    ) AS table_0 RIGHT JOIN dist1 AS table_8 USING (id)
	) AS avgsub;
2023-11-25 04:50:44.427 UTC [1100022] ERROR:  0A000: recursive complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:44.427 UTC [1100022] LOCATION:  CreateDistributedPlan, distributed_planner.c:1092
2023-11-25 04:50:44.427 UTC [1100022] STATEMENT:  WITH cte_0 AS (
	    SELECT table_0.id FROM dist1 AS table_0 FULL JOIN dist1 AS table_1 USING (id)
	)
	SELECT avg(table_5.id) FROM (
	    SELECT table_6.id FROM (
	        SELECT table_7.id FROM dist0 AS table_7 ORDER BY id LIMIT 87
	    ) AS table_6 INNER JOIN dist0 AS table_8 USING (id) WHERE table_8.id < 0 ORDER BY id
	) AS table_5 INNER JOIN dist0 AS table_9 USING (id);
2023-11-25 04:50:45.164 UTC [1100432] ERROR:  42601: parallel workers for vacuum must be between 0 and 1024 at character 9
2023-11-25 04:50:45.164 UTC [1100432] LOCATION:  ExecVacuum, vacuum.c:188
2023-11-25 04:50:45.164 UTC [1100432] STATEMENT:  VACUUM (PARALLEL -5) dist_table;
2023-11-25 04:50:45.165 UTC [1100432] ERROR:  42601: parallel option requires a value between 0 and 1024 at character 9
2023-11-25 04:50:45.165 UTC [1100432] LOCATION:  ExecVacuum, vacuum.c:176
2023-11-25 04:50:45.165 UTC [1100432] STATEMENT:  VACUUM (PARALLEL) dist_table;
2023-11-25 04:50:45.171 UTC [1100432] ERROR:  0A000: alter table command is currently unsupported
2023-11-25 04:50:45.171 UTC [1100432] DETAIL:  Only ADD|DROP COLUMN, SET|DROP NOT NULL, SET|DROP DEFAULT, ADD|DROP|VALIDATE CONSTRAINT, SET (), RESET (), ENABLE|DISABLE|NO FORCE|FORCE ROW LEVEL SECURITY, ATTACH|DETACH PARTITION and TYPE subcommands are supported.
2023-11-25 04:50:45.171 UTC [1100432] LOCATION:  ErrorIfUnsupportedAlterTableStmt, table.c:3506
2023-11-25 04:50:45.171 UTC [1100432] STATEMENT:  ALTER TABLE generated_col_table ALTER COLUMN b DROP EXPRESSION;
2023-11-25 04:50:45.175 UTC [1100433] ERROR:  0A000: cannot distribute relation: gen2
2023-11-25 04:50:45.175 UTC [1100433] DETAIL:  Distribution column must not use GENERATED ALWAYS AS (...) STORED.
2023-11-25 04:50:45.175 UTC [1100433] LOCATION:  EnsureRelationCanBeDistributed, create_distributed_table.c:1825
2023-11-25 04:50:45.175 UTC [1100433] STATEMENT:  select create_distributed_table('gen2', 'val2');
2023-11-25 04:50:45.195 UTC [1100432] WARNING:  0A000: "function myvarcharin(cstring,oid,integer)" has dependency on unsupported object "type myvarchar"
2023-11-25 04:50:45.195 UTC [1100432] DETAIL:  "function myvarcharin(cstring,oid,integer)" will be created only locally
2023-11-25 04:50:45.195 UTC [1100432] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:958
2023-11-25 04:50:45.195 UTC [1100432] WARNING:  0A000: "function myvarcharout(myvarchar)" has dependency on unsupported object "type myvarchar"
2023-11-25 04:50:45.195 UTC [1100432] DETAIL:  "function myvarcharout(myvarchar)" will be created only locally
2023-11-25 04:50:45.195 UTC [1100432] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:958
2023-11-25 04:50:45.196 UTC [1100432] ERROR:  0A000: "table my_table" has dependency on unsupported object "type myvarchar"
2023-11-25 04:50:45.196 UTC [1100432] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:958
2023-11-25 04:50:45.196 UTC [1100432] STATEMENT:  SELECT create_distributed_table('my_table', 'a');
2023-11-25 04:50:45.206 UTC [1100432] ERROR:  22023: EXPLAIN option WAL requires ANALYZE
2023-11-25 04:50:45.206 UTC [1100432] LOCATION:  ExplainQuery, explain.c:231
2023-11-25 04:50:45.206 UTC [1100432] STATEMENT:  EXPLAIN (WAL) INSERT INTO test_wal VALUES(1,11);
2023-11-25 04:50:45.213 UTC [1100433] ERROR:  XX000: Citus does not support COPY FROM with WHERE
2023-11-25 04:50:45.213 UTC [1100433] LOCATION:  ProcessCopyStmt, multi_copy.c:2934
2023-11-25 04:50:45.213 UTC [1100433] STATEMENT:  copy cptest from STDIN with csv where val < 4;
2023-11-25 04:50:45.213 UTC [1100433] ERROR:  42601: syntax error at or near "1" at character 1
2023-11-25 04:50:45.213 UTC [1100433] LOCATION:  scanner_yyerror, scan.l:1188
2023-11-25 04:50:45.213 UTC [1100433] STATEMENT:  1,6
	2,3
	3,2
	4,9
	5,4
	select sum(id), sum(val) from cptest;
2023-11-25 04:50:45.258 UTC [1100433] ERROR:  23503: insert or update on table "collection_users" violates foreign key constraint "collection_users_fkey"
2023-11-25 04:50:45.258 UTC [1100433] DETAIL:  Key (key, collection_id)=(1, 1000) is not present in table "collections_list".
2023-11-25 04:50:45.258 UTC [1100433] LOCATION:  ri_ReportViolation, ri_triggers.c:2596
2023-11-25 04:50:45.258 UTC [1100433] STATEMENT:  INSERT INTO collection_users VALUES (1, 1000, 1);
2023-11-25 04:50:45.295 UTC [1100433] ERROR:  23503: insert or update on table "collection_users_60028" violates foreign key constraint "collection_users_fkey_60028"
2023-11-25 04:50:45.295 UTC [1100433] DETAIL:  Key (key, collection_id)=(1, 1000) is not present in table "collections_list_60016".
2023-11-25 04:50:45.295 UTC [1100433] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:45.295 UTC [1100433] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:45.295 UTC [1100433] STATEMENT:  INSERT INTO collection_users VALUES (1, 1000, 1);
2023-11-25 04:50:45.325 UTC [1100433] ERROR:  25006: cannot execute UPDATE in a read-only transaction
2023-11-25 04:50:45.325 UTC [1100433] LOCATION:  PreventCommandIfReadOnly, utility.c:414
2023-11-25 04:50:45.325 UTC [1100433] STATEMENT:  UPDATE test SET y = 35;
2023-11-25 04:50:45.328 UTC [1100433] ERROR:  25006: cannot execute UPDATE in a read-only transaction
2023-11-25 04:50:45.328 UTC [1100433] LOCATION:  PreventCommandIfReadOnly, utility.c:414
2023-11-25 04:50:45.328 UTC [1100433] STATEMENT:  UPDATE test SET y = 40;
2023-11-25 04:50:45.335 UTC [1100433] ERROR:  0A000: Hash distributed partition columns may not use a non deterministic collation
2023-11-25 04:50:45.335 UTC [1100433] LOCATION:  EnsureRelationCanBeDistributed, create_distributed_table.c:1864
2023-11-25 04:50:45.335 UTC [1100433] STATEMENT:  select create_distributed_table('col_test', 'val');
2023-11-25 04:50:45.618 UTC [1100433] ERROR:  42501: permission denied for schema test_pg12
2023-11-25 04:50:45.618 UTC [1100433] LOCATION:  aclcheck_error, aclchk.c:3650
2023-11-25 04:50:45.618 UTC [1100433] STATEMENT:  ALTER TABLE test_pg12.superuser_columnar_table SET(columnar.chunk_group_row_limit = 100);
2023-11-25 04:50:45.618 UTC [1100433] ERROR:  42501: permission denied for schema test_pg12
2023-11-25 04:50:45.618 UTC [1100433] LOCATION:  aclcheck_error, aclchk.c:3650
2023-11-25 04:50:45.618 UTC [1100433] STATEMENT:  ALTER TABLE test_pg12.superuser_columnar_table RESET (columnar.chunk_group_row_limit);
2023-11-25 04:50:45.796 UTC [1100561] ERROR:  0A000: PROCESS_TOAST required with VACUUM FULL
2023-11-25 04:50:45.796 UTC [1100561] LOCATION:  vacuum, vacuum.c:351
2023-11-25 04:50:45.796 UTC [1100561] STATEMENT:  VACUUM (FULL, PROCESS_TOAST false) t1;
2023-11-25 04:50:45.800 UTC [1100561] ERROR:  42601: index_cleanup requires a Boolean value
2023-11-25 04:50:45.800 UTC [1100561] LOCATION:  defGetBoolean, define.c:152
2023-11-25 04:50:45.800 UTC [1100561] STATEMENT:  VACUUM (INDEX_CLEANUP "AUTOX") t1;
2023-11-25 04:50:45.820 UTC [1100561] ERROR:  42704: tablespace "test_tablespace1" does not exist
2023-11-25 04:50:45.820 UTC [1100561] LOCATION:  get_tablespace_oid, tablespace.c:1484
2023-11-25 04:50:45.820 UTC [1100561] STATEMENT:  reindex(TABLESPACE test_tablespace1) index idx;
2023-11-25 04:50:45.826 UTC [1100561] ERROR:  0A000: only simple column references are allowed in CREATE STATISTICS
2023-11-25 04:50:45.826 UTC [1100561] LOCATION:  AppendColumnNames, deparse_statistics_stmts.c:242
2023-11-25 04:50:45.826 UTC [1100561] STATEMENT:  CREATE STATISTICS s3 (ndistinct) ON date_trunc('month', a), date_trunc('day', a) FROM tbl1;
2023-11-25 04:50:45.836 UTC [1100561] ERROR:  XX000: ALTER TABLE .. DETACH PARTITION .. CONCURRENTLY commands are currently unsupported.
2023-11-25 04:50:45.836 UTC [1100561] LOCATION:  ErrorIfUnsupportedAlterTableStmt, table.c:3420
2023-11-25 04:50:45.836 UTC [1100561] STATEMENT:  ALTER TABLE par DETACH PARTITION par_2 CONCURRENTLY;
2023-11-25 04:50:45.836 UTC [1100561] ERROR:  XX000: ALTER TABLE .. DETACH PARTITION .. FINALIZE commands are currently unsupported.
2023-11-25 04:50:45.836 UTC [1100561] LOCATION:  ErrorIfUnsupportedAlterTableStmt, table.c:3398
2023-11-25 04:50:45.836 UTC [1100561] STATEMENT:  ALTER TABLE par DETACH PARTITION par_2 FINALIZE;
2023-11-25 04:50:45.910 UTC [1100574] LOG:  00000: deferred drop of orphaned resource pg14.col_compression_980010 on localhost:57637 completed
2023-11-25 04:50:45.910 UTC [1100574] LOCATION:  DropOrphanedResourcesForCleanup, shard_cleaner.c:298
2023-11-25 04:50:45.910 UTC [1100574] STATEMENT:  CALL citus_cleanup_orphaned_resources()
2023-11-25 04:50:45.952 UTC [1100561] LOG:  00000: deferred drop of orphaned resource pg14.col_compression_980010 on localhost:57638 completed
2023-11-25 04:50:45.952 UTC [1100561] CONTEXT:  SQL statement "CALL pg_catalog.citus_cleanup_orphaned_resources()"
	PL/pgSQL function public.wait_for_resource_cleanup() line 7 at CALL
2023-11-25 04:50:45.952 UTC [1100561] LOCATION:  DropOrphanedResourcesForCleanup, shard_cleaner.c:298
2023-11-25 04:50:45.952 UTC [1100561] STATEMENT:  SELECT public.wait_for_resource_cleanup();
2023-11-25 04:50:46.189 UTC [1100561] ERROR:  22004: jsonb subscript in assignment must not be null
2023-11-25 04:50:46.189 UTC [1100561] CONTEXT:  while executing command on localhost:57638
2023-11-25 04:50:46.189 UTC [1100561] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:46.189 UTC [1100561] STATEMENT:  update test_jsonb_subscript set test_json[NULL] = '1';
2023-11-25 04:50:46.211 UTC [1100561] ERROR:  42P01: invalid reference to FROM-clause entry for table "j1_tbl" at character 57
2023-11-25 04:50:46.211 UTC [1100561] HINT:  There is an entry for table "j1_tbl", but it cannot be referenced from this part of the query.
2023-11-25 04:50:46.211 UTC [1100561] LOCATION:  errorMissingRTE, parse_relation.c:3597
2023-11-25 04:50:46.211 UTC [1100561] STATEMENT:  SELECT * FROM (J1_TBL JOIN J2_TBL USING (i)) AS x WHERE J1_TBL.t = 'one' ORDER BY 1,2,3,4;
2023-11-25 04:50:46.212 UTC [1100561] ERROR:  42703: column x.t does not exist at character 55
2023-11-25 04:50:46.212 UTC [1100561] LOCATION:  errorMissingColumn, parse_relation.c:3656
2023-11-25 04:50:46.212 UTC [1100561] STATEMENT:  SELECT * FROM J1_TBL JOIN J2_TBL USING (i) AS x WHERE x.t = 'one' ORDER BY 1,2,3,4;
2023-11-25 04:50:46.212 UTC [1100561] ERROR:  42P01: missing FROM-clause entry for table "x" at character 63
2023-11-25 04:50:46.212 UTC [1100561] LOCATION:  errorMissingRTE, parse_relation.c:3608
2023-11-25 04:50:46.212 UTC [1100561] STATEMENT:  SELECT * FROM (J1_TBL JOIN J2_TBL USING (i) AS x) AS xx WHERE x.i = 1 ORDER BY 1,2,3,4;
2023-11-25 04:50:46.212 UTC [1100561] ERROR:  42712: table name "a1" specified more than once
2023-11-25 04:50:46.212 UTC [1100561] LOCATION:  checkNameSpaceConflicts, parse_relation.c:443
2023-11-25 04:50:46.212 UTC [1100561] STATEMENT:  SELECT * FROM J1_TBL a1 JOIN J2_TBL a2 USING (i) AS a1 ORDER BY 1,2,3,4;
2023-11-25 04:50:46.231 UTC [1100561] ERROR:  0A000: REINDEX TABLE queries on distributed partitioned tables are not supported
2023-11-25 04:50:46.231 UTC [1100561] LOCATION:  PreprocessReindexStmt, index.c:635
2023-11-25 04:50:46.231 UTC [1100561] STATEMENT:  REINDEX TABLE dist_part_table;
2023-11-25 04:50:46.239 UTC [1100561] ERROR:  0A000: recursive CTEs are not supported in distributed queries
2023-11-25 04:50:46.239 UTC [1100561] LOCATION:  RecursivelyPlanCTEs, recursive_planning.c:1089
2023-11-25 04:50:46.239 UTC [1100561] STATEMENT:  WITH RECURSIVE search_graph(f, t, label) AS (
	    SELECT * FROM graph0 g WHERE f = 1
	    UNION ALL
	    SELECT g.*
	        FROM graph0 g, search_graph sg
	        WHERE g.f = sg.t and  g.f = 1
	) SEARCH DEPTH FIRST BY f, t SET seq
	SELECT * FROM search_graph ORDER BY seq;
2023-11-25 04:50:46.240 UTC [1100561] ERROR:  0A000: recursive CTEs are not supported in distributed queries
2023-11-25 04:50:46.240 UTC [1100561] LOCATION:  RecursivelyPlanCTEs, recursive_planning.c:1089
2023-11-25 04:50:46.240 UTC [1100561] STATEMENT:  WITH RECURSIVE search_graph(f, t, label) AS (
	    SELECT * FROM graph0 g WHERE f = 1
	    UNION ALL
	    SELECT g.*
	        FROM graph0 g, search_graph sg
	        WHERE g.f = sg.t and  g.f = 1
	) SEARCH DEPTH FIRST BY f, t SET seq
	DELETE FROM graph0 WHERE t IN (SELECT t FROM search_graph ORDER BY seq);
2023-11-25 04:50:46.245 UTC [1100561] ERROR:  0A000: recursive CTEs are not supported in distributed queries
2023-11-25 04:50:46.245 UTC [1100561] LOCATION:  RecursivelyPlanCTEs, recursive_planning.c:1089
2023-11-25 04:50:46.245 UTC [1100561] STATEMENT:  WITH RECURSIVE search_graph(f, t, label) AS (
	    SELECT * FROM graph1 g WHERE f = 1
	    UNION ALL
	    SELECT g.*
	        FROM graph1 g, search_graph sg
	        WHERE g.f = sg.t and  g.f = 1
	) SEARCH DEPTH FIRST BY f, t SET seq
	SELECT * FROM search_graph ORDER BY seq;
2023-11-25 04:50:46.245 UTC [1100561] ERROR:  0A000: recursive CTEs are not supported in distributed queries
2023-11-25 04:50:46.245 UTC [1100561] LOCATION:  RecursivelyPlanCTEs, recursive_planning.c:1089
2023-11-25 04:50:46.245 UTC [1100561] STATEMENT:  WITH RECURSIVE search_graph(f, t, label) AS (
	    SELECT * FROM graph1 g WHERE f = 1
	    UNION ALL
	    SELECT g.*
	        FROM graph1 g, search_graph sg
	        WHERE g.f = sg.t and  g.f = 1
	) SEARCH DEPTH FIRST BY f, t SET seq
	DELETE FROM graph1 WHERE t IN (SELECT t FROM search_graph ORDER BY seq);
2023-11-25 04:50:46.246 UTC [1100561] ERROR:  0A000: recursive CTEs are not supported in distributed queries
2023-11-25 04:50:46.246 UTC [1100561] LOCATION:  RecursivelyPlanCTEs, recursive_planning.c:1089
2023-11-25 04:50:46.246 UTC [1100561] STATEMENT:  SELECT * FROM (
	    WITH RECURSIVE search_graph(f, t, label) AS (
	        SELECT *
	        FROM graph0 g
	        WHERE f = 1
	        UNION ALL SELECT g.*
	        FROM graph0 g, search_graph sg
	        WHERE g.f = sg.t AND g.f = 1
	    ) SEARCH DEPTH FIRST BY f, t SET seq
	    SELECT * FROM search_graph ORDER BY seq
	) as foo;
2023-11-25 04:50:46.253 UTC [1100561] ERROR:  42883: function "proc_with_out_param(date,int)" does not exist at character 36
2023-11-25 04:50:46.253 UTC [1100561] LOCATION:  regprocedurein, regproc.c:275
2023-11-25 04:50:46.253 UTC [1100561] STATEMENT:  SELECT create_distributed_function('proc_with_out_param(date,int)');
2023-11-25 04:50:46.533 UTC [1100561] ERROR:  42710: extension "postgres_fdw" already exists
2023-11-25 04:50:46.533 UTC [1100561] LOCATION:  CreateExtension, extension.c:1726
2023-11-25 04:50:46.533 UTC [1100561] STATEMENT:  CREATE EXTENSION postgres_fdw;
2023-11-25 04:50:47.734 UTC [1100668] LOG:  00000: The LSN of the target subscriptions on node localhost:57638 have caught up with the source LSN 
2023-11-25 04:50:47.734 UTC [1100668] LOCATION:  WaitForGroupedLogicalRepTargetsToCatchUp, multi_logical_replication.c:1842
2023-11-25 04:50:47.734 UTC [1100668] STATEMENT:  SELECT citus_move_shard_placement(980042, 'localhost', 57637, 'localhost', 57638, shard_transfer_mode := 'force_logical');
2023-11-25 04:50:47.735 UTC [1100668] LOG:  00000: The LSN of the target subscriptions on node localhost:57638 have caught up with the source LSN 
2023-11-25 04:50:47.735 UTC [1100668] LOCATION:  WaitForGroupedLogicalRepTargetsToCatchUp, multi_logical_replication.c:1842
2023-11-25 04:50:47.735 UTC [1100668] STATEMENT:  SELECT citus_move_shard_placement(980042, 'localhost', 57637, 'localhost', 57638, shard_transfer_mode := 'force_logical');
2023-11-25 04:50:47.736 UTC [1100668] LOG:  00000: The LSN of the target subscriptions on node localhost:57638 have caught up with the source LSN 
2023-11-25 04:50:47.736 UTC [1100668] LOCATION:  WaitForGroupedLogicalRepTargetsToCatchUp, multi_logical_replication.c:1842
2023-11-25 04:50:47.736 UTC [1100668] STATEMENT:  SELECT citus_move_shard_placement(980042, 'localhost', 57637, 'localhost', 57638, shard_transfer_mode := 'force_logical');
2023-11-25 04:50:47.771 UTC [1100683] LOG:  00000: cleaned up orphaned resource pg14.dist_table_1_980042 on localhost:57638 which was left behind after a failed operation
2023-11-25 04:50:47.771 UTC [1100683] LOCATION:  DropOrphanedResourcesForCleanup, shard_cleaner.c:305
2023-11-25 04:50:47.771 UTC [1100683] STATEMENT:  CALL citus_cleanup_orphaned_resources()
2023-11-25 04:50:47.772 UTC [1100683] LOG:  00000: cleaned up orphaned resource pg14.dist_table_2_980044 on localhost:57638 which was left behind after a failed operation
2023-11-25 04:50:47.772 UTC [1100683] LOCATION:  DropOrphanedResourcesForCleanup, shard_cleaner.c:305
2023-11-25 04:50:47.772 UTC [1100683] STATEMENT:  CALL citus_cleanup_orphaned_resources()
2023-11-25 04:50:48.813 UTC [1100668] LOG:  00000: The LSN of the target subscriptions on node localhost:57638 have caught up with the source LSN 
2023-11-25 04:50:48.813 UTC [1100668] LOCATION:  WaitForGroupedLogicalRepTargetsToCatchUp, multi_logical_replication.c:1842
2023-11-25 04:50:48.813 UTC [1100668] STATEMENT:  SELECT citus_move_shard_placement(980042, 'localhost', 57637, 'localhost', 57638, shard_transfer_mode := 'force_logical');
2023-11-25 04:50:48.814 UTC [1100668] LOG:  00000: The LSN of the target subscriptions on node localhost:57638 have caught up with the source LSN 
2023-11-25 04:50:48.814 UTC [1100668] LOCATION:  WaitForGroupedLogicalRepTargetsToCatchUp, multi_logical_replication.c:1842
2023-11-25 04:50:48.814 UTC [1100668] STATEMENT:  SELECT citus_move_shard_placement(980042, 'localhost', 57637, 'localhost', 57638, shard_transfer_mode := 'force_logical');
2023-11-25 04:50:48.815 UTC [1100668] LOG:  00000: The LSN of the target subscriptions on node localhost:57638 have caught up with the source LSN 
2023-11-25 04:50:48.815 UTC [1100668] LOCATION:  WaitForGroupedLogicalRepTargetsToCatchUp, multi_logical_replication.c:1842
2023-11-25 04:50:48.815 UTC [1100668] STATEMENT:  SELECT citus_move_shard_placement(980042, 'localhost', 57637, 'localhost', 57638, shard_transfer_mode := 'force_logical');
2023-11-25 04:50:49.146 UTC [1100731] ERROR:  42P17: parameter "locale" must be specified
2023-11-25 04:50:49.146 UTC [1100731] LOCATION:  DefineCollation, collationcmds.c:242
2023-11-25 04:50:49.146 UTC [1100731] STATEMENT:  CREATE COLLATION german_phonebook_test (provider = icu, lc_collate = 'de-u-co-phonebk');
2023-11-25 04:50:49.146 UTC [1100731] ERROR:  42P17: parameter "locale" must be specified
2023-11-25 04:50:49.146 UTC [1100731] LOCATION:  DefineCollation, collationcmds.c:242
2023-11-25 04:50:49.146 UTC [1100731] STATEMENT:  CREATE COLLATION german_phonebook_test (provider = icu, lc_collate = 'de-u-co-phonebk', lc_ctype = 'de-u-co-phonebk');
2023-11-25 04:50:49.323 UTC [1100731] ERROR:  XX000: cannot rename trigger "new_record_sale_trigger" on table "sale_newyork"
2023-11-25 04:50:49.323 UTC [1100731] HINT:  Rename the trigger on the partitioned table "sale" instead.
2023-11-25 04:50:49.323 UTC [1100731] LOCATION:  renametrig, trigger.c:1567
2023-11-25 04:50:49.323 UTC [1100731] STATEMENT:  ALTER TRIGGER "new_record_sale_trigger" ON "pg15"."sale_newyork" RENAME TO "another_trigger_name";
2023-11-25 04:50:49.330 UTC [1100731] ERROR:  2BP01: cannot drop column col_1 of table generated_stored_ref because other objects depend on it
2023-11-25 04:50:49.330 UTC [1100731] DETAIL:  column col_3 of table generated_stored_ref depends on column col_1 of table generated_stored_ref
	column col_5 of table generated_stored_ref depends on column col_1 of table generated_stored_ref
2023-11-25 04:50:49.330 UTC [1100731] HINT:  Use DROP ... CASCADE to drop the dependent objects too.
2023-11-25 04:50:49.330 UTC [1100731] LOCATION:  reportDependentObjects, dependency.c:1189
2023-11-25 04:50:49.330 UTC [1100731] STATEMENT:  ALTER TABLE generated_stored_ref DROP COLUMN col_1;
2023-11-25 04:50:49.360 UTC [1100767] LOG:  00000: cleaned up orphaned resource pg14.dist_table_1_980042 on localhost:57638 which was left behind after a failed operation
2023-11-25 04:50:49.360 UTC [1100767] LOCATION:  DropOrphanedResourcesForCleanup, shard_cleaner.c:305
2023-11-25 04:50:49.360 UTC [1100767] STATEMENT:  CALL citus_cleanup_orphaned_resources()
2023-11-25 04:50:49.361 UTC [1100767] LOG:  00000: cleaned up orphaned resource pg14.dist_table_2_980044 on localhost:57638 which was left behind after a failed operation
2023-11-25 04:50:49.361 UTC [1100767] LOCATION:  DropOrphanedResourcesForCleanup, shard_cleaner.c:305
2023-11-25 04:50:49.361 UTC [1100767] STATEMENT:  CALL citus_cleanup_orphaned_resources()
2023-11-25 04:50:49.521 UTC [1100731] ERROR:  0A000: MERGE command is not supported on reference tables yet
2023-11-25 04:50:49.521 UTC [1100731] LOCATION:  CheckIfRTETypeIsUnsupported, merge_planner.c:330
2023-11-25 04:50:49.521 UTC [1100731] STATEMENT:  MERGE INTO tbl1 USING tbl2 ON (true)
	WHEN MATCHED THEN DELETE;
2023-11-25 04:50:49.529 UTC [1100731] ERROR:  0A000: MERGE command is not supported on reference tables yet
2023-11-25 04:50:49.529 UTC [1100731] LOCATION:  CheckIfRTETypeIsUnsupported, merge_planner.c:330
2023-11-25 04:50:49.529 UTC [1100731] STATEMENT:  MERGE INTO tbl1 USING tbl2 ON (true)
	WHEN MATCHED THEN DELETE;
2023-11-25 04:50:49.590 UTC [1100809] ERROR:  0A000: MERGE command is only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:49.590 UTC [1100809] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:49.590 UTC [1100809] STATEMENT:  MERGE INTO tbl1 USING tbl2 ON (true)
	WHEN MATCHED THEN DELETE;
2023-11-25 04:50:49.591 UTC [1100809] ERROR:  0A000: MERGE command is only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:49.591 UTC [1100809] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:49.591 UTC [1100809] STATEMENT:  WITH targq AS (
	    SELECT * FROM tbl2
	)
	MERGE INTO tbl1 USING targq ON (true)
	WHEN MATCHED THEN DELETE;
2023-11-25 04:50:49.591 UTC [1100809] ERROR:  0A000: MERGE not supported in WITH query at character 6
2023-11-25 04:50:49.591 UTC [1100809] LOCATION:  transformWithClause, parse_cte.c:131
2023-11-25 04:50:49.591 UTC [1100809] STATEMENT:  WITH foo AS (
	  MERGE INTO tbl1 USING tbl2 ON (true)
	  WHEN MATCHED THEN DELETE
	) SELECT * FROM foo;
2023-11-25 04:50:49.591 UTC [1100809] ERROR:  0A000: MERGE not supported in COPY
2023-11-25 04:50:49.591 UTC [1100809] LOCATION:  DoCopy, copy.c:281
2023-11-25 04:50:49.591 UTC [1100809] STATEMENT:  COPY (
	  MERGE INTO tbl1 USING tbl2 ON (true)
	  WHEN MATCHED THEN DELETE
	) TO stdout;
2023-11-25 04:50:49.591 UTC [1100809] ERROR:  0A000: MERGE command is only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:49.591 UTC [1100809] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:49.591 UTC [1100809] STATEMENT:  MERGE INTO tbl1 t
	USING tbl2
	ON (true)
	WHEN MATCHED THEN
	    DO NOTHING;
2023-11-25 04:50:49.591 UTC [1100809] ERROR:  0A000: updating the distribution column is not allowed in MERGE actions
2023-11-25 04:50:49.591 UTC [1100809] LOCATION:  MergeQualAndTargetListFunctionsSupported, merge_planner.c:651
2023-11-25 04:50:49.591 UTC [1100809] STATEMENT:  MERGE INTO tbl1 t
	USING tbl2
	ON (true)
	WHEN MATCHED THEN
	    UPDATE SET x = (SELECT count(*) FROM tbl2);
2023-11-25 04:50:49.592 UTC [1100809] ERROR:  0A000: cannot distribute relation: numeric_negative_scale
2023-11-25 04:50:49.592 UTC [1100809] DETAIL:  Distribution column must not use numeric type with negative scale
2023-11-25 04:50:49.592 UTC [1100809] LOCATION:  EnsureRelationCanBeDistributed, create_distributed_table.c:1838
2023-11-25 04:50:49.592 UTC [1100809] STATEMENT:  SELECT create_distributed_table('numeric_negative_scale','numeric_column');
2023-11-25 04:50:49.609 UTC [1100809] ERROR:  0A000: cannot distribute relation: numeric_negative_scale_3037880381
2023-11-25 04:50:49.609 UTC [1100809] DETAIL:  Distribution column must not use numeric type with negative scale
2023-11-25 04:50:49.609 UTC [1100809] LOCATION:  EnsureRelationCanBeDistributed, create_distributed_table.c:1838
2023-11-25 04:50:49.609 UTC [1100809] STATEMENT:  SELECT alter_distributed_table('numeric_negative_scale',
	                                distribution_column := 'numeric_column');
2023-11-25 04:50:49.680 UTC [1100809] ERROR:  0A000: complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
2023-11-25 04:50:49.680 UTC [1100809] LOCATION:  JoinOrderList, multi_join_order.c:310
2023-11-25 04:50:49.680 UTC [1100809] STATEMENT:  SELECT count(*)
	FROM numeric_repartition_first f,
	     numeric_repartition_second s
	WHERE f.id = s.numeric_column;
2023-11-25 04:50:49.897 UTC [1100809] ERROR:  22P04: column name mismatch in header line field 2: got "data", expected "data_"
2023-11-25 04:50:49.897 UTC [1100809] CONTEXT:  COPY copy_test2, line 1: "id	data"
2023-11-25 04:50:49.897 UTC [1100809] LOCATION:  NextCopyFromRawFields, copyfromparse.c:806
2023-11-25 04:50:49.897 UTC [1100809] STATEMENT:  COPY copy_test2 FROM '/tmp/''copy_test.txt' WITH ( HEADER match, FORMAT text);
2023-11-25 04:50:49.964 UTC [1101005] ERROR:  42P01: relation "seq_non_exists" does not exist
2023-11-25 04:50:49.964 UTC [1101005] LOCATION:  RangeVarGetRelidExtended, namespace.c:433
2023-11-25 04:50:49.964 UTC [1101005] STATEMENT:  ALTER SEQUENCE seq_non_exists SET LOGGED;
2023-11-25 04:50:50.037 UTC [1101019] ERROR:  0A000: cannot create foreign key constraint
2023-11-25 04:50:50.037 UTC [1101019] DETAIL:  SET NULL or SET DEFAULT is not supported in ON DELETE operation when distribution key is included in the foreign key constraint
2023-11-25 04:50:50.037 UTC [1101019] LOCATION:  EnsureSupportedFKeyOnDistKey, foreign_constraint.c:548
2023-11-25 04:50:50.037 UTC [1101019] STATEMENT:  SELECT create_distributed_table('FKTABLE', 'tid');
2023-11-25 04:50:50.091 UTC [1101024] ERROR:  23505: duplicate key value violates unique constraint "idx2_null_distinct_test_960150"
2023-11-25 04:50:50.091 UTC [1101024] DETAIL:  Key (id, c2)=(1, null) already exists.
2023-11-25 04:50:50.091 UTC [1101024] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:50.091 UTC [1101024] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:50.091 UTC [1101024] STATEMENT:  INSERT INTO null_distinct_test VALUES (1, NULL, NULL, 'data4') ;
2023-11-25 04:50:50.103 UTC [1101024] ERROR:  23505: could not create unique index "uniq_c1_960150"
2023-11-25 04:50:50.103 UTC [1101024] DETAIL:  Key (id, c1)=(1, null) is duplicated.
2023-11-25 04:50:50.103 UTC [1101024] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:50.103 UTC [1101024] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:50.103 UTC [1101024] STATEMENT:  ALTER TABLE null_distinct_test ADD CONSTRAINT uniq_c1 UNIQUE NULLS NOT DISTINCT (id,c1);
2023-11-25 04:50:50.120 UTC [1101024] ERROR:  23505: duplicate key value violates unique constraint "reference_uniq_test_x_y_key_960154"
2023-11-25 04:50:50.120 UTC [1101024] DETAIL:  Key (x, y)=(1, null) already exists.
2023-11-25 04:50:50.120 UTC [1101024] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:50.120 UTC [1101024] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:50.120 UTC [1101024] STATEMENT:  INSERT INTO reference_uniq_test VALUES (1, NULL);
2023-11-25 04:50:50.121 UTC [1101024] WARNING:  01000: not propagating CLUSTER command for partitioned table to worker nodes
2023-11-25 04:50:50.121 UTC [1101024] HINT:  Provide a child partition table names in order to CLUSTER distributed partitioned tables.
2023-11-25 04:50:50.121 UTC [1101024] LOCATION:  PreprocessClusterStmt, cluster.c:85
2023-11-25 04:50:50.124 UTC [1101024] ERROR:  0A000: modifications on partitions when replication factor is greater than 1 is not supported
2023-11-25 04:50:50.124 UTC [1101024] HINT:  Run the query on the parent table "sale" instead.
2023-11-25 04:50:50.124 UTC [1101024] LOCATION:  DeferErrorIfPartitionTableNotSingleReplicated, distributed_planner.c:1224
2023-11-25 04:50:50.124 UTC [1101024] STATEMENT:  CLUSTER sale_newyork USING sale_newyork_pkey;
2023-11-25 04:50:50.147 UTC [1101024] WARNING:  01000: not propagating CLUSTER command for partitioned table to worker nodes
2023-11-25 04:50:50.147 UTC [1101024] HINT:  Provide a child partition table names in order to CLUSTER distributed partitioned tables.
2023-11-25 04:50:50.147 UTC [1101024] LOCATION:  PreprocessClusterStmt, cluster.c:85
2023-11-25 04:50:50.234 UTC [1101076] ERROR:  42501: permission denied for table events
2023-11-25 04:50:50.234 UTC [1101076] LOCATION:  aclcheck_error, aclchk.c:3650
2023-11-25 04:50:50.234 UTC [1101076] STATEMENT:  SELECT * FROM sec_invoker_view ORDER BY event_id;
2023-11-25 04:50:50.276 UTC [1101094] ERROR:  42501: permission denied for table events
2023-11-25 04:50:50.276 UTC [1101094] LOCATION:  aclcheck_error, aclchk.c:3650
2023-11-25 04:50:50.276 UTC [1101094] STATEMENT:  SELECT * FROM sec_definer_view ORDER BY event_id;
2023-11-25 04:50:50.289 UTC [1101094] ERROR:  0A000: cannot modify views when the query contains citus tables
2023-11-25 04:50:50.289 UTC [1101094] LOCATION:  DeferErrorIfModifyView, multi_router_planner.c:1137
2023-11-25 04:50:50.289 UTC [1101094] STATEMENT:  UPDATE sec_invoker_view SET event_id = 5;
2023-11-25 04:50:50.309 UTC [1101094] ERROR:  XX000: cannot create foreign key constraint since Citus does not support ON DELETE / UPDATE SET DEFAULT actions on the columns that default to sequences
2023-11-25 04:50:50.309 UTC [1101094] LOCATION:  ErrorIfUnsupportedForeignConstraintExists, foreign_constraint.c:274
2023-11-25 04:50:50.309 UTC [1101094] STATEMENT:  SELECT create_reference_table('set_on_default_test_referencing');
2023-11-25 04:50:50.569 UTC [1101094] ERROR:  XX000: cannot create foreign key constraint since Citus does not support ON DELETE / UPDATE SET DEFAULT actions on the columns that default to sequences
2023-11-25 04:50:50.569 UTC [1101094] LOCATION:  ErrorIfUnsupportedForeignConstraintExists, foreign_constraint.c:274
2023-11-25 04:50:50.569 UTC [1101094] STATEMENT:  CREATE TABLE set_on_default_test_referencing(
	    col_1 int, col_2 int, col_3 serial, col_4 int,
	    FOREIGN KEY(col_1, col_3)
	    REFERENCES set_on_default_test_referenced(col_1, col_3)
	    ON DELETE SET DEFAULT (col_3)
	);
2023-11-25 04:50:50.595 UTC [1101168] ERROR:  42883: operator does not exist: pg15.user_enum ~~ unknown at character 77
2023-11-25 04:50:50.595 UTC [1101168] HINT:  No operator matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:50:50.595 UTC [1101168] LOCATION:  op_error, parse_oper.c:647
2023-11-25 04:50:50.595 UTC [1101168] STATEMENT:  DECLARE c1 CURSOR FOR
	SELECT c0, c1 FROM pg15.foreign_table_test WHERE ((c1 ~~ 'foo')) LIMIT 1::bigint
2023-11-25 04:50:50.595 UTC [1101094] ERROR:  42883: operator does not exist: pg15.user_enum ~~ unknown
2023-11-25 04:50:50.595 UTC [1101094] HINT:  No operator matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:50:50.595 UTC [1101094] CONTEXT:  remote SQL command: SELECT c0, c1 FROM pg15.foreign_table_test WHERE ((c1 ~~ 'foo')) LIMIT 1::bigint
2023-11-25 04:50:50.595 UTC [1101094] LOCATION:  pgfdw_report_error, connection.c:895
2023-11-25 04:50:50.595 UTC [1101094] STATEMENT:  SELECT * FROM foreign_table WHERE c1 LIKE 'foo' LIMIT 1;
2023-11-25 04:50:50.596 UTC [1101168] ERROR:  42883: operator does not exist: pg15.user_enum ~~ unknown at character 77
2023-11-25 04:50:50.596 UTC [1101168] HINT:  No operator matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:50:50.596 UTC [1101168] LOCATION:  op_error, parse_oper.c:647
2023-11-25 04:50:50.596 UTC [1101168] STATEMENT:  DECLARE c1 CURSOR FOR
	SELECT c0, c1 FROM pg15.foreign_table_test WHERE ((c1 ~~ 'foo')) LIMIT 1::bigint
2023-11-25 04:50:50.596 UTC [1101094] ERROR:  42883: operator does not exist: pg15.user_enum ~~ unknown
2023-11-25 04:50:50.596 UTC [1101094] HINT:  No operator matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:50:50.596 UTC [1101094] CONTEXT:  remote SQL command: SELECT c0, c1 FROM pg15.foreign_table_test WHERE ((c1 ~~ 'foo')) LIMIT 1::bigint
2023-11-25 04:50:50.596 UTC [1101094] LOCATION:  pgfdw_report_error, connection.c:895
2023-11-25 04:50:50.596 UTC [1101094] STATEMENT:  SELECT * FROM foreign_table WHERE c1::text LIKE 'foo' LIMIT 1;
2023-11-25 04:50:50.632 UTC [1094385] LOG:  00000: checkpoint starting: immediate force wait
2023-11-25 04:50:50.632 UTC [1094385] LOCATION:  LogCheckpointStart, xlog.c:6089
2023-11-25 04:50:50.673 UTC [1094385] LOG:  00000: checkpoint complete: wrote 2800 buffers (17.1%); 0 WAL file(s) added, 0 removed, 3 recycled; write=0.011 s, sync=0.001 s, total=0.041 s; sync files=0, longest=0.000 s, average=0.000 s; distance=54951 kB, estimate=54951 kB
2023-11-25 04:50:50.673 UTC [1094385] LOCATION:  LogCheckpointEnd, xlog.c:6170
2023-11-25 04:50:51.048 UTC [1101197] ERROR:  22P02: invalid input syntax for type jsonpath: ""
2023-11-25 04:50:51.048 UTC [1101197] CONTEXT:  while executing command on localhost:57638
2023-11-25 04:50:51.048 UTC [1101197] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:51.048 UTC [1101197] STATEMENT:  SELECT sample, sample::jsonpath FROM jsonpath_test WHERE sample = '';
2023-11-25 04:50:51.055 UTC [1101197] ERROR:  22P02: invalid input syntax for type jsonpath: ""
2023-11-25 04:50:51.055 UTC [1101197] LOCATION:  jsonPathFromCstring, jsonpath.c:180
2023-11-25 04:50:51.055 UTC [1101197] STATEMENT:  WITH samples as (SELECT id, sample FROM jsonpath_test WHERE sample = '' OFFSET 0)
	SELECT sample, sample::jsonpath FROM samples;
2023-11-25 04:50:51.056 UTC [1101197] ERROR:  42601: LAST is allowed only in array subscripts
2023-11-25 04:50:51.056 UTC [1101197] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:51.056 UTC [1101197] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:51.056 UTC [1101197] STATEMENT:  SELECT sample, sample::jsonpath FROM jsonpath_test WHERE sample = 'last';
2023-11-25 04:50:51.065 UTC [1101197] ERROR:  42601: LAST is allowed only in array subscripts
2023-11-25 04:50:51.065 UTC [1101197] LOCATION:  flattenJsonPathParseItem, jsonpath.c:366
2023-11-25 04:50:51.065 UTC [1101197] STATEMENT:  WITH samples as (SELECT id, sample FROM jsonpath_test WHERE sample = 'last' OFFSET 0)
	SELECT sample, sample::jsonpath FROM samples;
2023-11-25 04:50:51.066 UTC [1101197] ERROR:  42601: trailing junk after numeric literal at or near "1.t" of jsonpath input
2023-11-25 04:50:51.066 UTC [1101197] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:51.066 UTC [1101197] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:51.066 UTC [1101197] STATEMENT:  SELECT sample, sample::jsonpath FROM jsonpath_test WHERE sample = '1.type()';
2023-11-25 04:50:51.076 UTC [1101197] ERROR:  42601: trailing junk after numeric literal at or near "1.t" of jsonpath input
2023-11-25 04:50:51.076 UTC [1101197] LOCATION:  jsonpath_yyerror, jsonpath_scan.l:286
2023-11-25 04:50:51.076 UTC [1101197] STATEMENT:  WITH samples as (SELECT id, sample FROM jsonpath_test WHERE sample = '1.type()' OFFSET 0)
	SELECT sample, sample::jsonpath FROM samples;
2023-11-25 04:50:51.077 UTC [1101197] ERROR:  2201B: invalid regular expression: parentheses () not balanced
2023-11-25 04:50:51.077 UTC [1101197] CONTEXT:  while executing command on localhost:57638
2023-11-25 04:50:51.077 UTC [1101197] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:51.077 UTC [1101197] STATEMENT:  SELECT sample, sample::jsonpath FROM jsonpath_test WHERE sample = '$ ? (@ like_regex "(invalid pattern")';
2023-11-25 04:50:51.086 UTC [1101197] ERROR:  2201B: invalid regular expression: parentheses () not balanced
2023-11-25 04:50:51.086 UTC [1101197] LOCATION:  RE_compile_and_cache, regexp.c:207
2023-11-25 04:50:51.086 UTC [1101197] STATEMENT:  WITH samples as (SELECT id, sample FROM jsonpath_test WHERE sample = '$ ? (@ like_regex "(invalid pattern")' OFFSET 0)
	SELECT sample, sample::jsonpath FROM samples;
2023-11-25 04:50:51.086 UTC [1101197] ERROR:  0A000: XQuery "x" flag (expanded regular expressions) is not implemented
2023-11-25 04:50:51.086 UTC [1101197] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:51.086 UTC [1101197] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:51.086 UTC [1101197] STATEMENT:  SELECT sample, sample::jsonpath FROM jsonpath_test WHERE sample = '$ ? (@ like_regex "pattern" flag "xsms")';
2023-11-25 04:50:51.095 UTC [1101197] ERROR:  0A000: XQuery "x" flag (expanded regular expressions) is not implemented
2023-11-25 04:50:51.095 UTC [1101197] LOCATION:  jspConvertRegexFlags, jsonpath_gram.y:582
2023-11-25 04:50:51.095 UTC [1101197] STATEMENT:  WITH samples as (SELECT id, sample FROM jsonpath_test WHERE sample = '$ ? (@ like_regex "pattern" flag "xsms")' OFFSET 0)
	SELECT sample, sample::jsonpath FROM samples;
2023-11-25 04:50:51.096 UTC [1101197] ERROR:  42601: @ is not allowed in root expressions
2023-11-25 04:50:51.096 UTC [1101197] CONTEXT:  while executing command on localhost:57638
2023-11-25 04:50:51.096 UTC [1101197] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:51.096 UTC [1101197] STATEMENT:  SELECT sample, sample::jsonpath FROM jsonpath_test WHERE sample = '@ + 1';
2023-11-25 04:50:51.104 UTC [1101197] ERROR:  42601: @ is not allowed in root expressions
2023-11-25 04:50:51.104 UTC [1101197] LOCATION:  flattenJsonPathParseItem, jsonpath.c:360
2023-11-25 04:50:51.104 UTC [1101197] STATEMENT:  WITH samples as (SELECT id, sample FROM jsonpath_test WHERE sample = '@ + 1' OFFSET 0)
	SELECT sample, sample::jsonpath FROM samples;
2023-11-25 04:50:51.105 UTC [1101197] ERROR:  42601: trailing junk after numeric literal at or near "00" of jsonpath input
2023-11-25 04:50:51.105 UTC [1101197] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:51.105 UTC [1101197] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:51.105 UTC [1101197] STATEMENT:  SELECT sample, sample::jsonpath FROM jsonpath_test WHERE sample = '00';
2023-11-25 04:50:51.115 UTC [1101197] ERROR:  42601: trailing junk after numeric literal at or near "00" of jsonpath input
2023-11-25 04:50:51.115 UTC [1101197] LOCATION:  jsonpath_yyerror, jsonpath_scan.l:286
2023-11-25 04:50:51.115 UTC [1101197] STATEMENT:  WITH samples as (SELECT id, sample FROM jsonpath_test WHERE sample = '00' OFFSET 0)
	SELECT sample, sample::jsonpath FROM samples;
2023-11-25 04:50:51.116 UTC [1101197] ERROR:  42601: trailing junk after numeric literal at or near "1.e" of jsonpath input
2023-11-25 04:50:51.116 UTC [1101197] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:51.116 UTC [1101197] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:51.116 UTC [1101197] STATEMENT:  SELECT sample, sample::jsonpath FROM jsonpath_test WHERE sample = '1.e';
2023-11-25 04:50:51.126 UTC [1101197] ERROR:  42601: trailing junk after numeric literal at or near "1.e" of jsonpath input
2023-11-25 04:50:51.126 UTC [1101197] LOCATION:  jsonpath_yyerror, jsonpath_scan.l:286
2023-11-25 04:50:51.126 UTC [1101197] STATEMENT:  WITH samples as (SELECT id, sample FROM jsonpath_test WHERE sample = '1.e' OFFSET 0)
	SELECT sample, sample::jsonpath FROM samples;
2023-11-25 04:50:51.127 UTC [1101197] ERROR:  42601: trailing junk after numeric literal at or near "1.2e3a" of jsonpath input
2023-11-25 04:50:51.127 UTC [1101197] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:51.127 UTC [1101197] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:51.127 UTC [1101197] STATEMENT:  SELECT sample, sample::jsonpath FROM jsonpath_test WHERE sample = '1.2e3a';
2023-11-25 04:50:51.136 UTC [1101197] ERROR:  42601: trailing junk after numeric literal at or near "1.2e3a" of jsonpath input
2023-11-25 04:50:51.136 UTC [1101197] LOCATION:  jsonpath_yyerror, jsonpath_scan.l:286
2023-11-25 04:50:51.136 UTC [1101197] STATEMENT:  WITH samples as (SELECT id, sample FROM jsonpath_test WHERE sample = '1.2e3a' OFFSET 0)
	SELECT sample, sample::jsonpath FROM samples;
2023-11-25 04:50:51.950 UTC [1101196] ERROR:  08006: connection to the remote node localhost:57637 failed with the following error: connection not open
2023-11-25 04:50:51.950 UTC [1101196] LOCATION:  ReportConnectionError, remote_commands.c:266
2023-11-25 04:50:51.950 UTC [1101196] STATEMENT:  SELECT count(*) FROM socket_test_table;
2023-11-25 04:50:53.157 UTC [1101557] WARNING:  01000: fake_tuple_insert
2023-11-25 04:50:53.157 UTC [1101557] LOCATION:  fake_tuple_insert, fake_am.c:204
2023-11-25 04:50:53.168 UTC [1101557] WARNING:  01000: fake_scan_getnextslot
2023-11-25 04:50:53.168 UTC [1101557] CONTEXT:  SQL statement "SELECT TRUE FROM test_tableam.test_hash_dist LIMIT 1"
2023-11-25 04:50:53.168 UTC [1101557] LOCATION:  fake_scan_getnextslot, fake_am.c:88
2023-11-25 04:50:53.176 UTC [1101557] WARNING:  01000: fake_scan_getnextslot
2023-11-25 04:50:53.176 UTC [1101557] LOCATION:  fake_scan_getnextslot, fake_am.c:88
2023-11-25 04:50:53.176 UTC [1101557] WARNING:  01000: fake_scan_getnextslot
2023-11-25 04:50:53.176 UTC [1101557] LOCATION:  fake_scan_getnextslot, fake_am.c:88
2023-11-25 04:50:53.178 UTC [1101557] WARNING:  01000: fake_scan_getnextslot
2023-11-25 04:50:53.178 UTC [1101557] DETAIL:  from localhost:57637
2023-11-25 04:50:53.178 UTC [1101557] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 04:50:53.178 UTC [1101557] WARNING:  01000: fake_scan_getnextslot
2023-11-25 04:50:53.178 UTC [1101557] DETAIL:  from localhost:57637
2023-11-25 04:50:53.178 UTC [1101557] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 04:50:53.178 UTC [1101557] WARNING:  01000: fake_scan_getnextslot
2023-11-25 04:50:53.178 UTC [1101557] DETAIL:  from localhost:57638
2023-11-25 04:50:53.178 UTC [1101557] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 04:50:53.178 UTC [1101557] WARNING:  01000: fake_scan_getnextslot
2023-11-25 04:50:53.178 UTC [1101557] DETAIL:  from localhost:57637
2023-11-25 04:50:53.178 UTC [1101557] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 04:50:53.178 UTC [1101557] WARNING:  01000: fake_scan_getnextslot
2023-11-25 04:50:53.178 UTC [1101557] DETAIL:  from localhost:57638
2023-11-25 04:50:53.178 UTC [1101557] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 04:50:53.180 UTC [1101557] WARNING:  01000: fake_tuple_insert
2023-11-25 04:50:53.180 UTC [1101557] DETAIL:  from localhost:57637
2023-11-25 04:50:53.180 UTC [1101557] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 04:50:53.181 UTC [1101557] WARNING:  01000: fake_scan_getnextslot
2023-11-25 04:50:53.181 UTC [1101557] DETAIL:  from localhost:57637
2023-11-25 04:50:53.181 UTC [1101557] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 04:50:53.181 UTC [1101557] ERROR:  XX000: fake_tuple_delete not implemented
2023-11-25 04:50:53.181 UTC [1101557] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:53.181 UTC [1101557] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:53.181 UTC [1101557] STATEMENT:  delete from test_hash_dist where id=1;
2023-11-25 04:50:53.183 UTC [1101557] WARNING:  01000: fake_scan_getnextslot
2023-11-25 04:50:53.183 UTC [1101557] DETAIL:  from localhost:57638
2023-11-25 04:50:53.183 UTC [1101557] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 04:50:53.183 UTC [1101557] WARNING:  01000: fake_tuple_insert
2023-11-25 04:50:53.183 UTC [1101557] LOCATION:  fake_tuple_insert, fake_am.c:204
2023-11-25 04:50:53.184 UTC [1101557] WARNING:  01000: fake_scan_getnextslot
2023-11-25 04:50:53.184 UTC [1101557] CONTEXT:  SQL statement "SELECT TRUE FROM test_tableam.test_ref LIMIT 1"
2023-11-25 04:50:53.184 UTC [1101557] LOCATION:  fake_scan_getnextslot, fake_am.c:88
2023-11-25 04:50:53.191 UTC [1101557] WARNING:  01000: fake_scan_getnextslot
2023-11-25 04:50:53.191 UTC [1101557] LOCATION:  fake_scan_getnextslot, fake_am.c:88
2023-11-25 04:50:53.191 UTC [1101557] WARNING:  01000: fake_scan_getnextslot
2023-11-25 04:50:53.191 UTC [1101557] LOCATION:  fake_scan_getnextslot, fake_am.c:88
2023-11-25 04:50:53.192 UTC [1101557] WARNING:  01000: fake_scan_getnextslot
2023-11-25 04:50:53.192 UTC [1101557] DETAIL:  from localhost:57637
2023-11-25 04:50:53.192 UTC [1101557] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 04:50:53.192 UTC [1101557] WARNING:  01000: fake_scan_getnextslot
2023-11-25 04:50:53.192 UTC [1101557] DETAIL:  from localhost:57637
2023-11-25 04:50:53.192 UTC [1101557] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 04:50:53.192 UTC [1101557] WARNING:  01000: fake_tuple_insert
2023-11-25 04:50:53.192 UTC [1101557] DETAIL:  from localhost:57637
2023-11-25 04:50:53.192 UTC [1101557] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 04:50:53.193 UTC [1101557] WARNING:  01000: fake_tuple_insert
2023-11-25 04:50:53.193 UTC [1101557] DETAIL:  from localhost:57638
2023-11-25 04:50:53.193 UTC [1101557] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 04:50:53.194 UTC [1101557] WARNING:  01000: fake_scan_getnextslot
2023-11-25 04:50:53.194 UTC [1101557] DETAIL:  from localhost:57637
2023-11-25 04:50:53.194 UTC [1101557] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 04:50:53.194 UTC [1101557] ERROR:  XX000: fake_tuple_delete not implemented
2023-11-25 04:50:53.194 UTC [1101557] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:53.194 UTC [1101557] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:53.194 UTC [1101557] STATEMENT:  delete from test_ref;
2023-11-25 04:50:53.196 UTC [1101557] WARNING:  01000: fake_scan_getnextslot
2023-11-25 04:50:53.196 UTC [1101557] DETAIL:  from localhost:57637
2023-11-25 04:50:53.196 UTC [1101557] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 04:50:53.196 UTC [1101557] ERROR:  XX000: fake_fetch_row_version not implemented
2023-11-25 04:50:53.196 UTC [1101557] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:50:53.196 UTC [1101557] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:50:53.196 UTC [1101557] STATEMENT:  update test_ref set a=2;
2023-11-25 04:50:53.197 UTC [1101557] WARNING:  01000: fake_scan_getnextslot
2023-11-25 04:50:53.197 UTC [1101557] CONTEXT:  SQL statement "SELECT TRUE FROM test_tableam.test_range_dist LIMIT 1"
2023-11-25 04:50:53.197 UTC [1101557] LOCATION:  fake_scan_getnextslot, fake_am.c:88
2023-11-25 04:50:53.197 UTC [1101557] WARNING:  01000: fake_scan_getnextslot
2023-11-25 04:50:53.197 UTC [1101557] CONTEXT:  SQL statement "SELECT TRUE FROM test_tableam.test_range_dist LIMIT 1"
2023-11-25 04:50:53.197 UTC [1101557] LOCATION:  fake_scan_getnextslot, fake_am.c:88
2023-11-25 04:50:53.204 UTC [1101557] WARNING:  01000: fake_scan_getnextslot
2023-11-25 04:50:53.204 UTC [1101557] DETAIL:  from localhost:57637
2023-11-25 04:50:53.204 UTC [1101557] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 04:50:53.204 UTC [1101557] WARNING:  01000: fake_scan_getnextslot
2023-11-25 04:50:53.204 UTC [1101557] DETAIL:  from localhost:57638
2023-11-25 04:50:53.204 UTC [1101557] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 04:50:53.204 UTC [1101557] WARNING:  01000: fake_tuple_insert
2023-11-25 04:50:53.204 UTC [1101557] DETAIL:  from localhost:57637
2023-11-25 04:50:53.204 UTC [1101557] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 04:50:53.251 UTC [1101584] WARNING:  01000: fake_tuple_insert
2023-11-25 04:50:53.251 UTC [1101584] LOCATION:  fake_tuple_insert, fake_am.c:204
2023-11-25 04:50:53.273 UTC [1101584] WARNING:  01000: fake_scan_getnextslot
2023-11-25 04:50:53.273 UTC [1101584] CONTEXT:  SQL statement "SELECT TRUE FROM test_tableam.test_partitioned_p2 LIMIT 1"
2023-11-25 04:50:53.273 UTC [1101584] LOCATION:  fake_scan_getnextslot, fake_am.c:88
2023-11-25 04:50:53.278 UTC [1101584] WARNING:  01000: fake_scan_getnextslot
2023-11-25 04:50:53.278 UTC [1101584] LOCATION:  fake_scan_getnextslot, fake_am.c:88
2023-11-25 04:50:53.279 UTC [1101584] WARNING:  01000: fake_scan_getnextslot
2023-11-25 04:50:53.279 UTC [1101584] LOCATION:  fake_scan_getnextslot, fake_am.c:88
2023-11-25 04:50:53.281 UTC [1101584] WARNING:  01000: fake_tuple_insert
2023-11-25 04:50:53.281 UTC [1101584] DETAIL:  from localhost:57637
2023-11-25 04:50:53.281 UTC [1101584] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 04:50:53.282 UTC [1101584] WARNING:  01000: fake_tuple_insert
2023-11-25 04:50:53.282 UTC [1101584] DETAIL:  from localhost:57638
2023-11-25 04:50:53.282 UTC [1101584] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 04:50:53.284 UTC [1101584] WARNING:  01000: fake_scan_getnextslot
2023-11-25 04:50:53.284 UTC [1101584] DETAIL:  from localhost:57637
2023-11-25 04:50:53.284 UTC [1101584] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 04:50:53.284 UTC [1101584] WARNING:  01000: fake_scan_getnextslot
2023-11-25 04:50:53.284 UTC [1101584] DETAIL:  from localhost:57638
2023-11-25 04:50:53.284 UTC [1101584] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 04:50:53.284 UTC [1101584] WARNING:  01000: fake_scan_getnextslot
2023-11-25 04:50:53.284 UTC [1101584] DETAIL:  from localhost:57638
2023-11-25 04:50:53.284 UTC [1101584] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 04:50:53.284 UTC [1101584] WARNING:  01000: fake_scan_getnextslot
2023-11-25 04:50:53.284 UTC [1101584] DETAIL:  from localhost:57637
2023-11-25 04:50:53.284 UTC [1101584] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 04:50:53.284 UTC [1101584] WARNING:  01000: fake_scan_getnextslot
2023-11-25 04:50:53.284 UTC [1101584] DETAIL:  from localhost:57638
2023-11-25 04:50:53.284 UTC [1101584] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 04:50:53.284 UTC [1101584] WARNING:  01000: fake_scan_getnextslot
2023-11-25 04:50:53.284 UTC [1101584] DETAIL:  from localhost:57638
2023-11-25 04:50:53.284 UTC [1101584] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 04:50:53.300 UTC [1101584] ERROR:  0A000: specifying a table access method is not supported on a partitioned table
2023-11-25 04:50:53.300 UTC [1101584] LOCATION:  DefineRelation, tablecmds.c:947
2023-11-25 04:50:53.300 UTC [1101584] STATEMENT:  CREATE TABLE test_partitioned(id int, p int, val int)
	PARTITION BY RANGE (p) USING fake_am;
2023-11-25 04:50:53.367 UTC [1101620] ERROR:  XX000: the backend has already been assigned a transaction id
2023-11-25 04:50:53.367 UTC [1101620] LOCATION:  assign_distributed_transaction_id, backend_data.c:168
2023-11-25 04:50:53.367 UTC [1101620] STATEMENT:  SELECT assign_distributed_transaction_id(51, 51, '2017-01-01 00:00:00+0');
2023-11-25 04:50:53.367 UTC [1101620] ERROR:  22012: division by zero
2023-11-25 04:50:53.367 UTC [1101620] LOCATION:  int4div, int.c:840
2023-11-25 04:50:53.367 UTC [1101620] STATEMENT:  SELECT 5 / 0;
2023-11-25 04:50:53.376 UTC [1101619] ERROR:  XX000: the intermediate result size exceeds citus.max_intermediate_result_size (currently 2 kB)
2023-11-25 04:50:53.376 UTC [1101619] DETAIL:  Citus restricts the size of intermediate results of complex subqueries and CTEs to avoid accidentally pulling large result sets into once place.
2023-11-25 04:50:53.376 UTC [1101619] HINT:  To run the current query, set citus.max_intermediate_result_size to a higher value or -1 to disable.
2023-11-25 04:50:53.376 UTC [1101619] LOCATION:  EnsureIntermediateSizeLimitNotExceeded, tuple_destination.c:164
2023-11-25 04:50:53.376 UTC [1101619] STATEMENT:  WITH cte AS MATERIALIZED
	(
		SELECT * FROM users_table
	),
	cte2 AS MATERIALIZED (
		SELECT * FROM events_table
	)
	SELECT cte.user_id, cte.value_2 FROM cte,cte2 ORDER BY 1,2 LIMIT 10;
2023-11-25 04:50:53.379 UTC [1101618] WARNING:  CIINF: Query could not find the intermediate result file "squares", it was mostly likely deleted due to an error in a parallel process within the same distributed transaction
2023-11-25 04:50:53.379 UTC [1101618] LOCATION:  ReadIntermediateResultsIntoFuncOutput, intermediate_results.c:869
2023-11-25 04:50:53.472 UTC [1101618] ERROR:  CIINF: Query could not find the intermediate result file "squares", it was mostly likely deleted due to an error in a parallel process within the same distributed transaction
2023-11-25 04:50:53.472 UTC [1101618] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:148
2023-11-25 04:50:53.472 UTC [1101618] STATEMENT:  SELECT x, x2
	FROM interesting_squares JOIN (SELECT * FROM read_intermediate_result('squares', 'binary') AS res (x text, x2 int)) squares ON (x = interested_in)
	WHERE user_id = 'jon' OR true
	ORDER BY x;
2023-11-25 04:50:53.473 UTC [1101618] ERROR:  CIINF: Query could not find the intermediate result file "squares", it was mostly likely deleted due to an error in a parallel process within the same distributed transaction
2023-11-25 04:50:53.473 UTC [1101618] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:148
2023-11-25 04:50:53.473 UTC [1101618] STATEMENT:  SELECT x, x2
	FROM interesting_squares JOIN (SELECT * FROM read_intermediate_result('squares', 'binary') AS res (x text, x2 int)) squares ON (x = interested_in)
	WHERE user_id = 'jon'
	ORDER BY x;
2023-11-25 04:50:53.474 UTC [1101618] ERROR:  22021: invalid byte sequence for encoding "UTF8": 0x00
2023-11-25 04:50:53.474 UTC [1101618] LOCATION:  report_invalid_encoding, mbutils.c:1665
2023-11-25 04:50:53.474 UTC [1101618] STATEMENT:  SELECT * FROM read_intermediate_result('squares', 'binary') AS res (x text, x2 int);
2023-11-25 04:50:53.474 UTC [1101618] ERROR:  22P02: invalid input syntax for type integer: "PGCOPY"
2023-11-25 04:50:53.474 UTC [1101618] LOCATION:  pg_strtoint32, numutils.c:232
2023-11-25 04:50:53.474 UTC [1101618] STATEMENT:  SELECT * FROM read_intermediate_result('squares', 'csv') AS res (x int, x2 int);
2023-11-25 04:50:53.481 UTC [1101618] ERROR:  22P04: COPY file signature not recognized
2023-11-25 04:50:53.481 UTC [1101618] LOCATION:  ReceiveCopyBinaryHeader, copyfromparse.c:198
2023-11-25 04:50:53.481 UTC [1101618] STATEMENT:  SELECT * FROM read_intermediate_result('stored_squares', 'binary') AS res (s intermediate_results.square_type);
BEGIN
COPY 0
SELECT 5
COMMIT
2023-11-25 04:50:53.512 UTC [1101618] ERROR:  XX000: cannot execute utility commands
2023-11-25 04:50:53.512 UTC [1101618] LOCATION:  ExecuteQueryIntoDestReceiver, multi_executor.c:664
2023-11-25 04:50:53.512 UTC [1101618] STATEMENT:  select broadcast_intermediate_result('a', 'create table foo(int serial)');
2023-11-25 04:50:53.512 UTC [1101618] ERROR:  XX000: cannot execute utility commands
2023-11-25 04:50:53.512 UTC [1101618] LOCATION:  ExecuteQueryIntoDestReceiver, multi_executor.c:664
2023-11-25 04:50:53.512 UTC [1101618] STATEMENT:  select broadcast_intermediate_result('a', 'prepare foo as select 1');
2023-11-25 04:50:53.512 UTC [1101618] ERROR:  XX000: cannot execute utility commands
2023-11-25 04:50:53.512 UTC [1101618] LOCATION:  ExecuteQueryIntoDestReceiver, multi_executor.c:664
2023-11-25 04:50:53.512 UTC [1101618] STATEMENT:  select create_intermediate_result('a', 'create table foo(int serial)');
2023-11-25 04:50:53.514 UTC [1101618] WARNING:  CIINF: Query could not find the intermediate result file "squares_1", it was mostly likely deleted due to an error in a parallel process within the same distributed transaction
2023-11-25 04:50:53.514 UTC [1101618] LOCATION:  ReadIntermediateResultsIntoFuncOutput, intermediate_results.c:869
2023-11-25 04:50:53.515 UTC [1101618] WARNING:  CIINF: Query could not find the intermediate result file "notexistingfile", it was mostly likely deleted due to an error in a parallel process within the same distributed transaction
2023-11-25 04:50:53.515 UTC [1101618] LOCATION:  ReadIntermediateResultsIntoFuncOutput, intermediate_results.c:869
2023-11-25 04:50:53.515 UTC [1101618] WARNING:  CIINF: Query could not find the intermediate result file "notexistingfile", it was mostly likely deleted due to an error in a parallel process within the same distributed transaction
2023-11-25 04:50:53.515 UTC [1101618] LOCATION:  ReadIntermediateResultsIntoFuncOutput, intermediate_results.c:869
2023-11-25 04:50:53.515 UTC [1101618] ERROR:  22004: null array element not allowed in this context
2023-11-25 04:50:53.515 UTC [1101618] LOCATION:  deconstruct_array, arrayfuncs.c:3525
2023-11-25 04:50:53.515 UTC [1101618] STATEMENT:  SELECT * FROM read_intermediate_results(ARRAY['squares_1', NULL], 'binary') AS res (x int, x2 int);
2023-11-25 04:50:53.515 UTC [1101618] WARNING:  CIINF: Query could not find the intermediate result file "squares_1", it was mostly likely deleted due to an error in a parallel process within the same distributed transaction
2023-11-25 04:50:53.515 UTC [1101618] LOCATION:  ReadIntermediateResultsIntoFuncOutput, intermediate_results.c:869
2023-11-25 04:50:53.526 UTC [1101619] ERROR:  XX000: the intermediate result size exceeds citus.max_intermediate_result_size (currently 2 kB)
2023-11-25 04:50:53.526 UTC [1101619] DETAIL:  Citus restricts the size of intermediate results of complex subqueries and CTEs to avoid accidentally pulling large result sets into once place.
2023-11-25 04:50:53.526 UTC [1101619] HINT:  To run the current query, set citus.max_intermediate_result_size to a higher value or -1 to disable.
2023-11-25 04:50:53.526 UTC [1101619] LOCATION:  EnsureIntermediateSizeLimitNotExceeded, tuple_destination.c:164
2023-11-25 04:50:53.526 UTC [1101619] STATEMENT:  WITH cte AS MATERIALIZED (SELECT * FROM users_table WHERE user_id IN (1,2,3,4,5))
	SELECT * FROM cte ORDER BY 1,2,3,4,5 LIMIT 10;
2023-11-25 04:50:53.528 UTC [1101619] ERROR:  XX000: the intermediate result size exceeds citus.max_intermediate_result_size (currently 0 kB)
2023-11-25 04:50:53.528 UTC [1101619] DETAIL:  Citus restricts the size of intermediate results of complex subqueries and CTEs to avoid accidentally pulling large result sets into once place.
2023-11-25 04:50:53.528 UTC [1101619] HINT:  To run the current query, set citus.max_intermediate_result_size to a higher value or -1 to disable.
2023-11-25 04:50:53.528 UTC [1101619] LOCATION:  EnsureIntermediateSizeLimitNotExceeded, tuple_destination.c:164
2023-11-25 04:50:53.528 UTC [1101619] STATEMENT:  WITH cte AS MATERIALIZED (SELECT * FROM users_table WHERE user_id=1),
	cte2 AS MATERIALIZED (SELECT * FROM users_table WHERE user_id=2),
	cte3 AS MATERIALIZED (SELECT * FROM users_table WHERE user_id=3),
	cte4 AS MATERIALIZED (SELECT * FROM users_table WHERE user_id=4),
	cte5 AS MATERIALIZED (SELECT * FROM users_table WHERE user_id=5)
	SELECT * FROM (
	(SELECT * FROM cte)
	UNION
	(SELECT * FROM cte2)
	UNION
	(SELECT * FROM cte3)
	UNION
	(SELECT * FROM cte4)
	UNION
	(SELECT * FROM cte5)
	)a ORDER BY 1,2,3,4,5 LIMIT 10;
2023-11-25 04:50:53.535 UTC [1101618] WARNING:  CIINF: Query could not find the intermediate result file "squares_1", it was mostly likely deleted due to an error in a parallel process within the same distributed transaction
2023-11-25 04:50:53.535 UTC [1101618] LOCATION:  ReadIntermediateResultsIntoFuncOutput, intermediate_results.c:869
2023-11-25 04:50:53.536 UTC [1101618] WARNING:  CIINF: Query could not find the intermediate result file "squares_2", it was mostly likely deleted due to an error in a parallel process within the same distributed transaction
2023-11-25 04:50:53.536 UTC [1101618] LOCATION:  ReadIntermediateResultsIntoFuncOutput, intermediate_results.c:869
2023-11-25 04:50:53.538 UTC [1101618] ERROR:  XX000: cannot connect to localhost:57635 to fetch intermediate results
2023-11-25 04:50:53.538 UTC [1101618] LOCATION:  fetch_intermediate_results, intermediate_results.c:929
2023-11-25 04:50:53.538 UTC [1101618] STATEMENT:  SELECT * FROM fetch_intermediate_results(ARRAY['squares_1', 'squares_2']::text[], 'localhost', 57635);
2023-11-25 04:50:53.540 UTC [1101618] WARNING:  CIINF: Query could not find the intermediate result file "squares_1", it was mostly likely deleted due to an error in a parallel process within the same distributed transaction
2023-11-25 04:50:53.540 UTC [1101618] LOCATION:  ReadIntermediateResultsIntoFuncOutput, intermediate_results.c:869
2023-11-25 04:50:53.540 UTC [1101618] WARNING:  CIINF: Query could not find the intermediate result file "squares_2", it was mostly likely deleted due to an error in a parallel process within the same distributed transaction
2023-11-25 04:50:53.540 UTC [1101618] LOCATION:  ReadIntermediateResultsIntoFuncOutput, intermediate_results.c:869
2023-11-25 04:50:53.543 UTC [1101619] ERROR:  XX000: the intermediate result size exceeds citus.max_intermediate_result_size (currently 3 kB)
2023-11-25 04:50:53.543 UTC [1101619] DETAIL:  Citus restricts the size of intermediate results of complex subqueries and CTEs to avoid accidentally pulling large result sets into once place.
2023-11-25 04:50:53.543 UTC [1101619] HINT:  To run the current query, set citus.max_intermediate_result_size to a higher value or -1 to disable.
2023-11-25 04:50:53.543 UTC [1101619] LOCATION:  EnsureIntermediateSizeLimitNotExceeded, tuple_destination.c:164
2023-11-25 04:50:53.543 UTC [1101619] STATEMENT:  WITH cte AS MATERIALIZED (
		WITH cte2 AS MATERIALIZED (
			SELECT * FROM users_table WHERE user_id IN (3,4,5,6)
		),
		cte3 AS MATERIALIZED(
			SELECT * FROM events_table WHERE event_type = 1
		)
		SELECT * FROM cte2, cte3 WHERE cte2.value_1 IN (SELECT value_2 FROM cte3)
	)
	SELECT count(*) FROM cte;
2023-11-25 04:50:53.546 UTC [1101619] ERROR:  XX000: the intermediate result size exceeds citus.max_intermediate_result_size (currently 2 kB)
2023-11-25 04:50:53.546 UTC [1101619] DETAIL:  Citus restricts the size of intermediate results of complex subqueries and CTEs to avoid accidentally pulling large result sets into once place.
2023-11-25 04:50:53.546 UTC [1101619] HINT:  To run the current query, set citus.max_intermediate_result_size to a higher value or -1 to disable.
2023-11-25 04:50:53.546 UTC [1101619] LOCATION:  EnsureIntermediateSizeLimitNotExceeded, tuple_destination.c:164
2023-11-25 04:50:53.546 UTC [1101619] STATEMENT:  WITH cte AS MATERIALIZED (
		WITH cte2 AS MATERIALIZED (
			SELECT * FROM users_table WHERE user_id IN (1, 2)
		),
		cte3 AS MATERIALIZED (
			SELECT * FROM users_table WHERE user_id = 3
		)
		SELECT * FROM cte2 UNION (SELECT * FROM cte3)
	),
	cte4 AS MATERIALIZED (
		SELECT * FROM events_table
	)
	SELECT * FROM cte UNION ALL
	SELECT * FROM cte4 ORDER BY 1,2,3,4,5 LIMIT 5;
2023-11-25 04:50:53.549 UTC [1101619] ERROR:  XX000: the intermediate result size exceeds citus.max_intermediate_result_size (currently 1 kB)
2023-11-25 04:50:53.549 UTC [1101619] DETAIL:  Citus restricts the size of intermediate results of complex subqueries and CTEs to avoid accidentally pulling large result sets into once place.
2023-11-25 04:50:53.549 UTC [1101619] HINT:  To run the current query, set citus.max_intermediate_result_size to a higher value or -1 to disable.
2023-11-25 04:50:53.549 UTC [1101619] LOCATION:  EnsureIntermediateSizeLimitNotExceeded, tuple_destination.c:164
2023-11-25 04:50:53.549 UTC [1101619] STATEMENT:  WITH cte AS MATERIALIZED (
		WITH cte2 AS MATERIALIZED (
			SELECT * FROM users_table WHERE user_id IN (1, 2)
		),
		cte3 AS MATERIALIZED (
			SELECT * FROM users_table WHERE user_id = 3
		)
		SELECT * FROM cte2 UNION (SELECT * FROM cte3)
	),
	cte4 AS MATERIALIZED (
		SELECT * FROM events_table
	)
	SELECT * FROM cte UNION ALL
	SELECT * FROM cte4 ORDER BY 1,2,3,4,5 LIMIT 5;
2023-11-25 04:50:53.554 UTC [1101618] ERROR:  22004: worker array object cannot contain null values
2023-11-25 04:50:53.554 UTC [1101618] LOCATION:  DeconstructArrayObject, array_type.c:43
2023-11-25 04:50:53.554 UTC [1101618] STATEMENT:  SELECT * FROM fetch_intermediate_results(ARRAY[NULL, 'squares_1', 'squares_2']::text[], 'localhost', 57637);
2023-11-25 04:50:53.556 UTC [1101618] WARNING:  CIINF: Query could not find the intermediate result file "squares_1", it was mostly likely deleted due to an error in a parallel process within the same distributed transaction
2023-11-25 04:50:53.556 UTC [1101618] LOCATION:  ReadIntermediateResultsIntoFuncOutput, intermediate_results.c:869
2023-11-25 04:50:53.556 UTC [1101618] WARNING:  CIINF: Query could not find the intermediate result file "squares_2", it was mostly likely deleted due to an error in a parallel process within the same distributed transaction
2023-11-25 04:50:53.556 UTC [1101618] LOCATION:  ReadIntermediateResultsIntoFuncOutput, intermediate_results.c:869
2023-11-25 04:50:54.230 UTC [1101916] ERROR:  XX000: query must be distributed and shouldn't require any merging on the coordinator.
2023-11-25 04:50:54.230 UTC [1101916] LOCATION:  partition_task_list_results, distributed_intermediate_results.c:60
2023-11-25 04:50:54.230 UTC [1101916] STATEMENT:  SELECT partition_task_list_results('test', $$ SELECT avg(a) FROM source_table $$, 'target_table');
2023-11-25 04:50:54.230 UTC [1101916] ERROR:  XX000: query must be distributed and shouldn't require any merging on the coordinator.
2023-11-25 04:50:54.230 UTC [1101916] LOCATION:  partition_task_list_results, distributed_intermediate_results.c:60
2023-11-25 04:50:54.230 UTC [1101916] STATEMENT:  SELECT partition_task_list_results('test', $$ SELECT * FROM generate_series(1, 2) $$, 'target_table');
2023-11-25 04:50:54.324 UTC [1101919] ERROR:  25001: cannot perform query with placements that were modified over multiple connections
2023-11-25 04:50:54.324 UTC [1101919] LOCATION:  FindPlacementListConnection, placement_connection.c:613
2023-11-25 04:50:54.324 UTC [1101919] STATEMENT:  SELECT COUNT(*) FROM test_table JOIN ref_test_table USING (id);
2023-11-25 04:50:54.742 UTC [1102094] ERROR:  0A000: repartitioning results of a tasklist is only supported when target relation is hash or range partitioned.
2023-11-25 04:50:54.742 UTC [1102094] LOCATION:  PartitionTasklistResults, distributed_intermediate_results.c:152
2023-11-25 04:50:54.742 UTC [1102094] STATEMENT:  CREATE TABLE distributed_result_info AS
	  SELECT * FROM redistribute_task_list_results('test', $$ SELECT * FROM source_table $$, 'target_table_reference');
2023-11-25 04:50:54.744 UTC [1102094] ERROR:  0A000: repartitioning results of a tasklist is only supported when target relation is hash or range partitioned.
2023-11-25 04:50:54.744 UTC [1102094] LOCATION:  PartitionTasklistResults, distributed_intermediate_results.c:152
2023-11-25 04:50:54.744 UTC [1102094] STATEMENT:  CREATE TABLE distributed_result_info AS
	  SELECT * FROM redistribute_task_list_results('test', $$ SELECT * FROM source_table $$, 'target_table_append');
2023-11-25 04:50:54.765 UTC [1094384] LOG:  00000: received SIGHUP, reloading configuration files
2023-11-25 04:50:54.765 UTC [1094384] LOCATION:  SIGHUP_handler, postmaster.c:2769
2023-11-25 04:50:54.766 UTC [1094384] LOG:  00000: parameter "deadlock_timeout" changed to "250ms"
2023-11-25 04:50:54.766 UTC [1094384] LOCATION:  ProcessConfigFileInternal, guc-file.l:453
2023-11-25 04:50:54.913 UTC [1101918] ERROR:  XX000: cannot EXPLAIN ANALYZE multiple queries
2023-11-25 04:50:54.913 UTC [1101918] LOCATION:  worker_save_query_explain_analyze, multi_explain.c:1048
2023-11-25 04:50:54.913 UTC [1101918] STATEMENT:  SELECT * FROM worker_save_query_explain_analyze('SELECT 1; SELECT 2', '{"costs": false, "timing": false, "summary": false}'::jsonb) as (a int);
2023-11-25 04:50:54.913 UTC [1101918] ERROR:  42703: column "x" does not exist at character 8
2023-11-25 04:50:54.913 UTC [1101918] LOCATION:  errorMissingColumn, parse_relation.c:3656
2023-11-25 04:50:54.913 UTC [1101918] STATEMENT:  SELECT * FROM worker_save_query_explain_analyze('SELECT x', '{"costs": false, "timing": false, "summary": false}'::jsonb) as (a int);
2023-11-25 04:50:54.913 UTC [1101918] ERROR:  XX000: Invalid explain analyze format: "invlaid_format"
2023-11-25 04:50:54.913 UTC [1101918] LOCATION:  ExtractFieldExplainFormat, multi_explain.c:1163
2023-11-25 04:50:54.913 UTC [1101918] STATEMENT:  SELECT * FROM worker_save_query_explain_analyze('SELECT 1', '{"format": "invlaid_format"}') as (a int);
2023-11-25 04:50:55.242 UTC [1101919] ERROR:  40P01: canceling the transaction since it was involved in a distributed deadlock
2023-11-25 04:50:55.242 UTC [1101919] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 04:50:55.242 UTC [1101919] STATEMENT:  SELECT id, pg_advisory_lock(15) FROM test_table;
2023-11-25 04:50:55.267 UTC [1101919] ERROR:  40P01: canceling the transaction since it was involved in a distributed deadlock
2023-11-25 04:50:55.267 UTC [1101919] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 04:50:55.267 UTC [1101919] STATEMENT:  SELECT id, pg_advisory_lock(15) FROM test_table;
2023-11-25 04:50:55.293 UTC [1101919] ERROR:  40P01: canceling the transaction since it was involved in a distributed deadlock
2023-11-25 04:50:55.293 UTC [1101919] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 04:50:55.293 UTC [1101919] STATEMENT:  SELECT id, pg_advisory_lock(15) FROM test_table;
2023-11-25 04:50:55.318 UTC [1101919] ERROR:  40P01: canceling the transaction since it was involved in a distributed deadlock
2023-11-25 04:50:55.318 UTC [1101919] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 04:50:55.318 UTC [1101919] STATEMENT:  SELECT id, pg_advisory_lock(15) FROM test_table;
2023-11-25 04:50:55.344 UTC [1101919] ERROR:  40P01: canceling the transaction since it was involved in a distributed deadlock
2023-11-25 04:50:55.344 UTC [1101919] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 04:50:55.344 UTC [1101919] STATEMENT:  SELECT id, pg_advisory_lock(15) FROM test_table;
2023-11-25 04:50:55.369 UTC [1101919] ERROR:  40P01: canceling the transaction since it was involved in a distributed deadlock
2023-11-25 04:50:55.369 UTC [1101919] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 04:50:55.369 UTC [1101919] STATEMENT:  SELECT id, pg_advisory_lock(15) FROM test_table;
2023-11-25 04:50:55.395 UTC [1101919] ERROR:  40P01: canceling the transaction since it was involved in a distributed deadlock
2023-11-25 04:50:55.395 UTC [1101919] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 04:50:55.395 UTC [1101919] STATEMENT:  SELECT id, pg_advisory_lock(15) FROM test_table;
2023-11-25 04:50:55.420 UTC [1101919] ERROR:  40P01: canceling the transaction since it was involved in a distributed deadlock
2023-11-25 04:50:55.420 UTC [1101919] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 04:50:55.420 UTC [1101919] STATEMENT:  SELECT id, pg_advisory_lock(15) FROM test_table;
2023-11-25 04:50:55.445 UTC [1101919] ERROR:  40P01: canceling the transaction since it was involved in a distributed deadlock
2023-11-25 04:50:55.445 UTC [1101919] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 04:50:55.445 UTC [1101919] STATEMENT:  SELECT id, pg_advisory_lock(15) FROM test_table;
2023-11-25 04:50:55.471 UTC [1101919] ERROR:  40P01: canceling the transaction since it was involved in a distributed deadlock
2023-11-25 04:50:55.471 UTC [1101919] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 04:50:55.471 UTC [1101919] STATEMENT:  SELECT id, pg_advisory_lock(15) FROM test_table;
2023-11-25 04:50:55.648 UTC [1094384] LOG:  00000: received SIGHUP, reloading configuration files
2023-11-25 04:50:55.648 UTC [1094384] LOCATION:  SIGHUP_handler, postmaster.c:2769
2023-11-25 04:50:55.648 UTC [1094384] LOG:  00000: parameter "deadlock_timeout" removed from configuration file, reset to default
2023-11-25 04:50:55.648 UTC [1094384] LOCATION:  ProcessConfigFileInternal, guc-file.l:388
2023-11-25 04:50:56.051 UTC [1101915] ERROR:  XX000: worker_partition_query_result can only be used in a transaction block
2023-11-25 04:50:56.051 UTC [1101915] LOCATION:  worker_partition_query_result, partitioned_intermediate_results.c:151
2023-11-25 04:50:56.051 UTC [1101915] STATEMENT:  SELECT * FROM worker_partition_query_result('squares_range',
	                                            'SELECT i, i * i FROM generate_series(1, 10) i',
	                                            1, 'range', '{0}'::text[], '{20}'::text[], true);
2023-11-25 04:50:56.052 UTC [1101915] ERROR:  42601: syntax error at or near "SELECxT" at character 1
2023-11-25 04:50:56.052 UTC [1101915] LOCATION:  scanner_yyerror, scan.l:1188
2023-11-25 04:50:56.052 UTC [1101915] STATEMENT:  SELECT worker_partition_query_result('squares_range',
	                                     'SELECxT i, i * i FROM generate_series(1, 10) i',
	                                     1, 'range',
	                                     '{0,21,41,61}'::text[],
	                                     '{20,40,60,100}'::text[],
	                                     true);
2023-11-25 04:50:56.052 UTC [1101915] ERROR:  42602: result key "squares_range/a/" contains invalid character
2023-11-25 04:50:56.052 UTC [1101915] HINT:  Result keys may only contain letters, numbers, underscores and hyphens.
2023-11-25 04:50:56.052 UTC [1101915] LOCATION:  QueryResultFileName, intermediate_results.c:652
2023-11-25 04:50:56.052 UTC [1101915] STATEMENT:  SELECT worker_partition_query_result('squares_range/a/',
	                                     'SELECT i, i * i FROM generate_series(1, 10) i',
	                                     1, 'range',
	                                     '{0,21,41,61}'::text[],
	                                     '{20,40,60,100}'::text[],
	                                     true);
2023-11-25 04:50:56.052 UTC [1101915] ERROR:  22023: number of partitions cannot be 0
2023-11-25 04:50:56.052 UTC [1101915] LOCATION:  worker_partition_query_result, partitioned_intermediate_results.c:176
2023-11-25 04:50:56.052 UTC [1101915] STATEMENT:  SELECT worker_partition_query_result('squares_range',
	                                     'SELECT i, i * i FROM generate_series(1, 10) i',
	                                     1, 'range', ARRAY[]::text[], ARRAY[]::text[], true);
2023-11-25 04:50:56.052 UTC [1101915] ERROR:  22023: only hash and range partitiong schemes are supported
2023-11-25 04:50:56.052 UTC [1101915] LOCATION:  worker_partition_query_result, partitioned_intermediate_results.c:135
2023-11-25 04:50:56.052 UTC [1101915] STATEMENT:  SELECT worker_partition_query_result('squares_range',
	                                     'SELECT i, i * i FROM generate_series(1, 10) i',
	                                     1, 'append',
	                                     '{0,21,41,61}'::text[],
	                                     '{20,40,60,100}'::text[],
	                                     true);
2023-11-25 04:50:56.053 UTC [1101915] ERROR:  22023: query must generate a set of rows
2023-11-25 04:50:56.053 UTC [1101915] LOCATION:  worker_partition_query_result, partitioned_intermediate_results.c:187
2023-11-25 04:50:56.053 UTC [1101915] STATEMENT:  SELECT worker_partition_query_result('squares_range',
	                                     'INSERT INTO t VALUES (1), (2)',
	                                     1, 'range',
	                                     '{0,21,41,61}'::text[],
	                                     '{20,40,60,100}'::text[],
	                                     true);
2023-11-25 04:50:56.053 UTC [1101915] ERROR:  22023: partition column index must be between 0 and 1
2023-11-25 04:50:56.053 UTC [1101915] LOCATION:  worker_partition_query_result, partitioned_intermediate_results.c:193
2023-11-25 04:50:56.053 UTC [1101915] STATEMENT:  SELECT worker_partition_query_result('squares_range',
	                                     'SELECT i, i * i FROM generate_series(1, 10) i',
	                                     -1, 'range',
	                                     '{0,21,41,61}'::text[],
	                                     '{20,40,60,100}'::text[],
	                                     true);
2023-11-25 04:50:56.054 UTC [1101915] ERROR:  22023: partition column index must be between 0 and 1
2023-11-25 04:50:56.054 UTC [1101915] LOCATION:  worker_partition_query_result, partitioned_intermediate_results.c:193
2023-11-25 04:50:56.054 UTC [1101915] STATEMENT:  SELECT worker_partition_query_result('squares_range',
	                                     'SELECT i, i * i FROM generate_series(1, 10) i',
	                                     2, 'range',
	                                     '{0,21,41,61}'::text[],
	                                     '{20,40,60,100}'::text[],
	                                     true);
2023-11-25 04:50:56.054 UTC [1101915] ERROR:  22023: min values and max values must have the same number of elements
2023-11-25 04:50:56.054 UTC [1101915] LOCATION:  worker_partition_query_result, partitioned_intermediate_results.c:167
2023-11-25 04:50:56.054 UTC [1101915] STATEMENT:  SELECT worker_partition_query_result('squares_range',
	                                     'SELECT i, i * i FROM generate_series(1, 10) i',
	                                     1, 'range',
	                                     '{0,21,41,61,101}'::text[],
	                                     '{20,40,60,100}'::text[],
	                                     true);
2023-11-25 04:50:56.054 UTC [1101915] ERROR:  XX000: hash partitioned table has uninitialized shards
2023-11-25 04:50:56.054 UTC [1101915] LOCATION:  ErrorIfInconsistentShardIntervals, metadata_cache.c:1976
2023-11-25 04:50:56.054 UTC [1101915] STATEMENT:  SELECT worker_partition_query_result('squares_hash',
	                                     'SELECT i, i * i FROM generate_series(1, 10) i',
	                                     1, 'hash',
	                                     '{NULL,21,41,61}'::text[],
	                                     '{20,40,60,100}'::text[],
	                                     true);
2023-11-25 04:50:56.054 UTC [1101915] ERROR:  XX000: cannot execute multiple utility events
2023-11-25 04:50:56.054 UTC [1101915] LOCATION:  ParseTreeRawStmt, worker_data_fetch_protocol.c:319
2023-11-25 04:50:56.054 UTC [1101915] STATEMENT:  SELECT worker_partition_query_result('squares_hash',
	                                     'SELECT i, i * i FROM generate_series(1, 10) i; SELECT 4, 16;',
	                                     1, 'hash',
	                                     '{NULL,21,41,61}'::text[],
	                                     '{20,40,60,100}'::text[],
	                                     true);
2023-11-25 04:50:56.578 UTC [1101915] ERROR:  55000: could not find shard for partition column value
2023-11-25 04:50:56.578 UTC [1101915] CONTEXT:  SQL statement "INSERT INTO t SELECT x, x * x * x FROM generate_series(1, 105) x"
	PL/pgSQL function test_partition_query_results(regclass,text,boolean) line 35 at EXECUTE
2023-11-25 04:50:56.578 UTC [1101915] LOCATION:  ShardIdForTuple, multi_copy.c:2614
2023-11-25 04:50:56.578 UTC [1101915] STATEMENT:  CALL test_partition_query_results('t', 'SELECT x, x * x * x FROM generate_series(1, 105) x');
2023-11-25 04:50:57.161 UTC [1102581] ERROR:  0A000: cannot perform distributed planning on this query
2023-11-25 04:50:57.161 UTC [1102581] DETAIL:  Cartesian products are currently unsupported
2023-11-25 04:50:57.161 UTC [1102581] LOCATION:  JoinSequenceArray, multi_physical_planner.c:3589
2023-11-25 04:50:57.161 UTC [1102581] STATEMENT:  SELECT count(*) FROM events_reference_table e1 CROSS JOIN events_table e2 CROSS JOIN users_table u;
2023-11-25 04:50:57.162 UTC [1102581] ERROR:  0A000: cannot perform distributed planning on this query
2023-11-25 04:50:57.162 UTC [1102581] DETAIL:  Cartesian products are currently unsupported
2023-11-25 04:50:57.162 UTC [1102581] LOCATION:  JoinSequenceArray, multi_physical_planner.c:3589
2023-11-25 04:50:57.162 UTC [1102581] STATEMENT:  SELECT count(*) FROM events_reference_table e1, events_table e2, users_table u;
2023-11-25 04:50:57.219 UTC [1102580] ERROR:  0A000: cannot perform distributed planning on this query
2023-11-25 04:50:57.219 UTC [1102580] DETAIL:  Cartesian products are currently unsupported
2023-11-25 04:50:57.219 UTC [1102580] LOCATION:  JoinSequenceArray, multi_physical_planner.c:3589
2023-11-25 04:50:57.219 UTC [1102580] STATEMENT:  SELECT
		avg(unit_price)
	FROM
		(SELECT
			l_orderkey,
			avg(o_totalprice / l_quantity) AS unit_price
		FROM
			lineitem_subquery,
			orders_subquery
		GROUP BY
			l_orderkey) AS unit_prices;
2023-11-25 04:50:57.359 UTC [1102579] ERROR:  0A000: complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
2023-11-25 04:50:57.359 UTC [1102579] LOCATION:  JoinOrderList, multi_join_order.c:310
2023-11-25 04:50:57.359 UTC [1102579] STATEMENT:  SELECT count(*) FROM lineitem, orders WHERE l_orderkey + 1 = o_orderkey;
2023-11-25 04:50:57.402 UTC [1102581] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:57.402 UTC [1102581] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:57.402 UTC [1102581] STATEMENT:  SELECT count(*) FROM users_table u1 CROSS JOIN users_ref_test_table ref2 LEFT JOIN users_table u2 ON (ref2.id = u2.user_id);
2023-11-25 04:50:57.403 UTC [1102581] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:57.403 UTC [1102581] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:57.403 UTC [1102581] STATEMENT:  SELECT count(*) FROM users_table u1 CROSS JOIN users_ref_test_table ref2 FULL JOIN users_table u2 ON (ref2.id = u2.user_id);
2023-11-25 04:50:57.403 UTC [1102581] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:57.403 UTC [1102581] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:57.403 UTC [1102581] STATEMENT:  SELECT count(*) FROM users_table u1 CROSS JOIN users_ref_test_table ref2 RIGHT JOIN users_table u2 ON (ref2.id = u2.user_id);
2023-11-25 04:50:57.449 UTC [1102584] ERROR:  0A000: complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
2023-11-25 04:50:57.449 UTC [1102584] LOCATION:  JoinOrderList, multi_join_order.c:310
2023-11-25 04:50:57.449 UTC [1102584] STATEMENT:  SELECT ("final_query"."event_types") as types, count(*) AS sumOfEventType
	FROM
	  ( SELECT *, random()
	   FROM
	     ( SELECT "t"."user_id", "t"."time", unnest("t"."collected_events") AS "event_types"
	      FROM
	        ( SELECT "t1"."user_id", min("t1"."time") AS "time", array_agg(("t1"."event") ORDER BY TIME ASC, event DESC) AS collected_events
	         FROM (
	                 (SELECT
	                    *
	                  FROM
	                   (SELECT
	                      "events"."user_id", "events"."time", 0 AS event
	                    FROM
	                      events_table as  "events"
	                    WHERE
	                      event_type IN (1, 2) ) events_subquery_1)
	                 UNION
	                 (SELECT
	                    *
	                  FROM
	                    (SELECT
	                        "events"."user_id", "events"."time", 1 AS event
	                     FROM
	                        events_table as "events"
	                     WHERE
	                      event_type IN (3, 4) ) events_subquery_2)
	               UNION
	                 (SELECT
	                    *
	                  FROM
	                    (SELECT
	                        "events"."user_id", "events"."time", 2 AS event
	                     FROM
	                        events_table as  "events",  users_table as "users"
	                     WHERE
	                      event_type IN (5, 6)  AND users.user_id != events.user_id ) events_subquery_3)
	                UNION
	                  (SELECT
	                      *
	                   FROM
	                    (SELECT
	                        "events"."user_id", "events"."time", 3 AS event
	                     FROM
	                      events_table as "events"
	                     WHERE
	                      event_type IN (4, 5)) events_subquery_4)) t1
	         GROUP BY "t1"."user_id") AS t) "q"
	INNER JOIN
	     (SELECT
	        "users"."user_id"
	      FROM
	        users_table as "users"
	      WHERE
	        value_1 > 0 and value_1 < 4) AS t
	    ON (t.user_id = q.user_id)) as final_query
	GROUP BY
	  types
	ORDER BY
	  types;
2023-11-25 04:50:57.575 UTC [1102580] ERROR:  0A000: cannot push down this subquery
2023-11-25 04:50:57.575 UTC [1102580] DETAIL:  Limit in subquery without limit in the outermost query is unsupported
2023-11-25 04:50:57.575 UTC [1102580] LOCATION:  DeferErrorIfCannotPushdownSubquery, query_pushdown_planning.c:1048
2023-11-25 04:50:57.575 UTC [1102580] STATEMENT:  SELECT
		avg(o_totalprice/l_quantity)
	FROM
			(SELECT
				l_orderkey,
				l_quantity
			FROM
				lineitem_subquery
			ORDER BY
				l_orderkey, l_quantity
			LIMIT 10
			) lineitem_quantities
		JOIN LATERAL
			(SELECT
				o_totalprice
			FROM
				orders_subquery
			WHERE
				lineitem_quantities.l_orderkey = o_orderkey) orders_price ON true;
2023-11-25 04:50:57.600 UTC [1102580] ERROR:  0A000: complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
2023-11-25 04:50:57.600 UTC [1102580] LOCATION:  JoinOrderList, multi_join_order.c:310
2023-11-25 04:50:57.600 UTC [1102580] STATEMENT:  SELECT l_orderkey
	FROM
		lineitem_subquery l
	JOIN
		orders_subquery o
	ON (l_orderkey::int8 = o_orderkey::int4)
	WHERE
		(o_orderkey < l_quantity + 3)
	ORDER BY l_orderkey DESC
	LIMIT 10;
2023-11-25 04:50:57.602 UTC [1102580] ERROR:  0A000: complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
2023-11-25 04:50:57.602 UTC [1102580] LOCATION:  JoinOrderList, multi_join_order.c:310
2023-11-25 04:50:57.602 UTC [1102580] STATEMENT:  SELECT l_orderkey
	FROM
		lineitem_subquery l
	JOIN
		orders_subquery o
	ON (l_orderkey::int4 = o_orderkey::int8)
	ORDER BY l_orderkey DESC
	LIMIT 10;
2023-11-25 04:50:57.611 UTC [1102583] ERROR:  0A000: cannot push down this subquery
2023-11-25 04:50:57.611 UTC [1102583] DETAIL:  Distinct on columns without partition column is currently unsupported
2023-11-25 04:50:57.611 UTC [1102583] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 04:50:57.611 UTC [1102583] STATEMENT:  SELECT a.user_id, avg(b.value_2) as subquery_avg
	FROM
		(SELECT
	      user_id
	   FROM
	      users_table
		 WHERE
	      (value_1 > 2)
		 GROUP BY
	      user_id
		 HAVING
	      count(distinct value_1) > 2
		) as a
		LEFT JOIN
		(SELECT
	      DISTINCT ON (value_2) value_2 , user_id, value_3
		 FROM
	      users_table
		 WHERE
	      (value_1 > 3)
		 ORDER BY
	      1,2,3
		) AS b
		USING (user_id)
	GROUP BY user_id;
2023-11-25 04:50:57.614 UTC [1102580] ERROR:  0A000: complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
2023-11-25 04:50:57.614 UTC [1102580] LOCATION:  JoinOrderList, multi_join_order.c:310
2023-11-25 04:50:57.614 UTC [1102580] STATEMENT:  SELECT l_orderkey
	FROM
		lineitem_subquery l
	JOIN
		orders_subquery o
	ON (l_orderkey = o_orderkey + 1)
	WHERE
		(o_orderkey < l_quantity)
	ORDER BY l_orderkey DESC
	LIMIT 10;
2023-11-25 04:50:57.614 UTC [1102580] ERROR:  0A000: complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
2023-11-25 04:50:57.614 UTC [1102580] LOCATION:  JoinOrderList, multi_join_order.c:310
2023-11-25 04:50:57.614 UTC [1102580] STATEMENT:  SELECT l_orderkey
	FROM
		lineitem_subquery l
	JOIN
		orders_subquery o
	ON (l_orderkey = o_orderkey + 1)
	ORDER BY l_orderkey DESC
	LIMIT 10;
2023-11-25 04:50:57.615 UTC [1102580] ERROR:  0A000: complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
2023-11-25 04:50:57.615 UTC [1102580] LOCATION:  JoinOrderList, multi_join_order.c:310
2023-11-25 04:50:57.615 UTC [1102580] STATEMENT:  SELECT l_orderkey
	FROM
		lineitem_subquery l
	JOIN
		orders_subquery o
	ON (l_orderkey < o_orderkey)
	WHERE
		(o_orderkey < l_quantity)
	ORDER BY l_orderkey DESC
	LIMIT 10;
2023-11-25 04:50:57.619 UTC [1102584] ERROR:  0A000: cannot push down this subquery
2023-11-25 04:50:57.619 UTC [1102584] DETAIL:  Limit clause is currently unsupported when a lateral subquery references a column from complex subqueries, CTEs or local tables
2023-11-25 04:50:57.619 UTC [1102584] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 04:50:57.619 UTC [1102584] STATEMENT:  SELECT "some_users_data".user_id, lastseen
	FROM
	     (SELECT user_id, max(time) AS lastseen
	      FROM
	        (SELECT user_id, time
	         FROM
	           (SELECT
	              user_id, time
	            FROM
	              events_table as "events"
	            WHERE
	              user_id > 1 and user_id < 4) "events_1"
	         ORDER BY
	           time DESC
	         LIMIT 1000) "recent_events_1"
	      GROUP BY
	        user_id
	      ORDER BY
	        max(TIME) DESC) "some_recent_users"
	   JOIN LATERAL
	     (SELECT
	        "users".user_id
	      FROM
	        users_table as "users"
	      WHERE
	        "users"."value_1" = "some_recent_users"."user_id" AND
	        users.value_2 > 1 and users.value_2 < 3
	      ORDER BY 1 LIMIT 1) "some_users_data"
	     ON TRUE
	ORDER BY
	  user_id
	limit 50;
2023-11-25 04:50:57.620 UTC [1102584] ERROR:  0A000: cannot push down this subquery
2023-11-25 04:50:57.620 UTC [1102584] DETAIL:  Limit clause is currently unsupported when a lateral subquery references a column from complex subqueries, CTEs or local tables
2023-11-25 04:50:57.620 UTC [1102584] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 04:50:57.620 UTC [1102584] STATEMENT:  SELECT "some_users_data".user_id, lastseen
	FROM
	     (SELECT 2 * user_id as user_id, max(time) AS lastseen
	      FROM
	        (SELECT user_id, time
	         FROM
	           (SELECT
	              user_id, time
	            FROM
	              events_table as "events"
	            WHERE
	              user_id > 1 and user_id < 4) "events_1"
	         ORDER BY
	           time DESC
	         LIMIT 1000) "recent_events_1"
	      GROUP BY
	        user_id
	      ORDER BY
	        max(TIME) DESC) "some_recent_users"
	   JOIN LATERAL
	     (SELECT
	        "users".user_id
	      FROM
	        users_table as "users"
	      WHERE
	        "users"."user_id" = "some_recent_users"."user_id" AND
	        users.value_2 > 1 and users.value_2 < 3
	      ORDER BY 1 LIMIT 1) "some_users_data"
	     ON TRUE
	ORDER BY
	  user_id
	limit 50;
2023-11-25 04:50:57.635 UTC [1102580] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:57.635 UTC [1102580] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:57.635 UTC [1102580] STATEMENT:  SELECT DISTINCT ON (t1.user_id) t1.user_id, t2.value_1, t2.value_2, t2.value_3
	FROM events_table t1
	LEFT JOIN users_table t2 ON t1.user_id > t2.user_id
	ORDER BY 1 DESC, 2 DESC, 3 DESC, 4 DESC
	LIMIT 5;
2023-11-25 04:50:57.650 UTC [1102584] ERROR:  0A000: cannot push down this subquery
2023-11-25 04:50:57.650 UTC [1102584] DETAIL:  Limit clause is currently unsupported when a lateral subquery references a column from complex subqueries, CTEs or local tables
2023-11-25 04:50:57.650 UTC [1102584] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 04:50:57.650 UTC [1102584] STATEMENT:  SELECT user_id, lastseen
	FROM
	  (SELECT
	      "some_users_data".user_id, lastseen
	   FROM
	     (SELECT
	        filter_users_1.user_id, time AS lastseen
	      FROM
	        (SELECT
	            user_where_1_1.user_id
	         FROM
	           (SELECT
	              "users"."user_id"
	            FROM
	              users_table as "users"
	            WHERE
	              user_id > 1 and user_id < 4 and value_1 > 2) user_where_1_1
	         INNER JOIN
	           (SELECT
	              "users"."user_id"
	            FROM
	              users_table as "users"
	            WHERE
	              user_id > 1 and user_id < 4  and value_2 > 3) user_where_1_join_1
	           ON ("user_where_1_1".user_id != "user_where_1_join_1".user_id)) filter_users_1
	      JOIN LATERAL
	        (SELECT
	            user_id, time
	         FROM
	          events_table as "events"
	         WHERE
	          user_id > 1 and user_id < 4 and user_id = filter_users_1.user_id
	         ORDER BY
	          time DESC
	         LIMIT 1) "last_events_1" ON true
	      ORDER BY time DESC
	      LIMIT 10) "some_recent_users"
	   JOIN LATERAL
	     (SELECT
	        "users".user_id
	      FROM
	        users_table  as "users"
	      WHERE
	        "users"."user_id" = "some_recent_users"."user_id" AND
	        "users"."value_2" > 4
	      ORDER BY 1 LIMIT 1) "some_users_data" ON true
	   ORDER BY
	    lastseen DESC
	   LIMIT 10) "some_users"
	ORDER BY
	  user_id DESC, lastseen DESC
	LIMIT 10;
2023-11-25 04:50:57.654 UTC [1102584] ERROR:  0A000: cannot push down this subquery
2023-11-25 04:50:57.654 UTC [1102584] DETAIL:  Limit clause is currently unsupported when a lateral subquery references a column from complex subqueries, CTEs or local tables
2023-11-25 04:50:57.654 UTC [1102584] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 04:50:57.654 UTC [1102584] STATEMENT:  SELECT user_id, lastseen
	FROM
	  (SELECT
	      "some_users_data".user_id, lastseen
	   FROM
	     (SELECT
	        filter_users_1.user_id, time AS lastseen
	      FROM
	        (SELECT
	            user_where_1_1.user_id
	         FROM
	           (SELECT
	              "users"."user_id"
	            FROM
	              users_table as "users"
	            WHERE
	              user_id > 1 and user_id < 4 and value_1 > 2) user_where_1_1
	         INNER JOIN
	           (SELECT
	              "users"."user_id", "users"."value_1"
	            FROM
	              users_table as "users"
	            WHERE
	              user_id > 1 and user_id < 4 and value_2 > 3) user_where_1_join_1
	           ON ("user_where_1_1".user_id = "user_where_1_join_1".value_1)) filter_users_1
	      JOIN LATERAL
	        (SELECT
	            user_id, time
	         FROM
	          events_table as "events"
	         WHERE
	          user_id > 1 and user_id < 4 and user_id = filter_users_1.user_id
	         ORDER BY
	          time DESC
	         LIMIT 1) "last_events_1" ON true
	      ORDER BY time DESC
	      LIMIT 10) "some_recent_users"
	   JOIN LATERAL
	     (SELECT
	        "users".user_id
	      FROM
	        users_table  as "users"
	      WHERE
	        "users"."user_id" = "some_recent_users"."user_id" AND
	        "users"."value_2" > 4
	      ORDER BY 1 LIMIT 1) "some_users_data" ON true
	   ORDER BY
	    lastseen DESC
	   LIMIT 10) "some_users"
	ORDER BY
	  user_id DESC, lastseen DESC
	LIMIT 10;
2023-11-25 04:50:57.655 UTC [1102584] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:57.655 UTC [1102584] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:57.655 UTC [1102584] STATEMENT:  SELECT user_id, lastseen
	FROM
	  (SELECT
	      "some_users_data".user_id, lastseen
	   FROM
	     (SELECT
	        filter_users_1.user_id, time AS lastseen
	      FROM
	        (SELECT
	            user_where_1_1.user_id
	         FROM
	           (SELECT
	              "users"."user_id"
	            FROM
	              users_table as "users"
	            WHERE
	              user_id > 1 and user_id < 3 and value_1 > 2) user_where_1_1
	         INNER JOIN
	           (SELECT
	              "users"."user_id", "users"."value_1"
	            FROM
	              users_table as "users"
	            WHERE
	              user_id > 1 and user_id < 3 and value_2 > 3) user_where_1_join_1
	           ON ("user_where_1_1".user_id = "user_where_1_join_1".user_id)) filter_users_1
	      JOIN LATERAL
	        (SELECT
	            user_id, time
	         FROM
	          events_table as "events"
	         WHERE
	          user_id > 1 and user_id < 3 and user_id != filter_users_1.user_id
	         ORDER BY
	          time DESC
	         LIMIT 1) "last_events_1" ON true
	      ORDER BY time DESC
	      LIMIT 10) "some_recent_users"
	   JOIN LATERAL
	     (SELECT
	        "users".user_id
	      FROM
	        users_table  as "users"
	      WHERE
	        "users"."user_id" = "some_recent_users"."user_id" AND
	        "users"."value_2" > 4
	      LIMIT 1) "some_users_data" ON true
	   ORDER BY
	    lastseen DESC
	   LIMIT 10) "some_users"
	ORDER BY
	  user_id DESC, lastseen DESC
	LIMIT 10;
2023-11-25 04:50:57.658 UTC [1102584] ERROR:  0A000: cannot push down this subquery
2023-11-25 04:50:57.658 UTC [1102584] DETAIL:  Limit clause is currently unsupported when a lateral subquery references a column from complex subqueries, CTEs or local tables
2023-11-25 04:50:57.658 UTC [1102584] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 04:50:57.658 UTC [1102584] STATEMENT:  SELECT user_id, lastseen
	FROM
	  (SELECT
	      "some_users_data".user_id, lastseen
	   FROM
	     (SELECT
	        filter_users_1.user_id, time AS lastseen
	      FROM
	        (SELECT
	            user_where_1_1.user_id
	         FROM
	           (SELECT
	              "users"."user_id"
	            FROM
	              users_table as "users"
	            WHERE
	              user_id > 1 and user_id < 3 and value_1 > 2) user_where_1_1
	         INNER JOIN
	           (SELECT
	              "users"."user_id", "users"."value_1"
	            FROM
	              users_table as "users"
	            WHERE
	              user_id > 1 and user_id < 3 and value_2 > 3) user_where_1_join_1
	           ON ("user_where_1_1".user_id = "user_where_1_join_1".user_id)) filter_users_1
	      JOIN LATERAL
	        (SELECT
	            user_id, time
	         FROM
	          events_table as "events"
	         WHERE
	          user_id > 1 and user_id < 3 and user_id = filter_users_1.user_id
	         ORDER BY
	          time DESC
	         LIMIT 1) "last_events_1" ON true
	      ORDER BY time DESC
	      LIMIT 10) "some_recent_users"
	   JOIN LATERAL
	     (SELECT
	        "users".user_id
	      FROM
	        users_table  as "users"
	      WHERE
	        "users"."value_1" = "some_recent_users"."user_id" AND
	        "users"."value_2" > 4
	      ORDER BY 1 LIMIT 1) "some_users_data" ON true
	   ORDER BY
	    lastseen DESC
	   LIMIT 10) "some_users"
	ORDER BY
	  user_id DESC, lastseen DESC
	LIMIT 10;
2023-11-25 04:50:57.668 UTC [1102580] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:57.668 UTC [1102580] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:57.668 UTC [1102580] STATEMENT:  SELECT DISTINCT ON (t1.user_id) t1.user_id, t2.value_1, t2.value_2, t2.value_3
	 FROM
	 users_table t0 LEFT JOIN
	 events_table t1  ON t0.user_id = trunc(t1.user_id)
	 LEFT JOIN users_reference_table t2 ON t1.user_id = trunc(t2.user_id)
	 ORDER BY 1 DESC, 2 DESC, 3 DESC, 4 DESC
	 LIMIT 5;
2023-11-25 04:50:57.724 UTC [1102580] ERROR:  0A000: cannot compute aggregate (distinct)
2023-11-25 04:50:57.724 UTC [1102580] DETAIL:  Only count(distinct) aggregate is supported in subqueries
2023-11-25 04:50:57.724 UTC [1102580] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4111
2023-11-25 04:50:57.724 UTC [1102580] STATEMENT:  SELECT
		sum(DISTINCT a)
	FROM (
		SELECT
			count(*) a
		FROM
			lineitem_subquery
		GROUP BY
		   l_orderkey
	) z;
2023-11-25 04:50:57.725 UTC [1102580] ERROR:  0A000: cannot compute aggregate (distinct)
2023-11-25 04:50:57.725 UTC [1102580] DETAIL:  Only count(distinct) aggregate is supported in subqueries
2023-11-25 04:50:57.725 UTC [1102580] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4111
2023-11-25 04:50:57.725 UTC [1102580] STATEMENT:  SELECT
		avg(DISTINCT a)
	FROM (
		SELECT
			count(*) a
		FROM
			lineitem_subquery
		GROUP BY
		   l_orderkey
	) z;
2023-11-25 04:50:57.759 UTC [1102583] ERROR:  0A000: cannot push down this subquery
2023-11-25 04:50:57.759 UTC [1102583] DETAIL:  Subqueries without a FROM clause can only contain immutable functions
2023-11-25 04:50:57.759 UTC [1102583] LOCATION:  DeferErrorIfCannotPushdownSubquery, query_pushdown_planning.c:1048
2023-11-25 04:50:57.759 UTC [1102583] STATEMENT:  SELECT count(*) as subquery_count
	FROM (
	  SELECT
	      user_id
	    FROM
	    users_table
	  WHERE
	    (value_1 = '1' OR value_1 = '3')
	  GROUP BY user_id
	  HAVING count(distinct value_1) = 2
	  ) as a
	  INNER JOIN (
	  SELECT
	    random()::int as user_id
	  ) AS b
	  ON a.user_id = b.user_id
	WHERE b.user_id IS NULL
	GROUP BY a.user_id;
2023-11-25 04:50:57.774 UTC [1102580] ERROR:  0A000: shard counts of co-located tables do not match
2023-11-25 04:50:57.774 UTC [1102580] LOCATION:  QueryPushdownSqlTaskList, multi_physical_planner.c:2210
2023-11-25 04:50:57.774 UTC [1102580] STATEMENT:  SELECT
		avg(unit_price)
	FROM
		(SELECT
			l_orderkey,
			avg(o_totalprice / l_quantity) AS unit_price
		FROM
			lineitem_subquery,
			orders_subquery
		WHERE
			l_orderkey = o_orderkey
		GROUP BY
			l_orderkey) AS unit_prices;
2023-11-25 04:50:57.781 UTC [1102584] ERROR:  0A000: cannot push down this subquery
2023-11-25 04:50:57.781 UTC [1102584] DETAIL:  Limit clause is currently unsupported when a lateral subquery references a column from complex subqueries, CTEs or local tables
2023-11-25 04:50:57.781 UTC [1102584] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 04:50:57.781 UTC [1102584] STATEMENT:  SELECT *
	FROM
	  (SELECT
	      "some_users_data".user_id, value_2
	   FROM
	     (SELECT user_id, max(value_2) AS value_2
	      FROM
	        (SELECT user_id, value_2
	         FROM
	           (SELECT
	              user_id, value_2
	            FROM
	              events_table as "events"
	            WHERE
	              user_id > 1 and user_id < 3) "events_1"
	         ORDER BY
	          value_2 DESC
	         LIMIT 10000) "recent_events_1"
	      GROUP BY
	        user_id
	      ORDER BY
	        max(value_2) DESC) "some_recent_users"
	   JOIN LATERAL
	     (SELECT
	        "users".user_id
	      FROM
	        users_table as "users"
	      WHERE
	        "users"."value_2" = "some_recent_users"."user_id" AND
	        value_2 > 4
	      ORDER BY 1 LIMIT 1) "some_users_data" ON true
	   ORDER BY
	    value_2 DESC
	   LIMIT 10) "some_users"
	ORDER BY
	    value_2 DESC, user_id DESC
	LIMIT 10;
2023-11-25 04:50:57.804 UTC [1102580] ERROR:  0A000: Subqueries in HAVING cannot refer to outer query
2023-11-25 04:50:57.804 UTC [1102580] LOCATION:  RecursivelyPlanSubqueriesAndCTEs, recursive_planning.c:323
2023-11-25 04:50:57.804 UTC [1102580] STATEMENT:  WITH cte_1 AS (SELECT b max FROM subquery_pruning_varchar_test_table)
	SELECT a
	FROM subquery_pruning_varchar_test_table
	JOIN cte_1 ON a = max::text
	GROUP BY a HAVING a = (SELECT a)
	ORDER BY 1;
2023-11-25 04:50:57.809 UTC [1102583] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:57.809 UTC [1102583] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:57.809 UTC [1102583] STATEMENT:  SELECT user_id, array_length(events_table, 1)
	FROM (
	  SELECT user_id, array_agg(event ORDER BY time) AS events_table
	  FROM (
	    SELECT u.user_id, e.event_type::text AS event, e.time
	    FROM users_table AS u,
	         events_table AS e
	    WHERE test_join_function_2(u.user_id, e.user_id)
	  ) t
	  GROUP BY user_id
	) q
	ORDER BY 2 DESC, 1;
2023-11-25 04:50:57.832 UTC [1102583] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:57.832 UTC [1102583] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:57.832 UTC [1102583] STATEMENT:  SELECT
	  count(*)
	FROM
	  (SELECT
	    event_type, random()
	  FROM
	    events_table, users_table
	  WHERE
	    events_table.user_id > users_table.user_id AND
	    events_table.time = users_table.time AND
	    events_table.value_2 IN (0, 4)
	  ) as foo;
2023-11-25 04:50:57.845 UTC [1102583] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:50:57.845 UTC [1102583] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:50:57.845 UTC [1102583] STATEMENT:  SELECT
	  count(*)
	FROM
	  (SELECT
	    event_type, random()
	  FROM
	    events_table, users_table
	  WHERE
	    events_table.user_id = users_table.user_id AND
	    events_table.value_2 IN (0, 4)
	  ) as foo,
	(SELECT
	    event_type, random()
	  FROM
	    events_table, users_table
	  WHERE
	    events_table.user_id = users_table.user_id AND
	    events_table.value_2 IN (1, 5)
	  ) as bar
	WHERE foo.event_type = bar.event_type;
2023-11-25 04:50:57.855 UTC [1102581] ERROR:  0A000: cannot perform distributed planning on this query
2023-11-25 04:50:57.855 UTC [1102581] DETAIL:  Cartesian products are currently unsupported
2023-11-25 04:50:57.855 UTC [1102581] LOCATION:  QueryTargetList, multi_physical_planner.c:818
2023-11-25 04:50:57.855 UTC [1102581] STATEMENT:  SELECT dist2.c0 FROM dist1, dist3, dist4, dist2 WHERE (dist3.c0) IN (dist4.c0);
2023-11-25 04:50:57.855 UTC [1102581] ERROR:  0A000: cannot perform distributed planning on this query
2023-11-25 04:50:57.855 UTC [1102581] DETAIL:  Cartesian products are currently unsupported
2023-11-25 04:50:57.855 UTC [1102581] LOCATION:  QueryTargetList, multi_physical_planner.c:818
2023-11-25 04:50:57.855 UTC [1102581] STATEMENT:  SELECT 1 FROM dist3, dist4, dist2 WHERE (dist3.c0) IN (dist4.c0);
2023-11-25 04:50:57.856 UTC [1102581] ERROR:  0A000: cannot perform distributed planning on this query
2023-11-25 04:50:57.856 UTC [1102581] DETAIL:  Cartesian products are currently unsupported
2023-11-25 04:50:57.856 UTC [1102581] LOCATION:  QueryTargetList, multi_physical_planner.c:818
2023-11-25 04:50:57.856 UTC [1102581] STATEMENT:  SELECT  FROM dist3, dist4, dist2 WHERE (dist3.c0) IN (dist4.c0);
2023-11-25 04:50:57.856 UTC [1102581] ERROR:  0A000: cannot perform distributed planning on this query
2023-11-25 04:50:57.856 UTC [1102581] DETAIL:  Cartesian products are currently unsupported
2023-11-25 04:50:57.856 UTC [1102581] LOCATION:  QueryTargetList, multi_physical_planner.c:818
2023-11-25 04:50:57.856 UTC [1102581] STATEMENT:  SELECT dist2.c0 FROM dist3, dist4, dist2 WHERE (dist3.c0) IN (dist4.c0);
2023-11-25 04:50:57.856 UTC [1102581] ERROR:  0A000: cannot perform distributed planning on this query
2023-11-25 04:50:57.856 UTC [1102581] DETAIL:  Cartesian products are currently unsupported
2023-11-25 04:50:57.856 UTC [1102581] LOCATION:  QueryTargetList, multi_physical_planner.c:818
2023-11-25 04:50:57.856 UTC [1102581] STATEMENT:  SELECT dist2.* FROM dist3, dist4, dist2 WHERE (dist3.c0) IN (dist4.c0);
2023-11-25 04:51:03.681 UTC [1103115] ERROR:  0A000: cannot perform distributed planning on this query because parameterized queries for SQL functions referencing distributed tables are not supported
2023-11-25 04:51:03.681 UTC [1103115] HINT:  Consider using PL/pgSQL functions instead.
2023-11-25 04:51:03.681 UTC [1103115] LOCATION:  distributed_planner, distributed_planner.c:301
2023-11-25 04:51:03.681 UTC [1103115] STATEMENT:  SELECT * FROM test_parameterized_sql_function(1);
2023-11-25 04:51:03.681 UTC [1103115] ERROR:  0A000: cannot perform distributed planning on this query because parameterized queries for SQL functions referencing distributed tables are not supported
2023-11-25 04:51:03.681 UTC [1103115] HINT:  Consider using PL/pgSQL functions instead.
2023-11-25 04:51:03.681 UTC [1103115] LOCATION:  GetRTEIdentity, distributed_planner.c:533
2023-11-25 04:51:03.681 UTC [1103115] STATEMENT:  SELECT (SELECT 1 FROM test_parameterized_sql limit 1) FROM test_parameterized_sql_function(1);
2023-11-25 04:51:03.682 UTC [1103115] ERROR:  0A000: could not create distributed plan
2023-11-25 04:51:03.682 UTC [1103115] DETAIL:  Possibly this is caused by the use of parameters in SQL functions, which is not supported in Citus.
2023-11-25 04:51:03.682 UTC [1103115] HINT:  Consider using PL/pgSQL functions instead.
2023-11-25 04:51:03.682 UTC [1103115] CONTEXT:  SQL function "test_parameterized_sql_function_in_subquery_where" statement 1
2023-11-25 04:51:03.682 UTC [1103115] LOCATION:  CreateDistributedPlannedStmt, distributed_planner.c:751
2023-11-25 04:51:03.682 UTC [1103115] STATEMENT:  SELECT test_parameterized_sql_function_in_subquery_where(1);
2023-11-25 04:51:03.731 UTC [1103115] ERROR:  23505: duplicate key value violates unique constraint "table_with_unique_constraint_a_key_1230009"
2023-11-25 04:51:03.731 UTC [1103115] DETAIL:  Key (a)=(4) already exists.
2023-11-25 04:51:03.731 UTC [1103115] CONTEXT:  while executing command on localhost:57638
	SQL function "insert_twice" statement 2
2023-11-25 04:51:03.731 UTC [1103115] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:03.731 UTC [1103115] STATEMENT:  SELECT insert_twice();
2023-11-25 04:51:04.490 UTC [1103116] ERROR:  0A000: cannot push down this subquery
2023-11-25 04:51:04.490 UTC [1103116] DETAIL:  Limit in subquery without limit in the outermost query is unsupported
2023-11-25 04:51:04.490 UTC [1103116] LOCATION:  DeferErrorIfCannotPushdownSubquery, query_pushdown_planning.c:1048
2023-11-25 04:51:04.490 UTC [1103116] STATEMENT:  SELECT * FROM recent_10_users;
2023-11-25 04:51:04.491 UTC [1103116] ERROR:  0A000: cannot push down this subquery
2023-11-25 04:51:04.491 UTC [1103116] DETAIL:  Limit in subquery without limit in the outermost query is unsupported
2023-11-25 04:51:04.491 UTC [1103116] LOCATION:  DeferErrorIfCannotPushdownSubquery, query_pushdown_planning.c:1048
2023-11-25 04:51:04.491 UTC [1103116] STATEMENT:  SELECT et.* FROM recent_10_users JOIN events_table et USING(user_id);
2023-11-25 04:51:04.999 UTC [1103114] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:51:04.999 UTC [1103114] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:51:04.999 UTC [1103114] STATEMENT:  SELECT
	  u1.user_id, count(*)
	FROM
	  events_table as e1, users_table as u1
	WHERE
	  event_type IN
	            (SELECT
	                event_type
	             FROM
	              events_reference_table as e2
	             WHERE
	              value_2 = 1 AND
	              value_3 > 3 AND
	              e1.value_2 > e2.value_2
	            )
	            AND u1.user_id > e1.user_id
	GROUP BY 1
	ORDER BY 2 DESC, 1 DESC
	LIMIT 5;
2023-11-25 04:51:05.011 UTC [1103116] ERROR:  0A000: cannot modify views when the query contains citus tables
2023-11-25 04:51:05.011 UTC [1103116] LOCATION:  DeferErrorIfModifyView, multi_router_planner.c:1137
2023-11-25 04:51:05.011 UTC [1103116] STATEMENT:  UPDATE small_view SET id = 1;
2023-11-25 04:51:05.011 UTC [1103116] ERROR:  0A000: cannot modify views when the query contains citus tables
2023-11-25 04:51:05.011 UTC [1103116] LOCATION:  DeferErrorIfModifyView, multi_router_planner.c:1137
2023-11-25 04:51:05.011 UTC [1103116] STATEMENT:  DELETE FROM small_view;
2023-11-25 04:51:05.011 UTC [1103116] ERROR:  0A000: cannot modify views when the query contains citus tables
2023-11-25 04:51:05.011 UTC [1103116] LOCATION:  DeferErrorIfModifyView, multi_router_planner.c:1137
2023-11-25 04:51:05.011 UTC [1103116] STATEMENT:  INSERT INTO small_view VALUES(8, 5) ON CONFLICT(tenant_id) DO UPDATE SET tenant_id=99;
2023-11-25 04:51:05.112 UTC [1103116] ERROR:  0A000: cannot modify views when the query contains citus tables
2023-11-25 04:51:05.112 UTC [1103116] LOCATION:  DeferErrorIfModifyView, multi_router_planner.c:1137
2023-11-25 04:51:05.112 UTC [1103116] STATEMENT:  INSERT INTO small_view VALUES(3, 3);
2023-11-25 04:51:05.224 UTC [1103114] ERROR:  0A000: FULL JOIN is only supported with merge-joinable or hash-joinable join conditions
2023-11-25 04:51:05.224 UTC [1103114] LOCATION:  populate_joinrel_with_paths, joinrels.c:853
2023-11-25 04:51:05.224 UTC [1103114] STATEMENT:  SELECT count(*) FROM users_ref_test_table ref1 INNER JOIN users_ref_test_table ref2 on ref1.id = ref2.id FULL JOIN user_buy_test_table ON (ref1.id > 5);
2023-11-25 04:51:05.224 UTC [1103114] ERROR:  0A000: FULL JOIN is only supported with merge-joinable or hash-joinable join conditions
2023-11-25 04:51:05.224 UTC [1103114] LOCATION:  populate_joinrel_with_paths, joinrels.c:853
2023-11-25 04:51:05.224 UTC [1103114] STATEMENT:  SELECT count(*) FROM users_ref_test_table ref1 INNER JOIN users_ref_test_table ref2 on ref1.id = ref2.id FULL JOIN user_buy_test_table ON (user_buy_test_table.user_id > 5);
2023-11-25 04:51:05.227 UTC [1103116] ERROR:  0A000: cannot modify views when the query contains citus tables
2023-11-25 04:51:05.227 UTC [1103116] LOCATION:  DeferErrorIfModifyView, multi_router_planner.c:1137
2023-11-25 04:51:05.227 UTC [1103116] STATEMENT:  UPDATE small_view SET id = 1;
2023-11-25 04:51:05.227 UTC [1103116] ERROR:  0A000: cannot modify views when the query contains citus tables
2023-11-25 04:51:05.227 UTC [1103116] LOCATION:  DeferErrorIfModifyView, multi_router_planner.c:1137
2023-11-25 04:51:05.227 UTC [1103116] STATEMENT:  DELETE FROM small_view;
2023-11-25 04:51:05.270 UTC [1103114] ERROR:  0A000: FULL JOIN is only supported with merge-joinable or hash-joinable join conditions
2023-11-25 04:51:05.270 UTC [1103114] LOCATION:  populate_joinrel_with_paths, joinrels.c:853
2023-11-25 04:51:05.270 UTC [1103114] STATEMENT:  SELECT count(*) FROM user_buy_test_table FULL JOIN users_ref_test_table ref1 INNER JOIN users_ref_test_table ref2 on ref1.id = ref2.id ON (ref1.id > 5);
2023-11-25 04:51:05.271 UTC [1103114] ERROR:  0A000: FULL JOIN is only supported with merge-joinable or hash-joinable join conditions
2023-11-25 04:51:05.271 UTC [1103114] LOCATION:  populate_joinrel_with_paths, joinrels.c:853
2023-11-25 04:51:05.271 UTC [1103114] STATEMENT:  SELECT count(*) FROM user_buy_test_table FULL JOIN users_ref_test_table ref1 INNER JOIN users_ref_test_table ref2 on ref1.id = ref2.id ON (user_buy_test_table.user_id > 5);
2023-11-25 04:51:05.308 UTC [1103114] ERROR:  0A000: FULL JOIN is only supported with merge-joinable or hash-joinable join conditions
2023-11-25 04:51:05.308 UTC [1103114] LOCATION:  populate_joinrel_with_paths, joinrels.c:853
2023-11-25 04:51:05.308 UTC [1103114] STATEMENT:  SELECT count(*) FROM (SELECT ref1.*, random() FROM users_ref_test_table ref1 INNER JOIN users_ref_test_table ref2 on ref1.id = ref2.id) as foo FULL JOIN user_buy_test_table ON (foo.id > 5);
2023-11-25 04:51:05.308 UTC [1103114] ERROR:  0A000: FULL JOIN is only supported with merge-joinable or hash-joinable join conditions
2023-11-25 04:51:05.308 UTC [1103114] LOCATION:  populate_joinrel_with_paths, joinrels.c:853
2023-11-25 04:51:05.308 UTC [1103114] STATEMENT:  SELECT count(*) FROM (SELECT ref1.*, random() FROM users_ref_test_table ref1 LEFT JOIN users_ref_test_table ref2 on ref1.id = ref2.id) as foo FULL JOIN user_buy_test_table ON (user_buy_test_table.user_id > 19);
2023-11-25 04:51:05.332 UTC [1103114] ERROR:  0A000: FULL JOIN is only supported with merge-joinable or hash-joinable join conditions
2023-11-25 04:51:05.332 UTC [1103114] LOCATION:  populate_joinrel_with_paths, joinrels.c:853
2023-11-25 04:51:05.332 UTC [1103114] STATEMENT:  SELECT count(*) FROM user_buy_test_table FULL JOIN (SELECT ref1.*, random() FROM users_ref_test_table ref1 INNER JOIN users_ref_test_table ref2 on ref1.id = ref2.id) as foo ON (foo.id > 5);
2023-11-25 04:51:05.332 UTC [1103114] ERROR:  0A000: FULL JOIN is only supported with merge-joinable or hash-joinable join conditions
2023-11-25 04:51:05.332 UTC [1103114] LOCATION:  populate_joinrel_with_paths, joinrels.c:853
2023-11-25 04:51:05.332 UTC [1103114] STATEMENT:  SELECT count(*) FROM user_buy_test_table FULL JOIN (SELECT ref1.*, random() FROM users_ref_test_table ref1 LEFT JOIN users_ref_test_table ref2 on ref1.id = ref2.id) as foo ON (user_buy_test_table.user_id > 19);
2023-11-25 04:51:05.469 UTC [1103116] ERROR:  55000: cannot insert into view "small_view"
2023-11-25 04:51:05.469 UTC [1103116] DETAIL:  Views that do not select from a single table or view are not automatically updatable.
2023-11-25 04:51:05.469 UTC [1103116] HINT:  To enable inserting into the view, provide an INSTEAD OF INSERT trigger or an unconditional ON INSERT DO INSTEAD rule.
2023-11-25 04:51:05.469 UTC [1103116] LOCATION:  rewriteTargetView, rewriteHandler.c:3096
2023-11-25 04:51:05.469 UTC [1103116] STATEMENT:  INSERT INTO small_view VALUES(3, 3);
2023-11-25 04:51:05.982 UTC [1103923] ERROR:  0A000: COMMIT is not allowed in an SQL function
2023-11-25 04:51:05.982 UTC [1103923] CONTEXT:  SQL function "test_procedure_commit" during startup
2023-11-25 04:51:05.982 UTC [1103923] LOCATION:  init_execution_state, functions.c:517
2023-11-25 04:51:05.982 UTC [1103923] STATEMENT:  CALL test_procedure_commit(2,5);
2023-11-25 04:51:05.991 UTC [1103923] ERROR:  0A000: ROLLBACK is not allowed in an SQL function
2023-11-25 04:51:05.991 UTC [1103923] CONTEXT:  SQL function "test_procedure_rollback" during startup
2023-11-25 04:51:05.991 UTC [1103923] LOCATION:  init_execution_state, functions.c:517
2023-11-25 04:51:05.991 UTC [1103923] STATEMENT:  CALL test_procedure_rollback(2,15);
2023-11-25 04:51:06.046 UTC [1103923] ERROR:  23505: duplicate key value violates unique constraint "idx_table_100503"
2023-11-25 04:51:06.046 UTC [1103923] DETAIL:  Key (id, org_id)=(2, 12) already exists.
2023-11-25 04:51:06.046 UTC [1103923] CONTEXT:  while executing command on localhost:57637
	SQL statement "INSERT INTO test_table VALUES (tt_id, tt_org_id)"
	PL/pgSQL function test_procedure_modify_insert(integer,integer) line 5 at SQL statement
2023-11-25 04:51:06.046 UTC [1103923] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:06.046 UTC [1103923] STATEMENT:  CALL test_procedure_modify_insert(2,12);
2023-11-25 04:51:06.060 UTC [1103924] ERROR:  0A000: input of anonymous composite types is not implemented
2023-11-25 04:51:06.060 UTC [1103924] LOCATION:  record_in, rowtypes.c:103
2023-11-25 04:51:06.060 UTC [1103924] STATEMENT:  SELECT ARRAY[(x,(y,x),y),(y,(x,y))] FROM test ORDER BY x, y;
2023-11-25 04:51:06.061 UTC [1103923] ERROR:  23505: duplicate key value violates unique constraint "idx_table_100503"
2023-11-25 04:51:06.061 UTC [1103923] DETAIL:  Key (id, org_id)=(2, 30) already exists.
2023-11-25 04:51:06.061 UTC [1103923] CONTEXT:  while executing command on localhost:57637
	SQL statement "INSERT INTO test_table VALUES (tt_id, tt_org_id)"
	PL/pgSQL function test_procedure_modify_insert_commit(integer,integer) line 5 at SQL statement
2023-11-25 04:51:06.061 UTC [1103923] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:06.061 UTC [1103923] STATEMENT:  CALL test_procedure_modify_insert_commit(2,30);
2023-11-25 04:51:06.073 UTC [1103924] ERROR:  0A000: input of anonymous composite types is not implemented
2023-11-25 04:51:06.073 UTC [1103924] LOCATION:  record_in, rowtypes.c:103
2023-11-25 04:51:06.073 UTC [1103924] STATEMENT:  SELECT ARRAY[[(x,(y,x))],[((x,x),y)]] FROM test ORDER BY x, y;
2023-11-25 04:51:06.092 UTC [1103924] ERROR:  0A000: input of anonymous composite types is not implemented
2023-11-25 04:51:06.092 UTC [1103924] LOCATION:  record_in, rowtypes.c:103
2023-11-25 04:51:06.092 UTC [1103924] STATEMENT:  SELECT CASE x WHEN 2 THEN (x, y, x) ELSE (y, x) END FROM test ORDER BY x, y;
2023-11-25 04:51:06.102 UTC [1103925] ERROR:  P0001: Task failed to execute
2023-11-25 04:51:06.102 UTC [1103925] CONTEXT:  PL/pgSQL function raise_failed_execution_func_join(text) line 8 at RAISE
2023-11-25 04:51:06.102 UTC [1103925] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:51:06.102 UTC [1103925] STATEMENT:  SELECT raise_failed_execution_func_join($$
	  WITH one_row AS (
	      SELECT * FROM table1 WHERE id=52
	      )
	  SELECT table1.id, table1.data
	  FROM one_row, table1, next_k_integers(one_row.id, 5) next_five_ids
	  WHERE table1.id = next_five_ids;
	$$);
2023-11-25 04:51:06.106 UTC [1103925] ERROR:  P0001: Task failed to execute
2023-11-25 04:51:06.106 UTC [1103925] CONTEXT:  PL/pgSQL function raise_failed_execution_func_join(text) line 8 at RAISE
2023-11-25 04:51:06.106 UTC [1103925] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:51:06.106 UTC [1103925] STATEMENT:  SELECT raise_failed_execution_func_join($$
	  SELECT * FROM table1 JOIN the_answer_to_life() the_answer ON (id = the_answer);
	$$);
2023-11-25 04:51:06.112 UTC [1103925] ERROR:  P0001: Task failed to execute
2023-11-25 04:51:06.112 UTC [1103925] CONTEXT:  PL/pgSQL function raise_failed_execution_func_join(text) line 8 at RAISE
2023-11-25 04:51:06.112 UTC [1103925] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:51:06.112 UTC [1103925] STATEMENT:  SELECT raise_failed_execution_func_join($$
	  SELECT *
	  FROM table1
	         JOIN next_k_integers(10,5) WITH ORDINALITY next_integers
	           ON (id = next_integers.result);
	$$);
2023-11-25 04:51:06.113 UTC [1103924] ERROR:  0A000: input of anonymous composite types is not implemented
2023-11-25 04:51:06.113 UTC [1103924] LOCATION:  record_in, rowtypes.c:103
2023-11-25 04:51:06.113 UTC [1103924] STATEMENT:  SELECT identity_returner((x, y)) FROM test ORDER BY x, y;
2023-11-25 04:51:06.119 UTC [1103925] ERROR:  P0001: Task failed to execute
2023-11-25 04:51:06.119 UTC [1103925] CONTEXT:  PL/pgSQL function raise_failed_execution_func_join(text) line 8 at RAISE
2023-11-25 04:51:06.119 UTC [1103925] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:51:06.119 UTC [1103925] STATEMENT:  SELECT raise_failed_execution_func_join($$
	  SELECT *
	  FROM table1
	         JOIN next_k_integers(10,5) WITH ORDINALITY next_integers
	           ON (id = next_integers.result)
	  ORDER BY id ASC;
	$$);
2023-11-25 04:51:06.129 UTC [1103924] ERROR:  0A000: input of anonymous composite types is not implemented
2023-11-25 04:51:06.129 UTC [1103924] LOCATION:  record_in, rowtypes.c:103
2023-11-25 04:51:06.129 UTC [1103924] STATEMENT:  SELECT array_agg((x, y)) FROM test;
2023-11-25 04:51:06.158 UTC [1103924] ERROR:  0A000: input of anonymous composite types is not implemented
2023-11-25 04:51:06.158 UTC [1103924] LOCATION:  record_in, rowtypes.c:103
2023-11-25 04:51:06.158 UTC [1103924] STATEMENT:  SELECT ARRAY[(x,(y,x),y),(y,(x,y))] FROM test WHERE x = 1 ORDER BY x, y;
2023-11-25 04:51:06.166 UTC [1103924] ERROR:  0A000: input of anonymous composite types is not implemented
2023-11-25 04:51:06.166 UTC [1103924] LOCATION:  record_in, rowtypes.c:103
2023-11-25 04:51:06.166 UTC [1103924] STATEMENT:  SELECT ARRAY[[(x,(y,x))],[((x,x),y)]] FROM test WHERE x = 1 ORDER BY x, y;
2023-11-25 04:51:06.172 UTC [1103924] ERROR:  0A000: input of anonymous composite types is not implemented
2023-11-25 04:51:06.172 UTC [1103924] LOCATION:  record_in, rowtypes.c:103
2023-11-25 04:51:06.172 UTC [1103924] STATEMENT:  SELECT CASE x WHEN 2 THEN (x, y, x) ELSE (y, x) END FROM test WHERE x = 1 ORDER BY x, y;
2023-11-25 04:51:06.182 UTC [1103924] ERROR:  0A000: input of anonymous composite types is not implemented
2023-11-25 04:51:06.182 UTC [1103924] LOCATION:  record_in, rowtypes.c:103
2023-11-25 04:51:06.182 UTC [1103924] STATEMENT:  SELECT identity_returner((x, y)) FROM test WHERE x = 1 ORDER BY x, y;
2023-11-25 04:51:06.190 UTC [1103924] ERROR:  0A000: input of anonymous composite types is not implemented
2023-11-25 04:51:06.190 UTC [1103924] LOCATION:  record_in, rowtypes.c:103
2023-11-25 04:51:06.190 UTC [1103924] STATEMENT:  SELECT array_agg((x, y)) FROM test WHERE x = 1;
2023-11-25 04:51:06.198 UTC [1103924] ERROR:  0A000: input of anonymous composite types is not implemented
2023-11-25 04:51:06.198 UTC [1103924] LOCATION:  record_in, rowtypes.c:103
2023-11-25 04:51:06.198 UTC [1103924] STATEMENT:  SELECT (x,table_returner(x)) FROM test WHERE x = 1 ORDER BY x, y;
2023-11-25 04:51:06.723 UTC [1103922] ERROR:  55000: materialized view "materialized_view" has not been populated
2023-11-25 04:51:06.723 UTC [1103922] HINT:  Use the REFRESH MATERIALIZED VIEW command.
2023-11-25 04:51:06.723 UTC [1103922] LOCATION:  ExecOpenScanRelation, execUtils.c:740
2023-11-25 04:51:06.723 UTC [1103922] STATEMENT:  SELECT count(*) FROM materialized_view;
2023-11-25 04:51:06.825 UTC [1103922] ERROR:  42809: cannot change materialized view "small_view"
2023-11-25 04:51:06.825 UTC [1103922] LOCATION:  CheckValidResultRel, execMain.c:1060
2023-11-25 04:51:06.825 UTC [1103922] STATEMENT:  UPDATE small_view SET id = 1;
2023-11-25 04:51:06.848 UTC [1103922] ERROR:  42809: cannot change materialized view "small_view"
2023-11-25 04:51:06.848 UTC [1103922] LOCATION:  CheckValidResultRel, execMain.c:1060
2023-11-25 04:51:06.848 UTC [1103922] STATEMENT:  INSERT INTO small_view VALUES(3, 3);
2023-11-25 04:51:06.936 UTC [1103922] ERROR:  42809: cannot change materialized view "small_view"
2023-11-25 04:51:06.936 UTC [1103922] LOCATION:  CheckValidResultRel, execMain.c:1060
2023-11-25 04:51:06.936 UTC [1103922] STATEMENT:  UPDATE small_view SET id = 1;
2023-11-25 04:51:06.936 UTC [1103922] ERROR:  42809: cannot change materialized view "small_view"
2023-11-25 04:51:06.936 UTC [1103922] LOCATION:  CheckValidResultRel, execMain.c:1060
2023-11-25 04:51:06.936 UTC [1103922] STATEMENT:  DELETE FROM small_view;
2023-11-25 04:51:07.210 UTC [1104290] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a reference table
2023-11-25 04:51:07.210 UTC [1104290] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:671
2023-11-25 04:51:07.210 UTC [1104290] STATEMENT:  SELECT
	  user_id
	FROM
	  users_reference_table
	WHERE
	  NOT EXISTS
	      (SELECT
	          value_2
	       FROM
	          events_table
	       WHERE
	          users_reference_table.user_id = events_table.user_id
	      )
	LIMIT 3;
2023-11-25 04:51:07.210 UTC [1104290] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a set returning function
2023-11-25 04:51:07.210 UTC [1104290] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:677
2023-11-25 04:51:07.210 UTC [1104290] STATEMENT:  SELECT
	  user_id
	FROM
	  (SELECT user_id FROM generate_series(1,10) AS series(user_id)) users_reference_table
	WHERE
	  NOT EXISTS
	      (SELECT
	          value_2
	       FROM
	          events_table
	       WHERE
	          users_reference_table.user_id = events_table.user_id
	      )
	LIMIT 3;
2023-11-25 04:51:07.211 UTC [1104290] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a CTE or subquery
2023-11-25 04:51:07.211 UTC [1104290] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:684
2023-11-25 04:51:07.211 UTC [1104290] STATEMENT:  SELECT
	  user_id
	FROM
	  (SELECT  5 AS user_id UNION ALL SELECT 6) users_reference_table
	WHERE
	  NOT EXISTS
	      (SELECT
	          value_2
	       FROM
	          events_table
	       WHERE
	          users_reference_table.user_id = events_table.user_id
	      )
	LIMIT 3;
2023-11-25 04:51:07.221 UTC [1104290] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a CTE or subquery
2023-11-25 04:51:07.221 UTC [1104290] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:684
2023-11-25 04:51:07.221 UTC [1104290] STATEMENT:  SELECT
	  DISTINCT user_id
	FROM
	  users_table RIGHT JOIN users_reference_table USING (user_id)
	WHERE
	  users_table.value_2 IN
	      (SELECT
	          value_2
	       FROM
	          events_table
	       WHERE
	          users_table.user_id = events_table.user_id
	      )
	ORDER BY user_id
	LIMIT 3;
2023-11-25 04:51:07.222 UTC [1104289] WARNING:  25001: SET TRANSACTION ISOLATION LEVEL must be called before any query
2023-11-25 04:51:07.222 UTC [1104289] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:51:07.222 UTC [1104289] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:07.222 UTC [1104289] ERROR:  XX000: failure on connection marked as essential: localhost:57637
2023-11-25 04:51:07.222 UTC [1104289] LOCATION:  MarkRemoteTransactionFailed, remote_transaction.c:840
2023-11-25 04:51:07.222 UTC [1104289] STATEMENT:  SET LOCAL TRANSACTION ISOLATION LEVEL REPEATABLE READ;
2023-11-25 04:51:07.223 UTC [1104289] ERROR:  25006: cannot execute INSERT in a read-only transaction
2023-11-25 04:51:07.223 UTC [1104289] LOCATION:  PreventCommandIfReadOnly, utility.c:414
2023-11-25 04:51:07.223 UTC [1104289] STATEMENT:  INSERT INTO test VALUES (2,2);
2023-11-25 04:51:07.228 UTC [1104289] WARNING:  25001: there is already a transaction in progress
2023-11-25 04:51:07.228 UTC [1104289] LOCATION:  BeginTransactionBlock, xact.c:3778
2023-11-25 04:51:07.229 UTC [1104289] WARNING:  25001: there is already a transaction in progress
2023-11-25 04:51:07.229 UTC [1104289] LOCATION:  BeginTransactionBlock, xact.c:3778
2023-11-25 04:51:07.229 UTC [1104289] ERROR:  25001: SET TRANSACTION ISOLATION LEVEL must be called before any query
2023-11-25 04:51:07.229 UTC [1104289] LOCATION:  call_enum_check_hook, guc.c:12007
2023-11-25 04:51:07.229 UTC [1104289] STATEMENT:  BEGIN TRANSACTION ISOLATION LEVEL REPEATABLE READ;
2023-11-25 04:51:07.244 UTC [1104290] ERROR:  0A000: cannot perform a lateral outer join when a distributed subquery references a reference table
2023-11-25 04:51:07.244 UTC [1104290] LOCATION:  DeferredErrorIfUnsupportedRecurringTuplesJoin, query_pushdown_planning.c:913
2023-11-25 04:51:07.244 UTC [1104290] STATEMENT:  SELECT user_id, value_2 FROM users_table WHERE
	  value_1 > 1 AND value_1 < 3
	  AND value_2 >= 5
	  AND user_id IN
	  (
			SELECT
			  e1.user_id
			FROM (
			  -- Get the first time each user viewed the homepage.
			  SELECT
			    user_id,
			    1 AS view_homepage,
			    min(time) AS view_homepage_time
			  FROM events_reference_table
			     WHERE
			     event_type IN (1, 2)
			  GROUP BY user_id
			) e1 LEFT JOIN LATERAL (
			  SELECT
			    user_id,
			    1 AS use_demo,
			    time AS use_demo_time
			  FROM events_table
			  WHERE
			    user_id = e1.user_id AND
			       event_type IN (2, 3)
			  ORDER BY time
			) e2 ON true LEFT JOIN LATERAL (
			  SELECT
			    user_id,
			    1 AS enter_credit_card,
			    time AS enter_credit_card_time
			  FROM  events_reference_table
			  WHERE
			    user_id = e2.user_id AND
			    event_type IN (3, 4)
			  ORDER BY time
			) e3 ON true LEFT JOIN LATERAL (
			  SELECT
			    1 AS submit_card_info,
			    user_id,
			    time AS enter_credit_card_time
			  FROM  events_reference_table
			  WHERE
			    user_id = e3.user_id AND
			    event_type IN (4, 5)
			  ORDER BY time
			) e4 ON true LEFT JOIN LATERAL (
			  SELECT
			    1 AS see_bought_screen
			  FROM  events_reference_table
			  WHERE
			    user_id = e4.user_id AND
			    event_type IN (5, 6)
			  ORDER BY time
			) e5 ON true
			group by e1.user_id
			HAVING sum(submit_card_info) > 0
	)
	ORDER BY 1, 2;
2023-11-25 04:51:07.281 UTC [1104290] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a reference table
2023-11-25 04:51:07.281 UTC [1104290] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:671
2023-11-25 04:51:07.281 UTC [1104290] STATEMENT:  SELECT user_id,
	       count(*)
	FROM users_reference_table
	WHERE value_2 > ALL
	    (SELECT min(value_2)
	     FROM events_table
	     WHERE event_type > 2 AND users_reference_table.user_id = events_table.user_id
	     GROUP BY user_id)
	GROUP BY user_id
	HAVING count(*) > 3
	ORDER BY 2 DESC,
	         1 DESC
	LIMIT 5;
2023-11-25 04:51:08.135 UTC [1104560] ERROR:  23505: duplicate key value violates unique constraint "test_forcepushdown_pkey_900015"
2023-11-25 04:51:08.135 UTC [1104560] DETAIL:  Key (intcol)=(3) already exists.
2023-11-25 04:51:08.135 UTC [1104560] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.test_forcepushdown VALUES (a)"
	PL/pgSQL function forcepushdown_schema.insert_data(integer) line 3 at SQL statement
	while executing command on localhost:57638
2023-11-25 04:51:08.135 UTC [1104560] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:08.135 UTC [1104560] STATEMENT:  SELECT insert_data(3);
2023-11-25 04:51:08.135 UTC [1104560] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 04:51:08.135 UTC [1104560] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 04:51:08.135 UTC [1104560] STATEMENT:  INSERT INTO forcepushdown_schema.test_forcepushdown VALUES (5);
2023-11-25 04:51:08.136 UTC [1104560] ERROR:  23505: duplicate key value violates unique constraint "test_forcepushdown_pkey_900000"
2023-11-25 04:51:08.136 UTC [1104560] DETAIL:  Key (intcol)=(8) already exists.
2023-11-25 04:51:08.136 UTC [1104560] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:51:08.136 UTC [1104560] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:08.136 UTC [1104560] STATEMENT:  INSERT INTO forcepushdown_schema.test_forcepushdown VALUES (8);
2023-11-25 04:51:08.137 UTC [1104560] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 04:51:08.137 UTC [1104560] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 04:51:08.137 UTC [1104560] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.test_forcepushdown VALUES (a+1)"
	PL/pgSQL function forcepushdown_schema.insert_data_non_distarg(integer) line 3 at SQL statement
	while executing command on localhost:57637
2023-11-25 04:51:08.137 UTC [1104560] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:08.137 UTC [1104560] STATEMENT:  SELECT insert_data_non_distarg(9);
2023-11-25 04:51:08.138 UTC [1104560] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 04:51:08.138 UTC [1104560] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 04:51:08.138 UTC [1104560] CONTEXT:  SQL statement "UPDATE forcepushdown_schema.test_forcepushdown SET data = 'non-default'"
	PL/pgSQL function forcepushdown_schema.update_data_nonlocal(integer) line 3 at SQL statement
	while executing command on localhost:57637
2023-11-25 04:51:08.138 UTC [1104560] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:08.138 UTC [1104560] STATEMENT:  SELECT update_data_nonlocal(12);
2023-11-25 04:51:08.139 UTC [1104560] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 04:51:08.139 UTC [1104560] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 04:51:08.139 UTC [1104560] STATEMENT:  INSERT INTO forcepushdown_schema.test_forcepushdown VALUES (13);
2023-11-25 04:51:08.141 UTC [1104560] ERROR:  XX000: cannot execute a distributed query from a query on a shard
2023-11-25 04:51:08.141 UTC [1104560] DETAIL:  Executing a distributed query in a function call that may be pushed to a remote node can lead to incorrect results.
2023-11-25 04:51:08.141 UTC [1104560] HINT:  Avoid nesting of distributed queries or use alter user current_user set citus.allow_nested_distributed_execution to on to allow it with possible incorrectness.
2023-11-25 04:51:08.141 UTC [1104560] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.test_forcepushdown VALUES (a)"
	PL/pgSQL function forcepushdown_schema.insert_data(integer) line 3 at SQL statement
	while executing command on localhost:57638
2023-11-25 04:51:08.141 UTC [1104560] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:08.141 UTC [1104560] STATEMENT:  SELECT insert_data(intcol+17) from test_forcepushdown where intcol = 1;
2023-11-25 04:51:08.142 UTC [1104560] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 04:51:08.142 UTC [1104560] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 04:51:08.142 UTC [1104560] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.test_forcepushdown_noncolocate VALUES (a)"
	PL/pgSQL function forcepushdown_schema.insert_data_noncolocation(integer) line 4 at SQL statement
	while executing command on localhost:57638
2023-11-25 04:51:08.142 UTC [1104560] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:08.142 UTC [1104560] STATEMENT:  SELECT insert_data_noncolocation(19);
2023-11-25 04:51:08.142 UTC [1104560] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 04:51:08.142 UTC [1104560] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 04:51:08.142 UTC [1104560] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.test_forcepushdown_noncolocate VALUES (a)"
	PL/pgSQL function forcepushdown_schema.insert_data_noncolocation(integer) line 4 at SQL statement
	while executing command on localhost:57638
2023-11-25 04:51:08.142 UTC [1104560] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:08.142 UTC [1104560] STATEMENT:  SELECT insert_data_noncolocation(19);
2023-11-25 04:51:08.192 UTC [1104560] ERROR:  XX000: cannot execute a distributed query from a query on a shard
2023-11-25 04:51:08.192 UTC [1104560] DETAIL:  Executing a distributed query in a function call that may be pushed to a remote node can lead to incorrect results.
2023-11-25 04:51:08.192 UTC [1104560] HINT:  Avoid nesting of distributed queries or use alter user current_user set citus.allow_nested_distributed_execution to on to allow it with possible incorrectness.
2023-11-25 04:51:08.192 UTC [1104560] CONTEXT:  SQL statement "SELECT max(id)::numeric+1               FROM forcepushdown_schema.test_nested WHERE id = $1"
	PL/pgSQL function forcepushdown_schema.inner_force_delegation_function(integer) line 4 at SQL statement
	while executing command on localhost:57638
2023-11-25 04:51:08.192 UTC [1104560] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:08.192 UTC [1104560] STATEMENT:  SELECT inner_force_delegation_function(id) FROM test_nested WHERE id = 300;
2023-11-25 04:51:08.193 UTC [1104560] ERROR:  XX000: cannot execute a distributed query from a query on a shard
2023-11-25 04:51:08.193 UTC [1104560] DETAIL:  Executing a distributed query in a function call that may be pushed to a remote node can lead to incorrect results.
2023-11-25 04:51:08.193 UTC [1104560] HINT:  Avoid nesting of distributed queries or use alter user current_user set citus.allow_nested_distributed_execution to on to allow it with possible incorrectness.
2023-11-25 04:51:08.193 UTC [1104560] CONTEXT:  SQL statement "SELECT max(id)::numeric+1               FROM forcepushdown_schema.test_nested WHERE id = $1"
	PL/pgSQL function forcepushdown_schema.inner_force_delegation_function(integer) line 4 at SQL statement
	while executing command on localhost:57637
2023-11-25 04:51:08.193 UTC [1104560] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:08.193 UTC [1104560] STATEMENT:  SELECT inner_force_delegation_function((SELECT id+112 FROM test_nested WHERE id=400));
2023-11-25 04:51:08.267 UTC [1104560] ERROR:  XX000: cannot execute a distributed query from a query on a shard
2023-11-25 04:51:08.267 UTC [1104560] DETAIL:  Executing a distributed query in a function call that may be pushed to a remote node can lead to incorrect results.
2023-11-25 04:51:08.267 UTC [1104560] HINT:  Avoid nesting of distributed queries or use alter user current_user set citus.allow_nested_distributed_execution to on to allow it with possible incorrectness.
2023-11-25 04:51:08.267 UTC [1104560] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.test_forcepushdown SELECT(a+1)"
	PL/pgSQL function forcepushdown_schema.insert_select_data(integer) line 3 at SQL statement
	while executing command on localhost:57637
2023-11-25 04:51:08.267 UTC [1104560] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:08.267 UTC [1104560] STATEMENT:  SELECT insert_select_data(20);
2023-11-25 04:51:08.278 UTC [1104560] ERROR:  XX000: cannot execute a distributed query from a query on a shard
2023-11-25 04:51:08.278 UTC [1104560] DETAIL:  Executing a distributed query in a function call that may be pushed to a remote node can lead to incorrect results.
2023-11-25 04:51:08.278 UTC [1104560] HINT:  Avoid nesting of distributed queries or use alter user current_user set citus.allow_nested_distributed_execution to on to allow it with possible incorrectness.
2023-11-25 04:51:08.278 UTC [1104560] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.test_forcepushdown SELECT(a+1)"
	PL/pgSQL function forcepushdown_schema.insert_select_data(integer) line 3 at SQL statement
	while executing command on localhost:57638
2023-11-25 04:51:08.278 UTC [1104560] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:08.278 UTC [1104560] STATEMENT:  SELECT insert_select_data(22);
2023-11-25 04:51:08.305 UTC [1104560] ERROR:  XX000: cannot execute a distributed query from a query on a shard
2023-11-25 04:51:08.305 UTC [1104560] DETAIL:  Executing a distributed query in a function call that may be pushed to a remote node can lead to incorrect results.
2023-11-25 04:51:08.305 UTC [1104560] HINT:  Avoid nesting of distributed queries or use alter user current_user set citus.allow_nested_distributed_execution to on to allow it with possible incorrectness.
2023-11-25 04:51:08.305 UTC [1104560] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.test_forcepushdown(intcol)
			SELECT intcol FROM forcepushdown_schema.test_forcepushdown_noncolocate"
	PL/pgSQL function forcepushdown_schema.insert_select_data_nonlocal(integer) line 3 at SQL statement
	while executing command on localhost:57638
2023-11-25 04:51:08.305 UTC [1104560] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:08.305 UTC [1104560] STATEMENT:  SELECT insert_select_data_nonlocal(41);
2023-11-25 04:51:08.408 UTC [1104560] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 04:51:08.408 UTC [1104560] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 04:51:08.408 UTC [1104560] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.test_forcepushdown_char VALUES (a)"
	PL/pgSQL function forcepushdown_schema.insert_data_char(character) line 3 at SQL statement
	while executing command on localhost:57638
2023-11-25 04:51:08.408 UTC [1104560] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:08.408 UTC [1104560] STATEMENT:  SELECT insert_data_char('CHAR');
2023-11-25 04:51:08.415 UTC [1104560] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 04:51:08.415 UTC [1104560] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 04:51:08.415 UTC [1104560] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.test_forcepushdown_char VALUES (a)"
	PL/pgSQL function forcepushdown_schema.insert_data_char(character) line 3 at SQL statement
	while executing command on localhost:57638
2023-11-25 04:51:08.415 UTC [1104560] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:08.415 UTC [1104560] STATEMENT:  SELECT insert_data_char('CHAR');
2023-11-25 04:51:08.503 UTC [1104560] ERROR:  XX000: cannot execute a distributed query from a query on a shard
2023-11-25 04:51:08.503 UTC [1104560] DETAIL:  Executing a distributed query in a function call that may be pushed to a remote node can lead to incorrect results.
2023-11-25 04:51:08.503 UTC [1104560] HINT:  Avoid nesting of distributed queries or use alter user current_user set citus.allow_nested_distributed_execution to on to allow it with possible incorrectness.
2023-11-25 04:51:08.503 UTC [1104560] CONTEXT:  SQL statement "SELECT result          FROM forcepushdown_schema.test_subquery WHERE data =
			(SELECT data FROM forcepushdown_schema.test_subquery WHERE data = a)"
	PL/pgSQL function forcepushdown_schema.select_data(integer) line 4 at SQL statement
	while executing command on localhost:57637
2023-11-25 04:51:08.503 UTC [1104560] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:08.503 UTC [1104560] STATEMENT:  SELECT select_data(100);
2023-11-25 04:51:08.511 UTC [1104560] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 04:51:08.511 UTC [1104560] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 04:51:08.511 UTC [1104560] CONTEXT:  SQL statement "SELECT data          FROM forcepushdown_schema.test_subquery WHERE data =
			(SELECT id FROM forcepushdown_schema.test_non_colocated WHERE id = a)"
	PL/pgSQL function forcepushdown_schema.select_data_noncolocate(integer) line 5 at SQL statement
	while executing command on localhost:57637
2023-11-25 04:51:08.511 UTC [1104560] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:08.511 UTC [1104560] STATEMENT:  SELECT select_data_noncolocate(100);
2023-11-25 04:51:08.518 UTC [1104560] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 04:51:08.518 UTC [1104560] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 04:51:08.518 UTC [1104560] CONTEXT:  SQL statement "SELECT data          FROM forcepushdown_schema.test_subquery WHERE data =
			(SELECT id FROM forcepushdown_schema.test_non_colocated WHERE id = a)"
	PL/pgSQL function forcepushdown_schema.select_data_noncolocate(integer) line 5 at SQL statement
	while executing command on localhost:57637
2023-11-25 04:51:08.518 UTC [1104560] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:08.518 UTC [1104560] STATEMENT:  SELECT select_data_noncolocate(100);
2023-11-25 04:51:08.532 UTC [1104560] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 04:51:08.532 UTC [1104560] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 04:51:08.532 UTC [1104560] CONTEXT:  SQL statement "WITH ins AS (INSERT INTO forcepushdown_schema.test_subquery VALUES (a+1) RETURNING data)
			SELECT ins.data          FROM forcepushdown_schema.test_subquery, ins WHERE forcepushdown_schema.test_subquery.data = a"
	PL/pgSQL function forcepushdown_schema.insert_data_cte_nondist(integer) line 5 at SQL statement
	while executing command on localhost:57637
2023-11-25 04:51:08.532 UTC [1104560] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:08.532 UTC [1104560] STATEMENT:  SELECT insert_data_cte_nondist(400);
2023-11-25 04:51:08.539 UTC [1104560] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 04:51:08.539 UTC [1104560] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 04:51:08.539 UTC [1104560] CONTEXT:  SQL statement "WITH ins AS (INSERT INTO forcepushdown_schema.test_subquery VALUES (a+1) RETURNING data)
			SELECT ins.data          FROM forcepushdown_schema.test_subquery, ins WHERE forcepushdown_schema.test_subquery.data = a"
	PL/pgSQL function forcepushdown_schema.insert_data_cte_nondist(integer) line 5 at SQL statement
	while executing command on localhost:57637
2023-11-25 04:51:08.539 UTC [1104560] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:08.539 UTC [1104560] STATEMENT:  SELECT insert_data_cte_nondist(400);
2023-11-25 04:51:08.563 UTC [1104560] ERROR:  XX000: cannot execute a distributed query from a query on a shard
2023-11-25 04:51:08.563 UTC [1104560] DETAIL:  Executing a distributed query in a function call that may be pushed to a remote node can lead to incorrect results.
2023-11-25 04:51:08.563 UTC [1104560] HINT:  Avoid nesting of distributed queries or use alter user current_user set citus.allow_nested_distributed_execution to on to allow it with possible incorrectness.
2023-11-25 04:51:08.563 UTC [1104560] CONTEXT:  SQL statement "SELECT result          FROM forcepushdown_schema.test_subquery WHERE data =
			(SELECT data FROM forcepushdown_schema.test_subquery WHERE data = a)"
	PL/pgSQL function forcepushdown_schema.select_data(integer) line 4 at SQL statement
	while executing command on localhost:57637
2023-11-25 04:51:08.563 UTC [1104560] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:08.563 UTC [1104560] STATEMENT:  SELECT 1,2,3 FROM select_data(100);
2023-11-25 04:51:08.583 UTC [1104560] ERROR:  42883: function test_prepare(integer, integer) does not exist
2023-11-25 04:51:08.583 UTC [1104560] LOCATION:  LookupFuncWithArgs, parse_func.c:2444
2023-11-25 04:51:08.583 UTC [1104560] STATEMENT:  DROP FUNCTION test_prepare(int, int);
2023-11-25 04:51:08.588 UTC [1104560] ERROR:  42883: function outer_test_prepare(integer, integer) does not exist
2023-11-25 04:51:08.588 UTC [1104560] LOCATION:  LookupFuncWithArgs, parse_func.c:2444
2023-11-25 04:51:08.588 UTC [1104560] STATEMENT:  DROP FUNCTION outer_test_prepare(int, int);
2023-11-25 04:51:08.603 UTC [1104560] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 04:51:08.603 UTC [1104560] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 04:51:08.603 UTC [1104560] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.table_test_prepare VALUES (y, x)"
	PL/pgSQL function forcepushdown_schema.test_prepare(integer,integer) line 5 at SQL statement
	while executing command on localhost:57638
	SQL statement "SELECT FROM test_prepare(x, y)"
	PL/pgSQL function outer_test_prepare(integer,integer) line 5 at PERFORM
2023-11-25 04:51:08.603 UTC [1104560] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:08.603 UTC [1104560] STATEMENT:  SELECT outer_test_prepare(1,2);
2023-11-25 04:51:08.686 UTC [1104560] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 04:51:08.686 UTC [1104560] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 04:51:08.686 UTC [1104560] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.testnested_table VALUES (x,x)"
	PL/pgSQL function forcepushdown_schema.inner_fn(integer) line 4 at SQL statement
	SQL statement "SELECT 1 FROM forcepushdown_schema.inner_fn(z)"
	PL/pgSQL function forcepushdown_schema.outer_fn(integer,integer) line 6 at PERFORM
	while executing command on localhost:57638
2023-11-25 04:51:08.686 UTC [1104560] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:08.686 UTC [1104560] STATEMENT:  SELECT outer_fn(1, 2);
2023-11-25 04:51:08.693 UTC [1104560] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 04:51:08.693 UTC [1104560] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 04:51:08.693 UTC [1104560] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.testnested_table VALUES (x,x)"
	PL/pgSQL function forcepushdown_schema.inner_fn(integer) line 4 at SQL statement
	SQL statement "SELECT 1 FROM forcepushdown_schema.inner_fn(z)"
	PL/pgSQL function forcepushdown_schema.outer_fn(integer,integer) line 6 at PERFORM
	while executing command on localhost:57638
2023-11-25 04:51:08.693 UTC [1104560] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:08.693 UTC [1104560] STATEMENT:  SELECT outer_fn(1, 2);
2023-11-25 04:51:08.708 UTC [1104560] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 04:51:08.708 UTC [1104560] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 04:51:08.708 UTC [1104560] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.testnested_table VALUES (y,y)"
	PL/pgSQL function forcepushdown_schema.force_push_inner(integer) line 4 at SQL statement
	SQL statement "SELECT forcepushdown_schema.force_push_inner(x+1) LIMIT 1"
	PL/pgSQL function forcepushdown_schema.force_push_outer(integer) line 5 at PERFORM
	while executing command on localhost:57637
2023-11-25 04:51:08.708 UTC [1104560] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:08.708 UTC [1104560] STATEMENT:  SELECT force_push_outer(7);
2023-11-25 04:51:08.714 UTC [1104560] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 04:51:08.714 UTC [1104560] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 04:51:08.714 UTC [1104560] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.testnested_table VALUES (y,y)"
	PL/pgSQL function forcepushdown_schema.force_push_inner(integer) line 4 at SQL statement
	SQL statement "SELECT forcepushdown_schema.force_push_inner(x+1) LIMIT 1"
	PL/pgSQL function forcepushdown_schema.force_push_outer(integer) line 5 at PERFORM
	while executing command on localhost:57637
2023-11-25 04:51:08.714 UTC [1104560] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:08.714 UTC [1104560] STATEMENT:  SELECT force_push_outer(8);
2023-11-25 04:51:08.715 UTC [1104560] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 04:51:08.715 UTC [1104560] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 04:51:08.715 UTC [1104560] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.testnested_table VALUES (y,y)"
	PL/pgSQL function forcepushdown_schema.force_push_inner(integer) line 4 at SQL statement
	SQL statement "SELECT forcepushdown_schema.force_push_inner(x+1) LIMIT 1"
	PL/pgSQL function forcepushdown_schema.force_push_outer(integer) line 5 at PERFORM
	while executing command on localhost:57637
2023-11-25 04:51:08.715 UTC [1104560] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:08.715 UTC [1104560] STATEMENT:  SELECT force_push_outer(14);
2023-11-25 04:51:08.742 UTC [1104560] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 04:51:08.742 UTC [1104560] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 04:51:08.742 UTC [1104560] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.testnested_table VALUES (y,y)"
	PL/pgSQL function forcepushdown_schema.force_push_2(integer) line 4 at SQL statement
	SQL statement "SELECT forcepushdown_schema.force_push_2(x+1) LIMIT 1"
	PL/pgSQL function forcepushdown_schema.force_push_1(integer) line 5 at PERFORM
	while executing command on localhost:57637
2023-11-25 04:51:08.742 UTC [1104560] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:08.742 UTC [1104560] STATEMENT:  SELECT force_push_1(7);
2023-11-25 04:51:08.743 UTC [1104560] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 04:51:08.743 UTC [1104560] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 04:51:08.743 UTC [1104560] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.testnested_table VALUES (y,y)"
	PL/pgSQL function forcepushdown_schema.force_push_2(integer) line 4 at SQL statement
	SQL statement "SELECT forcepushdown_schema.force_push_2(x+1) LIMIT 1"
	PL/pgSQL function forcepushdown_schema.force_push_1(integer) line 5 at PERFORM
	while executing command on localhost:57638
2023-11-25 04:51:08.743 UTC [1104560] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:08.743 UTC [1104560] STATEMENT:  SELECT force_push_1(13);
2023-11-25 04:51:08.765 UTC [1104560] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 04:51:08.765 UTC [1104560] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 04:51:08.765 UTC [1104560] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.testnested_table VALUES (x+1,x+1)"
	PL/pgSQL function forcepushdown_schema.force_push_outer(integer) line 5 at SQL statement
	while executing command on localhost:57637
2023-11-25 04:51:08.765 UTC [1104560] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:08.765 UTC [1104560] STATEMENT:  SELECT force_push_outer(7);
2023-11-25 04:51:09.188 UTC [1104752] ERROR:  0A000: cannot push down this subquery
2023-11-25 04:51:09.188 UTC [1104752] DETAIL:  Shards of relations in subquery need to have 1-to-1 shard partitioning
2023-11-25 04:51:09.188 UTC [1104752] LOCATION:  ErrorIfUnsupportedShardDistribution, multi_physical_planner.c:2432
2023-11-25 04:51:09.188 UTC [1104752] STATEMENT:  SELECT * FROM test_table_1 full join test_table_2 using(id);
2023-11-25 04:51:09.882 UTC [1105258] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:51:09.882 UTC [1105258] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:51:09.882 UTC [1105258] STATEMENT:  SELECT
	  user_id
	FROM
	  users_table
	WHERE
	  value_2 >
	          (SELECT
	              max(value_2)
	           FROM
	              events_table
	           WHERE
	              users_table.user_id > events_table.user_id AND event_type = 1 AND
	              users_table.time = events_table.time
	           GROUP BY
	              user_id
	          )
	GROUP BY user_id
	HAVING count(*) > 1
	ORDER BY user_id
	LIMIT 5;
2023-11-25 04:51:09.941 UTC [1105258] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:51:09.941 UTC [1105258] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:51:09.941 UTC [1105258] STATEMENT:  SELECT user_id, value_2 FROM users_table WHERE
	  value_1 > 1 AND value_1 < 2
	  AND value_2 >= 1
	  AND user_id IN
	  (
			SELECT
			  e1.user_id
			FROM (
			  -- Get the first time each user viewed the homepage.
			  SELECT
			    user_id,
			    1 AS view_homepage,
			    min(time) AS view_homepage_time
			  FROM events_table
			     WHERE
			     event_type IN (0, 1)
			  GROUP BY user_id
			) e1 LEFT JOIN LATERAL (
			  SELECT
			    user_id,
			    1 AS use_demo,
			    time AS use_demo_time
			  FROM events_table
			  WHERE
			    user_id = e1.user_id AND
			       event_type IN (1, 2)
			  ORDER BY time
			) e2 ON true LEFT JOIN LATERAL (
			  SELECT
			    user_id,
			    1 AS enter_credit_card,
			    time AS enter_credit_card_time
			  FROM  events_table
			  WHERE
			    user_id = e2.user_id AND
			    event_type IN (2, 3)
			  ORDER BY time
			) e3 ON true LEFT JOIN LATERAL (
			  SELECT
			    1 AS submit_card_info,
			    user_id,
			    time AS enter_credit_card_time
			  FROM  events_table
			  WHERE
			    value_2 = e3.user_id AND
			    event_type IN (3, 4)
			  ORDER BY time
			) e4 ON true LEFT JOIN LATERAL (
			  SELECT
			    1 AS see_bought_screen
			  FROM  events_table
			  WHERE
			    user_id = e4.user_id AND
			    event_type IN (5, 6)
			  ORDER BY time
			) e5 ON true
			group by e1.user_id
			HAVING sum(submit_card_info) > 0
	);
2023-11-25 04:51:09.958 UTC [1105258] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:51:09.958 UTC [1105258] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:51:09.958 UTC [1105258] STATEMENT:  SELECT user_id, array_length(events_table, 1)
	FROM (
	  SELECT user_id, array_agg(event ORDER BY time) AS events_table
	  FROM (
	    SELECT
	    	u.user_id, e.event_type::text AS event, e.time
	    FROM
	    	users_table AS u,
	        events_table AS e
	    WHERE u.user_id = e.user_id AND
	    		u.user_id IN
	    		(
	    			SELECT
	    				user_id
	    			FROM
	    				users_table
	    			WHERE value_2 >= 5
				    AND  EXISTS (SELECT user_id FROM events_table WHERE event_type > 1 AND event_type <= 3 AND value_3 > 1 AND user_id = users_table.user_id)
					AND  NOT EXISTS (SELECT user_id FROM events_table WHERE event_type > 3 AND event_type <= 4  AND value_3 > 1 AND user_id != users_table.user_id)
	    		)
	  ) t
	  GROUP BY user_id
	) q
	ORDER BY 2 DESC, 1;
2023-11-25 04:51:10.016 UTC [1105259] ERROR:  0A000: could not create distributed plan
2023-11-25 04:51:10.016 UTC [1105259] DETAIL:  Possibly this is caused by the use of parameters in SQL functions, which is not supported in Citus.
2023-11-25 04:51:10.016 UTC [1105259] HINT:  Consider using PL/pgSQL functions instead.
2023-11-25 04:51:10.016 UTC [1105259] CONTEXT:  SQL function "sql_subquery_test" statement 1
2023-11-25 04:51:10.016 UTC [1105259] LOCATION:  CreateDistributedPlannedStmt, distributed_planner.c:751
2023-11-25 04:51:10.016 UTC [1105259] STATEMENT:  SELECT sql_subquery_test(1,1);
2023-11-25 04:51:10.029 UTC [1105258] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:51:10.029 UTC [1105258] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:51:10.029 UTC [1105258] STATEMENT:  SELECT user_id, value_2 FROM users_table WHERE
	  value_1 = 1
	  AND value_2 >= 2
	  AND NOT EXISTS (SELECT user_id FROM events_table WHERE event_type=1 AND value_3 > 1 AND test_join_function(events_table.user_id, users_table.user_id))
	ORDER BY 1 DESC, 2 DESC
	LIMIT 3;
2023-11-25 04:51:10.064 UTC [1105257] ERROR:  0A000: complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
2023-11-25 04:51:10.064 UTC [1105257] LOCATION:  JoinOrderList, multi_join_order.c:310
2023-11-25 04:51:10.064 UTC [1105257] STATEMENT:  SELECT user_id, sum(counter)
	FROM (
	    SELECT user_id, sum(value_2) AS counter FROM users_table GROUP BY user_id
	      UNION
	    SELECT events_table.user_id, sum(events_table.value_2) AS counter FROM events_table, users_table WHERE users_table.user_id > events_table.user_id GROUP BY 1
	) user_id
	GROUP BY user_id;
2023-11-25 04:51:10.064 UTC [1105257] ERROR:  0A000: complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
2023-11-25 04:51:10.064 UTC [1105257] LOCATION:  JoinOrderList, multi_join_order.c:310
2023-11-25 04:51:10.064 UTC [1105257] STATEMENT:  SELECT user_id, sum(counter)
	FROM (
	    SELECT user_id, sum(value_2) AS counter FROM users_table GROUP BY user_id
	      UNION ALL
	    SELECT events_table.user_id, sum(events_table.value_2) AS counter FROM events_table, users_table WHERE users_table.user_id > events_table.user_id GROUP BY 1
	) user_id
	GROUP BY user_id;
2023-11-25 04:51:10.127 UTC [1105257] ERROR:  0A000: cannot perform distributed planning on this query because parameterized queries for SQL functions referencing distributed tables are not supported
2023-11-25 04:51:10.127 UTC [1105257] HINT:  Consider using PL/pgSQL functions instead.
2023-11-25 04:51:10.127 UTC [1105257] LOCATION:  distributed_planner, distributed_planner.c:301
2023-11-25 04:51:10.127 UTC [1105257] STATEMENT:  SELECT user_id FROM users_table
	UNION SELECT u.user_id FROM users_table, users_udf() u;
2023-11-25 04:51:11.020 UTC [1105256] ERROR:  40P01: canceling the transaction since it was involved in a distributed deadlock
2023-11-25 04:51:11.020 UTC [1105256] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 04:51:11.020 UTC [1105256] STATEMENT:  SELECT id, pg_advisory_lock(15) FROM t_lock;
2023-11-25 04:51:11.032 UTC [1105256] ERROR:  57014: canceling statement due to statement timeout
2023-11-25 04:51:11.032 UTC [1105256] LOCATION:  ProcessInterrupts, postgres.c:3326
2023-11-25 04:51:11.032 UTC [1105256] STATEMENT:  INSERT INTO t_unrelated SELECT i FROM generate_series(1, 10) i;
2023-11-25 04:51:11.475 UTC [1105511] ERROR:  25001: cannot perform query on placements that were modified in this transaction by a different user
2023-11-25 04:51:11.475 UTC [1105511] LOCATION:  FindPlacementListConnection, placement_connection.c:647
2023-11-25 04:51:11.475 UTC [1105511] STATEMENT:  SELECT * FROM t;
2023-11-25 04:51:11.475 UTC [1105511] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 04:51:11.475 UTC [1105511] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 04:51:11.475 UTC [1105511] STATEMENT:  INSERT INTO t values (1);
2023-11-25 04:51:11.475 UTC [1105511] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 04:51:11.475 UTC [1105511] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 04:51:11.475 UTC [1105511] STATEMENT:  SELECT * from t;
2023-11-25 04:51:11.494 UTC [1105511] ERROR:  25001: cannot perform query on placements that were modified in this transaction by a different user
2023-11-25 04:51:11.494 UTC [1105511] LOCATION:  FindPlacementListConnection, placement_connection.c:647
2023-11-25 04:51:11.494 UTC [1105511] STATEMENT:  INSERT INTO t values (2);
2023-11-25 04:51:11.531 UTC [1105511] ERROR:  25001: cannot perform query on placements that were modified in this transaction by a different user
2023-11-25 04:51:11.531 UTC [1105511] CONTEXT:  COPY t, line 1: "1"
2023-11-25 04:51:11.531 UTC [1105511] LOCATION:  FindPlacementListConnection, placement_connection.c:647
2023-11-25 04:51:11.531 UTC [1105511] STATEMENT:  COPY t FROM STDIN;
2023-11-25 04:51:11.640 UTC [1105509] LOG:  00000: join order: [ "multi_outer_join_left_hash" ]
2023-11-25 04:51:11.640 UTC [1105509] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:11.640 UTC [1105509] STATEMENT:  SELECT
		min(r_custkey), max(r_custkey)
	FROM
		multi_outer_join_left_hash a RIGHT JOIN multi_outer_join_right_reference b ON (l_custkey = r_custkey);
2023-11-25 04:51:11.662 UTC [1105509] ERROR:  XX000: hash partitioned table has overlapping shards
2023-11-25 04:51:11.662 UTC [1105509] LOCATION:  ErrorIfInconsistentShardIntervals, metadata_cache.c:1981
2023-11-25 04:51:11.662 UTC [1105509] STATEMENT:  SELECT
		min(l_custkey), max(l_custkey)
	FROM
		multi_outer_join_left_hash a LEFT JOIN multi_outer_join_right_hash b ON (l_custkey = r_custkey);
2023-11-25 04:51:11.825 UTC [1105509] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:51:11.825 UTC [1105509] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:51:11.825 UTC [1105509] STATEMENT:  SELECT
		count(*)
	FROM
		multi_outer_join_left_hash a LEFT JOIN multi_outer_join_right_hash b ON (l_nationkey = r_nationkey);
2023-11-25 04:51:11.846 UTC [1105509] LOG:  00000: join order: [ "multi_outer_join_left_hash" ]
2023-11-25 04:51:11.846 UTC [1105509] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:11.846 UTC [1105509] STATEMENT:  SELECT
		min(r_custkey), max(r_custkey)
	FROM
		multi_outer_join_left_hash a RIGHT JOIN multi_outer_join_right_reference b ON (l_custkey = r_custkey);
2023-11-25 04:51:11.857 UTC [1105509] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:51:11.857 UTC [1105509] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:51:11.857 UTC [1105509] STATEMENT:  SELECT
		*
	FROM
		multi_outer_join_left_hash l1
		LEFT JOIN multi_outer_join_right_reference r1 ON (l1.l_custkey = r1.r_custkey)
		LEFT JOIN multi_outer_join_right_reference r2 ON (l1.l_custkey  = r2.r_custkey)
		RIGHT JOIN multi_outer_join_left_hash l2 ON (r2.r_custkey = l2.l_custkey);
2023-11-25 04:51:11.857 UTC [1105509] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:51:11.857 UTC [1105509] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:51:11.857 UTC [1105509] STATEMENT:  SELECT
		*
	FROM
		multi_outer_join_left_hash l1
		LEFT JOIN multi_outer_join_right_reference r1 ON (l1.l_custkey = r1.r_custkey)
		LEFT JOIN multi_outer_join_right_reference r2 ON (l1.l_custkey  = r2.r_custkey)
		RIGHT JOIN multi_outer_join_left_hash l2 ON (r2.r_custkey = l2.l_custkey)
	WHERE
		r1.r_custkey is NULL;
2023-11-25 04:51:11.865 UTC [1105509] LOG:  00000: join order: [ "multi_outer_join_left_hash" ]
2023-11-25 04:51:11.865 UTC [1105509] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:11.865 UTC [1105509] STATEMENT:  SELECT
		l_custkey, r_custkey, t_custkey
	FROM
		multi_outer_join_left_hash l1
		LEFT JOIN multi_outer_join_right_hash r1 ON (l1.l_custkey = r1.r_custkey)
		RIGHT JOIN multi_outer_join_third_reference t1 ON (r1.r_custkey  = t1.t_custkey)
	ORDER BY 1,2,3;
2023-11-25 04:51:11.866 UTC [1105509] LOG:  00000: join order: [ "multi_outer_join_right_hash" ]
2023-11-25 04:51:11.866 UTC [1105509] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:11.866 UTC [1105509] STATEMENT:  SELECT
		l_custkey, r_custkey, t_custkey
	FROM
		multi_outer_join_left_hash l1
		LEFT JOIN multi_outer_join_right_hash r1 ON (l1.l_custkey = r1.r_custkey)
		RIGHT JOIN multi_outer_join_third_reference t1 ON (r1.r_custkey  = t1.t_custkey)
	ORDER BY 1,2,3;
2023-11-25 04:51:11.907 UTC [1105509] LOG:  00000: join order: [ "multi_outer_join_left_hash" ]
2023-11-25 04:51:11.907 UTC [1105509] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:11.907 UTC [1105509] STATEMENT:  SELECT
		l_custkey, t_custkey
	FROM
		multi_outer_join_left_hash l1
		FULL JOIN multi_outer_join_third_reference t1 ON (l1.l_custkey = t1.t_custkey)
	ORDER BY 1,2;
2023-11-25 04:51:11.946 UTC [1105508] ERROR:  0A000: cannot compute aggregate (distinct)
2023-11-25 04:51:11.946 UTC [1105508] DETAIL:  table partitioning is unsuitable for aggregate (distinct)
2023-11-25 04:51:11.946 UTC [1105508] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4205
2023-11-25 04:51:11.946 UTC [1105508] STATEMENT:  SELECT SUM(distinct l_partkey) FROM lineitem_hash;
2023-11-25 04:51:11.946 UTC [1105508] ERROR:  0A000: cannot compute aggregate (distinct)
2023-11-25 04:51:11.946 UTC [1105508] DETAIL:  table partitioning is unsuitable for aggregate (distinct)
2023-11-25 04:51:11.946 UTC [1105508] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4205
2023-11-25 04:51:11.946 UTC [1105508] STATEMENT:  SELECT l_shipmode, sum(distinct l_partkey) FROM lineitem_hash GROUP BY l_shipmode;
2023-11-25 04:51:12.361 UTC [1105877] ERROR:  0A000: could not run distributed query with FOR UPDATE/SHARE commands
2023-11-25 04:51:12.361 UTC [1105877] HINT:  Consider using an equality filter on the distributed table's partition column.
2023-11-25 04:51:12.361 UTC [1105877] LOCATION:  DeferErrorIfQueryNotSupported, multi_logical_planner.c:945
2023-11-25 04:51:12.361 UTC [1105877] STATEMENT:  SELECT * FROM
		test_table_1_rf1 as tt1 INNER JOIN test_table_1_rf1 as tt2 on tt1.id = tt2.id
		ORDER BY 1
		FOR UPDATE;
2023-11-25 04:51:12.362 UTC [1105877] ERROR:  0A000: could not run distributed query with FOR UPDATE/SHARE commands
2023-11-25 04:51:12.362 UTC [1105877] LOCATION:  DeferErrorIfQueryNotSupported, multi_logical_planner.c:945
2023-11-25 04:51:12.362 UTC [1105877] STATEMENT:  SELECT * FROM
		test_table_1_rf1 as tt1 INNER JOIN test_table_3_rf2 as tt3 on tt1.id = tt3.id
		WHERE tt1.id = 1
		ORDER BY 1
		FOR UPDATE;
2023-11-25 04:51:12.362 UTC [1105877] ERROR:  0A000: could not run distributed query with FOR UPDATE/SHARE commands
2023-11-25 04:51:12.362 UTC [1105877] LOCATION:  DeferErrorIfQueryNotSupported, multi_logical_planner.c:945
2023-11-25 04:51:12.362 UTC [1105877] STATEMENT:  SELECT * FROM
		test_table_1_rf1 as tt1 INNER JOIN test_table_3_rf2 as tt3 on tt1.id = tt3.id
		ORDER BY 1
		FOR UPDATE;
2023-11-25 04:51:12.362 UTC [1105877] ERROR:  0A000: could not run distributed query with FOR UPDATE/SHARE commands
2023-11-25 04:51:12.362 UTC [1105877] HINT:  Consider using an equality filter on the distributed table's partition column.
2023-11-25 04:51:12.362 UTC [1105877] LOCATION:  DeferErrorIfQueryNotSupported, multi_logical_planner.c:945
2023-11-25 04:51:12.362 UTC [1105877] STATEMENT:  SELECT * FROM
		test_table_3_rf2 as tt3 INNER JOIN test_table_4_rf2 as tt4 on tt3.id = tt4.id
		WHERE tt3.id = 1
		ORDER BY 1
		FOR UPDATE;
2023-11-25 04:51:12.401 UTC [1105875] ERROR:  23505: duplicate key value violates unique constraint "reference_table_test_fourth_pkey_1250003"
2023-11-25 04:51:12.401 UTC [1105875] DETAIL:  Key (value_2)=(1) already exists.
2023-11-25 04:51:12.401 UTC [1105875] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:51:12.401 UTC [1105875] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:12.401 UTC [1105875] STATEMENT:  INSERT INTO reference_table_test_fourth VALUES (1, 1.0, '1', '2016-12-01');
2023-11-25 04:51:12.402 UTC [1105875] ERROR:  23502: null value in column "value_2" of relation "reference_table_test_fourth_1250003" violates not-null constraint
2023-11-25 04:51:12.402 UTC [1105875] DETAIL:  Failing row contains (1, null, 1.0, 2016-12-01 00:00:00).
2023-11-25 04:51:12.402 UTC [1105875] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:51:12.402 UTC [1105875] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:12.402 UTC [1105875] STATEMENT:  INSERT INTO reference_table_test_fourth (value_1, value_3, value_4) VALUES (1, '1.0', '2016-12-01');
2023-11-25 04:51:12.515 UTC [1105875] LOG:  00000: join order: [ "colocated_table_test" ][ reference join "reference_table_test" ]
2023-11-25 04:51:12.515 UTC [1105875] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:12.515 UTC [1105875] STATEMENT:  SELECT
		reference_table_test.value_1
	FROM
		reference_table_test, colocated_table_test
	WHERE
		colocated_table_test.value_1 = reference_table_test.value_1
	ORDER BY 1;
2023-11-25 04:51:12.522 UTC [1105875] LOG:  00000: join order: [ "colocated_table_test" ][ reference join "reference_table_test" ]
2023-11-25 04:51:12.522 UTC [1105875] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:12.522 UTC [1105875] STATEMENT:  SELECT
		colocated_table_test.value_2
	FROM
		reference_table_test, colocated_table_test
	WHERE
		colocated_table_test.value_2 = reference_table_test.value_2
	ORDER BY 1;
2023-11-25 04:51:12.532 UTC [1105875] LOG:  00000: join order: [ "colocated_table_test" ][ reference join "reference_table_test" ]
2023-11-25 04:51:12.532 UTC [1105875] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:12.532 UTC [1105875] STATEMENT:  SELECT
		colocated_table_test.value_2
	FROM
		colocated_table_test, reference_table_test
	WHERE
		reference_table_test.value_1 = colocated_table_test.value_1
	ORDER BY 1;
2023-11-25 04:51:12.540 UTC [1105875] LOG:  00000: join order: [ "colocated_table_test_2" ][ cartesian product reference join "reference_table_test" ][ dual partition join "colocated_table_test" ]
2023-11-25 04:51:12.540 UTC [1105875] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:12.540 UTC [1105875] STATEMENT:  SELECT
		colocated_table_test.value_2
	FROM
		reference_table_test, colocated_table_test, colocated_table_test_2
	WHERE
		colocated_table_test.value_2 = reference_table_test.value_2
	ORDER BY colocated_table_test.value_2;
2023-11-25 04:51:12.693 UTC [1105875] LOG:  00000: join order: [ "colocated_table_test" ][ reference join "reference_table_test" ][ local partition join "colocated_table_test_2" ]
2023-11-25 04:51:12.693 UTC [1105875] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:12.693 UTC [1105875] STATEMENT:  SELECT
		colocated_table_test.value_2
	FROM
		reference_table_test, colocated_table_test, colocated_table_test_2
	WHERE
		colocated_table_test.value_1 = colocated_table_test_2.value_1 AND colocated_table_test.value_2 = reference_table_test.value_2
	ORDER BY 1;
2023-11-25 04:51:12.699 UTC [1105875] LOG:  00000: join order: [ "colocated_table_test" ][ reference join "reference_table_test" ][ dual partition join "colocated_table_test_2" ]
2023-11-25 04:51:12.699 UTC [1105875] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:12.699 UTC [1105875] STATEMENT:  SELECT
		colocated_table_test.value_2
	FROM
		reference_table_test, colocated_table_test, colocated_table_test_2
	WHERE
		colocated_table_test.value_2 = colocated_table_test_2.value_2 AND colocated_table_test.value_2 = reference_table_test.value_2
	ORDER BY colocated_table_test.value_2;
2023-11-25 04:51:12.843 UTC [1105875] LOG:  00000: join order: [ "colocated_table_test" ][ reference join "reference_table_test" ][ dual partition join "colocated_table_test_2" ]
2023-11-25 04:51:12.843 UTC [1105875] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:12.843 UTC [1105875] STATEMENT:  SELECT
		reference_table_test.value_2
	FROM
		reference_table_test, colocated_table_test, colocated_table_test_2
	WHERE
		colocated_table_test.value_1 = reference_table_test.value_1 AND colocated_table_test_2.value_1 = reference_table_test.value_1
	ORDER BY reference_table_test.value_2;
2023-11-25 04:51:13.033 UTC [1105875] ERROR:  0A000: relation reference_table_test should be a hash distributed table
2023-11-25 04:51:13.033 UTC [1105875] LOCATION:  EnsureHashDistributedTable, metadata_utility.c:2284
2023-11-25 04:51:13.033 UTC [1105875] STATEMENT:  SELECT update_distributed_table_colocation('colocated_table_test_2', colocate_with => 'reference_table_test');
2023-11-25 04:51:13.033 UTC [1105875] ERROR:  0A000: relation reference_table_test_fifth should be a hash distributed table
2023-11-25 04:51:13.033 UTC [1105875] LOCATION:  EnsureHashDistributedTable, metadata_utility.c:2284
2023-11-25 04:51:13.033 UTC [1105875] STATEMENT:  SELECT update_distributed_table_colocation('reference_table_test', colocate_with => 'reference_table_test_fifth');
2023-11-25 04:51:13.172 UTC [1106430] ERROR:  XX000: relation "reference_schema.reference_table_ddl" is a reference table
2023-11-25 04:51:13.172 UTC [1106430] DETAIL:  We currently don't support creating shards on reference tables
2023-11-25 04:51:13.172 UTC [1106430] LOCATION:  master_create_empty_shard, stage_protocol.c:143
2023-11-25 04:51:13.172 UTC [1106430] STATEMENT:  SELECT master_create_empty_shard('reference_schema.reference_table_ddl');
2023-11-25 04:51:13.503 UTC [1106491] ERROR:  42704: type "hll" does not exist at character 51
2023-11-25 04:51:13.503 UTC [1106491] LOCATION:  typenameType, parse_type.c:270
2023-11-25 04:51:13.503 UTC [1106491] STATEMENT:  CREATE TABLE daily_uniques(day date, unique_users hll);
2023-11-25 04:51:13.519 UTC [1106491] ERROR:  42P01: relation "daily_uniques" does not exist at character 33
2023-11-25 04:51:13.519 UTC [1106491] LOCATION:  RangeVarGetRelidExtended, namespace.c:433
2023-11-25 04:51:13.519 UTC [1106491] STATEMENT:  SELECT create_distributed_table('daily_uniques', 'day');
2023-11-25 04:51:13.571 UTC [1106492] ERROR:  42883: function tdigest(double precision, integer) does not exist at character 37
2023-11-25 04:51:13.571 UTC [1106492] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:51:13.571 UTC [1106492] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 04:51:13.571 UTC [1106492] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT tdigest(latency, 100)
	FROM latencies;
2023-11-25 04:51:13.571 UTC [1106492] ERROR:  42883: function tdigest(double precision, integer) does not exist at character 40
2023-11-25 04:51:13.571 UTC [1106492] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:51:13.571 UTC [1106492] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 04:51:13.571 UTC [1106492] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT a, tdigest(latency, 100)
	FROM latencies
	GROUP BY a;
2023-11-25 04:51:13.571 UTC [1106492] ERROR:  42883: function tdigest(double precision, integer) does not exist at character 40
2023-11-25 04:51:13.571 UTC [1106492] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:51:13.571 UTC [1106492] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 04:51:13.571 UTC [1106492] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT b, tdigest(latency, 100)
	FROM latencies
	GROUP BY b;
2023-11-25 04:51:13.571 UTC [1106492] ERROR:  42883: function tdigest_percentile(double precision, integer, numeric) does not exist at character 37
2023-11-25 04:51:13.571 UTC [1106492] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:51:13.571 UTC [1106492] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 04:51:13.571 UTC [1106492] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT tdigest_percentile(latency, 100, 0.99)
	FROM latencies;
2023-11-25 04:51:13.571 UTC [1106492] ERROR:  42883: function tdigest_percentile(double precision, integer, numeric) does not exist at character 40
2023-11-25 04:51:13.571 UTC [1106492] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:51:13.571 UTC [1106492] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 04:51:13.571 UTC [1106492] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT a, tdigest_percentile(latency, 100, 0.99)
	FROM latencies
	GROUP BY a;
2023-11-25 04:51:13.572 UTC [1106492] ERROR:  42883: function tdigest_percentile(double precision, integer, numeric) does not exist at character 40
2023-11-25 04:51:13.572 UTC [1106492] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:51:13.572 UTC [1106492] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 04:51:13.572 UTC [1106492] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT b, tdigest_percentile(latency, 100, 0.99)
	FROM latencies
	GROUP BY b;
2023-11-25 04:51:13.572 UTC [1106492] ERROR:  42883: function tdigest_percentile(double precision, integer, numeric[]) does not exist at character 37
2023-11-25 04:51:13.572 UTC [1106492] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:51:13.572 UTC [1106492] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 04:51:13.572 UTC [1106492] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT tdigest_percentile(latency, 100, ARRAY[0.99, 0.95])
	FROM latencies;
2023-11-25 04:51:13.572 UTC [1106492] ERROR:  42883: function tdigest_percentile(double precision, integer, numeric[]) does not exist at character 40
2023-11-25 04:51:13.572 UTC [1106492] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:51:13.572 UTC [1106492] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 04:51:13.572 UTC [1106492] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT a, tdigest_percentile(latency, 100, ARRAY[0.99, 0.95])
	FROM latencies
	GROUP BY a;
2023-11-25 04:51:13.572 UTC [1106492] ERROR:  42883: function tdigest_percentile(double precision, integer, numeric[]) does not exist at character 40
2023-11-25 04:51:13.572 UTC [1106492] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:51:13.572 UTC [1106492] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 04:51:13.572 UTC [1106492] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT b, tdigest_percentile(latency, 100, ARRAY[0.99, 0.95])
	FROM latencies
	GROUP BY b;
2023-11-25 04:51:13.572 UTC [1106492] ERROR:  42883: function tdigest_percentile_of(double precision, integer, integer) does not exist at character 37
2023-11-25 04:51:13.572 UTC [1106492] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:51:13.572 UTC [1106492] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 04:51:13.572 UTC [1106492] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT tdigest_percentile_of(latency, 100, 9000)
	FROM latencies;
2023-11-25 04:51:13.572 UTC [1106492] ERROR:  42883: function tdigest_percentile_of(double precision, integer, integer) does not exist at character 40
2023-11-25 04:51:13.572 UTC [1106492] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:51:13.572 UTC [1106492] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 04:51:13.572 UTC [1106492] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT a, tdigest_percentile_of(latency, 100, 9000)
	FROM latencies
	GROUP BY a;
2023-11-25 04:51:13.573 UTC [1106492] ERROR:  42883: function tdigest_percentile_of(double precision, integer, integer) does not exist at character 40
2023-11-25 04:51:13.573 UTC [1106492] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:51:13.573 UTC [1106492] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 04:51:13.573 UTC [1106492] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT b, tdigest_percentile_of(latency, 100, 9000)
	FROM latencies
	GROUP BY b;
2023-11-25 04:51:13.573 UTC [1106492] ERROR:  42883: function tdigest_percentile_of(double precision, integer, integer[]) does not exist at character 37
2023-11-25 04:51:13.573 UTC [1106492] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:51:13.573 UTC [1106492] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 04:51:13.573 UTC [1106492] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT tdigest_percentile_of(latency, 100, ARRAY[9000, 9500])
	FROM latencies;
2023-11-25 04:51:13.573 UTC [1106492] ERROR:  42883: function tdigest_percentile_of(double precision, integer, integer[]) does not exist at character 40
2023-11-25 04:51:13.573 UTC [1106492] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:51:13.573 UTC [1106492] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 04:51:13.573 UTC [1106492] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT a, tdigest_percentile_of(latency, 100, ARRAY[9000, 9500])
	FROM latencies
	GROUP BY a;
2023-11-25 04:51:13.573 UTC [1106492] ERROR:  42883: function tdigest_percentile_of(double precision, integer, integer[]) does not exist at character 40
2023-11-25 04:51:13.573 UTC [1106492] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:51:13.573 UTC [1106492] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 04:51:13.573 UTC [1106492] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT b, tdigest_percentile_of(latency, 100, ARRAY[9000, 9500])
	FROM latencies
	GROUP BY b;
2023-11-25 04:51:13.573 UTC [1106492] ERROR:  42883: function tdigest(double precision, integer) does not exist at character 8
2023-11-25 04:51:13.573 UTC [1106492] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:51:13.573 UTC [1106492] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 04:51:13.573 UTC [1106492] STATEMENT:  SELECT tdigest(latency, 100) FROM latencies;
2023-11-25 04:51:13.573 UTC [1106492] ERROR:  42883: function tdigest_percentile(double precision, integer, numeric) does not exist at character 8
2023-11-25 04:51:13.573 UTC [1106492] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:51:13.573 UTC [1106492] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 04:51:13.573 UTC [1106492] STATEMENT:  SELECT tdigest_percentile(latency, 100, 0.99) FROM latencies;
2023-11-25 04:51:13.573 UTC [1106492] ERROR:  42883: function tdigest_percentile(double precision, integer, numeric[]) does not exist at character 8
2023-11-25 04:51:13.573 UTC [1106492] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:51:13.573 UTC [1106492] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 04:51:13.573 UTC [1106492] STATEMENT:  SELECT tdigest_percentile(latency, 100, ARRAY[0.99, 0.95]) FROM latencies;
2023-11-25 04:51:13.574 UTC [1106492] ERROR:  42883: function tdigest_percentile_of(double precision, integer, integer) does not exist at character 8
2023-11-25 04:51:13.574 UTC [1106492] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:51:13.574 UTC [1106492] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 04:51:13.574 UTC [1106492] STATEMENT:  SELECT tdigest_percentile_of(latency, 100, 9000) FROM latencies;
2023-11-25 04:51:13.574 UTC [1106492] ERROR:  42883: function tdigest_percentile_of(double precision, integer, integer[]) does not exist at character 8
2023-11-25 04:51:13.574 UTC [1106492] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:51:13.574 UTC [1106492] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 04:51:13.574 UTC [1106492] STATEMENT:  SELECT tdigest_percentile_of(latency, 100, ARRAY[9000, 9500]) FROM latencies;
2023-11-25 04:51:13.574 UTC [1106492] ERROR:  42704: type "tdigest" does not exist at character 47
2023-11-25 04:51:13.574 UTC [1106492] LOCATION:  typenameType, parse_type.c:270
2023-11-25 04:51:13.574 UTC [1106492] STATEMENT:  CREATE TABLE latencies_rollup (a int, tdigest tdigest);
2023-11-25 04:51:13.574 UTC [1106492] ERROR:  42P01: relation "latencies_rollup" does not exist at character 33
2023-11-25 04:51:13.574 UTC [1106492] LOCATION:  RangeVarGetRelidExtended, namespace.c:433
2023-11-25 04:51:13.574 UTC [1106492] STATEMENT:  SELECT create_distributed_table('latencies_rollup', 'a', colocate_with => 'latencies');
2023-11-25 04:51:13.574 UTC [1106492] ERROR:  42P01: relation "latencies_rollup" does not exist at character 13
2023-11-25 04:51:13.574 UTC [1106492] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 04:51:13.574 UTC [1106492] STATEMENT:  INSERT INTO latencies_rollup
	SELECT a, tdigest(latency, 100)
	FROM latencies
	GROUP BY a;
2023-11-25 04:51:13.574 UTC [1106492] ERROR:  42P01: relation "latencies_rollup" does not exist at character 59
2023-11-25 04:51:13.574 UTC [1106492] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 04:51:13.574 UTC [1106492] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT tdigest(tdigest)
	FROM latencies_rollup;
2023-11-25 04:51:13.574 UTC [1106492] ERROR:  42P01: relation "latencies_rollup" does not exist at character 62
2023-11-25 04:51:13.574 UTC [1106492] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 04:51:13.574 UTC [1106492] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT a, tdigest(tdigest)
	FROM latencies_rollup
	GROUP BY a;
2023-11-25 04:51:13.574 UTC [1106492] ERROR:  42P01: relation "latencies_rollup" does not exist at character 76
2023-11-25 04:51:13.574 UTC [1106492] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 04:51:13.574 UTC [1106492] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT tdigest_percentile(tdigest, 0.99)
	FROM latencies_rollup;
2023-11-25 04:51:13.574 UTC [1106492] ERROR:  42P01: relation "latencies_rollup" does not exist at character 79
2023-11-25 04:51:13.574 UTC [1106492] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 04:51:13.574 UTC [1106492] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT a, tdigest_percentile(tdigest, 0.99)
	FROM latencies_rollup
	GROUP BY a;
2023-11-25 04:51:13.574 UTC [1106492] ERROR:  42P01: relation "latencies_rollup" does not exist at character 89
2023-11-25 04:51:13.574 UTC [1106492] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 04:51:13.574 UTC [1106492] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT tdigest_percentile(tdigest, ARRAY[0.99, 0.95])
	FROM latencies_rollup;
2023-11-25 04:51:13.575 UTC [1106492] ERROR:  42P01: relation "latencies_rollup" does not exist at character 92
2023-11-25 04:51:13.575 UTC [1106492] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 04:51:13.575 UTC [1106492] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT a, tdigest_percentile(tdigest, ARRAY[0.99, 0.95])
	FROM latencies_rollup
	GROUP BY a;
2023-11-25 04:51:13.575 UTC [1106492] ERROR:  42P01: relation "latencies_rollup" does not exist at character 79
2023-11-25 04:51:13.575 UTC [1106492] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 04:51:13.575 UTC [1106492] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT tdigest_percentile_of(tdigest, 9000)
	FROM latencies_rollup;
2023-11-25 04:51:13.575 UTC [1106492] ERROR:  42P01: relation "latencies_rollup" does not exist at character 82
2023-11-25 04:51:13.575 UTC [1106492] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 04:51:13.575 UTC [1106492] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT a, tdigest_percentile_of(tdigest, 9000)
	FROM latencies_rollup
	GROUP BY a;
2023-11-25 04:51:13.575 UTC [1106492] ERROR:  42P01: relation "latencies_rollup" does not exist at character 92
2023-11-25 04:51:13.575 UTC [1106492] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 04:51:13.575 UTC [1106492] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT tdigest_percentile_of(tdigest, ARRAY[9000, 9500])
	FROM latencies_rollup;
2023-11-25 04:51:13.575 UTC [1106492] ERROR:  42P01: relation "latencies_rollup" does not exist at character 95
2023-11-25 04:51:13.575 UTC [1106492] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 04:51:13.575 UTC [1106492] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT a, tdigest_percentile_of(tdigest, ARRAY[9000, 9500])
	FROM latencies_rollup
	GROUP BY a;
2023-11-25 04:51:13.575 UTC [1106492] ERROR:  42P01: relation "latencies_rollup" does not exist at character 30
2023-11-25 04:51:13.575 UTC [1106492] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 04:51:13.575 UTC [1106492] STATEMENT:  SELECT tdigest(tdigest) FROM latencies_rollup;
2023-11-25 04:51:13.575 UTC [1106492] ERROR:  42P01: relation "latencies_rollup" does not exist at character 47
2023-11-25 04:51:13.575 UTC [1106492] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 04:51:13.575 UTC [1106492] STATEMENT:  SELECT tdigest_percentile(tdigest, 0.99) FROM latencies_rollup;
2023-11-25 04:51:13.575 UTC [1106492] ERROR:  42P01: relation "latencies_rollup" does not exist at character 60
2023-11-25 04:51:13.575 UTC [1106492] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 04:51:13.575 UTC [1106492] STATEMENT:  SELECT tdigest_percentile(tdigest, ARRAY[0.99, 0.95]) FROM latencies_rollup;
2023-11-25 04:51:13.575 UTC [1106492] ERROR:  42P01: relation "latencies_rollup" does not exist at character 50
2023-11-25 04:51:13.575 UTC [1106492] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 04:51:13.575 UTC [1106492] STATEMENT:  SELECT tdigest_percentile_of(tdigest, 9000) FROM latencies_rollup;
2023-11-25 04:51:13.575 UTC [1106492] ERROR:  42P01: relation "latencies_rollup" does not exist at character 63
2023-11-25 04:51:13.575 UTC [1106492] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 04:51:13.575 UTC [1106492] STATEMENT:  SELECT tdigest_percentile_of(tdigest, ARRAY[9000, 9500]) FROM latencies_rollup;
2023-11-25 04:51:13.580 UTC [1106490] ERROR:  0A000: cannot compute aggregate (distinct)
2023-11-25 04:51:13.580 UTC [1106490] DETAIL:  table partitioning is unsuitable for aggregate (distinct)
2023-11-25 04:51:13.580 UTC [1106490] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4205
2023-11-25 04:51:13.580 UTC [1106490] STATEMENT:  select key, sum2(distinct val), sum2_strict(distinct val), psum(distinct val, valf::int), psum_strict(distinct val, valf::int) from aggdata group by key order by key;
2023-11-25 04:51:13.584 UTC [1106490] ERROR:  XX000: unsupported aggregate function sum2
2023-11-25 04:51:13.584 UTC [1106490] LOCATION:  GetAggregateType, multi_logical_optimizer.c:3481
2023-11-25 04:51:13.584 UTC [1106490] STATEMENT:  select key, sum2(val order by valf), sum2_strict(val order by valf), psum(val, valf::int order by valf), psum_strict(val, valf::int order by valf) from aggdata group by key order by key;
2023-11-25 04:51:13.610 UTC [1106491] ERROR:  42883: function hll_hash_integer(integer) does not exist at character 72
2023-11-25 04:51:13.610 UTC [1106491] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:51:13.610 UTC [1106491] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 04:51:13.610 UTC [1106491] STATEMENT:  SELECT hll_cardinality(hll_union_agg(agg))
	FROM (
	  SELECT hll_add_agg(hll_hash_integer(user_id)) AS agg
	  FROM raw_table)a;
2023-11-25 04:51:13.610 UTC [1106491] ERROR:  42P01: relation "daily_uniques" does not exist at character 13
2023-11-25 04:51:13.610 UTC [1106491] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 04:51:13.610 UTC [1106491] STATEMENT:  INSERT INTO daily_uniques
	  SELECT day, hll_add_agg(hll_hash_integer(user_id))
	  FROM raw_table
	  GROUP BY 1;
2023-11-25 04:51:13.610 UTC [1106491] ERROR:  42P01: relation "daily_uniques" does not exist at character 48
2023-11-25 04:51:13.610 UTC [1106491] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 04:51:13.610 UTC [1106491] STATEMENT:  SELECT day, hll_cardinality(unique_users)
	FROM daily_uniques
	WHERE day >= '2018-06-20' and day <= '2018-06-30'
	ORDER BY 2 DESC,1
	LIMIT 10;
2023-11-25 04:51:13.610 UTC [1106491] ERROR:  42P01: relation "daily_uniques" does not exist at character 58
2023-11-25 04:51:13.610 UTC [1106491] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 04:51:13.610 UTC [1106491] STATEMENT:  SELECT hll_cardinality(hll_union_agg(unique_users))
	FROM daily_uniques
	WHERE day >= '2018-05-24'::date AND day <= '2018-05-31'::date;
2023-11-25 04:51:13.610 UTC [1106491] ERROR:  42P01: relation "daily_uniques" does not exist at character 92
2023-11-25 04:51:13.610 UTC [1106491] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 04:51:13.610 UTC [1106491] STATEMENT:  SELECT EXTRACT(MONTH FROM day) AS month, hll_cardinality(hll_union_agg(unique_users))
	FROM daily_uniques
	WHERE day >= '2018-06-23' AND day <= '2018-07-01'
	GROUP BY 1
	ORDER BY 1;
2023-11-25 04:51:13.611 UTC [1106491] ERROR:  42P01: relation "daily_uniques" does not exist at character 79
2023-11-25 04:51:13.611 UTC [1106491] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 04:51:13.611 UTC [1106491] STATEMENT:  SELECT day, hll_cardinality(hll_union_agg(unique_users) OVER seven_days)
	FROM daily_uniques
	WINDOW seven_days AS (ORDER BY day ASC ROWS 6 PRECEDING)
	ORDER BY 1;
2023-11-25 04:51:13.611 UTC [1106491] ERROR:  42P01: relation "daily_uniques" does not exist at character 127
2023-11-25 04:51:13.611 UTC [1106491] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 04:51:13.611 UTC [1106491] STATEMENT:  SELECT day, (hll_cardinality(hll_union_agg(unique_users) OVER two_days)) - hll_cardinality(unique_users) AS lost_uniques
	FROM daily_uniques
	WINDOW two_days AS (ORDER BY day ASC ROWS 1 PRECEDING)
	ORDER BY 1;
2023-11-25 04:51:13.611 UTC [1106491] ERROR:  42P01: relation "daily_uniques" does not exist at character 69
2023-11-25 04:51:13.611 UTC [1106491] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 04:51:13.611 UTC [1106491] STATEMENT:  EXPLAIN(COSTS OFF)
	SELECT
	  day, hll_union_agg(unique_users)
	FROM
	  daily_uniques
	GROUP BY(1);
2023-11-25 04:51:13.611 UTC [1106491] ERROR:  42P01: relation "daily_uniques" does not exist at character 69
2023-11-25 04:51:13.611 UTC [1106491] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 04:51:13.611 UTC [1106491] STATEMENT:  EXPLAIN(COSTS OFF)
	SELECT
	  day, hll_union_agg(unique_users)
	FROM
	  daily_uniques
	GROUP BY(1);
2023-11-25 04:51:13.611 UTC [1106491] ERROR:  42P01: relation "daily_uniques" does not exist at character 100
2023-11-25 04:51:13.611 UTC [1106491] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 04:51:13.611 UTC [1106491] STATEMENT:  EXPLAIN(COSTS OFF)
	SELECT
	  day, hll_union_agg(unique_users) || hll_union_agg(unique_users)
	FROM
	  daily_uniques
	GROUP BY(1);
2023-11-25 04:51:13.611 UTC [1106491] ERROR:  42P01: relation "daily_uniques" does not exist at character 100
2023-11-25 04:51:13.611 UTC [1106491] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 04:51:13.611 UTC [1106491] STATEMENT:  EXPLAIN(COSTS OFF)
	SELECT
	  day, hll_union_agg(unique_users) || hll_union_agg(unique_users)
	FROM
	  daily_uniques
	GROUP BY(1);
2023-11-25 04:51:13.611 UTC [1106491] ERROR:  42P01: relation "daily_uniques" does not exist at character 86
2023-11-25 04:51:13.611 UTC [1106491] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 04:51:13.611 UTC [1106491] STATEMENT:  EXPLAIN(COSTS OFF)
	SELECT
	  day, hll_cardinality(hll_union_agg(unique_users))
	FROM
	  daily_uniques
	GROUP BY(1);
2023-11-25 04:51:13.612 UTC [1106491] ERROR:  42P01: relation "daily_uniques" does not exist at character 86
2023-11-25 04:51:13.612 UTC [1106491] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 04:51:13.612 UTC [1106491] STATEMENT:  EXPLAIN(COSTS OFF)
	SELECT
	  day, hll_cardinality(hll_union_agg(unique_users))
	FROM
	  daily_uniques
	GROUP BY(1);
2023-11-25 04:51:13.612 UTC [1106491] ERROR:  42P01: relation "daily_uniques" does not exist at character 86
2023-11-25 04:51:13.612 UTC [1106491] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 04:51:13.612 UTC [1106491] STATEMENT:  EXPLAIN(COSTS OFF)
	SELECT
	  day, hll_cardinality(hll_union_agg(unique_users))
	FROM
	  daily_uniques
	GROUP BY(1);
2023-11-25 04:51:13.612 UTC [1106491] ERROR:  42P01: relation "daily_uniques" does not exist at character 86
2023-11-25 04:51:13.612 UTC [1106491] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 04:51:13.612 UTC [1106491] STATEMENT:  EXPLAIN(COSTS OFF)
	SELECT
	  day, hll_cardinality(hll_union_agg(unique_users))
	FROM
	  daily_uniques
	GROUP BY(1)
	HAVING hll_cardinality(hll_union_agg(unique_users)) > 1;
2023-11-25 04:51:13.626 UTC [1106491] ERROR:  42P01: table "daily_uniques" does not exist
2023-11-25 04:51:13.626 UTC [1106491] LOCATION:  DropErrorMsgNonExistent, tablecmds.c:1290
2023-11-25 04:51:13.626 UTC [1106491] STATEMENT:  DROP TABLE daily_uniques;
2023-11-25 04:51:13.774 UTC [1106491] ERROR:  42883: function topn_add_agg(text) does not exist at character 42
2023-11-25 04:51:13.774 UTC [1106491] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:51:13.774 UTC [1106491] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 04:51:13.774 UTC [1106491] STATEMENT:  SELECT (topn(agg, 10)).*
	FROM (
	  SELECT topn_add_agg(user_id::text) AS agg
	  FROM customer_reviews
	  )a
	ORDER BY 2 DESC, 1;
2023-11-25 04:51:13.775 UTC [1106491] ERROR:  42883: function topn_add_agg(text) does not exist at character 44
2023-11-25 04:51:13.775 UTC [1106491] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:51:13.775 UTC [1106491] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 04:51:13.775 UTC [1106491] STATEMENT:  INSERT INTO popular_reviewer
	  SELECT day, topn_add_agg(user_id::text)
	  FROM customer_reviews
	  GROUP BY 1;
2023-11-25 04:51:13.775 UTC [1106491] ERROR:  42883: function topn(jsonb, integer) does not exist at character 14
2023-11-25 04:51:13.775 UTC [1106491] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:51:13.775 UTC [1106491] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 04:51:13.775 UTC [1106491] STATEMENT:  SELECT day, (topn(reviewers, 10)).*
	FROM popular_reviewer
	WHERE day >= '2018-06-20' and day <= '2018-06-30'
	ORDER BY 3 DESC, 1, 2
	LIMIT 10;
2023-11-25 04:51:13.775 UTC [1106491] ERROR:  42883: function topn_union_agg(jsonb) does not exist at character 41
2023-11-25 04:51:13.775 UTC [1106491] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:51:13.775 UTC [1106491] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 04:51:13.775 UTC [1106491] STATEMENT:  SELECT (topn(agg, 10)).*
	FROM (
		SELECT topn_union_agg(reviewers) AS agg
		FROM popular_reviewer
		WHERE day >= '2018-05-24'::date AND day <= '2018-05-31'::date
		)a
	ORDER BY 2 DESC, 1;
2023-11-25 04:51:13.776 UTC [1106491] ERROR:  42883: function topn_union_agg(jsonb) does not exist at character 81
2023-11-25 04:51:13.776 UTC [1106491] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:51:13.776 UTC [1106491] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 04:51:13.776 UTC [1106491] STATEMENT:  SELECT month, (topn(agg, 5)).*
	FROM (
		SELECT EXTRACT(MONTH FROM day) AS month, topn_union_agg(reviewers) AS agg
		FROM popular_reviewer
		WHERE day >= '2018-06-23' AND day <= '2018-07-01'
		GROUP BY 1
		ORDER BY 1
		)a
	ORDER BY 1, 3 DESC, 2;
2023-11-25 04:51:13.776 UTC [1106491] ERROR:  42883: function topn_union_agg(jsonb) does not exist at character 14
2023-11-25 04:51:13.776 UTC [1106491] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:51:13.776 UTC [1106491] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 04:51:13.776 UTC [1106491] STATEMENT:  SELECT (topn(topn_union_agg(reviewers), 10)).*
	FROM popular_reviewer
	WHERE day >= '2018-05-24'::date AND day <= '2018-05-31'::date
	ORDER BY 2 DESC, 1;
2023-11-25 04:51:13.776 UTC [1106491] ERROR:  42883: function topn_add_agg(text) does not exist at character 14
2023-11-25 04:51:13.776 UTC [1106491] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:51:13.776 UTC [1106491] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 04:51:13.776 UTC [1106491] STATEMENT:  SELECT (topn(topn_add_agg(user_id::text), 10)).*
	FROM customer_reviews
	ORDER BY 2 DESC, 1;
2023-11-25 04:51:13.776 UTC [1106491] ERROR:  42883: function topn_union_agg(jsonb) does not exist at character 51
2023-11-25 04:51:13.776 UTC [1106491] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:51:13.776 UTC [1106491] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 04:51:13.776 UTC [1106491] STATEMENT:  SELECT day, (topn(agg, 10)).*
	FROM (
		SELECT day, topn_union_agg(reviewers) OVER seven_days AS agg
		FROM popular_reviewer
		WINDOW seven_days AS (ORDER BY day ASC ROWS 6 PRECEDING)
		)a
	ORDER BY 3 DESC, 1, 2
	LIMIT 10;
2023-11-25 04:51:13.776 UTC [1106491] ERROR:  42883: function topn_add_agg(text) does not exist at character 19
2023-11-25 04:51:13.776 UTC [1106491] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:51:13.776 UTC [1106491] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 04:51:13.776 UTC [1106491] STATEMENT:  SELECT day, (topn(topn_add_agg(user_id::text) OVER seven_days, 10)).*
	FROM customer_reviews
	WINDOW seven_days AS (ORDER BY day ASC ROWS 6 PRECEDING)
	ORDER BY 3 DESC, 1, 2
	LIMIT 10;
2023-11-25 04:51:13.951 UTC [1106490] ERROR:  XX000: unsupported aggregate function first
2023-11-25 04:51:13.951 UTC [1106490] LOCATION:  GetAggregateType, multi_logical_optimizer.c:3481
2023-11-25 04:51:13.951 UTC [1106490] STATEMENT:  SELECT key, first(val ORDER BY id), last(val ORDER BY id)
	FROM aggdata GROUP BY key ORDER BY key;
2023-11-25 04:51:13.953 UTC [1106490] ERROR:  XX000: unsupported aggregate function first
2023-11-25 04:51:13.953 UTC [1106490] LOCATION:  GetAggregateType, multi_logical_optimizer.c:3481
2023-11-25 04:51:13.953 UTC [1106490] STATEMENT:  SELECT id%5, first(val ORDER BY key), last(val ORDER BY key)
	FROM aggdata GROUP BY id%5 ORDER BY id%5;
2023-11-25 04:51:14.057 UTC [1106490] ERROR:  0A000: could not run distributed query with GROUPING
2023-11-25 04:51:14.057 UTC [1106490] HINT:  Consider using an equality filter on the distributed table's partition column.
2023-11-25 04:51:14.057 UTC [1106490] LOCATION:  DeferErrorIfQueryNotSupported, multi_logical_planner.c:945
2023-11-25 04:51:14.057 UTC [1106490] STATEMENT:  select grouping(id)
	from aggdata group by id order by 1 limit 3;
2023-11-25 04:51:14.057 UTC [1106490] ERROR:  0A000: could not run distributed query with GROUPING
2023-11-25 04:51:14.057 UTC [1106490] HINT:  Consider using an equality filter on the distributed table's partition column.
2023-11-25 04:51:14.057 UTC [1106490] LOCATION:  DeferErrorIfQueryNotSupported, multi_logical_planner.c:945
2023-11-25 04:51:14.057 UTC [1106490] STATEMENT:  select key, grouping(val)
	from aggdata group by key, val order by 1, 2;
2023-11-25 04:51:14.057 UTC [1106490] ERROR:  0A000: could not run distributed query with GROUPING
2023-11-25 04:51:14.057 UTC [1106490] HINT:  Consider using an equality filter on the distributed table's partition column.
2023-11-25 04:51:14.057 UTC [1106490] LOCATION:  DeferErrorIfQueryNotSupported, multi_logical_planner.c:945
2023-11-25 04:51:14.057 UTC [1106490] STATEMENT:  select key, grouping(val), sum(distinct valf)
	from aggdata group by key, val order by 1, 2;
2023-11-25 04:51:14.058 UTC [1106490] ERROR:  XX000: worker_partial_agg_sfunc could not confirm type correctness
2023-11-25 04:51:14.058 UTC [1106490] LOCATION:  worker_partial_agg_sfunc, aggregate_utils.c:517
2023-11-25 04:51:14.058 UTC [1106490] STATEMENT:  select pg_catalog.worker_partial_agg('string_agg(text,text)'::regprocedure, id) from nulltable;
2023-11-25 04:51:14.058 UTC [1106490] ERROR:  XX000: worker_partial_agg_sfunc could not confirm type correctness
2023-11-25 04:51:14.058 UTC [1106490] LOCATION:  worker_partial_agg_sfunc, aggregate_utils.c:517
2023-11-25 04:51:14.058 UTC [1106490] STATEMENT:  select pg_catalog.worker_partial_agg('sum(int8)'::regprocedure, id) from nulltable;
2023-11-25 04:51:14.058 UTC [1106490] ERROR:  XX000: coord_combine_agg_ffunc could not confirm type correctness
2023-11-25 04:51:14.058 UTC [1106490] LOCATION:  coord_combine_agg_ffunc, aggregate_utils.c:817
2023-11-25 04:51:14.058 UTC [1106490] STATEMENT:  select pg_catalog.coord_combine_agg('sum(float8)'::regprocedure, id::text::cstring, null::text) from nulltable;
2023-11-25 04:51:14.058 UTC [1106490] ERROR:  XX000: coord_combine_agg_ffunc could not confirm type correctness
2023-11-25 04:51:14.058 UTC [1106490] LOCATION:  coord_combine_agg_ffunc, aggregate_utils.c:817
2023-11-25 04:51:14.058 UTC [1106490] STATEMENT:  select pg_catalog.coord_combine_agg('avg(float8)'::regprocedure, ARRAY[id,id,id]::text::cstring, null::text) from nulltable;
2023-11-25 04:51:14.093 UTC [1106490] ERROR:  42883: function aggregate_support.square_func(integer) does not exist
2023-11-25 04:51:14.093 UTC [1106490] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:51:14.093 UTC [1106490] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:51:14.093 UTC [1106490] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:14.093 UTC [1106490] STATEMENT:  SELECT square_func(5), a FROM t1 GROUP BY a;
2023-11-25 04:51:14.100 UTC [1106490] ERROR:  42883: function aggregate_support.square_func(integer) does not exist
2023-11-25 04:51:14.100 UTC [1106490] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 04:51:14.100 UTC [1106490] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:51:14.100 UTC [1106490] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:14.100 UTC [1106490] STATEMENT:  SELECT square_func(5), a, count(a) FROM t1 GROUP BY a;
2023-11-25 04:51:14.152 UTC [1106490] WARNING:  0A000: "function dummy_fnc(dummy_tbl,double precision)" has dependency to "table dummy_tbl" that is not in Citus' metadata
2023-11-25 04:51:14.152 UTC [1106490] DETAIL:  "function dummy_fnc(dummy_tbl,double precision)" will be created only locally
2023-11-25 04:51:14.152 UTC [1106490] HINT:  Distribute "table dummy_tbl" first to distribute "function dummy_fnc(dummy_tbl,double precision)"
2023-11-25 04:51:14.152 UTC [1106490] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:950
2023-11-25 04:51:14.153 UTC [1106490] WARNING:  0A000: "function dependent_agg(double precision)" has dependency to "table dummy_tbl" that is not in Citus' metadata
2023-11-25 04:51:14.153 UTC [1106490] DETAIL:  "function dependent_agg(double precision)" will be created only locally
2023-11-25 04:51:14.153 UTC [1106490] HINT:  Distribute "table dummy_tbl" first to distribute "function dependent_agg(double precision)"
2023-11-25 04:51:14.153 UTC [1106490] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:950
2023-11-25 04:51:14.937 UTC [1106867] ERROR:  0A000: array_agg (distinct) is unsupported
2023-11-25 04:51:14.937 UTC [1106867] LOCATION:  DeferErrorIfUnsupportedArrayAggregate, multi_logical_optimizer.c:4016
2023-11-25 04:51:14.937 UTC [1106867] STATEMENT:  SELECT array_agg(distinct l_orderkey) FROM lineitem;
2023-11-25 04:51:14.938 UTC [1106867] ERROR:  0A000: array_agg with order by is unsupported
2023-11-25 04:51:14.938 UTC [1106867] LOCATION:  DeferErrorIfUnsupportedArrayAggregate, multi_logical_optimizer.c:4008
2023-11-25 04:51:14.938 UTC [1106867] STATEMENT:  SELECT array_agg(l_orderkey ORDER BY l_partkey) FROM lineitem;
2023-11-25 04:51:14.938 UTC [1106867] ERROR:  0A000: array_agg with order by is unsupported
2023-11-25 04:51:14.938 UTC [1106867] LOCATION:  DeferErrorIfUnsupportedArrayAggregate, multi_logical_optimizer.c:4008
2023-11-25 04:51:14.938 UTC [1106867] STATEMENT:  SELECT array_agg(distinct l_orderkey ORDER BY l_orderkey) FROM lineitem;
2023-11-25 04:51:15.184 UTC [1106868] ERROR:  0A000: subquery in LIMIT is not supported in multi-shard queries
2023-11-25 04:51:15.184 UTC [1106868] LOCATION:  DeferErrorIfQueryNotSupported, multi_logical_planner.c:945
2023-11-25 04:51:15.184 UTC [1106868] STATEMENT:  SELECT l_orderkey FROM lineitem_hash ORDER BY l_orderkey LIMIT (SELECT 10);
2023-11-25 04:51:15.184 UTC [1106868] ERROR:  0A000: subquery in OFFSET is not supported in multi-shard queries
2023-11-25 04:51:15.184 UTC [1106868] LOCATION:  DeferErrorIfQueryNotSupported, multi_logical_planner.c:945
2023-11-25 04:51:15.184 UTC [1106868] STATEMENT:  SELECT l_orderkey FROM lineitem_hash ORDER BY l_orderkey LIMIT 10 OFFSET (SELECT 10);
2023-11-25 04:51:15.390 UTC [1107032] ERROR:  0A000: json_object_agg (distinct) is unsupported
2023-11-25 04:51:15.390 UTC [1107032] LOCATION:  DeferErrorIfUnsupportedJsonAggregate, multi_logical_optimizer.c:4051
2023-11-25 04:51:15.390 UTC [1107032] STATEMENT:  SELECT json_object_agg(distinct l_shipmode, l_orderkey) FROM lineitem;
2023-11-25 04:51:15.391 UTC [1107032] ERROR:  0A000: json_object_agg with order by is unsupported
2023-11-25 04:51:15.391 UTC [1107032] LOCATION:  DeferErrorIfUnsupportedJsonAggregate, multi_logical_optimizer.c:4051
2023-11-25 04:51:15.391 UTC [1107032] STATEMENT:  SELECT json_object_agg(l_shipmode, l_orderkey ORDER BY l_shipmode) FROM lineitem;
2023-11-25 04:51:15.391 UTC [1107032] ERROR:  0A000: json_object_agg with order by is unsupported
2023-11-25 04:51:15.391 UTC [1107032] LOCATION:  DeferErrorIfUnsupportedJsonAggregate, multi_logical_optimizer.c:4051
2023-11-25 04:51:15.391 UTC [1107032] STATEMENT:  SELECT json_object_agg(distinct l_orderkey, l_shipmode ORDER BY l_orderkey) FROM lineitem;
2023-11-25 04:51:15.394 UTC [1107027] ERROR:  0A000: jsonb_agg (distinct) is unsupported
2023-11-25 04:51:15.394 UTC [1107027] LOCATION:  DeferErrorIfUnsupportedJsonAggregate, multi_logical_optimizer.c:4051
2023-11-25 04:51:15.394 UTC [1107027] STATEMENT:  SELECT jsonb_agg(distinct l_orderkey) FROM lineitem;
2023-11-25 04:51:15.394 UTC [1107027] ERROR:  0A000: jsonb_agg with order by is unsupported
2023-11-25 04:51:15.394 UTC [1107027] LOCATION:  DeferErrorIfUnsupportedJsonAggregate, multi_logical_optimizer.c:4051
2023-11-25 04:51:15.394 UTC [1107027] STATEMENT:  SELECT jsonb_agg(l_orderkey ORDER BY l_partkey) FROM lineitem;
2023-11-25 04:51:15.394 UTC [1107027] ERROR:  0A000: jsonb_agg with order by is unsupported
2023-11-25 04:51:15.394 UTC [1107027] LOCATION:  DeferErrorIfUnsupportedJsonAggregate, multi_logical_optimizer.c:4051
2023-11-25 04:51:15.394 UTC [1107027] STATEMENT:  SELECT jsonb_agg(distinct l_orderkey ORDER BY l_orderkey) FROM lineitem;
2023-11-25 04:51:15.398 UTC [1107031] ERROR:  0A000: json_agg (distinct) is unsupported
2023-11-25 04:51:15.398 UTC [1107031] LOCATION:  DeferErrorIfUnsupportedJsonAggregate, multi_logical_optimizer.c:4051
2023-11-25 04:51:15.398 UTC [1107031] STATEMENT:  SELECT json_agg(distinct l_orderkey) FROM lineitem;
2023-11-25 04:51:15.398 UTC [1107031] ERROR:  0A000: json_agg with order by is unsupported
2023-11-25 04:51:15.398 UTC [1107031] LOCATION:  DeferErrorIfUnsupportedJsonAggregate, multi_logical_optimizer.c:4051
2023-11-25 04:51:15.398 UTC [1107031] STATEMENT:  SELECT json_agg(l_orderkey ORDER BY l_partkey) FROM lineitem;
2023-11-25 04:51:15.398 UTC [1107031] ERROR:  0A000: json_agg with order by is unsupported
2023-11-25 04:51:15.398 UTC [1107031] LOCATION:  DeferErrorIfUnsupportedJsonAggregate, multi_logical_optimizer.c:4051
2023-11-25 04:51:15.398 UTC [1107031] STATEMENT:  SELECT json_agg(distinct l_orderkey ORDER BY l_orderkey) FROM lineitem;
2023-11-25 04:51:15.428 UTC [1107029] ERROR:  0A000: jsonb_object_agg (distinct) is unsupported
2023-11-25 04:51:15.428 UTC [1107029] LOCATION:  DeferErrorIfUnsupportedJsonAggregate, multi_logical_optimizer.c:4051
2023-11-25 04:51:15.428 UTC [1107029] STATEMENT:  SELECT jsonb_object_agg(distinct l_shipmode, l_orderkey) FROM lineitem;
2023-11-25 04:51:15.428 UTC [1107029] ERROR:  0A000: jsonb_object_agg with order by is unsupported
2023-11-25 04:51:15.428 UTC [1107029] LOCATION:  DeferErrorIfUnsupportedJsonAggregate, multi_logical_optimizer.c:4051
2023-11-25 04:51:15.428 UTC [1107029] STATEMENT:  SELECT jsonb_object_agg(l_shipmode, l_orderkey ORDER BY l_shipmode) FROM lineitem;
2023-11-25 04:51:15.430 UTC [1107029] ERROR:  0A000: jsonb_object_agg with order by is unsupported
2023-11-25 04:51:15.430 UTC [1107029] LOCATION:  DeferErrorIfUnsupportedJsonAggregate, multi_logical_optimizer.c:4051
2023-11-25 04:51:15.430 UTC [1107029] STATEMENT:  SELECT jsonb_object_agg(distinct l_orderkey, l_shipmode ORDER BY l_orderkey) FROM lineitem;
2023-11-25 04:51:15.584 UTC [1107035] ERROR:  0A000: Subqueries in HAVING cannot refer to outer query
2023-11-25 04:51:15.584 UTC [1107035] LOCATION:  RecursivelyPlanSubqueriesAndCTEs, recursive_planning.c:323
2023-11-25 04:51:15.584 UTC [1107035] STATEMENT:  select     s_i_id, sum(s_order_cnt) as ordercount
	from     stock s
	where   s_order_cnt > (select sum(s_order_cnt) * .005 as where_query from stock)
	group by s_i_id
	having   (select max(s_order_cnt) > 2 as having_query from stock where s_i_id = s.s_i_id)
	order by s_i_id;
2023-11-25 04:51:15.584 UTC [1107035] ERROR:  0A000: Subqueries in HAVING cannot refer to outer query
2023-11-25 04:51:15.584 UTC [1107035] LOCATION:  RecursivelyPlanSubqueriesAndCTEs, recursive_planning.c:323
2023-11-25 04:51:15.584 UTC [1107035] STATEMENT:  select     s_i_id, sum(s_order_cnt) as ordercount
	from     stock s
	group by s_i_id
	having   (select max(s_order_cnt) > 2 as having_query from stock where s_i_id = s.s_i_id)
	order by s_i_id;
2023-11-25 04:51:15.746 UTC [1107036] ERROR:  0A000: complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
2023-11-25 04:51:15.746 UTC [1107036] LOCATION:  JoinOrderList, multi_join_order.c:310
2023-11-25 04:51:15.746 UTC [1107036] STATEMENT:  SELECT *
	FROM
	    test t1 JOIN test t2 USING (y), -- causes repartition, which makes this not routable or pushdownable
	    test a,
	    test b
	WHERE t2.y - a.x - b.x = 0
	ORDER BY 1,2,3;
2023-11-25 04:51:15.917 UTC [1107034] LOG:  00000: join order: [ "order_line" ]
2023-11-25 04:51:15.917 UTC [1107034] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:15.917 UTC [1107034] STATEMENT:  SELECT
	    ol_number,
	    sum(ol_quantity) as sum_qty,
	    sum(ol_amount) as sum_amount,
	    avg(ol_quantity) as avg_qty,
	    avg(ol_amount) as avg_amount,
	    count(*) as count_order
	FROM order_line
	WHERE ol_delivery_d > '2007-01-02 00:00:00.000000'
	GROUP BY ol_number
	ORDER BY ol_number;
2023-11-25 04:51:15.921 UTC [1107034] LOG:  00000: join order: [ "stock" ][ reference join "supplier" ][ reference join "nation" ][ reference join "region" ]
2023-11-25 04:51:15.921 UTC [1107034] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:15.921 UTC [1107034] STATEMENT:  SELECT
	    su_suppkey,
	    su_name,
	    n_name,
	    i_id,
	    i_name,
	    su_address,
	    su_phone,
	    su_comment
	FROM
	    item,
	    supplier,
	    stock,
	    nation,
	    region,
	    (SELECT
	         s_i_id AS m_i_id,
	         min(s_quantity) as m_s_quantity
	     FROM
	         stock,
	         supplier,
	         nation,
	         region
	     WHERE mod((s_w_id*s_i_id),10000)=su_suppkey
	       AND su_nationkey=n_nationkey
	       AND n_regionkey=r_regionkey
	       AND r_name LIKE 'Europ%'
	     GROUP BY s_i_id) m
	WHERE i_id = s_i_id
	  AND mod((s_w_id * s_i_id), 10000) = su_suppkey
	  AND su_nationkey = n_nationkey
	  AND n_regionkey = r_regionkey
	  AND i_data LIKE '%b'
	  AND r_name LIKE 'Europ%'
	  AND i_id = m_i_id
	  AND s_quantity = m_s_quantity
	ORDER BY
	    n_name,
	    su_name,
	    i_id;
2023-11-25 04:51:15.928 UTC [1107034] LOG:  00000: join order: [ "customer" ][ local partition join "oorder" ][ local partition join "new_order" ][ local partition join "order_line" ]
2023-11-25 04:51:15.928 UTC [1107034] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:15.928 UTC [1107034] STATEMENT:  SELECT
	    ol_o_id,
	    ol_w_id,
	    ol_d_id,
	    sum(ol_amount) AS revenue,
	    o_entry_d
	FROM
	    customer,
	    new_order,
	    oorder,
	    order_line
	WHERE c_state LIKE 'C%' -- used to ba A%, but C% works with our small data
	  AND c_id = o_c_id
	  AND c_w_id = o_w_id
	  AND c_d_id = o_d_id
	  AND no_w_id = o_w_id
	  AND no_d_id = o_d_id
	  AND no_o_id = o_id
	  AND ol_w_id = o_w_id
	  AND ol_d_id = o_d_id
	  AND ol_o_id = o_id
	  AND o_entry_d > '2007-01-02 00:00:00.000000'
	GROUP BY
	    ol_o_id,
	    ol_w_id,
	    ol_d_id,
	    o_entry_d
	ORDER BY
	    revenue DESC,
	    o_entry_d;
2023-11-25 04:51:15.937 UTC [1107034] LOG:  00000: join order: [ "customer" ][ local partition join "oorder" ][ local partition join "order_line" ][ local partition join "stock" ][ reference join "supplier" ][ reference join "nation" ][ reference join "region" ]
2023-11-25 04:51:15.937 UTC [1107034] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:15.937 UTC [1107034] STATEMENT:  SELECT
	    n_name,
	    sum(ol_amount) AS revenue
	FROM
	    customer,
	    oorder,
	    order_line,
	    stock,
	    supplier,
	    nation,
	    region
	WHERE c_id = o_c_id
	  AND c_w_id = o_w_id
	  AND c_d_id = o_d_id
	  AND ol_o_id = o_id
	  AND ol_w_id = o_w_id
	  AND ol_d_id=o_d_id
	  AND ol_w_id = s_w_id
	  AND ol_i_id = s_i_id
	  AND mod((s_w_id * s_i_id),10000) = su_suppkey
	-- our dataset does not have the supplier in the same nation as the customer causing this
	-- join to filter out all the data. We verify later on that we can actually perform an
	-- ascii(substr(c_state,1,1)) == reference table column join later on so it should not
	-- matter we skip this filter here.
	--AND ascii(substr(c_state,1,1)) = su_nationkey
	  AND su_nationkey = n_nationkey
	  AND n_regionkey = r_regionkey
	  AND r_name = 'Europe'
	  AND o_entry_d >= '2007-01-02 00:00:00.000000'
	GROUP BY n_name
	ORDER BY revenue DESC;
2023-11-25 04:51:15.946 UTC [1107034] LOG:  00000: join order: [ "order_line" ]
2023-11-25 04:51:15.946 UTC [1107034] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:15.946 UTC [1107034] STATEMENT:  SELECT
	    sum(ol_amount) AS revenue
	FROM order_line
	WHERE ol_delivery_d >= '1999-01-01 00:00:00.000000'
	  AND ol_delivery_d < '2020-01-01 00:00:00.000000'
	  AND ol_quantity BETWEEN 1 AND 100000;
2023-11-25 04:51:15.952 UTC [1107034] LOG:  00000: join order: [ "order_line" ][ local partition join "oorder" ][ local partition join "customer" ][ reference join "nation" ][ reference join "nation" ][ reference join "supplier" ][ dual partition join "stock" ]
2023-11-25 04:51:15.952 UTC [1107034] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:15.952 UTC [1107034] STATEMENT:  SELECT
	    su_nationkey as supp_nation,
	    substr(c_state,1,1) as cust_nation,
	    extract(year from o_entry_d) as l_year,
	    sum(ol_amount) as revenue
	FROM
	    supplier,
	    stock,
	    order_line,
	    oorder,
	    customer,
	    nation n1,
	    nation n2
	WHERE ol_supply_w_id = s_w_id
	  AND ol_i_id = s_i_id
	  AND mod((s_w_id * s_i_id), 10000) = su_suppkey
	  AND ol_w_id = o_w_id
	  AND ol_d_id = o_d_id
	  AND ol_o_id = o_id
	  AND c_id = o_c_id
	  AND c_w_id = o_w_id
	  AND c_d_id = o_d_id
	  AND su_nationkey = n1.n_nationkey
	  AND ascii(substr(c_state,1,1)) = n2.n_nationkey
	  AND (
	         (n1.n_name = 'Germany' AND n2.n_name = 'Cambodia')
	      OR (n1.n_name = 'Cambodia' AND n2.n_name = 'Germany')
	      )
	  AND ol_delivery_d BETWEEN '2007-01-02 00:00:00.000000' AND '2012-01-02 00:00:00.000000'
	GROUP BY
	    su_nationkey,
	    substr(c_state,1,1),
	    extract(year from o_entry_d)
	ORDER BY
	    su_nationkey,
	    cust_nation,
	    l_year;
2023-11-25 04:51:16.037 UTC [1107034] LOG:  00000: join order: [ "order_line" ][ reference join "item" ][ local partition join "oorder" ][ local partition join "customer" ][ reference join "nation" ][ reference join "region" ][ dual partition join "stock" ][ reference join "supplier" ][ reference join "nation" ]
2023-11-25 04:51:16.037 UTC [1107034] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:16.037 UTC [1107034] STATEMENT:  SELECT
	    extract(year from o_entry_d) as l_year,
	    sum(case when n2.n_name = 'Germany' then ol_amount else 0 end) / sum(ol_amount) as mkt_share
	FROM
	    item,
	    supplier,
	    stock,
	    order_line,
	    oorder,
	    customer,
	    nation n1,
	    nation n2,
	    region
	WHERE i_id = s_i_id
	  AND ol_i_id = s_i_id
	  AND ol_supply_w_id = s_w_id
	  AND mod((s_w_id * s_i_id),10000) = su_suppkey
	  AND ol_w_id = o_w_id
	  AND ol_d_id = o_d_id
	  AND ol_o_id = o_id
	  AND c_id = o_c_id
	  AND c_w_id = o_w_id
	  AND c_d_id = o_d_id
	  AND n1.n_nationkey = ascii(substr(c_state,1,1))
	  AND n1.n_regionkey = r_regionkey
	  AND ol_i_id < 1000
	  AND r_name = 'Europe'
	  AND su_nationkey = n2.n_nationkey
	  AND o_entry_d BETWEEN '2007-01-02 00:00:00.000000' AND '2012-01-02 00:00:00.000000'
	  AND i_data LIKE '%b'
	  AND i_id = ol_i_id
	GROUP BY extract(YEAR FROM o_entry_d)
	ORDER BY l_year;
2023-11-25 04:51:16.121 UTC [1107034] LOG:  00000: join order: [ "order_line" ][ reference join "item" ][ local partition join "oorder" ][ dual partition join "stock" ][ reference join "supplier" ][ reference join "nation" ]
2023-11-25 04:51:16.121 UTC [1107034] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:16.121 UTC [1107034] STATEMENT:  SELECT
	    n_name,
	    extract(year from o_entry_d) as l_year,
	    sum(ol_amount) as sum_profit
	FROM
	    item,
	    stock,
	    supplier,
	    order_line,
	    oorder,
	    nation
	WHERE ol_i_id = s_i_id
	  AND ol_supply_w_id = s_w_id
	  AND mod((s_w_id * s_i_id), 10000) = su_suppkey
	  AND ol_w_id = o_w_id
	  AND ol_d_id = o_d_id
	  AND ol_o_id = o_id
	  AND ol_i_id = i_id
	  AND su_nationkey = n_nationkey
	  AND i_data LIKE '%b' -- this used to be %BB but that will not work with our small dataset
	GROUP BY
	    n_name,
	    extract(YEAR FROM o_entry_d)
	ORDER BY
	    n_name,
	    l_year DESC;
2023-11-25 04:51:16.206 UTC [1107034] LOG:  00000: join order: [ "customer" ][ reference join "nation" ][ local partition join "oorder" ][ local partition join "order_line" ]
2023-11-25 04:51:16.206 UTC [1107034] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:16.206 UTC [1107034] STATEMENT:  SELECT
	    c_id,
	    c_last,
	    sum(ol_amount) AS revenue,
	    c_city,
	    c_phone,
	    n_name
	FROM
	    customer,
	    oorder,
	    order_line,
	    nation
	WHERE c_id = o_c_id
	  AND c_w_id = o_w_id
	  AND c_d_id = o_d_id
	  AND ol_w_id = o_w_id
	  AND ol_d_id = o_d_id
	  AND ol_o_id = o_id
	  AND o_entry_d >= '2007-01-02 00:00:00.000000'
	  AND o_entry_d <= ol_delivery_d
	  AND n_nationkey = ascii(substr(c_state,1,1))
	GROUP BY
	    c_id,
	    c_last,
	    c_city,
	    c_phone,
	    n_name
	ORDER BY revenue DESC;
2023-11-25 04:51:16.211 UTC [1107034] LOG:  00000: join order: [ "stock" ][ reference join "supplier" ][ reference join "nation" ]
2023-11-25 04:51:16.211 UTC [1107034] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:16.211 UTC [1107034] STATEMENT:  SELECT
	    s_i_id,
	    sum(s_order_cnt) AS ordercount
	FROM
	    stock,
	    supplier,
	    nation
	WHERE mod((s_w_id * s_i_id),10000) = su_suppkey
	  AND su_nationkey = n_nationkey
	  AND n_name = 'Germany'
	GROUP BY s_i_id
	HAVING sum(s_order_cnt) >
	         (SELECT sum(s_order_cnt) * .005
	          FROM
	              stock,
	              supplier,
	              nation
	          WHERE mod((s_w_id * s_i_id),10000) = su_suppkey
	            AND su_nationkey = n_nationkey
	            AND n_name = 'Germany')
	ORDER BY ordercount DESC;
2023-11-25 04:51:16.216 UTC [1107034] LOG:  00000: join order: [ "oorder" ][ local partition join "order_line" ]
2023-11-25 04:51:16.216 UTC [1107034] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:16.216 UTC [1107034] STATEMENT:  SELECT
	    o_ol_cnt,
	    sum(case when o_carrier_id = 1 or o_carrier_id = 2 then 1 else 0 end) as high_line_count,
	    sum(case when o_carrier_id <> 1 and o_carrier_id <> 2 then 1 else 0 end) as low_line_count
	FROM
	    oorder,
	    order_line
	WHERE ol_w_id = o_w_id
	  AND ol_d_id = o_d_id
	  AND ol_o_id = o_id
	  AND o_entry_d <= ol_delivery_d
	  AND ol_delivery_d < '2020-01-01 00:00:00.000000'
	GROUP BY o_ol_cnt
	ORDER BY o_ol_cnt;
2023-11-25 04:51:16.225 UTC [1107034] LOG:  00000: join order: [ "order_line" ][ reference join "item" ]
2023-11-25 04:51:16.225 UTC [1107034] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:16.225 UTC [1107034] STATEMENT:  SELECT
	    100.00 * sum(CASE WHEN i_data LIKE 'PR%' THEN ol_amount ELSE 0 END) / (1+sum(ol_amount)) AS promo_revenue
	FROM
	    order_line,
	    item
	WHERE ol_i_id = i_id
	  AND ol_delivery_d >= '2007-01-02 00:00:00.000000'
	  AND ol_delivery_d < '2020-01-02 00:00:00.000000';
2023-11-25 04:51:16.228 UTC [1107034] LOG:  00000: join order: [ "order_line" ][ dual partition join "stock" ]
2023-11-25 04:51:16.228 UTC [1107034] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:16.228 UTC [1107034] STATEMENT:  WITH revenue (supplier_no, total_revenue) AS (
	    SELECT
	        mod((s_w_id * s_i_id),10000) AS supplier_no,
	        sum(ol_amount) AS total_revenue
	    FROM
	        order_line,
	        stock
	    WHERE ol_i_id = s_i_id
	      AND ol_supply_w_id = s_w_id
	      AND ol_delivery_d >= '2007-01-02 00:00:00.000000'
	    GROUP BY mod((s_w_id * s_i_id),10000))
	SELECT
	    su_suppkey,
	    su_name,
	    su_address,
	    su_phone,
	    total_revenue
	FROM
	    supplier,
	    revenue
	WHERE su_suppkey = supplier_no
	  AND total_revenue = (SELECT max(total_revenue) FROM revenue)
	ORDER BY su_suppkey;
2023-11-25 04:51:16.316 UTC [1107034] LOG:  00000: join order: [ "order_line" ][ reference join "item" ]
2023-11-25 04:51:16.316 UTC [1107034] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:16.316 UTC [1107034] STATEMENT:  SELECT
	       sum(ol_amount) / 2.0 AS avg_yearly
	FROM
	    order_line,
	    (SELECT
	         i_id,
	         avg(ol_quantity) AS a
	     FROM
	         item,
	         order_line
	     WHERE i_data LIKE '%b'
	       AND ol_i_id = i_id
	     GROUP BY i_id) t
	WHERE ol_i_id = t.i_id;
2023-11-25 04:51:16.322 UTC [1107034] LOG:  00000: join order: [ "customer" ][ local partition join "oorder" ][ local partition join "order_line" ]
2023-11-25 04:51:16.322 UTC [1107034] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:16.322 UTC [1107034] STATEMENT:  SELECT
	    c_last,
	    c_id o_id,
	    o_entry_d,
	    o_ol_cnt,
	    sum(ol_amount)
	FROM
	    customer,
	    oorder,
	    order_line
	WHERE c_id = o_c_id
	  AND c_w_id = o_w_id
	  AND c_d_id = o_d_id
	  AND ol_w_id = o_w_id
	  AND ol_d_id = o_d_id
	  AND ol_o_id = o_id
	GROUP BY
	    o_id,
	    o_w_id,
	    o_d_id,
	    c_id,
	    c_last,
	    o_entry_d,
	    o_ol_cnt
	HAVING sum(ol_amount) > 5 -- was 200, but thats too big for the dataset
	ORDER BY
	    sum(ol_amount) DESC,
	    o_entry_d;
2023-11-25 04:51:16.326 UTC [1107034] LOG:  00000: join order: [ "order_line" ][ reference join "item" ]
2023-11-25 04:51:16.326 UTC [1107034] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:16.326 UTC [1107034] STATEMENT:  SELECT
	    sum(ol_amount) AS revenue
	FROM
	    order_line,
	     item
	WHERE (     ol_i_id = i_id
	        AND i_data LIKE '%a'
	        AND ol_quantity >= 1
	        AND ol_quantity <= 10
	        AND i_price BETWEEN 1 AND 400000
	        AND ol_w_id IN (1,2,3))
	   OR (     ol_i_id = i_id
	        AND i_data LIKE '%b'
	        AND ol_quantity >= 1
	        AND ol_quantity <= 10
	        AND i_price BETWEEN 1 AND 400000
	        AND ol_w_id IN (1,2,4))
	   OR (     ol_i_id = i_id
	        AND i_data LIKE '%c'
	        AND ol_quantity >= 1
	        AND ol_quantity <= 10
	        AND i_price BETWEEN 1 AND 400000
	        AND ol_w_id IN (1,5,3));
2023-11-25 04:51:16.330 UTC [1107034] LOG:  00000: join order: [ "stock" ][ reference join "item" ][ dual partition join "order_line" ]
2023-11-25 04:51:16.330 UTC [1107034] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:16.330 UTC [1107034] STATEMENT:  SELECT
	    su_name,
	    su_address
	FROM
	    supplier,
	    nation
	WHERE su_suppkey in
	      (SELECT
	           mod(s_i_id * s_w_id, 10000)
	       FROM
	           stock,
	           order_line
	       WHERE s_i_id IN
	             (SELECT i_id
	              FROM item
	              WHERE i_data LIKE 'co%')
	       AND ol_i_id = s_i_id
	       AND ol_delivery_d > '2008-05-23 12:00:00' -- was 2010, but our order is in 2008
	       GROUP BY s_i_id, s_w_id, s_quantity
	       HAVING   2*s_quantity > sum(ol_quantity))
	  AND su_nationkey = n_nationkey
	  AND n_name = 'Germany'
	ORDER BY su_name;
2023-11-25 04:51:16.420 UTC [1107034] LOG:  00000: join order: [ "customer" ]
2023-11-25 04:51:16.420 UTC [1107034] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:16.420 UTC [1107034] STATEMENT:  SELECT
	    substr(c_state,1,1) AS country,
	    count(*) AS numcust,
	    sum(c_balance) AS totacctbal
	FROM customer
	WHERE substr(c_phone,1,1) in ('1','2','3','4','5','6','7')
	  AND c_balance > (SELECT avg(c_BALANCE)
	                   FROM customer
	                   WHERE  c_balance > 0.00
	                     AND substr(c_phone,1,1) in ('1','2','3','4','5','6','7'))
	  AND NOT exists (SELECT *
	                  FROM oorder
	                  WHERE o_c_id = c_id
	                    AND o_w_id = c_w_id
	                    AND o_d_id = c_d_id)
	GROUP BY substr(c_state,1,1)
	ORDER BY substr(c_state,1,1);
2023-11-25 04:51:16.429 UTC [1107034] LOG:  00000: join order: [ "order_line" ][ local partition join "oorder" ][ local partition join "customer" ][ reference join "nation" ][ reference join "nation" ][ reference join "supplier" ][ single hash partition join "stock" ]
2023-11-25 04:51:16.429 UTC [1107034] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:16.429 UTC [1107034] STATEMENT:  SELECT
	    su_nationkey as supp_nation,
	    substr(c_state,1,1) as cust_nation,
	    extract(year from o_entry_d) as l_year,
	    sum(ol_amount) as revenue
	FROM
	    supplier,
	    stock,
	    order_line,
	    oorder,
	    customer,
	    nation n1,
	    nation n2
	WHERE ol_supply_w_id = s_w_id
	  AND ol_i_id = s_i_id
	  AND mod((s_w_id * s_i_id), 10000) = su_suppkey
	  AND ol_w_id = o_w_id
	  AND ol_d_id = o_d_id
	  AND ol_o_id = o_id
	  AND c_id = o_c_id
	  AND c_w_id = o_w_id
	  AND c_d_id = o_d_id
	  AND su_nationkey = n1.n_nationkey
	  AND ascii(substr(c_state,1,1)) = n2.n_nationkey
	  AND (
	        (n1.n_name = 'Germany' AND n2.n_name = 'Cambodia')
	        OR (n1.n_name = 'Cambodia' AND n2.n_name = 'Germany')
	    )
	  AND ol_delivery_d BETWEEN '2007-01-02 00:00:00.000000' AND '2012-01-02 00:00:00.000000'
	GROUP BY
	    su_nationkey,
	    substr(c_state,1,1),
	    extract(year from o_entry_d)
	ORDER BY
	    su_nationkey,
	    cust_nation,
	    l_year;
2023-11-25 04:51:16.487 UTC [1107034] LOG:  00000: join order: [ "order_line" ][ reference join "item" ][ local partition join "oorder" ][ local partition join "customer" ][ reference join "nation" ][ reference join "region" ][ single hash partition join "stock" ][ reference join "supplier" ][ reference join "nation" ]
2023-11-25 04:51:16.487 UTC [1107034] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:16.487 UTC [1107034] STATEMENT:  SELECT
	    extract(year from o_entry_d) as l_year,
	    sum(case when n2.n_name = 'Germany' then ol_amount else 0 end) / sum(ol_amount) as mkt_share
	FROM
	    item,
	    supplier,
	    stock,
	    order_line,
	    oorder,
	    customer,
	    nation n1,
	    nation n2,
	    region
	WHERE i_id = s_i_id
	  AND ol_i_id = s_i_id
	  AND ol_supply_w_id = s_w_id
	  AND mod((s_w_id * s_i_id),10000) = su_suppkey
	  AND ol_w_id = o_w_id
	  AND ol_d_id = o_d_id
	  AND ol_o_id = o_id
	  AND c_id = o_c_id
	  AND c_w_id = o_w_id
	  AND c_d_id = o_d_id
	  AND n1.n_nationkey = ascii(substr(c_state,1,1))
	  AND n1.n_regionkey = r_regionkey
	  AND ol_i_id < 1000
	  AND r_name = 'Europe'
	  AND su_nationkey = n2.n_nationkey
	  AND o_entry_d BETWEEN '2007-01-02 00:00:00.000000' AND '2012-01-02 00:00:00.000000'
	  AND i_data LIKE '%b'
	  AND i_id = ol_i_id
	GROUP BY extract(YEAR FROM o_entry_d)
	ORDER BY l_year;
2023-11-25 04:51:16.544 UTC [1107034] LOG:  00000: join order: [ "order_line" ][ reference join "item" ][ local partition join "oorder" ][ single hash partition join "stock" ][ reference join "supplier" ][ reference join "nation" ]
2023-11-25 04:51:16.544 UTC [1107034] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:16.544 UTC [1107034] STATEMENT:  SELECT
	    n_name,
	    extract(year from o_entry_d) as l_year,
	    sum(ol_amount) as sum_profit
	FROM
	    item,
	    stock,
	    supplier,
	    order_line,
	    oorder,
	    nation
	WHERE ol_i_id = s_i_id
	  AND ol_supply_w_id = s_w_id
	  AND mod((s_w_id * s_i_id), 10000) = su_suppkey
	  AND ol_w_id = o_w_id
	  AND ol_d_id = o_d_id
	  AND ol_o_id = o_id
	  AND ol_i_id = i_id
	  AND su_nationkey = n_nationkey
	  AND i_data LIKE '%b' -- this used to be %BB but that will not work with our small dataset
	GROUP BY
	    n_name,
	    extract(YEAR FROM o_entry_d)
	ORDER BY
	    n_name,
	    l_year DESC;
2023-11-25 04:51:17.237 UTC [1107919] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:51:17.237 UTC [1107919] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:51:17.237 UTC [1107919] STATEMENT:  select  s_i_id
	    from stock, order_line
	    where
	        s_i_id in (select i_im_id from item)
	        AND s_i_id = ol_i_id;
2023-11-25 04:51:17.237 UTC [1107919] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:51:17.237 UTC [1107919] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:51:17.237 UTC [1107919] STATEMENT:  select  s_i_id
	    from stock, order_line
	    where
	        s_i_id not in (select i_id from item)
	        AND s_i_id = ol_i_id;
2023-11-25 04:51:17.237 UTC [1107919] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:51:17.237 UTC [1107919] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:51:17.237 UTC [1107919] STATEMENT:  select  s_i_id
	    from stock, order_line
	    where
	        s_i_id not in (select i_im_id from item)
	        AND s_i_id = ol_i_id;
2023-11-25 04:51:17.241 UTC [1107919] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:51:17.241 UTC [1107919] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:51:17.241 UTC [1107919] STATEMENT:  select  s_i_id
	    from stock, order_line
	    where
	        s_i_id in (select i_id from item)
	        AND s_i_id not in (select i_id from item)
	        AND s_i_id = ol_i_id;
2023-11-25 04:51:17.241 UTC [1107919] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:51:17.241 UTC [1107919] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:51:17.241 UTC [1107919] STATEMENT:  select  s_i_id
	    from stock, order_line
	    where
	        s_i_id in (select i_id from item)
	        AND s_i_id not in (select i_im_id from item)
	        AND s_i_id = ol_i_id;
2023-11-25 04:51:17.241 UTC [1107919] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:51:17.241 UTC [1107919] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:51:17.241 UTC [1107919] STATEMENT:  select  s_i_id
	    from stock, order_line
	    where
	        s_i_id in (select i_im_id from item)
	        AND s_i_id not in (select i_id from item)
	        AND s_i_id = ol_i_id;
2023-11-25 04:51:17.242 UTC [1107919] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:51:17.242 UTC [1107919] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:51:17.242 UTC [1107919] STATEMENT:  select  s_i_id
	    from stock, order_line
	    where
	        s_i_id in (select i_im_id from item)
	        AND s_i_id not in (select i_im_id from item)
	        AND s_i_id = ol_i_id;
2023-11-25 04:51:17.758 UTC [1108249] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:51:17.758 UTC [1108249] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:51:17.758 UTC [1108249] STATEMENT:  SELECT count(*)
	FROM distributed_table u1
	JOIN local_table u2 USING(value)
	JOIN LATERAL
	  (SELECT value,
	          random()
	   FROM distributed_table
	   WHERE u2.value = 15) AS u3 USING (value)
	WHERE (u2.value > 2
	       AND FALSE);
2023-11-25 04:51:18.003 UTC [1108366] LOG:  00000: join order: [ "single_hash_repartition_first" ][ single hash partition join "single_hash_repartition_second" ]
2023-11-25 04:51:18.003 UTC [1108366] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:18.003 UTC [1108366] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		single_hash_repartition_first t1, single_hash_repartition_second t2
	WHERE
		t1.id = t2.sum;
2023-11-25 04:51:18.003 UTC [1108366] ERROR:  XX000: the query contains a join that requires repartitioning
2023-11-25 04:51:18.003 UTC [1108366] HINT:  Set citus.enable_repartition_joins to on to enable repartitioning
2023-11-25 04:51:18.003 UTC [1108366] LOCATION:  JobExecutorType, multi_server_executor.c:68
2023-11-25 04:51:18.003 UTC [1108366] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		single_hash_repartition_first t1, single_hash_repartition_second t2
	WHERE
		t1.id = t2.sum;
2023-11-25 04:51:18.003 UTC [1108366] LOG:  00000: join order: [ "single_hash_repartition_second" ][ single hash partition join "single_hash_repartition_first" ]
2023-11-25 04:51:18.003 UTC [1108366] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:18.003 UTC [1108366] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		single_hash_repartition_second t1, single_hash_repartition_first t2
	WHERE
		t2.sum = t1.id;
2023-11-25 04:51:18.004 UTC [1108366] ERROR:  XX000: the query contains a join that requires repartitioning
2023-11-25 04:51:18.004 UTC [1108366] HINT:  Set citus.enable_repartition_joins to on to enable repartitioning
2023-11-25 04:51:18.004 UTC [1108366] LOCATION:  JobExecutorType, multi_server_executor.c:68
2023-11-25 04:51:18.004 UTC [1108366] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		single_hash_repartition_second t1, single_hash_repartition_first t2
	WHERE
		t2.sum = t1.id;
2023-11-25 04:51:18.004 UTC [1108366] LOG:  00000: join order: [ "single_hash_repartition_second" ][ reference join "ref_table" ][ single hash partition join "single_hash_repartition_first" ]
2023-11-25 04:51:18.004 UTC [1108366] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:18.004 UTC [1108366] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		ref_table r1, single_hash_repartition_second t1, single_hash_repartition_first t2
	WHERE
		r1.id = t1.id AND t2.sum = t1.id;
2023-11-25 04:51:18.005 UTC [1108366] ERROR:  XX000: the query contains a join that requires repartitioning
2023-11-25 04:51:18.005 UTC [1108366] HINT:  Set citus.enable_repartition_joins to on to enable repartitioning
2023-11-25 04:51:18.005 UTC [1108366] LOCATION:  JobExecutorType, multi_server_executor.c:68
2023-11-25 04:51:18.005 UTC [1108366] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		ref_table r1, single_hash_repartition_second t1, single_hash_repartition_first t2
	WHERE
		r1.id = t1.id AND t2.sum = t1.id;
2023-11-25 04:51:18.005 UTC [1108366] LOG:  00000: join order: [ "single_hash_repartition_first" ][ local partition join "single_hash_repartition_first" ][ single hash partition join "single_hash_repartition_second" ]
2023-11-25 04:51:18.005 UTC [1108366] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:18.005 UTC [1108366] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		single_hash_repartition_first t1, single_hash_repartition_first t2, single_hash_repartition_second t3
	WHERE
		t1.id = t2.id AND t1.sum = t3.id;
2023-11-25 04:51:18.006 UTC [1108366] ERROR:  XX000: the query contains a join that requires repartitioning
2023-11-25 04:51:18.006 UTC [1108366] HINT:  Set citus.enable_repartition_joins to on to enable repartitioning
2023-11-25 04:51:18.006 UTC [1108366] LOCATION:  JobExecutorType, multi_server_executor.c:68
2023-11-25 04:51:18.006 UTC [1108366] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		single_hash_repartition_first t1, single_hash_repartition_first t2, single_hash_repartition_second t3
	WHERE
		t1.id = t2.id AND t1.sum = t3.id;
2023-11-25 04:51:18.006 UTC [1108366] LOG:  00000: join order: [ "single_hash_repartition_first" ][ single hash partition join "single_hash_repartition_second" ][ dual partition join "single_hash_repartition_first" ]
2023-11-25 04:51:18.006 UTC [1108366] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:18.006 UTC [1108366] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		single_hash_repartition_first t1, single_hash_repartition_first t2, single_hash_repartition_second t3
	WHERE
		t1.sum = t2.sum AND t1.sum = t3.id;
2023-11-25 04:51:18.008 UTC [1108366] ERROR:  XX000: the query contains a join that requires repartitioning
2023-11-25 04:51:18.008 UTC [1108366] HINT:  Set citus.enable_repartition_joins to on to enable repartitioning
2023-11-25 04:51:18.008 UTC [1108366] LOCATION:  JobExecutorType, multi_server_executor.c:68
2023-11-25 04:51:18.008 UTC [1108366] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		single_hash_repartition_first t1, single_hash_repartition_first t2, single_hash_repartition_second t3
	WHERE
		t1.sum = t2.sum AND t1.sum = t3.id;
2023-11-25 04:51:18.008 UTC [1108366] LOG:  00000: join order: [ "single_hash_repartition_second" ][ cartesian product "single_hash_repartition_first" ][ dual partition join "single_hash_repartition_first" ]
2023-11-25 04:51:18.008 UTC [1108366] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:18.008 UTC [1108366] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		single_hash_repartition_first t1, single_hash_repartition_first t2, single_hash_repartition_second t3
	WHERE
		t1.id = t2.id AND t1.avg = t3.id;
2023-11-25 04:51:18.009 UTC [1108366] ERROR:  0A000: cannot perform distributed planning on this query
2023-11-25 04:51:18.009 UTC [1108366] DETAIL:  Cartesian products are currently unsupported
2023-11-25 04:51:18.009 UTC [1108366] LOCATION:  JoinSequenceArray, multi_physical_planner.c:3589
2023-11-25 04:51:18.009 UTC [1108366] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		single_hash_repartition_first t1, single_hash_repartition_first t2, single_hash_repartition_second t3
	WHERE
		t1.id = t2.id AND t1.avg = t3.id;
2023-11-25 04:51:18.009 UTC [1108366] LOG:  00000: join order: [ "single_hash_repartition_first" ][ single hash partition join "single_hash_repartition_second" ]
2023-11-25 04:51:18.009 UTC [1108366] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:18.009 UTC [1108366] STATEMENT:  EXPLAIN WITH cte1 AS
	(
		SELECT
			t1.id * t2.avg as data
		FROM
			single_hash_repartition_first t1, single_hash_repartition_second t2
		WHERE
			t1.id = t2.sum
		AND t1.sum > 5000
		ORDER BY 1 DESC
		LIMIT 50
	)
	SELECT
		count(*)
	FROM
		cte1, single_hash_repartition_first
	WHERE
		cte1.data > single_hash_repartition_first.id;
2023-11-25 04:51:18.010 UTC [1108366] LOG:  00000: join order: [ "single_hash_repartition_first" ][ single hash partition join "single_hash_repartition_second" ]
2023-11-25 04:51:18.010 UTC [1108366] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:18.010 UTC [1108366] STATEMENT:  EXPLAIN WITH cte1 AS
	(
		SELECT
			t1.id * t2.avg as data
		FROM
			single_hash_repartition_first t1, single_hash_repartition_second t2
		WHERE
			t1.id = t2.sum
		AND t1.sum > 5000
		ORDER BY 1 DESC
		LIMIT 50
	)
	SELECT
		count(*)
	FROM
		cte1, single_hash_repartition_first
	WHERE
		cte1.data > single_hash_repartition_first.id;
2023-11-25 04:51:18.011 UTC [1108366] ERROR:  XX000: the query contains a join that requires repartitioning
2023-11-25 04:51:18.011 UTC [1108366] HINT:  Set citus.enable_repartition_joins to on to enable repartitioning
2023-11-25 04:51:18.011 UTC [1108366] LOCATION:  JobExecutorType, multi_server_executor.c:68
2023-11-25 04:51:18.011 UTC [1108366] STATEMENT:  EXPLAIN WITH cte1 AS
	(
		SELECT
			t1.id * t2.avg as data
		FROM
			single_hash_repartition_first t1, single_hash_repartition_second t2
		WHERE
			t1.id = t2.sum
		AND t1.sum > 5000
		ORDER BY 1 DESC
		LIMIT 50
	)
	SELECT
		count(*)
	FROM
		cte1, single_hash_repartition_first
	WHERE
		cte1.data > single_hash_repartition_first.id;
2023-11-25 04:51:18.011 UTC [1108366] LOG:  00000: join order: [ "single_hash_repartition_first" ][ single hash partition join "single_hash_repartition_second" ][ single hash partition join "single_hash_repartition_second" ]
2023-11-25 04:51:18.011 UTC [1108366] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:18.011 UTC [1108366] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		single_hash_repartition_first t1, single_hash_repartition_second t2, single_hash_repartition_second t3
	WHERE
		t1.id = t2.sum AND t2.sum = t3.id;
2023-11-25 04:51:18.012 UTC [1108366] ERROR:  XX000: the query contains a join that requires repartitioning
2023-11-25 04:51:18.012 UTC [1108366] HINT:  Set citus.enable_repartition_joins to on to enable repartitioning
2023-11-25 04:51:18.012 UTC [1108366] LOCATION:  JobExecutorType, multi_server_executor.c:68
2023-11-25 04:51:18.012 UTC [1108366] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		single_hash_repartition_first t1, single_hash_repartition_second t2, single_hash_repartition_second t3
	WHERE
		t1.id = t2.sum AND t2.sum = t3.id;
2023-11-25 04:51:18.013 UTC [1108366] LOG:  00000: join order: [ "single_hash_repartition_second" ][ single hash partition join "single_hash_repartition_second" ][ single hash partition join "single_hash_repartition_first" ]
2023-11-25 04:51:18.013 UTC [1108366] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:18.013 UTC [1108366] STATEMENT:  EXPLAIN SELECT
		avg(t1.avg + t2.avg)
	FROM
		single_hash_repartition_first t1, single_hash_repartition_second t2, single_hash_repartition_second t3
	WHERE
		t1.id = t2.sum AND t2.id = t3.sum
	LIMIT 10;
2023-11-25 04:51:18.014 UTC [1108366] ERROR:  XX000: the query contains a join that requires repartitioning
2023-11-25 04:51:18.014 UTC [1108366] HINT:  Set citus.enable_repartition_joins to on to enable repartitioning
2023-11-25 04:51:18.014 UTC [1108366] LOCATION:  JobExecutorType, multi_server_executor.c:68
2023-11-25 04:51:18.014 UTC [1108366] STATEMENT:  EXPLAIN SELECT
		avg(t1.avg + t2.avg)
	FROM
		single_hash_repartition_first t1, single_hash_repartition_second t2, single_hash_repartition_second t3
	WHERE
		t1.id = t2.sum AND t2.id = t3.sum
	LIMIT 10;
2023-11-25 04:51:18.015 UTC [1108366] LOG:  00000: join order: [ "single_hash_repartition_first" ][ single hash partition join "single_hash_repartition_second" ]
2023-11-25 04:51:18.015 UTC [1108366] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:18.015 UTC [1108366] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		single_hash_repartition_first t1, single_hash_repartition_second t2
	WHERE
		t1.id = t2.sum;
2023-11-25 04:51:18.015 UTC [1108366] ERROR:  XX000: the query contains a join that requires repartitioning
2023-11-25 04:51:18.015 UTC [1108366] HINT:  Set citus.enable_repartition_joins to on to enable repartitioning
2023-11-25 04:51:18.015 UTC [1108366] LOCATION:  JobExecutorType, multi_server_executor.c:68
2023-11-25 04:51:18.015 UTC [1108366] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		single_hash_repartition_first t1, single_hash_repartition_second t2
	WHERE
		t1.id = t2.sum;
2023-11-25 04:51:18.016 UTC [1108366] LOG:  00000: join order: [ "single_hash_repartition_first" ][ single hash partition join "single_hash_repartition_second" ]
2023-11-25 04:51:18.016 UTC [1108366] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:18.016 UTC [1108366] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		single_hash_repartition_first t1, single_hash_repartition_second t2
	WHERE
		t1.sum = t2.id;
2023-11-25 04:51:18.016 UTC [1108366] ERROR:  XX000: the query contains a join that requires repartitioning
2023-11-25 04:51:18.016 UTC [1108366] HINT:  Set citus.enable_repartition_joins to on to enable repartitioning
2023-11-25 04:51:18.016 UTC [1108366] LOCATION:  JobExecutorType, multi_server_executor.c:68
2023-11-25 04:51:18.016 UTC [1108366] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		single_hash_repartition_first t1, single_hash_repartition_second t2
	WHERE
		t1.sum = t2.id;
2023-11-25 04:51:18.021 UTC [1108365] ERROR:  0A000: cannot push down this subquery
2023-11-25 04:51:18.021 UTC [1108365] DETAIL:  Limit clause is currently unsupported when a lateral subquery references a column from a reference table (ref)
2023-11-25 04:51:18.021 UTC [1108365] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 04:51:18.021 UTC [1108365] STATEMENT:  SELECT count(*)
	FROM ref,
	    LATERAL (
	        SELECT
	            test.y
	        FROM test
	        WHERE
	            test.y = ref.a
	        LIMIT 2
	    ) q;
2023-11-25 04:51:18.021 UTC [1108365] ERROR:  0A000: cannot push down this subquery
2023-11-25 04:51:18.021 UTC [1108365] DETAIL:  Limit clause is currently unsupported when a lateral subquery references a column from a VALUES clause
2023-11-25 04:51:18.021 UTC [1108365] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 04:51:18.021 UTC [1108365] STATEMENT:  SELECT count(*)
	FROM (VALUES (1), (3)) ref(a),
	    LATERAL (
	        SELECT
	            test.y
	        FROM test
	        WHERE
	            test.y = ref.a
	        LIMIT 2
	    ) q;
2023-11-25 04:51:18.022 UTC [1108365] ERROR:  0A000: cannot push down this subquery
2023-11-25 04:51:18.022 UTC [1108365] DETAIL:  Limit clause is currently unsupported when a lateral subquery references a column from complex subqueries, CTEs or local tables
2023-11-25 04:51:18.022 UTC [1108365] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 04:51:18.022 UTC [1108365] STATEMENT:  WITH ref(a) as (select y from test)
	SELECT count(*)
	FROM ref,
	    LATERAL (
	        SELECT
	            test.y
	        FROM test
	        WHERE
	            test.y = ref.a
	        LIMIT 2
	    ) q;
2023-11-25 04:51:18.022 UTC [1108365] ERROR:  0A000: cannot push down this subquery
2023-11-25 04:51:18.022 UTC [1108365] DETAIL:  Limit clause is currently unsupported when a lateral subquery references a column from a table function (ref)
2023-11-25 04:51:18.022 UTC [1108365] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 04:51:18.022 UTC [1108365] STATEMENT:  SELECT count(*)
	FROM generate_series(1, 3) ref(a),
	    LATERAL (
	        SELECT
	            test.y
	        FROM test
	        WHERE
	            test.y = ref.a
	        LIMIT 2
	    ) q;
2023-11-25 04:51:18.022 UTC [1108365] ERROR:  0A000: cannot push down this subquery
2023-11-25 04:51:18.022 UTC [1108365] DETAIL:  Limit clause is currently unsupported when a lateral subquery references a column from a subquery without FROM (ref)
2023-11-25 04:51:18.022 UTC [1108365] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 04:51:18.022 UTC [1108365] STATEMENT:  SELECT count(*)
	FROM (SELECT generate_series(1, 3)) ref(a),
	    LATERAL (
	        SELECT
	            test.y
	        FROM test
	        WHERE
	            test.y = ref.a
	        LIMIT 2
	    ) q;
2023-11-25 04:51:18.022 UTC [1108365] ERROR:  0A000: cannot push down this subquery
2023-11-25 04:51:18.022 UTC [1108365] DETAIL:  Limit clause is currently unsupported when a lateral subquery references a column from a reference table (ref_table)
2023-11-25 04:51:18.022 UTC [1108365] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 04:51:18.022 UTC [1108365] STATEMENT:  SELECT count(*)
	FROM ref ref_table,
	    (VALUES (1), (3)) rec_values(a),
	    LATERAL (
	        SELECT
	            test.y
	        FROM test
	        WHERE
	            test.y = ref_table.a
	        LIMIT 2
	    ) q;
2023-11-25 04:51:18.022 UTC [1108365] ERROR:  0A000: cannot push down this subquery
2023-11-25 04:51:18.022 UTC [1108365] DETAIL:  Limit clause is currently unsupported when a lateral subquery references a column from a VALUES clause
2023-11-25 04:51:18.022 UTC [1108365] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 04:51:18.022 UTC [1108365] STATEMENT:  SELECT count(*)
	FROM ref as ref_table,
	    (VALUES (1), (3)) ref_values(a),
	    LATERAL (
	        SELECT
	            test.y
	        FROM test
	        WHERE
	            test.y = ref_values.a
	        LIMIT 2
	    ) q;
2023-11-25 04:51:18.023 UTC [1108365] ERROR:  0A000: cannot push down this subquery
2023-11-25 04:51:18.023 UTC [1108365] DETAIL:  Limit clause is currently unsupported when a lateral subquery references a column from a reference table (ref_outer)
2023-11-25 04:51:18.023 UTC [1108365] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 04:51:18.023 UTC [1108365] STATEMENT:  SELECT count(*) FROM
	    ref ref_outer,
	    LATERAL (
	        SELECT * FROM
	            LATERAL ( SELECT *
	            FROM ref ref_inner,
	                LATERAL (
	                    SELECT
	                        test.y
	                    FROM test
	                    WHERE
	                        test.y = ref_outer.a
	                    LIMIT 2
	                ) q
	            ) q2
	    ) q3;
2023-11-25 04:51:18.023 UTC [1108365] ERROR:  0A000: cannot push down this subquery
2023-11-25 04:51:18.023 UTC [1108365] DETAIL:  Limit clause is currently unsupported when a lateral subquery references a column from a reference table (ref_inner)
2023-11-25 04:51:18.023 UTC [1108365] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 04:51:18.023 UTC [1108365] STATEMENT:  SELECT count(*) FROM
	    ref ref_outer,
	    LATERAL (
	        SELECT * FROM
	            LATERAL ( SELECT *
	            FROM ref ref_inner,
	                LATERAL (
	                    SELECT
	                        test.y
	                    FROM test
	                    WHERE
	                        test.y = ref_inner.a
	                    LIMIT 2
	                ) q
	            ) q2
	    ) q3;
2023-11-25 04:51:18.023 UTC [1108365] ERROR:  0A000: cannot push down this subquery
2023-11-25 04:51:18.023 UTC [1108365] DETAIL:  Limit clause is currently unsupported when a lateral subquery references a column from a reference table (ref)
2023-11-25 04:51:18.023 UTC [1108365] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 04:51:18.023 UTC [1108365] STATEMENT:  SELECT count(*)
	FROM ref,
	    LATERAL (
	        SELECT
	            test.x
	        FROM test
	        WHERE
	            test.x = ref.a
	        LIMIT 2
	    ) q;
2023-11-25 04:51:18.023 UTC [1108365] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:51:18.023 UTC [1108365] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:51:18.023 UTC [1108365] STATEMENT:  SELECT count(*)
	FROM test,
	    LATERAL (
	        SELECT
	            test_2.x
	        FROM test test_2
	        WHERE
	            test_2.x = test.y
	        LIMIT 2
	    ) q ;
2023-11-25 04:51:18.023 UTC [1108365] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:51:18.023 UTC [1108365] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:51:18.023 UTC [1108365] STATEMENT:  SELECT count(*)
	FROM ref JOIN test on ref.b = test.x,
	    LATERAL (
	        SELECT
	            test_2.x
	        FROM test test_2
	        WHERE
	            test_2.x = ref.a
	        LIMIT 2
	    ) q
	;
2023-11-25 04:51:18.024 UTC [1108365] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:51:18.024 UTC [1108365] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:51:18.024 UTC [1108365] STATEMENT:  SELECT count(*)
	FROM ref JOIN test on ref.b = test.x,
	    LATERAL (
	        SELECT
	            test_2.y
	        FROM test test_2
	        WHERE
	            test_2.y = ref.a
	        LIMIT 2
	    ) q
	;
2023-11-25 04:51:18.060 UTC [1108366] LOG:  00000: join order: [ "test_numeric" ][ single hash partition join "test_numeric" ]
2023-11-25 04:51:18.060 UTC [1108366] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:18.060 UTC [1108366] STATEMENT:  SELECT count(*) FROM test_numeric t1 JOIN test_numeric as t2 ON (t1.a = t2.b);
2023-11-25 04:51:18.245 UTC [1108366] LOG:  00000: join order: [ "dist_1" ][ single hash partition join "dist_1" ]
2023-11-25 04:51:18.245 UTC [1108366] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:18.245 UTC [1108366] STATEMENT:  SELECT COUNT(*) FROM dist_1 f, dist_1 s WHERE f.a = s.b;
2023-11-25 04:51:19.129 UTC [1108819] LOG:  00000: join order: [ "lineitem" ]
2023-11-25 04:51:19.129 UTC [1108819] CONTEXT:  PL/pgSQL function public.coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 04:51:19.129 UTC [1108819] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:19.129 UTC [1108819] STATEMENT:  SELECT public.coordinator_plan($Q$
	EXPLAIN (COSTS FALSE)
	SELECT l_orderkey, l_linenumber, l_shipdate FROM lineitem WHERE l_orderkey = 9030 or l_orderkey = 1;
	$Q$);
2023-11-25 04:51:19.132 UTC [1108819] LOG:  00000: join order: [ "lineitem" ][ local partition join "orders" ]
2023-11-25 04:51:19.132 UTC [1108819] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:19.132 UTC [1108819] STATEMENT:  EXPLAIN (COSTS FALSE)
	SELECT sum(l_linenumber), avg(l_linenumber) FROM lineitem, orders
		WHERE l_orderkey = o_orderkey;
2023-11-25 04:51:19.135 UTC [1108819] LOG:  00000: join order: [ "lineitem" ]
2023-11-25 04:51:19.135 UTC [1108819] CONTEXT:  PL/pgSQL function public.coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 04:51:19.135 UTC [1108819] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:19.135 UTC [1108819] STATEMENT:  SELECT public.coordinator_plan($Q$
	EXPLAIN (COSTS FALSE)
	SELECT l_orderkey, l_linenumber, l_shipdate FROM lineitem WHERE l_orderkey = 9030;
	$Q$);
2023-11-25 04:51:19.136 UTC [1108819] LOG:  00000: join order: [ "lineitem" ][ dual partition join "orders" ]
2023-11-25 04:51:19.136 UTC [1108819] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:19.136 UTC [1108819] STATEMENT:  EXPLAIN (COSTS FALSE)
	SELECT sum(l_linenumber), avg(l_linenumber) FROM lineitem, orders
		WHERE l_partkey = o_custkey;
2023-11-25 04:51:19.137 UTC [1108819] LOG:  00000: join order: [ "lineitem" ]
2023-11-25 04:51:19.137 UTC [1108819] CONTEXT:  PL/pgSQL function public.coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 04:51:19.137 UTC [1108819] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:19.137 UTC [1108819] STATEMENT:  SELECT public.coordinator_plan($Q$
	EXPLAIN (COSTS FALSE)
	SELECT l_orderkey, l_linenumber, l_shipdate FROM lineitem WHERE l_orderkey = 9030;
	$Q$);
2023-11-25 04:51:19.138 UTC [1108819] LOG:  00000: join order: [ "lineitem" ][ dual partition join "orders" ]
2023-11-25 04:51:19.138 UTC [1108819] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:19.138 UTC [1108819] STATEMENT:  EXPLAIN (COSTS FALSE)
	SELECT sum(l_linenumber), avg(l_linenumber) FROM lineitem, orders
		WHERE l_partkey = o_custkey;
2023-11-25 04:51:19.140 UTC [1108819] LOG:  00000: join order: [ "lineitem" ][ dual partition join "orders" ]
2023-11-25 04:51:19.140 UTC [1108819] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 04:51:19.140 UTC [1108819] STATEMENT:  EXPLAIN (COSTS FALSE)
	SELECT sum(l_linenumber), avg(l_linenumber) FROM lineitem, orders
		WHERE l_partkey = o_custkey;
2023-11-25 04:51:19.357 UTC [1108870] ERROR:  0A000: modifying the partition value of rows is not allowed
2023-11-25 04:51:19.357 UTC [1108870] LOCATION:  TargetlistAndFunctionsSupported, multi_router_planner.c:585
2023-11-25 04:51:19.357 UTC [1108870] STATEMENT:  UPDATE test SET a = 5, c = 5;
2023-11-25 04:51:19.358 UTC [1108870] ERROR:  0A000: modifying the partition value of rows is not allowed
2023-11-25 04:51:19.358 UTC [1108870] LOCATION:  TargetlistAndFunctionsSupported, multi_router_planner.c:585
2023-11-25 04:51:19.358 UTC [1108870] STATEMENT:  UPDATE test SET a = 5, c = d, d =3;
2023-11-25 04:51:19.358 UTC [1108870] ERROR:  0A000: modifying the partition value of rows is not allowed
2023-11-25 04:51:19.358 UTC [1108870] LOCATION:  TargetlistAndFunctionsSupported, multi_router_planner.c:585
2023-11-25 04:51:19.358 UTC [1108870] STATEMENT:  UPDATE test SET c = d, d =3;
2023-11-25 04:51:19.358 UTC [1108870] ERROR:  0A000: modifying the partition value of rows is not allowed
2023-11-25 04:51:19.358 UTC [1108870] LOCATION:  TargetlistAndFunctionsSupported, multi_router_planner.c:585
2023-11-25 04:51:19.358 UTC [1108870] STATEMENT:  UPDATE test SET d=c, c = d;
2023-11-25 04:51:19.372 UTC [1108870] ERROR:  42601: multiple assignments to same column "c"
2023-11-25 04:51:19.372 UTC [1108870] LOCATION:  process_matched_tle, rewriteHandler.c:1105
2023-11-25 04:51:19.372 UTC [1108870] STATEMENT:  UPDATE test SET c = c, c = 3;
2023-11-25 04:51:19.386 UTC [1108870] ERROR:  0A000: modifying the partition value of rows is not allowed
2023-11-25 04:51:19.386 UTC [1108870] LOCATION:  ErrorIfOnConflictNotSupported, multi_router_planner.c:1255
2023-11-25 04:51:19.386 UTC [1108870] STATEMENT:  INSERT INTO test (c,d) VALUES(3,4) ON CONFLICT(c) DO UPDATE SET c=7;
2023-11-25 04:51:19.389 UTC [1108869] ERROR:  42501: permission denied for table reference_table_1
2023-11-25 04:51:19.389 UTC [1108869] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:51:19.389 UTC [1108869] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:19.389 UTC [1108869] STATEMENT:  INSERT INTO reference_table_2 SELECT * FROM reference_table_1;
2023-11-25 04:51:19.390 UTC [1108870] ERROR:  0A000: modifying the partition value of rows is not allowed
2023-11-25 04:51:19.390 UTC [1108870] LOCATION:  ErrorIfOnConflictNotSupported, multi_router_planner.c:1255
2023-11-25 04:51:19.390 UTC [1108870] STATEMENT:  INSERT INTO test (d,c) VALUES(3,4) ON CONFLICT(c) DO UPDATE SET c=7;
2023-11-25 04:51:19.412 UTC [1108870] ERROR:  42601: multiple assignments to same column "c"
2023-11-25 04:51:19.412 UTC [1108870] LOCATION:  process_matched_tle, rewriteHandler.c:1105
2023-11-25 04:51:19.412 UTC [1108870] STATEMENT:  UPDATE test SET c = c, c = 3;
2023-11-25 04:51:19.481 UTC [1108870] ERROR:  0A000: modifying the partition value of rows is not allowed
2023-11-25 04:51:19.481 UTC [1108870] LOCATION:  TargetlistAndFunctionsSupported, multi_router_planner.c:585
2023-11-25 04:51:19.481 UTC [1108870] STATEMENT:  UPDATE test SET a = 5,d  = 2, c = 5 FROM (SELECT * FROM test LIMIT 10) t2 WHERE t2.d = test.c  and test.c = 6;
2023-11-25 04:51:19.715 UTC [1109038] ERROR:  XX000: multi-task query about to be executed
2023-11-25 04:51:19.715 UTC [1109038] HINT:  Queries are split to multiple tasks if they have to be split into several queries on the workers.
2023-11-25 04:51:19.715 UTC [1109038] LOCATION:  FinalizePlan, distributed_planner.c:1392
2023-11-25 04:51:19.715 UTC [1109038] STATEMENT:  SELECT * FROM multi_task_table;
2023-11-25 04:51:19.740 UTC [1109038] ERROR:  XX000: multi-task query about to be executed
2023-11-25 04:51:19.740 UTC [1109038] HINT:  Queries are split to multiple tasks if they have to be split into several queries on the workers.
2023-11-25 04:51:19.740 UTC [1109038] LOCATION:  FinalizePlan, distributed_planner.c:1392
2023-11-25 04:51:19.740 UTC [1109038] STATEMENT:  INSERT INTO summary_table SELECT id, SUM(order_count) FROM raw_table GROUP BY id;
2023-11-25 04:51:19.750 UTC [1109038] ERROR:  XX000: multi-task query about to be executed
2023-11-25 04:51:19.750 UTC [1109038] HINT:  Queries are split to multiple tasks if they have to be split into several queries on the workers.
2023-11-25 04:51:19.750 UTC [1109038] LOCATION:  FinalizePlan, distributed_planner.c:1392
2023-11-25 04:51:19.750 UTC [1109038] STATEMENT:  INSERT INTO summary_table SELECT id, SUM(order_count) FROM raw_table GROUP BY id;
2023-11-25 04:51:20.013 UTC [1109158] ERROR:  42P01: relation "customer_few" does not exist at character 52
2023-11-25 04:51:20.013 UTC [1109158] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 04:51:20.013 UTC [1109158] STATEMENT:  SELECT customer_key, c_name, c_address
	       FROM customer_few ORDER BY customer_key LIMIT 5;
2023-11-25 04:51:20.098 UTC [1109158] ERROR:  34000: cursor "noholdcursor" does not exist
2023-11-25 04:51:20.098 UTC [1109158] LOCATION:  PerformPortalFetch, portalcmds.c:187
2023-11-25 04:51:20.098 UTC [1109158] STATEMENT:  FETCH ABSOLUTE 5 FROM noHoldCursor;
2023-11-25 04:51:20.297 UTC [1109190] ERROR:  XX000: cannot modify table "on_update_fkey_table" because there was a parallel operation on a distributed table in the transaction
2023-11-25 04:51:20.297 UTC [1109190] DETAIL:  When there is a foreign key to a reference table, Citus needs to perform all operations over a single connection per node to ensure consistency.
2023-11-25 04:51:20.297 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.297 UTC [1109190] LOCATION:  SetupExecutionModeForAlterTable, table.c:3638
2023-11-25 04:51:20.297 UTC [1109190] STATEMENT:  ALTER TABLE on_update_fkey_table DROP COLUMN value_1 CASCADE;
2023-11-25 04:51:20.300 UTC [1109190] ERROR:  XX000: cannot modify table "on_update_fkey_table" because there was a parallel operation on a distributed table in the transaction
2023-11-25 04:51:20.300 UTC [1109190] DETAIL:  When there is a foreign key to a reference table, Citus needs to perform all operations over a single connection per node to ensure consistency.
2023-11-25 04:51:20.300 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.300 UTC [1109190] LOCATION:  SetupExecutionModeForAlterTable, table.c:3638
2023-11-25 04:51:20.300 UTC [1109190] STATEMENT:  ALTER TABLE on_update_fkey_table DROP COLUMN value_1 CASCADE;
2023-11-25 04:51:20.305 UTC [1109190] ERROR:  XX000: cannot execute parallel DDL on table "on_update_fkey_table" after SELECT command on reference table "reference_table" because there is a foreign key between them and "reference_table" has been accessed in this transaction
2023-11-25 04:51:20.305 UTC [1109190] DETAIL:  When there is a foreign key to a reference table, Citus needs to perform all operations over a single connection per node to ensure consistency.
2023-11-25 04:51:20.305 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.305 UTC [1109190] LOCATION:  CheckConflictingParallelRelationAccesses, relation_access_tracking.c:880
2023-11-25 04:51:20.305 UTC [1109190] STATEMENT:  ALTER TABLE on_update_fkey_table ADD COLUMN X INT;
2023-11-25 04:51:20.309 UTC [1109190] ERROR:  XX000: cannot execute parallel DDL on table "on_update_fkey_table" after SELECT command on reference table "transitive_reference_table" because there is a foreign key between them and "transitive_reference_table" has been accessed in this transaction
2023-11-25 04:51:20.309 UTC [1109190] DETAIL:  When there is a foreign key to a reference table, Citus needs to perform all operations over a single connection per node to ensure consistency.
2023-11-25 04:51:20.309 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.309 UTC [1109190] LOCATION:  CheckConflictingParallelRelationAccesses, relation_access_tracking.c:880
2023-11-25 04:51:20.309 UTC [1109190] STATEMENT:  ALTER TABLE on_update_fkey_table ADD COLUMN X INT;
2023-11-25 04:51:20.345 UTC [1109190] ERROR:  23503: insert or update on table "on_update_fkey_table_2380002" violates foreign key constraint "fkey_2380002"
2023-11-25 04:51:20.345 UTC [1109190] DETAIL:  Key (value_1)=(101) is not present in table "reference_table_2380001".
2023-11-25 04:51:20.345 UTC [1109190] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:51:20.345 UTC [1109190] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:51:20.345 UTC [1109190] STATEMENT:  UPDATE on_update_fkey_table SET value_1 = 101 WHERE id = 1;
2023-11-25 04:51:20.345 UTC [1109190] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 04:51:20.345 UTC [1109190] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 04:51:20.345 UTC [1109190] STATEMENT:  UPDATE on_update_fkey_table SET value_1 = 101 WHERE id = 2;
2023-11-25 04:51:20.345 UTC [1109190] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 04:51:20.345 UTC [1109190] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 04:51:20.345 UTC [1109190] STATEMENT:  UPDATE on_update_fkey_table SET value_1 = 101 WHERE id = 3;
2023-11-25 04:51:20.345 UTC [1109190] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 04:51:20.345 UTC [1109190] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 04:51:20.345 UTC [1109190] STATEMENT:  UPDATE on_update_fkey_table SET value_1 = 101 WHERE id = 4;
2023-11-25 04:51:20.370 UTC [1109190] ERROR:  XX000: insert or update on table "on_update_fkey_table_2380005" violates foreign key constraint "fkey_2380005"
2023-11-25 04:51:20.370 UTC [1109190] DETAIL:  Key (value_1)=(101) is not present in table "reference_table_2380001".
2023-11-25 04:51:20.370 UTC [1109190] LOCATION:  ReportCopyError, multi_copy.c:1193
2023-11-25 04:51:20.370 UTC [1109190] STATEMENT:  COPY on_update_fkey_table FROM STDIN WITH CSV;
2023-11-25 04:51:20.505 UTC [1109190] ERROR:  XX000: cannot modify table "reference_table" because there was a parallel operation on a distributed table
2023-11-25 04:51:20.505 UTC [1109190] DETAIL:  When there is a foreign key to a reference table or to a local table, Citus needs to perform all operations over a single connection per node to ensure consistency.
2023-11-25 04:51:20.505 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.505 UTC [1109190] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:795
2023-11-25 04:51:20.505 UTC [1109190] STATEMENT:  UPDATE reference_table SET id = 101 WHERE id = 99;
2023-11-25 04:51:20.508 UTC [1109190] ERROR:  XX000: cannot modify table "transitive_reference_table" because there was a parallel operation on a distributed table
2023-11-25 04:51:20.508 UTC [1109190] DETAIL:  When there is a foreign key to a reference table or to a local table, Citus needs to perform all operations over a single connection per node to ensure consistency.
2023-11-25 04:51:20.508 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.508 UTC [1109190] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:795
2023-11-25 04:51:20.508 UTC [1109190] STATEMENT:  UPDATE transitive_reference_table SET id = 101 WHERE id = 99;
2023-11-25 04:51:20.512 UTC [1109190] ERROR:  XX000: cannot execute DDL on table "reference_table" because there was a parallel SELECT access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 04:51:20.512 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.512 UTC [1109190] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 04:51:20.512 UTC [1109190] STATEMENT:  ALTER TABLE reference_table ADD COLUMN X INT;
2023-11-25 04:51:20.516 UTC [1109190] ERROR:  XX000: cannot execute DDL on table "transitive_reference_table" because there was a parallel SELECT access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 04:51:20.516 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.516 UTC [1109190] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 04:51:20.516 UTC [1109190] STATEMENT:  ALTER TABLE transitive_reference_table ADD COLUMN X INT;
2023-11-25 04:51:20.523 UTC [1109190] ERROR:  XX000: cannot execute DDL on table "reference_table" because there was a parallel SELECT access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 04:51:20.523 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.523 UTC [1109190] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 04:51:20.523 UTC [1109190] STATEMENT:  ALTER TABLE reference_table ALTER COLUMN id SET DATA TYPE smallint;
2023-11-25 04:51:20.530 UTC [1109190] ERROR:  XX000: cannot execute DDL on table "transitive_reference_table" because there was a parallel SELECT access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 04:51:20.530 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.530 UTC [1109190] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 04:51:20.530 UTC [1109190] STATEMENT:  ALTER TABLE transitive_reference_table ALTER COLUMN id SET DATA TYPE smallint;
2023-11-25 04:51:20.536 UTC [1109190] ERROR:  XX000: cannot execute DDL on table "reference_table" because there was a parallel SELECT access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 04:51:20.536 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.536 UTC [1109190] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 04:51:20.536 UTC [1109190] STATEMENT:  TRUNCATE reference_table CASCADE;
2023-11-25 04:51:20.541 UTC [1109190] ERROR:  XX000: cannot execute DDL on table "transitive_reference_table" because there was a parallel SELECT access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 04:51:20.541 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.541 UTC [1109190] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 04:51:20.541 UTC [1109190] STATEMENT:  TRUNCATE transitive_reference_table CASCADE;
2023-11-25 04:51:20.557 UTC [1109190] ERROR:  XX000: cannot execute DDL on table because there was a parallel SELECT access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 04:51:20.557 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.557 UTC [1109190] CONTEXT:  SQL statement "SELECT citus_drop_all_shards(v_obj.objid, v_obj.schema_name, v_obj.object_name, drop_shards_metadata_only := false)"
	PL/pgSQL function citus_drop_trigger() line 25 at PERFORM
2023-11-25 04:51:20.557 UTC [1109190] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:761
2023-11-25 04:51:20.557 UTC [1109190] STATEMENT:  DROP TABLE reference_table CASCADE;
2023-11-25 04:51:20.575 UTC [1109190] ERROR:  XX000: cannot execute DML on table "reference_table" because there was a parallel DML access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 04:51:20.575 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.575 UTC [1109190] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 04:51:20.575 UTC [1109190] STATEMENT:  UPDATE reference_table SET id = 160 WHERE id = 15;
2023-11-25 04:51:20.579 UTC [1109190] ERROR:  XX000: cannot execute DML on table "transitive_reference_table" because there was a parallel DML access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 04:51:20.579 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.579 UTC [1109190] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 04:51:20.579 UTC [1109190] STATEMENT:  UPDATE transitive_reference_table SET id = 160 WHERE id = 15;
2023-11-25 04:51:20.583 UTC [1109190] ERROR:  XX000: cannot execute DML on table "reference_table" because there was a parallel DML access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 04:51:20.583 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.583 UTC [1109190] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 04:51:20.583 UTC [1109190] STATEMENT:  UPDATE reference_table SET id = 160 WHERE id = 15;
2023-11-25 04:51:20.587 UTC [1109190] ERROR:  XX000: cannot execute DML on table "transitive_reference_table" because there was a parallel DML access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 04:51:20.587 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.587 UTC [1109190] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 04:51:20.587 UTC [1109190] STATEMENT:  UPDATE transitive_reference_table SET id = 160 WHERE id = 15;
2023-11-25 04:51:20.591 UTC [1109190] ERROR:  XX000: cannot execute DDL on table "reference_table" because there was a parallel DML access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 04:51:20.591 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.591 UTC [1109190] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 04:51:20.591 UTC [1109190] STATEMENT:  ALTER TABLE reference_table ADD COLUMN X INT;
2023-11-25 04:51:20.596 UTC [1109190] ERROR:  XX000: cannot execute DDL on table "transitive_reference_table" because there was a parallel DML access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 04:51:20.596 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.596 UTC [1109190] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 04:51:20.596 UTC [1109190] STATEMENT:  ALTER TABLE transitive_reference_table ADD COLUMN X INT;
2023-11-25 04:51:20.603 UTC [1109190] ERROR:  XX000: cannot execute DDL on table "reference_table" because there was a parallel DML access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 04:51:20.603 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.603 UTC [1109190] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 04:51:20.603 UTC [1109190] STATEMENT:  ALTER TABLE reference_table ALTER COLUMN id SET DATA TYPE smallint;
2023-11-25 04:51:20.610 UTC [1109190] ERROR:  XX000: cannot execute DDL on table "transitive_reference_table" because there was a parallel DML access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 04:51:20.610 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.610 UTC [1109190] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 04:51:20.610 UTC [1109190] STATEMENT:  ALTER TABLE transitive_reference_table ALTER COLUMN id SET DATA TYPE smallint;
2023-11-25 04:51:20.615 UTC [1109190] ERROR:  XX000: cannot execute SELECT on table "reference_table" because there was a parallel DDL access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 04:51:20.615 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.615 UTC [1109190] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 04:51:20.615 UTC [1109190] STATEMENT:  SELECT count(*) FROM reference_table;
2023-11-25 04:51:20.619 UTC [1109190] ERROR:  XX000: cannot execute SELECT on table "transitive_reference_table" because there was a parallel DDL access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 04:51:20.619 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.619 UTC [1109190] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 04:51:20.619 UTC [1109190] STATEMENT:  SELECT count(*) FROM transitive_reference_table;
2023-11-25 04:51:20.639 UTC [1109190] ERROR:  XX000: cannot execute SELECT on table "reference_table" because there was a parallel DDL access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 04:51:20.639 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.639 UTC [1109190] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 04:51:20.639 UTC [1109190] STATEMENT:  SELECT count(*) FROM reference_table;
2023-11-25 04:51:20.643 UTC [1109190] ERROR:  XX000: cannot execute SELECT on table "transitive_reference_table" because there was a parallel DDL access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 04:51:20.643 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.643 UTC [1109190] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 04:51:20.643 UTC [1109190] STATEMENT:  SELECT count(*) FROM transitive_reference_table;
2023-11-25 04:51:20.647 UTC [1109190] ERROR:  XX000: cannot execute DML on table "reference_table" because there was a parallel DDL access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 04:51:20.647 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.647 UTC [1109190] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 04:51:20.647 UTC [1109190] STATEMENT:  UPDATE reference_table SET id = 160 WHERE id = 15;
2023-11-25 04:51:20.651 UTC [1109190] ERROR:  XX000: cannot execute DML on table "transitive_reference_table" because there was a parallel DDL access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 04:51:20.651 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.651 UTC [1109190] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 04:51:20.651 UTC [1109190] STATEMENT:  UPDATE transitive_reference_table SET id = 160 WHERE id = 15;
2023-11-25 04:51:20.656 UTC [1109190] ERROR:  XX000: cannot execute DDL on table "reference_table" because there was a parallel DDL access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 04:51:20.656 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.656 UTC [1109190] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 04:51:20.656 UTC [1109190] STATEMENT:  ALTER TABLE reference_table ADD COLUMN X int;
2023-11-25 04:51:20.661 UTC [1109190] ERROR:  XX000: cannot execute DDL on table "transitive_reference_table" because there was a parallel DDL access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 04:51:20.661 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.661 UTC [1109190] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 04:51:20.661 UTC [1109190] STATEMENT:  ALTER TABLE transitive_reference_table ADD COLUMN X int;
2023-11-25 04:51:20.664 UTC [1109190] ERROR:  XX000: cannot modify table "on_update_fkey_table" because there was a parallel operation on a distributed table in the transaction
2023-11-25 04:51:20.664 UTC [1109190] DETAIL:  When there is a foreign key to a reference table, Citus needs to perform all operations over a single connection per node to ensure consistency.
2023-11-25 04:51:20.664 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.664 UTC [1109190] LOCATION:  SetupExecutionModeForAlterTable, table.c:3638
2023-11-25 04:51:20.664 UTC [1109190] STATEMENT:  ALTER TABLE on_update_fkey_table ALTER COLUMN value_1 SET DATA TYPE smallint;
2023-11-25 04:51:20.675 UTC [1109190] ERROR:  XX000: cannot execute DML on table "reference_table" because there was a parallel DML access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 04:51:20.675 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.675 UTC [1109190] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 04:51:20.675 UTC [1109190] STATEMENT:  DELETE FROM reference_table  WHERE id = 99;
2023-11-25 04:51:20.686 UTC [1109190] ERROR:  XX000: cannot execute DML on table "transitive_reference_table" because there was a parallel DML access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 04:51:20.686 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.686 UTC [1109190] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 04:51:20.686 UTC [1109190] STATEMENT:  DELETE FROM transitive_reference_table  WHERE id = 99;
2023-11-25 04:51:20.696 UTC [1109190] ERROR:  XX000: cannot execute DML on table "reference_table" because there was a parallel DML access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 04:51:20.696 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.696 UTC [1109190] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 04:51:20.696 UTC [1109190] STATEMENT:  UPDATE reference_table SET id = 101 WHERE id = 99;
2023-11-25 04:51:20.706 UTC [1109190] ERROR:  XX000: cannot execute DML on table "transitive_reference_table" because there was a parallel DML access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 04:51:20.706 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.706 UTC [1109190] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 04:51:20.706 UTC [1109190] STATEMENT:  UPDATE transitive_reference_table SET id = 101 WHERE id = 99;
2023-11-25 04:51:20.711 UTC [1109190] ERROR:  XX000: cannot modify table "reference_table" because there was a parallel operation on a distributed table
2023-11-25 04:51:20.711 UTC [1109190] DETAIL:  When there is a foreign key to a reference table or to a local table, Citus needs to perform all operations over a single connection per node to ensure consistency.
2023-11-25 04:51:20.711 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.711 UTC [1109190] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:795
2023-11-25 04:51:20.711 UTC [1109190] STATEMENT:  UPDATE reference_table SET id = 101 WHERE id = 99;
2023-11-25 04:51:20.711 UTC [1109190] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 04:51:20.711 UTC [1109190] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 04:51:20.711 UTC [1109190] STATEMENT:  UPDATE on_update_fkey_table SET value_1 = 5 WHERE id != 11;
2023-11-25 04:51:20.745 UTC [1109190] ERROR:  XX000: cannot distribute relation "test_table_2" in this transaction because it has a foreign key to a reference table
2023-11-25 04:51:20.745 UTC [1109190] DETAIL:  If a hash distributed table has a foreign key to a reference table, it has to be created in sequential mode before any parallel commands have been executed in the same transaction
2023-11-25 04:51:20.745 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.745 UTC [1109190] LOCATION:  CanUseExclusiveConnections, create_distributed_table.c:2256
2023-11-25 04:51:20.745 UTC [1109190] STATEMENT:  SELECT create_distributed_table('test_table_2', 'id');
2023-11-25 04:51:20.746 UTC [1109190] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 04:51:20.746 UTC [1109190] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 04:51:20.746 UTC [1109190] STATEMENT:  SET LOCAL client_min_messages TO ERROR;
2023-11-25 04:51:20.746 UTC [1109190] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 04:51:20.746 UTC [1109190] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 04:51:20.746 UTC [1109190] STATEMENT:  DROP TABLE test_table_1 CASCADE;
2023-11-25 04:51:20.787 UTC [1109190] ERROR:  XX000: cannot modify table "test_table_2" because there was a parallel operation on a distributed table in the transaction
2023-11-25 04:51:20.787 UTC [1109190] DETAIL:  When there is a foreign key to a reference table, Citus needs to perform all operations over a single connection per node to ensure consistency.
2023-11-25 04:51:20.787 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.787 UTC [1109190] LOCATION:  SetupExecutionModeForAlterTable, table.c:3638
2023-11-25 04:51:20.787 UTC [1109190] STATEMENT:  ALTER TABLE test_table_2 ADD CONSTRAINT c_check FOREIGN KEY (value_1) REFERENCES test_table_1(id);
2023-11-25 04:51:20.788 UTC [1109190] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 04:51:20.788 UTC [1109190] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 04:51:20.788 UTC [1109190] STATEMENT:  SET LOCAL client_min_messages TO ERROR;
2023-11-25 04:51:20.788 UTC [1109190] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 04:51:20.788 UTC [1109190] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 04:51:20.788 UTC [1109190] STATEMENT:  DROP TABLE test_table_1, test_table_2;
2023-11-25 04:51:20.821 UTC [1109190] ERROR:  XX000: cannot modify table "test_table_2" because there was a parallel operation on a distributed table in the transaction
2023-11-25 04:51:20.821 UTC [1109190] DETAIL:  When there is a foreign key to a reference table, Citus needs to perform all operations over a single connection per node to ensure consistency.
2023-11-25 04:51:20.821 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.821 UTC [1109190] LOCATION:  SetupExecutionModeForAlterTable, table.c:3638
2023-11-25 04:51:20.821 UTC [1109190] STATEMENT:  ALTER TABLE test_table_2 ADD CONSTRAINT c_check FOREIGN KEY (value_1) REFERENCES test_table_1(id);
2023-11-25 04:51:20.822 UTC [1109190] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 04:51:20.822 UTC [1109190] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 04:51:20.822 UTC [1109190] STATEMENT:  SET LOCAL client_min_messages TO ERROR;
2023-11-25 04:51:20.822 UTC [1109190] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 04:51:20.822 UTC [1109190] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 04:51:20.822 UTC [1109190] STATEMENT:  DROP TABLE test_table_1 CASCADE;
2023-11-25 04:51:20.843 UTC [1109190] ERROR:  XX000: cannot distribute "test_table_2" in sequential mode because a parallel query was executed in this transaction
2023-11-25 04:51:20.843 UTC [1109190] HINT:  If you have manually set citus.multi_shard_modify_mode to 'sequential', try with 'parallel' option. 
2023-11-25 04:51:20.843 UTC [1109190] LOCATION:  CanUseExclusiveConnections, create_distributed_table.c:2269
2023-11-25 04:51:20.843 UTC [1109190] STATEMENT:  SELECT create_distributed_table('test_table_2', 'id');
2023-11-25 04:51:20.843 UTC [1109190] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 04:51:20.843 UTC [1109190] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 04:51:20.843 UTC [1109190] STATEMENT:  CREATE TABLE test_table_1(id int PRIMARY KEY);
2023-11-25 04:51:20.844 UTC [1109190] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 04:51:20.844 UTC [1109190] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 04:51:20.844 UTC [1109190] STATEMENT:  SELECT create_reference_table('test_table_1');
2023-11-25 04:51:20.844 UTC [1109190] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 04:51:20.844 UTC [1109190] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 04:51:20.844 UTC [1109190] STATEMENT:  ALTER TABLE test_table_2 ADD CONSTRAINT c_check FOREIGN KEY (value_1) REFERENCES test_table_1(id);
2023-11-25 04:51:20.844 UTC [1109190] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 04:51:20.844 UTC [1109190] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 04:51:20.844 UTC [1109190] STATEMENT:  SET LOCAL client_min_messages TO ERROR;
2023-11-25 04:51:20.844 UTC [1109190] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 04:51:20.844 UTC [1109190] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 04:51:20.844 UTC [1109190] STATEMENT:  DROP TABLE test_table_1 CASCADE;
2023-11-25 04:51:20.969 UTC [1109190] ERROR:  XX000: cannot execute DML on table "reference_table" because there was a parallel DML access to distributed table "distributed_table" in the same transaction
2023-11-25 04:51:20.969 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.969 UTC [1109190] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 04:51:20.969 UTC [1109190] STATEMENT:  WITH t1 AS (DELETE FROM distributed_table RETURNING id),
		t2 AS (DELETE FROM reference_table RETURNING id)
		SELECT count(*) FROM distributed_table, t1, t2 WHERE  value_1 = t1.id AND value_1 = t2.id;
2023-11-25 04:51:20.972 UTC [1109190] ERROR:  XX000: cannot execute DML on table "reference_table" because there was a parallel DML access to distributed table "distributed_table" in the same transaction
2023-11-25 04:51:20.972 UTC [1109190] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 04:51:20.972 UTC [1109190] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 04:51:20.972 UTC [1109190] STATEMENT:  WITH t1 AS (DELETE FROM distributed_table RETURNING id)
		DELETE FROM reference_table RETURNING id;
2023-11-25 04:51:21.370 UTC [1109360] ERROR:  XX000: you cannot alter access method of a partitioned table
2023-11-25 04:51:21.370 UTC [1109360] LOCATION:  AlterTableSetAccessMethod, alter_table.c:487
2023-11-25 04:51:21.370 UTC [1109360] STATEMENT:  SELECT alter_table_set_access_method('partitioned_table', 'columnar');
2023-11-25 04:51:21.412 UTC [1109360] ERROR:  P0001: partition column of partitioned_table cannot be cast to a timestamptz
2023-11-25 04:51:21.412 UTC [1109360] CONTEXT:  PL/pgSQL function alter_old_partitions_set_access_method(regclass,timestamp with time zone,name) line 14 at RAISE
2023-11-25 04:51:21.412 UTC [1109360] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:51:21.412 UTC [1109360] STATEMENT:  CALL alter_old_partitions_set_access_method('partitioned_table', '2021-01-01', 'columnar');
2023-11-25 04:51:21.771 UTC [1109360] ERROR:  0A000: Foreign keys and AFTER ROW triggers are not supported for columnar tables
2023-11-25 04:51:21.771 UTC [1109360] HINT:  Consider an AFTER STATEMENT trigger instead.
2023-11-25 04:51:21.771 UTC [1109360] CONTEXT:  SQL statement "ALTER TABLE alter_table_set_access_method.test_fk_p ATTACH PARTITION alter_table_set_access_method.test_fk_p1 FOR VALUES FROM (10) TO (20);"
2023-11-25 04:51:21.771 UTC [1109360] LOCATION:  ColumnarTriggerCreateHook, columnar_tableam.c:2140
2023-11-25 04:51:21.771 UTC [1109360] STATEMENT:  select alter_table_set_access_method('test_fk_p1', 'columnar');
2023-11-25 04:51:21.772 UTC [1109360] ERROR:  XX000: the access method of alter_table_set_access_method.same_access_method is already heap
2023-11-25 04:51:21.772 UTC [1109360] LOCATION:  AlterTableSetAccessMethod, alter_table.c:514
2023-11-25 04:51:21.772 UTC [1109360] STATEMENT:  SELECT alter_table_set_access_method('same_access_method', 'heap');
2023-11-25 04:51:21.777 UTC [1109360] WARNING:  0A000: "view v_local" has dependency to "table local" that is not in Citus' metadata
2023-11-25 04:51:21.777 UTC [1109360] DETAIL:  "view v_local" will be created only locally
2023-11-25 04:51:21.777 UTC [1109360] HINT:  Distribute "table local" first to distribute "view v_local"
2023-11-25 04:51:21.777 UTC [1109360] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:950
2023-11-25 04:51:21.922 UTC [1109360] ERROR:  XX000: you cannot alter access method of a view
2023-11-25 04:51:21.922 UTC [1109360] LOCATION:  AlterTableSetAccessMethod, alter_table.c:492
2023-11-25 04:51:21.922 UTC [1109360] STATEMENT:  select alter_table_set_access_method('view_test_view','columnar');
2023-11-25 04:51:22.419 UTC [1109451] ERROR:  XX000: cannot complete operation because table is a partition
2023-11-25 04:51:22.419 UTC [1109451] HINT:  the parent table is "partitioned_table"
2023-11-25 04:51:22.419 UTC [1109451] LOCATION:  EnsureTableNotPartition, alter_table.c:1172
2023-11-25 04:51:22.419 UTC [1109451] STATEMENT:  SELECT alter_distributed_table('partitioned_table_1_5', shard_count := 10, distribution_column := 'a');
2023-11-25 04:51:22.626 UTC [1109451] WARNING:  01000: foreign key table_with_references_a1_fkey will be dropped
2023-11-25 04:51:22.626 UTC [1109451] LOCATION:  WarningsForDroppingForeignKeysWithDistributedTables, alter_table.c:2096
2023-11-25 04:51:22.626 UTC [1109451] WARNING:  01000: foreign key referencing_dist_table_a_fkey will be dropped
2023-11-25 04:51:22.626 UTC [1109451] LOCATION:  WarningsForDroppingForeignKeysWithDistributedTables, alter_table.c:2096
2023-11-25 04:51:23.104 UTC [1109451] LOG:  00000: performing blocking isolate_tenant_to_new_shard 
2023-11-25 04:51:23.104 UTC [1109451] LOCATION:  SplitShard, shard_split.c:507
2023-11-25 04:51:23.104 UTC [1109451] STATEMENT:  SELECT 1 FROM isolate_tenant_to_new_shard('shard_split_table', 5, shard_transfer_mode => 'block_writes');
2023-11-25 04:51:23.106 UTC [1109451] LOG:  00000: creating child shards for isolate_tenant_to_new_shard
2023-11-25 04:51:23.106 UTC [1109451] LOCATION:  BlockingShardSplit, shard_split.c:571
2023-11-25 04:51:23.106 UTC [1109451] STATEMENT:  SELECT 1 FROM isolate_tenant_to_new_shard('shard_split_table', 5, shard_transfer_mode => 'block_writes');
2023-11-25 04:51:23.114 UTC [1109451] LOG:  00000: performing copy for isolate_tenant_to_new_shard
2023-11-25 04:51:23.114 UTC [1109451] LOCATION:  BlockingShardSplit, shard_split.c:577
2023-11-25 04:51:23.114 UTC [1109451] STATEMENT:  SELECT 1 FROM isolate_tenant_to_new_shard('shard_split_table', 5, shard_transfer_mode => 'block_writes');
2023-11-25 04:51:23.115 UTC [1109451] LOG:  00000: creating auxillary structures (indexes, stats, replicaindentities, triggers) for isolate_tenant_to_new_shard
2023-11-25 04:51:23.115 UTC [1109451] LOCATION:  BlockingShardSplit, shard_split.c:587
2023-11-25 04:51:23.115 UTC [1109451] STATEMENT:  SELECT 1 FROM isolate_tenant_to_new_shard('shard_split_table', 5, shard_transfer_mode => 'block_writes');
2023-11-25 04:51:23.115 UTC [1109451] LOG:  00000: marking deferred cleanup of source shard(s) for isolate_tenant_to_new_shard
2023-11-25 04:51:23.115 UTC [1109451] LOCATION:  BlockingShardSplit, shard_split.c:609
2023-11-25 04:51:23.115 UTC [1109451] STATEMENT:  SELECT 1 FROM isolate_tenant_to_new_shard('shard_split_table', 5, shard_transfer_mode => 'block_writes');
2023-11-25 04:51:23.117 UTC [1109451] LOG:  00000: creating foreign key constraints (if any) for isolate_tenant_to_new_shard
2023-11-25 04:51:23.117 UTC [1109451] LOCATION:  BlockingShardSplit, shard_split.c:625
2023-11-25 04:51:23.117 UTC [1109451] STATEMENT:  SELECT 1 FROM isolate_tenant_to_new_shard('shard_split_table', 5, shard_transfer_mode => 'block_writes');
2023-11-25 04:51:23.149 UTC [1109451] ERROR:  XX000: you have to specify at least one of the distribution_column, shard_count or colocate_with parameters
2023-11-25 04:51:23.149 UTC [1109451] LOCATION:  CheckAlterDistributedTableConversionParameters, alter_table.c:1813
2023-11-25 04:51:23.149 UTC [1109451] STATEMENT:  SELECT alter_distributed_table('dist_table');
2023-11-25 04:51:23.149 UTC [1109451] ERROR:  XX000: you have to specify at least one of the distribution_column, shard_count or colocate_with parameters
2023-11-25 04:51:23.149 UTC [1109451] LOCATION:  CheckAlterDistributedTableConversionParameters, alter_table.c:1813
2023-11-25 04:51:23.149 UTC [1109451] STATEMENT:  SELECT alter_distributed_table('dist_table', cascade_to_colocated := false);
2023-11-25 04:51:23.149 UTC [1109451] ERROR:  XX000: this call doesn't change any properties of the table
2023-11-25 04:51:23.149 UTC [1109451] HINT:  check citus_tables view to see current properties of the table
2023-11-25 04:51:23.149 UTC [1109451] LOCATION:  CheckAlterDistributedTableConversionParameters, alter_table.c:1891
2023-11-25 04:51:23.149 UTC [1109451] STATEMENT:  SELECT alter_distributed_table('dist_table', distribution_column := 'b');
2023-11-25 04:51:23.149 UTC [1109451] ERROR:  XX000: this call doesn't change any properties of the table
2023-11-25 04:51:23.149 UTC [1109451] HINT:  check citus_tables view to see current properties of the table
2023-11-25 04:51:23.149 UTC [1109451] LOCATION:  CheckAlterDistributedTableConversionParameters, alter_table.c:1891
2023-11-25 04:51:23.149 UTC [1109451] STATEMENT:  SELECT alter_distributed_table('dist_table', shard_count := 10);
2023-11-25 04:51:23.169 UTC [1109451] ERROR:  XX000: this call doesn't change any properties of the table
2023-11-25 04:51:23.169 UTC [1109451] HINT:  check citus_tables view to see current properties of the table
2023-11-25 04:51:23.169 UTC [1109451] LOCATION:  CheckAlterDistributedTableConversionParameters, alter_table.c:1891
2023-11-25 04:51:23.169 UTC [1109451] STATEMENT:  SELECT alter_distributed_table('dist_table', colocate_with := 'colocation_table');
2023-11-25 04:51:23.216 UTC [1109451] ERROR:  XX000: distribution_column cannot be cascaded to colocated tables
2023-11-25 04:51:23.216 UTC [1109451] LOCATION:  CheckAlterDistributedTableConversionParameters, alter_table.c:1873
2023-11-25 04:51:23.216 UTC [1109451] STATEMENT:  SELECT alter_distributed_table('dist_table', distribution_column := 'b', cascade_to_colocated := true);
2023-11-25 04:51:23.216 UTC [1109451] ERROR:  XX000: distribution_column cannot be cascaded to colocated tables
2023-11-25 04:51:23.216 UTC [1109451] LOCATION:  CheckAlterDistributedTableConversionParameters, alter_table.c:1873
2023-11-25 04:51:23.216 UTC [1109451] STATEMENT:  SELECT alter_distributed_table('dist_table', distribution_column := 'b', shard_count:=12, colocate_with:='colocation_table_2', cascade_to_colocated := true);
2023-11-25 04:51:23.217 UTC [1109451] ERROR:  XX000: shard_count or colocate_with is necessary for cascading to colocated tables
2023-11-25 04:51:23.217 UTC [1109451] LOCATION:  CheckAlterDistributedTableConversionParameters, alter_table.c:1879
2023-11-25 04:51:23.217 UTC [1109451] STATEMENT:  SELECT alter_distributed_table('dist_table', cascade_to_colocated := true);
2023-11-25 04:51:23.217 UTC [1109451] ERROR:  XX000: colocate_with := 'none' cannot be cascaded to colocated tables
2023-11-25 04:51:23.217 UTC [1109451] LOCATION:  CheckAlterDistributedTableConversionParameters, alter_table.c:1899
2023-11-25 04:51:23.217 UTC [1109451] STATEMENT:  SELECT alter_distributed_table('dist_table', colocate_with := 'none', cascade_to_colocated := true);
2023-11-25 04:51:23.217 UTC [1109451] ERROR:  XX000: cascade_to_colocated parameter is necessary
2023-11-25 04:51:23.217 UTC [1109451] DETAIL:  this table is colocated with some other tables
2023-11-25 04:51:23.217 UTC [1109451] HINT:  cascade_to_colocated := false will break the current colocation, cascade_to_colocated := true will change the shard count of colocated tables too.
2023-11-25 04:51:23.217 UTC [1109451] LOCATION:  CheckAlterDistributedTableConversionParameters, alter_table.c:1907
2023-11-25 04:51:23.217 UTC [1109451] STATEMENT:  SELECT alter_distributed_table('dist_table', shard_count := 14);
2023-11-25 04:51:23.246 UTC [1109451] ERROR:  XX000: shard_count cannot be different than the shard count of the table in colocate_with
2023-11-25 04:51:23.246 UTC [1109451] HINT:  if no shard_count is specified shard count will be same with colocate_with table's
2023-11-25 04:51:23.246 UTC [1109451] LOCATION:  CheckAlterDistributedTableConversionParameters, alter_table.c:1927
2023-11-25 04:51:23.246 UTC [1109451] STATEMENT:  SELECT alter_distributed_table('dist_table', colocate_with := 'colocation_table', shard_count := 16);
2023-11-25 04:51:23.253 UTC [1109451] ERROR:  XX000: cannot colocate with different_type_table because data type of its distribution column is different than dist_table
2023-11-25 04:51:23.253 UTC [1109451] LOCATION:  CheckAlterDistributedTableConversionParameters, alter_table.c:1968
2023-11-25 04:51:23.253 UTC [1109451] STATEMENT:  SELECT alter_distributed_table('dist_table', colocate_with := 'different_type_table');
2023-11-25 04:51:23.253 UTC [1109451] ERROR:  XX000: cannot colocate with different_type_table and change distribution column to a because data type of column a is different then the distribution column of the different_type_table
2023-11-25 04:51:23.253 UTC [1109451] LOCATION:  CheckAlterDistributedTableConversionParameters, alter_table.c:1959
2023-11-25 04:51:23.253 UTC [1109451] STATEMENT:  SELECT alter_distributed_table('dist_table', distribution_column := 'a', colocate_with := 'different_type_table');
2023-11-25 04:51:23.253 UTC [1109451] ERROR:  XX000: shard_count cannot be 0
2023-11-25 04:51:23.253 UTC [1109451] HINT:  if you no longer want this to be a distributed table you can try undistribute_table() function
2023-11-25 04:51:23.253 UTC [1109451] LOCATION:  CheckAlterDistributedTableConversionParameters, alter_table.c:1864
2023-11-25 04:51:23.253 UTC [1109451] STATEMENT:  SELECT alter_distributed_table('dist_table', shard_count := 0);
2023-11-25 04:51:23.257 UTC [1109451] ERROR:  XX000: cannot colocate with reference_table because it is not a distributed table
2023-11-25 04:51:23.257 UTC [1109451] LOCATION:  CheckAlterDistributedTableConversionParameters, alter_table.c:1855
2023-11-25 04:51:23.257 UTC [1109451] STATEMENT:  SELECT alter_distributed_table('dist_table', colocate_with:='reference_table');
2023-11-25 04:51:23.258 UTC [1109451] ERROR:  0A000: relation append_table should be a hash distributed table
2023-11-25 04:51:23.258 UTC [1109451] LOCATION:  EnsureHashDistributedTable, metadata_utility.c:2284
2023-11-25 04:51:23.258 UTC [1109451] STATEMENT:  SELECT alter_distributed_table('append_table', shard_count:=6);
2023-11-25 04:51:25.310 UTC [1109645] ERROR:  0A000: cannot push down subquery on the target list
2023-11-25 04:51:25.310 UTC [1109645] DETAIL:  Subqueries in the SELECT part of the query can only be pushed down if they happen before aggregates and window functions
2023-11-25 04:51:25.310 UTC [1109645] LOCATION:  MultiLogicalPlanOptimize, multi_logical_optimizer.c:479
2023-11-25 04:51:25.310 UTC [1109645] STATEMENT:  /*
	 * Test that we don't get a crash. See #5248.
	 */
	SELECT   subq_3.c15 AS c0,
	         subq_3.c0  AS c1,
	         subq_3.c15 AS c2,
	         subq_0.c1  AS c3,
	         pg_catalog.String_agg( Cast(
	                                      (
	                                      SELECT tgargs
	                                      FROM   pg_catalog.pg_trigger limit 1 offset 1) AS BYTEA), Cast(
	                                                                                                      (
	                                                                                                      SELECT minimum_value
	                                                                                                      FROM   columnar.chunk limit 1 offset 5) AS BYTEA)) OVER (partition BY subq_3.c10 ORDER BY subq_3.c12,subq_0.c2) AS c4,
	         subq_0.c1                                                                                                                                                                                                    AS c5
	FROM     (
	                    SELECT     ref_1.address                      AS c0,
	                               ref_1.error                        AS c1,
	                               sample_0.NAME                      AS c2,
	                               sample_2.trftosql                  AS c3
	                    FROM       pg_catalog.pg_statio_all_sequences AS ref_0
	                    INNER JOIN pg_catalog.pg_hba_file_rules       AS ref_1
	                    ON         ((
	                                                 SELECT pg_catalog.Max(aggnumdirectargs)
	                                                 FROM   pg_catalog.pg_aggregate) <= ref_0.blks_hit)
	                    INNER JOIN countries  AS sample_0 TABLESAMPLE system (6.4)
	                    INNER JOIN local_data AS sample_1 TABLESAMPLE bernoulli (8)
	                    ON         ((
	                                                     true)
	                               OR         (
	                                                     sample_0.NAME IS NOT NULL))
	                    INNER JOIN pg_catalog.pg_transform AS sample_2 TABLESAMPLE bernoulli (1.2)
	                    INNER JOIN pg_catalog.pg_language  AS ref_2
	                    ON         ((
	                                                 SELECT shard_cost_function
	                                                 FROM   pg_catalog.pg_dist_rebalance_strategy limit 1 offset 1) IS NULL)
	                    RIGHT JOIN pg_catalog.pg_index AS sample_3 TABLESAMPLE system (0.3)
	                    ON         ((
	                                                     cast(NULL AS bpchar) ~<=~ cast(NULL AS bpchar))
	                               OR         ((
	                                                                EXISTS
	                                                                (
	                                                                       SELECT sample_3.indnkeyatts        AS c0,
	                                                                              sample_2.trflang            AS c1,
	                                                                              sample_2.trftype            AS c2
	                                                                       FROM   pg_catalog.pg_statistic_ext AS sample_4 TABLESAMPLE bernoulli (8.6)
	                                                                       WHERE  sample_2.trftype IS NOT NULL))
	                                          AND        (
	                                                                false)))
	                    ON         (
	                                          EXISTS
	                                          (
	                                                 SELECT sample_0.id           AS c0,
	                                                        sample_3.indisprimary AS c1
	                                                 FROM   orgs           AS sample_5 TABLESAMPLE system (5.3)
	                                                 WHERE  false))
	                    ON         (
	                                          cast(NULL AS float8) >
	                                          (
	                                                 SELECT pg_catalog.avg(enumsortorder)
	                                                 FROM   pg_catalog.pg_enum) )
	                    WHERE      cast(COALESCE(
	                               CASE
	                                          WHEN ref_1.auth_method ~>=~ ref_1.auth_method THEN cast(NULL AS path)
	                                          ELSE cast(NULL AS path)
	                               END , cast(NULL AS path)) AS path) = cast(NULL AS path)) AS subq_0,
	         lateral
	         (
	                SELECT
	                       (
	                              SELECT pg_catalog.stddev(total_time)
	                              FROM   pg_catalog.pg_stat_user_functions) AS c0,
	                       subq_0.c1                                        AS c1,
	                       subq_2.c0                                        AS c2,
	                       subq_0.c2                                        AS c3,
	                       subq_0.c0                                        AS c4,
	                       cast(COALESCE(subq_2.c0, subq_2.c0) AS text)     AS c5,
	                       subq_2.c0                                        AS c6,
	                       subq_2.c1                                        AS c7,
	                       subq_2.c1                                        AS c8,
	                       subq_2.c1                                        AS c9,
	                       subq_0.c3                                        AS c10,
	                       pg_catalog.pg_stat_get_db_temp_files( cast(
	                                                                   (
	                                                                   SELECT objoid
	                                                                   FROM   pg_catalog.pg_description limit 1 offset 1) AS oid)) AS c11,
	                       subq_0.c3                                                                                               AS c12,
	                       subq_2.c1                                                                                               AS c13,
	                       subq_0.c0                                                                                               AS c14,
	                       subq_0.c3                                                                                               AS c15,
	                       subq_0.c3                                                                                               AS c16,
	                       subq_0.c1                                                                                               AS c17,
	                       subq_0.c2                                                                                               AS c18
	                FROM   (
	                              SELECT subq_1.c2                        AS c0,
	                                     subq_0.c3                        AS c1
	                              FROM   information_schema.element_types AS ref_3,
	                                     lateral
	                                     (
	                                            SELECT subq_0.c1            AS c0,
	                                                   sample_6.info        AS c1,
	                                                   subq_0.c2            AS c2,
	                                                   subq_0.c3            AS c3,
	                                                   ref_3.domain_default AS c4,
	                                                   sample_6.user_id     AS c5,
	                                                   ref_3.collation_name AS c6
	                                            FROM   orders        AS sample_6 TABLESAMPLE system (3.8)
	                                            WHERE  sample_6.price = sample_6.org_id limit 58) AS subq_1
	                              WHERE  (
	                                            subq_1.c2 <= subq_0.c2)
	                              AND    (
	                                            cast(NULL AS line) ?-| cast(NULL AS line)) limit 59) AS subq_2
	                WHERE  cast(COALESCE(pg_catalog.age( cast(
	                                                           (
	                                                           SELECT pg_catalog.max(event_time)
	                                                           FROM   events) AS "timestamp")),
	                       (
	                              SELECT write_lag
	                              FROM   pg_catalog.pg_stat_replication limit 1 offset 3) ) AS "interval") >
	                       (
	                              SELECT utc_offset
	                              FROM   pg_catalog.pg_timezone_names limit 1 offset 4) limit 91) AS subq_3
	WHERE    pg_catalog.pg_backup_stop() > cast(NULL AS record) limit 100;
2023-11-25 04:51:25.647 UTC [1109762] ERROR:  22023: relation with OID 31834 does not exist
2023-11-25 04:51:25.647 UTC [1109762] LOCATION:  EnsureRelationExists, create_distributed_table.c:969
2023-11-25 04:51:25.647 UTC [1109762] STATEMENT:  SELECT undistribute_table('dist_table'), create_distributed_table('dist_table', 'a');
2023-11-25 04:51:25.713 UTC [1109762] ERROR:  XX000: cannot complete operation because table referenced_table is referenced by a foreign key
2023-11-25 04:51:25.713 UTC [1109762] HINT:  Use cascade option to undistribute all the relations involved in a foreign key relationship with undistribute_table.referenced_table by executing SELECT undistribute_table($$undistribute_table.referenced_table$$, cascade_via_foreign_keys=>true)
2023-11-25 04:51:25.713 UTC [1109762] LOCATION:  EnsureTableNotReferenced, alter_table.c:1129
2023-11-25 04:51:25.713 UTC [1109762] STATEMENT:  SELECT undistribute_table('referenced_table');
2023-11-25 04:51:25.713 UTC [1109762] ERROR:  XX000: cannot complete operation because table referencing_table has a foreign key
2023-11-25 04:51:25.713 UTC [1109762] HINT:  Use cascade option to undistribute all the relations involved in a foreign key relationship with undistribute_table.referencing_table by executing SELECT undistribute_table($$undistribute_table.referencing_table$$, cascade_via_foreign_keys=>true)
2023-11-25 04:51:25.713 UTC [1109762] LOCATION:  EnsureTableNotReferencing, alter_table.c:1100
2023-11-25 04:51:25.713 UTC [1109762] STATEMENT:  SELECT undistribute_table('referencing_table');
2023-11-25 04:51:25.761 UTC [1109762] ERROR:  XX000: cannot complete operation because table is a partition
2023-11-25 04:51:25.761 UTC [1109762] HINT:  the parent table is "partitioned_table"
2023-11-25 04:51:25.761 UTC [1109762] LOCATION:  EnsureTableNotPartition, alter_table.c:1172
2023-11-25 04:51:25.761 UTC [1109762] STATEMENT:  SELECT undistribute_table('partitioned_table_1_5');
2023-11-25 04:51:25.860 UTC [1109762] ERROR:  XX000: cannot alter table because an extension depends on it
2023-11-25 04:51:25.860 UTC [1109762] LOCATION:  ErrorIfUnsupportedCascadeObjects, alter_table.c:1405
2023-11-25 04:51:25.860 UTC [1109762] STATEMENT:  SELECT undistribute_table ('extension_table');
2023-11-25 04:51:25.877 UTC [1109762] ERROR:  XX000: cannot alter table because an extension depends on it
2023-11-25 04:51:25.877 UTC [1109762] LOCATION:  ErrorIfUnsupportedCascadeObjects, alter_table.c:1405
2023-11-25 04:51:25.877 UTC [1109762] STATEMENT:  SELECT undistribute_table('rule_table_2');
2023-11-25 04:51:26.100 UTC [1109833] WARNING:  01000: Error on node with node id 16: failed to connect to localhost:0
2023-11-25 04:51:26.100 UTC [1109833] CONTEXT:  PL/pgSQL function run_command_on_all_nodes(text,boolean,boolean) line 46 at RAISE
2023-11-25 04:51:26.100 UTC [1109833] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:51:26.214 UTC [1109883] ERROR:  P0001: the coordinator is not added to the metadata
2023-11-25 04:51:26.214 UTC [1109883] HINT:  Add the node as a coordinator by using: SELECT citus_set_coordinator_host('<hostname>')
2023-11-25 04:51:26.214 UTC [1109883] CONTEXT:  PL/pgSQL function run_command_on_coordinator(text,boolean) line 36 at RAISE
2023-11-25 04:51:26.214 UTC [1109883] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 04:51:26.214 UTC [1109883] STATEMENT:  SELECT success, result FROM run_command_on_coordinator('select inet_server_port()');
2023-11-25 04:51:26.406 UTC [1094384] LOG:  00000: received SIGHUP, reloading configuration files
2023-11-25 04:51:26.406 UTC [1094384] LOCATION:  SIGHUP_handler, postmaster.c:2769
2023-11-25 04:51:26.407 UTC [1094384] LOG:  00000: parameter "citus.background_task_queue_interval" changed to "1s"
2023-11-25 04:51:26.407 UTC [1094384] LOCATION:  ProcessConfigFileInternal, guc-file.l:453
2023-11-25 04:51:27.410 UTC [1094454] LOG:  00000: found scheduled background tasks, starting new background task queue monitor
2023-11-25 04:51:27.410 UTC [1094454] CONTEXT:  Citus maintenance daemon for database 16384 user 10
2023-11-25 04:51:27.410 UTC [1094454] LOCATION:  CitusMaintenanceDaemonMain, maintenanced.c:772
2023-11-25 04:51:27.413 UTC [1109943] LOG:  00000: task jobid/taskid started: 1450000/1450000
2023-11-25 04:51:27.413 UTC [1109943] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:27.413 UTC [1109943] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 04:51:27.415 UTC [1109943] LOG:  00000: task jobid/taskid succeeded: 1450000/1450000
2023-11-25 04:51:27.415 UTC [1109943] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:27.415 UTC [1109943] LOCATION:  ConsumeExecutorQueue, background_jobs.c:777
2023-11-25 04:51:28.420 UTC [1094454] LOG:  00000: found scheduled background tasks, starting new background task queue monitor
2023-11-25 04:51:28.420 UTC [1094454] CONTEXT:  Citus maintenance daemon for database 16384 user 10
2023-11-25 04:51:28.420 UTC [1094454] LOCATION:  CitusMaintenanceDaemonMain, maintenanced.c:772
2023-11-25 04:51:28.422 UTC [1109945] LOG:  00000: task jobid/taskid started: 1450001/1450001
2023-11-25 04:51:28.422 UTC [1109945] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:28.422 UTC [1109945] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 04:51:29.414 UTC [1109946] ERROR:  57014: canceling statement due to user request
2023-11-25 04:51:29.414 UTC [1109946] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450001/1450001)
2023-11-25 04:51:29.414 UTC [1109946] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 04:51:29.415 UTC [1109945] LOG:  00000: task jobid/taskid is cancelled: 1450001/1450001
2023-11-25 04:51:29.415 UTC [1109945] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:29.415 UTC [1109945] LOCATION:  TaskConcurrentCancelCheck, background_jobs.c:720
2023-11-25 04:51:29.416 UTC [1094384] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450001/1450001)" (PID 1109946) exited with exit code 1
2023-11-25 04:51:29.416 UTC [1094384] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 04:51:30.419 UTC [1094454] LOG:  00000: found scheduled background tasks, starting new background task queue monitor
2023-11-25 04:51:30.419 UTC [1094454] CONTEXT:  Citus maintenance daemon for database 16384 user 10
2023-11-25 04:51:30.419 UTC [1094454] LOCATION:  CitusMaintenanceDaemonMain, maintenanced.c:772
2023-11-25 04:51:30.422 UTC [1109947] LOG:  00000: task jobid/taskid started: 1450002/1450002
2023-11-25 04:51:30.422 UTC [1109947] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:30.422 UTC [1109947] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 04:51:30.424 UTC [1109948] ERROR:  22012: division by zero
2023-11-25 04:51:30.424 UTC [1109948] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450002/1450002)
2023-11-25 04:51:30.424 UTC [1109948] LOCATION:  int4div, int.c:840
2023-11-25 04:51:30.424 UTC [1109947] LOG:  00000: task jobid/taskid failed: 1450002/1450002
2023-11-25 04:51:30.424 UTC [1109947] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:30.424 UTC [1109947] LOCATION:  ConsumeExecutorQueue, background_jobs.c:770
2023-11-25 04:51:30.425 UTC [1094384] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450002/1450002)" (PID 1109948) exited with exit code 1
2023-11-25 04:51:30.425 UTC [1094384] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 04:51:32.429 UTC [1094454] LOG:  00000: found scheduled background tasks, starting new background task queue monitor
2023-11-25 04:51:32.429 UTC [1094454] CONTEXT:  Citus maintenance daemon for database 16384 user 10
2023-11-25 04:51:32.429 UTC [1094454] LOCATION:  CitusMaintenanceDaemonMain, maintenanced.c:772
2023-11-25 04:51:32.432 UTC [1109949] LOG:  00000: task jobid/taskid started: 1450003/1450003
2023-11-25 04:51:32.432 UTC [1109949] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:32.432 UTC [1109949] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 04:51:32.434 UTC [1109949] LOG:  00000: task jobid/taskid succeeded: 1450003/1450003
2023-11-25 04:51:32.434 UTC [1109949] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:32.434 UTC [1109949] LOCATION:  ConsumeExecutorQueue, background_jobs.c:777
2023-11-25 04:51:32.436 UTC [1109949] LOG:  00000: task jobid/taskid started: 1450003/1450004
2023-11-25 04:51:32.436 UTC [1109949] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:32.436 UTC [1109949] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 04:51:32.438 UTC [1109949] LOG:  00000: task jobid/taskid succeeded: 1450003/1450004
2023-11-25 04:51:32.438 UTC [1109949] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:32.438 UTC [1109949] LOCATION:  ConsumeExecutorQueue, background_jobs.c:777
2023-11-25 04:51:32.440 UTC [1109949] LOG:  00000: task jobid/taskid started: 1450003/1450005
2023-11-25 04:51:32.440 UTC [1109949] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:32.440 UTC [1109949] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 04:51:32.442 UTC [1109949] LOG:  00000: task jobid/taskid succeeded: 1450003/1450005
2023-11-25 04:51:32.442 UTC [1109949] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:32.442 UTC [1109949] LOCATION:  ConsumeExecutorQueue, background_jobs.c:777
2023-11-25 04:51:33.447 UTC [1094454] LOG:  00000: found scheduled background tasks, starting new background task queue monitor
2023-11-25 04:51:33.447 UTC [1094454] CONTEXT:  Citus maintenance daemon for database 16384 user 10
2023-11-25 04:51:33.447 UTC [1094454] LOCATION:  CitusMaintenanceDaemonMain, maintenanced.c:772
2023-11-25 04:51:33.449 UTC [1109953] LOG:  00000: task jobid/taskid started: 1450004/1450006
2023-11-25 04:51:33.449 UTC [1109953] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:33.449 UTC [1109953] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 04:51:33.452 UTC [1109953] LOG:  00000: task jobid/taskid succeeded: 1450004/1450006
2023-11-25 04:51:33.452 UTC [1109953] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:33.452 UTC [1109953] LOCATION:  ConsumeExecutorQueue, background_jobs.c:777
2023-11-25 04:51:33.453 UTC [1109953] LOG:  00000: task jobid/taskid started: 1450004/1450007
2023-11-25 04:51:33.453 UTC [1109953] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:33.453 UTC [1109953] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 04:51:33.455 UTC [1109955] ERROR:  22012: division by zero
2023-11-25 04:51:33.455 UTC [1109955] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450004/1450007)
2023-11-25 04:51:33.455 UTC [1109955] LOCATION:  int4div, int.c:840
2023-11-25 04:51:33.456 UTC [1109953] LOG:  00000: task jobid/taskid failed: 1450004/1450007
2023-11-25 04:51:33.456 UTC [1109953] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:33.456 UTC [1109953] LOCATION:  ConsumeExecutorQueue, background_jobs.c:770
2023-11-25 04:51:33.456 UTC [1094384] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450004/1450007)" (PID 1109955) exited with exit code 1
2023-11-25 04:51:33.456 UTC [1094384] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 04:51:35.461 UTC [1094454] LOG:  00000: found scheduled background tasks, starting new background task queue monitor
2023-11-25 04:51:35.461 UTC [1094454] CONTEXT:  Citus maintenance daemon for database 16384 user 10
2023-11-25 04:51:35.461 UTC [1094454] LOCATION:  CitusMaintenanceDaemonMain, maintenanced.c:772
2023-11-25 04:51:35.464 UTC [1109956] LOG:  00000: task jobid/taskid started: 1450005/1450009
2023-11-25 04:51:35.464 UTC [1109956] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:35.464 UTC [1109956] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 04:51:35.465 UTC [1109956] LOG:  00000: task jobid/taskid started: 1450005/1450010
2023-11-25 04:51:35.465 UTC [1109956] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:35.465 UTC [1109956] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 04:51:35.465 UTC [1109956] LOG:  00000: task jobid/taskid started: 1450005/1450011
2023-11-25 04:51:35.465 UTC [1109956] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:35.465 UTC [1109956] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 04:51:35.466 UTC [1109956] LOG:  00000: task jobid/taskid started: 1450006/1450012
2023-11-25 04:51:35.466 UTC [1109956] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:35.466 UTC [1109956] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 04:51:35.466 UTC [1109956] WARNING:  01000: unable to start background worker for background task execution
2023-11-25 04:51:35.466 UTC [1109956] DETAIL:  Already reached the maximum number of task executors: 4/4
2023-11-25 04:51:35.466 UTC [1109956] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:35.466 UTC [1109956] LOCATION:  NewExecutorExceedsCitusLimit, background_jobs.c:491
2023-11-25 04:51:36.402 UTC [1109960] ERROR:  57014: canceling statement due to user request
2023-11-25 04:51:36.402 UTC [1109960] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450006/1450012)
2023-11-25 04:51:36.402 UTC [1109960] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 04:51:36.402 UTC [1109956] LOG:  00000: task jobid/taskid is cancelled: 1450006/1450012
2023-11-25 04:51:36.402 UTC [1109956] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:36.402 UTC [1109956] LOCATION:  TaskConcurrentCancelCheck, background_jobs.c:720
2023-11-25 04:51:36.403 UTC [1094384] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450006/1450012)" (PID 1109960) exited with exit code 1
2023-11-25 04:51:36.403 UTC [1094384] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 04:51:36.404 UTC [1109956] LOG:  00000: able to start a background worker with 0 seconds delay
2023-11-25 04:51:36.404 UTC [1109956] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:36.404 UTC [1109956] LOCATION:  CheckAndResetLastWorkerAllocationFailure, background_jobs.c:679
2023-11-25 04:51:36.404 UTC [1109956] LOG:  00000: task jobid/taskid started: 1450007/1450013
2023-11-25 04:51:36.404 UTC [1109956] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:36.404 UTC [1109956] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 04:51:37.404 UTC [1109957] ERROR:  57014: canceling statement due to user request
2023-11-25 04:51:37.404 UTC [1109957] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450005/1450009)
2023-11-25 04:51:37.404 UTC [1109957] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 04:51:37.404 UTC [1109959] ERROR:  57014: canceling statement due to user request
2023-11-25 04:51:37.404 UTC [1109959] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450005/1450011)
2023-11-25 04:51:37.404 UTC [1109959] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 04:51:37.404 UTC [1109958] ERROR:  57014: canceling statement due to user request
2023-11-25 04:51:37.404 UTC [1109958] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450005/1450010)
2023-11-25 04:51:37.404 UTC [1109958] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 04:51:37.404 UTC [1109956] LOG:  00000: task jobid/taskid is cancelled: 1450005/1450011
2023-11-25 04:51:37.404 UTC [1109956] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:37.404 UTC [1109956] LOCATION:  TaskConcurrentCancelCheck, background_jobs.c:720
2023-11-25 04:51:37.406 UTC [1094384] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450005/1450009)" (PID 1109957) exited with exit code 1
2023-11-25 04:51:37.406 UTC [1094384] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 04:51:37.406 UTC [1094384] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450005/1450010)" (PID 1109958) exited with exit code 1
2023-11-25 04:51:37.406 UTC [1094384] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 04:51:37.406 UTC [1094384] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450005/1450011)" (PID 1109959) exited with exit code 1
2023-11-25 04:51:37.406 UTC [1094384] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 04:51:37.406 UTC [1109956] LOG:  00000: task jobid/taskid is cancelled: 1450005/1450009
2023-11-25 04:51:37.406 UTC [1109956] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:37.406 UTC [1109956] LOCATION:  TaskConcurrentCancelCheck, background_jobs.c:720
2023-11-25 04:51:37.406 UTC [1109956] LOG:  00000: task jobid/taskid is cancelled: 1450005/1450010
2023-11-25 04:51:37.406 UTC [1109956] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:37.406 UTC [1109956] LOCATION:  TaskConcurrentCancelCheck, background_jobs.c:720
2023-11-25 04:51:37.406 UTC [1109961] ERROR:  57014: canceling statement due to user request
2023-11-25 04:51:37.406 UTC [1109961] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450007/1450013)
2023-11-25 04:51:37.406 UTC [1109961] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 04:51:37.406 UTC [1109956] LOG:  00000: task jobid/taskid is cancelled: 1450007/1450013
2023-11-25 04:51:37.406 UTC [1109956] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:37.406 UTC [1109956] LOCATION:  TaskConcurrentCancelCheck, background_jobs.c:720
2023-11-25 04:51:37.408 UTC [1094384] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450007/1450013)" (PID 1109961) exited with exit code 1
2023-11-25 04:51:37.408 UTC [1094384] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 04:51:38.408 UTC [1094454] LOG:  00000: found scheduled background tasks, starting new background task queue monitor
2023-11-25 04:51:38.408 UTC [1094454] CONTEXT:  Citus maintenance daemon for database 16384 user 10
2023-11-25 04:51:38.408 UTC [1094454] LOCATION:  CitusMaintenanceDaemonMain, maintenanced.c:772
2023-11-25 04:51:38.411 UTC [1109962] LOG:  00000: task jobid/taskid started: 1450008/1450014
2023-11-25 04:51:38.411 UTC [1109962] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:38.411 UTC [1109962] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 04:51:38.411 UTC [1109962] LOG:  00000: task jobid/taskid started: 1450008/1450015
2023-11-25 04:51:38.411 UTC [1109962] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:38.411 UTC [1109962] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 04:51:38.411 UTC [1109962] LOG:  00000: task jobid/taskid started: 1450008/1450016
2023-11-25 04:51:38.411 UTC [1109962] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:38.411 UTC [1109962] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 04:51:38.412 UTC [1109962] LOG:  00000: task jobid/taskid started: 1450009/1450017
2023-11-25 04:51:38.412 UTC [1109962] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:38.412 UTC [1109962] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 04:51:38.412 UTC [1109962] WARNING:  01000: unable to start background worker for background task execution
2023-11-25 04:51:38.412 UTC [1109962] DETAIL:  Already reached the maximum number of task executors: 4/4
2023-11-25 04:51:38.412 UTC [1109962] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:38.412 UTC [1109962] LOCATION:  NewExecutorExceedsCitusLimit, background_jobs.c:491
2023-11-25 04:51:39.410 UTC [1094384] LOG:  00000: received SIGHUP, reloading configuration files
2023-11-25 04:51:39.410 UTC [1094384] LOCATION:  SIGHUP_handler, postmaster.c:2769
2023-11-25 04:51:39.411 UTC [1094384] LOG:  00000: parameter "citus.max_background_task_executors" changed to "5"
2023-11-25 04:51:39.411 UTC [1094384] LOCATION:  ProcessConfigFileInternal, guc-file.l:453
2023-11-25 04:51:39.412 UTC [1109962] LOG:  00000: able to start a background worker with 0 seconds delay
2023-11-25 04:51:39.412 UTC [1109962] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:39.412 UTC [1109962] LOCATION:  CheckAndResetLastWorkerAllocationFailure, background_jobs.c:679
2023-11-25 04:51:39.412 UTC [1109962] LOG:  00000: task jobid/taskid started: 1450010/1450018
2023-11-25 04:51:39.412 UTC [1109962] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:39.412 UTC [1109962] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 04:51:40.413 UTC [1109963] ERROR:  57014: canceling statement due to user request
2023-11-25 04:51:40.413 UTC [1109963] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450008/1450014)
2023-11-25 04:51:40.413 UTC [1109963] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 04:51:40.413 UTC [1109965] ERROR:  57014: canceling statement due to user request
2023-11-25 04:51:40.413 UTC [1109965] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450008/1450016)
2023-11-25 04:51:40.413 UTC [1109965] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 04:51:40.413 UTC [1109964] ERROR:  57014: canceling statement due to user request
2023-11-25 04:51:40.413 UTC [1109964] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450008/1450015)
2023-11-25 04:51:40.413 UTC [1109964] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 04:51:40.414 UTC [1109962] LOG:  00000: task jobid/taskid is cancelled: 1450008/1450015
2023-11-25 04:51:40.414 UTC [1109962] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:40.414 UTC [1109962] LOCATION:  TaskConcurrentCancelCheck, background_jobs.c:720
2023-11-25 04:51:40.415 UTC [1094384] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450008/1450016)" (PID 1109965) exited with exit code 1
2023-11-25 04:51:40.415 UTC [1094384] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 04:51:40.415 UTC [1094384] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450008/1450014)" (PID 1109963) exited with exit code 1
2023-11-25 04:51:40.415 UTC [1094384] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 04:51:40.415 UTC [1094384] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450008/1450015)" (PID 1109964) exited with exit code 1
2023-11-25 04:51:40.415 UTC [1094384] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 04:51:40.415 UTC [1109962] LOG:  00000: task jobid/taskid is cancelled: 1450008/1450014
2023-11-25 04:51:40.415 UTC [1109962] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:40.415 UTC [1109962] LOCATION:  TaskConcurrentCancelCheck, background_jobs.c:720
2023-11-25 04:51:40.415 UTC [1109962] LOG:  00000: task jobid/taskid is cancelled: 1450008/1450016
2023-11-25 04:51:40.415 UTC [1109962] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:40.415 UTC [1109962] LOCATION:  TaskConcurrentCancelCheck, background_jobs.c:720
2023-11-25 04:51:40.415 UTC [1109966] ERROR:  57014: canceling statement due to user request
2023-11-25 04:51:40.415 UTC [1109966] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450009/1450017)
2023-11-25 04:51:40.415 UTC [1109966] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 04:51:40.415 UTC [1109962] LOG:  00000: task jobid/taskid is cancelled: 1450009/1450017
2023-11-25 04:51:40.415 UTC [1109962] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:40.415 UTC [1109962] LOCATION:  TaskConcurrentCancelCheck, background_jobs.c:720
2023-11-25 04:51:40.417 UTC [1094384] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450009/1450017)" (PID 1109966) exited with exit code 1
2023-11-25 04:51:40.417 UTC [1094384] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 04:51:40.417 UTC [1109967] ERROR:  57014: canceling statement due to user request
2023-11-25 04:51:40.417 UTC [1109967] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450010/1450018)
2023-11-25 04:51:40.417 UTC [1109967] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 04:51:40.417 UTC [1109962] LOG:  00000: task jobid/taskid is cancelled: 1450010/1450018
2023-11-25 04:51:40.417 UTC [1109962] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:40.417 UTC [1109962] LOCATION:  TaskConcurrentCancelCheck, background_jobs.c:720
2023-11-25 04:51:40.418 UTC [1094384] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450010/1450018)" (PID 1109967) exited with exit code 1
2023-11-25 04:51:40.418 UTC [1094384] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 04:51:41.416 UTC [1094454] LOG:  00000: found scheduled background tasks, starting new background task queue monitor
2023-11-25 04:51:41.416 UTC [1094454] CONTEXT:  Citus maintenance daemon for database 16384 user 10
2023-11-25 04:51:41.416 UTC [1094454] LOCATION:  CitusMaintenanceDaemonMain, maintenanced.c:772
2023-11-25 04:51:41.418 UTC [1109970] LOG:  00000: task jobid/taskid started: 1450011/1450019
2023-11-25 04:51:41.418 UTC [1109970] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:41.418 UTC [1109970] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 04:51:41.419 UTC [1109970] LOG:  00000: task jobid/taskid started: 1450012/1450020
2023-11-25 04:51:41.419 UTC [1109970] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:41.419 UTC [1109970] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 04:51:42.419 UTC [1109970] LOG:  00000: handling termination signal
2023-11-25 04:51:42.419 UTC [1109970] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:42.419 UTC [1109970] LOCATION:  CitusBackgroundTaskQueueMonitorMain, background_jobs.c:1232
2023-11-25 04:51:42.419 UTC [1109972] FATAL:  57P01: terminating background worker "Citus Background Task Queue Executor: regression/postgres for (1450012/1450020)" due to administrator command
2023-11-25 04:51:42.419 UTC [1109972] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450012/1450020)
2023-11-25 04:51:42.419 UTC [1109972] LOCATION:  ProcessInterrupts, postgres.c:3213
2023-11-25 04:51:42.419 UTC [1109971] FATAL:  57P01: terminating background worker "Citus Background Task Queue Executor: regression/postgres for (1450011/1450019)" due to administrator command
2023-11-25 04:51:42.419 UTC [1109971] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450011/1450019)
2023-11-25 04:51:42.419 UTC [1109971] LOCATION:  ProcessInterrupts, postgres.c:3213
2023-11-25 04:51:42.419 UTC [1109970] LOG:  00000: task jobid/taskid failed: 1450012/1450020
2023-11-25 04:51:42.419 UTC [1109970] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:42.419 UTC [1109970] LOCATION:  ConsumeExecutorQueue, background_jobs.c:770
2023-11-25 04:51:42.421 UTC [1094384] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450012/1450020)" (PID 1109972) exited with exit code 1
2023-11-25 04:51:42.421 UTC [1094384] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 04:51:42.421 UTC [1094384] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450011/1450019)" (PID 1109971) exited with exit code 1
2023-11-25 04:51:42.421 UTC [1094384] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 04:51:42.421 UTC [1109970] LOG:  00000: task jobid/taskid failed: 1450011/1450019
2023-11-25 04:51:42.421 UTC [1109970] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:42.421 UTC [1109970] LOCATION:  ConsumeExecutorQueue, background_jobs.c:770
2023-11-25 04:51:45.428 UTC [1094454] LOG:  00000: found scheduled background tasks, starting new background task queue monitor
2023-11-25 04:51:45.428 UTC [1094454] CONTEXT:  Citus maintenance daemon for database 16384 user 10
2023-11-25 04:51:45.428 UTC [1094454] LOCATION:  CitusMaintenanceDaemonMain, maintenanced.c:772
2023-11-25 04:51:45.431 UTC [1109973] LOG:  00000: task jobid/taskid started: 1450013/1450021
2023-11-25 04:51:45.431 UTC [1109973] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:45.431 UTC [1109973] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 04:51:45.431 UTC [1109973] LOG:  00000: task jobid/taskid started: 1450014/1450022
2023-11-25 04:51:45.431 UTC [1109973] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:45.431 UTC [1109973] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 04:51:46.430 UTC [1109973] LOG:  00000: handling cancellation signal
2023-11-25 04:51:46.430 UTC [1109973] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:46.430 UTC [1109973] LOCATION:  CitusBackgroundTaskQueueMonitorMain, background_jobs.c:1239
2023-11-25 04:51:46.431 UTC [1109974] ERROR:  57014: canceling statement due to user request
2023-11-25 04:51:46.431 UTC [1109974] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450013/1450021)
2023-11-25 04:51:46.431 UTC [1109974] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 04:51:46.431 UTC [1109973] LOG:  00000: task jobid/taskid is cancelled: 1450014/1450022
2023-11-25 04:51:46.431 UTC [1109973] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:46.431 UTC [1109973] LOCATION:  TaskConcurrentCancelCheck, background_jobs.c:720
2023-11-25 04:51:46.431 UTC [1109975] ERROR:  57014: canceling statement due to user request
2023-11-25 04:51:46.431 UTC [1109975] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450014/1450022)
2023-11-25 04:51:46.431 UTC [1109975] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 04:51:46.433 UTC [1094384] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450013/1450021)" (PID 1109974) exited with exit code 1
2023-11-25 04:51:46.433 UTC [1094384] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 04:51:46.433 UTC [1094384] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450014/1450022)" (PID 1109975) exited with exit code 1
2023-11-25 04:51:46.433 UTC [1094384] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 04:51:46.433 UTC [1109973] LOG:  00000: task jobid/taskid is cancelled: 1450013/1450021
2023-11-25 04:51:46.433 UTC [1109973] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:46.433 UTC [1109973] LOCATION:  TaskConcurrentCancelCheck, background_jobs.c:720
2023-11-25 04:51:47.436 UTC [1094454] LOG:  00000: found scheduled background tasks, starting new background task queue monitor
2023-11-25 04:51:47.436 UTC [1094454] CONTEXT:  Citus maintenance daemon for database 16384 user 10
2023-11-25 04:51:47.436 UTC [1094454] LOCATION:  CitusMaintenanceDaemonMain, maintenanced.c:772
2023-11-25 04:51:47.438 UTC [1109976] LOG:  00000: task jobid/taskid started: 1450015/1450023
2023-11-25 04:51:47.438 UTC [1109976] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:47.438 UTC [1109976] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 04:51:47.439 UTC [1109976] LOG:  00000: task jobid/taskid started: 1450016/1450024
2023-11-25 04:51:47.439 UTC [1109976] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:47.439 UTC [1109976] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 04:51:47.441 UTC [1109976] LOG:  00000: task jobid/taskid succeeded: 1450016/1450024
2023-11-25 04:51:47.441 UTC [1109976] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:47.441 UTC [1109976] LOCATION:  ConsumeExecutorQueue, background_jobs.c:777
2023-11-25 04:51:48.436 UTC [1109977] ERROR:  57014: canceling statement due to user request
2023-11-25 04:51:48.436 UTC [1109977] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450015/1450023)
2023-11-25 04:51:48.436 UTC [1109977] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 04:51:48.436 UTC [1109976] LOG:  00000: task jobid/taskid is cancelled: 1450015/1450023
2023-11-25 04:51:48.436 UTC [1109976] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:48.436 UTC [1109976] LOCATION:  TaskConcurrentCancelCheck, background_jobs.c:720
2023-11-25 04:51:48.438 UTC [1094384] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450015/1450023)" (PID 1109977) exited with exit code 1
2023-11-25 04:51:48.438 UTC [1094384] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 04:51:49.441 UTC [1094454] LOG:  00000: found scheduled background tasks, starting new background task queue monitor
2023-11-25 04:51:49.441 UTC [1094454] CONTEXT:  Citus maintenance daemon for database 16384 user 10
2023-11-25 04:51:49.441 UTC [1094454] LOCATION:  CitusMaintenanceDaemonMain, maintenanced.c:772
2023-11-25 04:51:49.443 UTC [1109980] LOG:  00000: task jobid/taskid started: 1450017/1450025
2023-11-25 04:51:49.443 UTC [1109980] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:49.443 UTC [1109980] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 04:51:49.444 UTC [1109980] LOG:  00000: task jobid/taskid started: 1450017/1450026
2023-11-25 04:51:49.444 UTC [1109980] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:49.444 UTC [1109980] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 04:51:51.448 UTC [1109980] LOG:  00000: task jobid/taskid succeeded: 1450017/1450025
2023-11-25 04:51:51.448 UTC [1109980] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:51.448 UTC [1109980] LOCATION:  ConsumeExecutorQueue, background_jobs.c:777
2023-11-25 04:51:51.450 UTC [1109980] LOG:  00000: task jobid/taskid started: 1450017/1450027
2023-11-25 04:51:51.450 UTC [1109980] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:51.450 UTC [1109980] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 04:51:51.450 UTC [1109980] LOG:  00000: task jobid/taskid succeeded: 1450017/1450026
2023-11-25 04:51:51.450 UTC [1109980] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:51.450 UTC [1109980] LOCATION:  ConsumeExecutorQueue, background_jobs.c:777
2023-11-25 04:51:52.418 UTC [1094384] LOG:  00000: received SIGHUP, reloading configuration files
2023-11-25 04:51:52.418 UTC [1094384] LOCATION:  SIGHUP_handler, postmaster.c:2769
2023-11-25 04:51:52.419 UTC [1094384] LOG:  00000: parameter "citus.max_background_task_executors_per_node" changed to "2"
2023-11-25 04:51:52.419 UTC [1094384] LOCATION:  ProcessConfigFileInternal, guc-file.l:453
2023-11-25 04:51:52.420 UTC [1109980] LOG:  00000: task jobid/taskid started: 1450017/1450028
2023-11-25 04:51:52.420 UTC [1109980] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:52.420 UTC [1109980] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 04:51:52.421 UTC [1109980] LOG:  00000: task jobid/taskid started: 1450017/1450029
2023-11-25 04:51:52.421 UTC [1109980] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:52.421 UTC [1109980] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 04:51:55.457 UTC [1109980] LOG:  00000: task jobid/taskid succeeded: 1450017/1450027
2023-11-25 04:51:55.457 UTC [1109980] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:55.457 UTC [1109980] LOCATION:  ConsumeExecutorQueue, background_jobs.c:777
2023-11-25 04:51:55.459 UTC [1109980] LOG:  00000: task jobid/taskid started: 1450017/1450030
2023-11-25 04:51:55.459 UTC [1109980] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:55.459 UTC [1109980] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 04:51:56.425 UTC [1109980] LOG:  00000: task jobid/taskid succeeded: 1450017/1450028
2023-11-25 04:51:56.425 UTC [1109980] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:56.425 UTC [1109980] LOCATION:  ConsumeExecutorQueue, background_jobs.c:777
2023-11-25 04:51:56.427 UTC [1109980] LOG:  00000: task jobid/taskid started: 1450017/1450031
2023-11-25 04:51:56.427 UTC [1109980] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:56.427 UTC [1109980] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 04:51:56.427 UTC [1109980] LOG:  00000: task jobid/taskid succeeded: 1450017/1450029
2023-11-25 04:51:56.427 UTC [1109980] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:56.427 UTC [1109980] LOCATION:  ConsumeExecutorQueue, background_jobs.c:777
2023-11-25 04:51:57.428 UTC [1094384] LOG:  00000: received SIGHUP, reloading configuration files
2023-11-25 04:51:57.428 UTC [1094384] LOCATION:  SIGHUP_handler, postmaster.c:2769
2023-11-25 04:51:57.428 UTC [1094384] LOG:  00000: parameter "citus.max_background_task_executors_per_node" changed to "3"
2023-11-25 04:51:57.428 UTC [1094384] LOCATION:  ProcessConfigFileInternal, guc-file.l:453
2023-11-25 04:51:57.429 UTC [1109980] LOG:  00000: task jobid/taskid started: 1450017/1450032
2023-11-25 04:51:57.429 UTC [1109980] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:57.429 UTC [1109980] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 04:51:58.437 UTC [1094384] LOG:  00000: received SIGHUP, reloading configuration files
2023-11-25 04:51:58.437 UTC [1094384] LOCATION:  SIGHUP_handler, postmaster.c:2769
2023-11-25 04:51:58.438 UTC [1109986] ERROR:  57014: canceling statement due to user request
2023-11-25 04:51:58.438 UTC [1109986] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450017/1450030)
2023-11-25 04:51:58.438 UTC [1109986] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 04:51:58.438 UTC [1094384] LOG:  00000: parameter "citus.max_background_task_executors_per_node" removed from configuration file, reset to default
2023-11-25 04:51:58.438 UTC [1094384] LOCATION:  ProcessConfigFileInternal, guc-file.l:388
2023-11-25 04:51:58.438 UTC [1109980] LOG:  00000: task jobid/taskid failed: 1450017/1450030
2023-11-25 04:51:58.438 UTC [1109980] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:51:58.438 UTC [1109980] LOCATION:  ConsumeExecutorQueue, background_jobs.c:770
2023-11-25 04:51:58.439 UTC [1094384] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450017/1450030)" (PID 1109986) exited with exit code 1
2023-11-25 04:51:58.439 UTC [1094384] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 04:52:02.437 UTC [1109980] LOG:  00000: task jobid/taskid succeeded: 1450017/1450031
2023-11-25 04:52:02.437 UTC [1109980] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:52:02.437 UTC [1109980] LOCATION:  ConsumeExecutorQueue, background_jobs.c:777
2023-11-25 04:52:03.439 UTC [1109980] LOG:  00000: task jobid/taskid succeeded: 1450017/1450032
2023-11-25 04:52:03.439 UTC [1109980] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:52:03.439 UTC [1109980] LOCATION:  ConsumeExecutorQueue, background_jobs.c:777
2023-11-25 04:52:03.445 UTC [1109980] LOG:  00000: task jobid/taskid started: 1450017/1450030
2023-11-25 04:52:03.445 UTC [1109980] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:52:03.445 UTC [1109980] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 04:52:03.447 UTC [1109991] ERROR:  57014: canceling statement due to user request
2023-11-25 04:52:03.447 UTC [1109991] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450017/1450030)
2023-11-25 04:52:03.447 UTC [1109991] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 04:52:03.447 UTC [1109980] LOG:  00000: task jobid/taskid is cancelled: 1450017/1450030
2023-11-25 04:52:03.447 UTC [1109980] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 04:52:03.447 UTC [1109980] LOCATION:  TaskConcurrentCancelCheck, background_jobs.c:720
2023-11-25 04:52:03.448 UTC [1094384] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450017/1450030)" (PID 1109991) exited with exit code 1
2023-11-25 04:52:03.448 UTC [1094384] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 04:52:04.448 UTC [1094384] LOG:  00000: received SIGHUP, reloading configuration files
2023-11-25 04:52:04.448 UTC [1094384] LOCATION:  SIGHUP_handler, postmaster.c:2769
2023-11-25 04:52:04.458 UTC [1094384] LOG:  00000: received SIGHUP, reloading configuration files
2023-11-25 04:52:04.458 UTC [1094384] LOCATION:  SIGHUP_handler, postmaster.c:2769
2023-11-25 04:52:04.459 UTC [1094384] LOG:  00000: parameter "citus.background_task_queue_interval" removed from configuration file, reset to default
2023-11-25 04:52:04.459 UTC [1094384] LOCATION:  ProcessConfigFileInternal, guc-file.l:388
2023-11-25 04:52:04.459 UTC [1094384] LOG:  00000: parameter "citus.max_background_task_executors" removed from configuration file, reset to default
2023-11-25 04:52:04.459 UTC [1094384] LOCATION:  ProcessConfigFileInternal, guc-file.l:388
2023-11-25 04:52:04.571 UTC [1110006] ERROR:  22P02: invalid input syntax for type cluster_clock: "(-1, 100)" at character 39
2023-11-25 04:52:04.571 UTC [1110006] LOCATION:  cluster_clock_in_internal, type_utils.c:71
2023-11-25 04:52:04.571 UTC [1110006] STATEMENT:  INSERT INTO cluster_clock_type values('(-1, 100)');
2023-11-25 04:52:04.571 UTC [1110006] ERROR:  22P02: invalid input syntax for type cluster_clock: "(100, -1)" at character 39
2023-11-25 04:52:04.571 UTC [1110006] LOCATION:  cluster_clock_in_internal, type_utils.c:82
2023-11-25 04:52:04.571 UTC [1110006] STATEMENT:  INSERT INTO cluster_clock_type values('(100, -1)');
2023-11-25 04:52:04.571 UTC [1110006] ERROR:  22P02: invalid input syntax for type cluster_clock: "(4398046511104, 100)" at character 39
2023-11-25 04:52:04.571 UTC [1110006] LOCATION:  cluster_clock_in_internal, type_utils.c:71
2023-11-25 04:52:04.571 UTC [1110006] STATEMENT:  INSERT INTO cluster_clock_type values('(4398046511104, 100)');
2023-11-25 04:52:04.571 UTC [1110006] ERROR:  22P02: invalid input syntax for type cluster_clock: "(0, 4194304)" at character 39
2023-11-25 04:52:04.571 UTC [1110006] LOCATION:  cluster_clock_in_internal, type_utils.c:82
2023-11-25 04:52:04.571 UTC [1110006] STATEMENT:  INSERT INTO cluster_clock_type values('(0, 4194304)');
2023-11-25 04:52:04.574 UTC [1110006] ERROR:  23505: duplicate key value violates unique constraint "cluster_clock_type_cc_key"
2023-11-25 04:52:04.574 UTC [1110006] DETAIL:  Key (cc)=((100,1)) already exists.
2023-11-25 04:52:04.574 UTC [1110006] LOCATION:  _bt_check_unique, nbtinsert.c:664
2023-11-25 04:52:04.574 UTC [1110006] STATEMENT:  INSERT INTO cluster_clock_type values('(100, 1)');
2023-11-25 04:52:04.574 UTC [1110006] ERROR:  23505: duplicate key value violates unique constraint "cluster_clock_type_cc_key"
2023-11-25 04:52:04.574 UTC [1110006] DETAIL:  Key (cc)=((100,200)) already exists.
2023-11-25 04:52:04.574 UTC [1110006] LOCATION:  _bt_check_unique, nbtinsert.c:664
2023-11-25 04:52:04.574 UTC [1110006] STATEMENT:  INSERT INTO cluster_clock_type values('(100, 200)');
2023-11-25 04:52:04.575 UTC [1110006] ERROR:  23505: duplicate key value violates unique constraint "cluster_clock_type_cc_key"
2023-11-25 04:52:04.575 UTC [1110006] DETAIL:  Key (cc)=((100,100)) already exists.
2023-11-25 04:52:04.575 UTC [1110006] LOCATION:  _bt_check_unique, nbtinsert.c:664
2023-11-25 04:52:04.575 UTC [1110006] STATEMENT:  INSERT INTO cluster_clock_type values('(100, 100)');
2023-11-25 04:52:04.690 UTC [1110006] WARNING:  01000: GUC enable_cluster_clock is off
2023-11-25 04:52:04.690 UTC [1110006] LOCATION:  PrepareAndSetTransactionClock, causal_clock.c:419
2023-11-25 04:52:04.692 UTC [1110006] ERROR:  42501: permission denied for sequence pg_dist_clock_logical_seq
2023-11-25 04:52:04.692 UTC [1110006] LOCATION:  do_setval, sequence.c:967
2023-11-25 04:52:04.692 UTC [1110006] STATEMENT:  SELECT setval('pg_dist_clock_logical_seq', 100, true);
2023-11-25 04:52:04.804 UTC [1110050] LOG:  00000: deferred drop of orphaned resource alter_distributed_table.shard_split_table_362791 on localhost:57637 completed
2023-11-25 04:52:04.804 UTC [1110050] LOCATION:  DropOrphanedResourcesForCleanup, shard_cleaner.c:298
2023-11-25 04:52:04.804 UTC [1110050] STATEMENT:  CALL citus_cleanup_orphaned_resources()
2023-11-25 04:52:05.273 UTC [1110044] WARNING:  0A000: "view pg_source_view" has dependency to "table pg_source" that is not in Citus' metadata
2023-11-25 04:52:05.273 UTC [1110044] DETAIL:  "view pg_source_view" will be created only locally
2023-11-25 04:52:05.273 UTC [1110044] HINT:  Distribute "table pg_source" first to distribute "view pg_source_view"
2023-11-25 04:52:05.273 UTC [1110044] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:950
2023-11-25 04:52:05.435 UTC [1110044] ERROR:  0A000: non-IMMUTABLE functions are not yet supported in MERGE sql with distributed tables 
2023-11-25 04:52:05.435 UTC [1110044] LOCATION:  MergeQuerySupported, merge_planner.c:134
2023-11-25 04:52:05.435 UTC [1110044] STATEMENT:  MERGE INTO target_serial sda
	USING source_serial sdn
	ON sda.id = sdn.id
	WHEN NOT matched THEN
	       INSERT (id, z) VALUES (id, z);
2023-11-25 04:52:05.448 UTC [1110044] ERROR:  0A000: cannot assign to system column "ctid" at character 110
2023-11-25 04:52:05.448 UTC [1110044] LOCATION:  transformAssignedExpr, parse_target.c:480
2023-11-25 04:52:05.448 UTC [1110044] STATEMENT:  MERGE INTO target_set
	USING source_set AS foo ON target_set.t1 = foo.s1
	WHEN MATCHED THEN
	        UPDATE SET ctid = '(0,100)';
2023-11-25 04:52:05.448 UTC [1110044] ERROR:  0A000: cannot pushdown the subquery since not all subqueries in the UNION have the partition column in the same position
2023-11-25 04:52:05.448 UTC [1110044] DETAIL:  Each leaf query of the UNION should return the partition column in the same position and all joins must be on the partition column
2023-11-25 04:52:05.448 UTC [1110044] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:582
2023-11-25 04:52:05.448 UTC [1110044] STATEMENT:  MERGE INTO target_set
	USING (SELECT s1,s2 FROM source_set UNION SELECT s2,s1 FROM source_set) AS foo ON target_set.t1 = foo.s1
	WHEN MATCHED THEN
	        UPDATE SET t2 = t2 + 1;
2023-11-25 04:52:05.448 UTC [1110044] ERROR:  0A000: cannot push down this subquery
2023-11-25 04:52:05.448 UTC [1110044] DETAIL:  Limit clause is currently unsupported when a subquery references a column from another query
2023-11-25 04:52:05.448 UTC [1110044] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 04:52:05.448 UTC [1110044] STATEMENT:  MERGE INTO target_set
	USING (SELECT 2 as s3, source_set.* FROM (SELECT * FROM source_set LIMIT 1) as foo LEFT JOIN source_set USING( s1)) AS foo
	ON target_set.t1 = foo.s1
	WHEN MATCHED THEN UPDATE SET t2 = t2 + 1
	WHEN NOT MATCHED THEN INSERT VALUES(s1, s3);
2023-11-25 04:52:05.448 UTC [1110044] ERROR:  0A000: MERGE command is only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:52:05.448 UTC [1110044] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:52:05.448 UTC [1110044] STATEMENT:  EXPLAIN
	WITH cte_1 AS (DELETE FROM target_json)
	MERGE INTO target_json sda
	USING source_json sdn
	ON sda.id = sdn.id
	WHEN NOT matched THEN
		INSERT (id, z) VALUES (sdn.id, 5);
2023-11-25 04:52:05.449 UTC [1110044] ERROR:  0A000: cannot push down this subquery
2023-11-25 04:52:05.449 UTC [1110044] DETAIL:  could not run distributed query with GROUPING SETS, CUBE, or ROLLUP
2023-11-25 04:52:05.449 UTC [1110044] LOCATION:  DeferErrorIfCannotPushdownSubquery, query_pushdown_planning.c:1048
2023-11-25 04:52:05.449 UTC [1110044] STATEMENT:  MERGE INTO citus_target t
	USING (SELECT count(*), id FROM citus_source GROUP BY GROUPING SETS (id, val)) subq
	ON subq.id = t.id
	WHEN MATCHED AND t.id > 350 THEN
	    UPDATE SET val = t.val || 'Updated'
	WHEN NOT MATCHED THEN
	        INSERT VALUES (subq.id, 99)
	WHEN MATCHED AND t.id < 350 THEN
	        DELETE;
2023-11-25 04:52:05.449 UTC [1110044] ERROR:  0A000: cannot push down this subquery
2023-11-25 04:52:05.449 UTC [1110044] DETAIL:  could not run distributed query with GROUPING SETS, CUBE, or ROLLUP
2023-11-25 04:52:05.449 UTC [1110044] LOCATION:  DeferErrorIfCannotPushdownSubquery, query_pushdown_planning.c:1048
2023-11-25 04:52:05.449 UTC [1110044] STATEMENT:  WITH subq AS
	(
	SELECT count(*), id FROM citus_source GROUP BY GROUPING SETS (id, val)
	)
	MERGE INTO citus_target t
	USING subq
	ON subq.id = t.id
	WHEN MATCHED AND t.id > 350 THEN
	    UPDATE SET val = t.val || 'Updated'
	WHEN NOT MATCHED THEN
	        INSERT VALUES (subq.id, 99)
	WHEN MATCHED AND t.id < 350 THEN
	        DELETE;
2023-11-25 04:52:05.449 UTC [1110044] ERROR:  0A000: cannot perform MERGE INSERT with DEFAULTS
2023-11-25 04:52:05.449 UTC [1110044] LOCATION:  InsertDistributionColumnMatchesSource, merge_planner.c:546
2023-11-25 04:52:05.449 UTC [1110044] STATEMENT:  MERGE INTO citus_target t
	USING citus_source s
	ON t.id = s.id
	WHEN NOT MATCHED THEN
	  INSERT DEFAULT VALUES;
2023-11-25 04:52:05.449 UTC [1110044] ERROR:  0A000: MERGE INSERT must refer a source column for distribution column 
2023-11-25 04:52:05.449 UTC [1110044] LOCATION:  InsertDistributionColumnMatchesSource, merge_planner.c:583
2023-11-25 04:52:05.449 UTC [1110044] STATEMENT:  MERGE INTO citus_target t
	USING citus_source s
	ON t.id = s.id
	WHEN NOT MATCHED THEN
	  INSERT VALUES(10000);
2023-11-25 04:52:05.449 UTC [1110044] ERROR:  0A000: MERGE INSERT must refer a source column for distribution column 
2023-11-25 04:52:05.449 UTC [1110044] LOCATION:  InsertDistributionColumnMatchesSource, merge_planner.c:583
2023-11-25 04:52:05.449 UTC [1110044] STATEMENT:  MERGE INTO citus_target t
	USING citus_source s
	ON t.id = s.id
	WHEN NOT MATCHED THEN
	  INSERT (id) VALUES(1000);
2023-11-25 04:52:05.449 UTC [1110044] ERROR:  0A000: MERGE INSERT must use the source table distribution column value
2023-11-25 04:52:05.449 UTC [1110044] LOCATION:  InsertDistributionColumnMatchesSource, merge_planner.c:575
2023-11-25 04:52:05.449 UTC [1110044] STATEMENT:  MERGE INTO t1 t
	USING s1 s
	ON t.id = s.id
	WHEN NOT MATCHED THEN
	  INSERT (id) VALUES(s.val);
2023-11-25 04:52:05.450 UTC [1110044] ERROR:  0A000: MERGE INSERT must have distribution column as value
2023-11-25 04:52:05.450 UTC [1110044] LOCATION:  InsertDistributionColumnMatchesSource, merge_planner.c:592
2023-11-25 04:52:05.450 UTC [1110044] STATEMENT:  MERGE INTO t1 t
	USING s1 s
	ON t.id = s.id
	WHEN NOT MATCHED THEN
	  INSERT (val) VALUES(s.val);
2023-11-25 04:52:05.450 UTC [1110044] ERROR:  0A000: updating the distribution column is not allowed in MERGE actions
2023-11-25 04:52:05.450 UTC [1110044] LOCATION:  MergeQualAndTargetListFunctionsSupported, merge_planner.c:651
2023-11-25 04:52:05.450 UTC [1110044] STATEMENT:  MERGE INTO target_cj t
	  USING source_cj1 s
	  ON t.tid = s.sid1 AND t.tid = 2
	  WHEN MATCHED THEN
	    UPDATE SET tid = tid + 9, src = src || ' updated by merge'
	  WHEN NOT MATCHED THEN
	    INSERT VALUES (sid1, 'inserted by merge', val1);
2023-11-25 04:52:05.450 UTC [1110044] ERROR:  0A000: cannot execute MERGE on relation "foreign_table"
2023-11-25 04:52:05.450 UTC [1110044] DETAIL:  This operation is not supported for foreign tables.
2023-11-25 04:52:05.450 UTC [1110044] LOCATION:  transformMergeStmt, parse_merge.c:180
2023-11-25 04:52:05.450 UTC [1110044] STATEMENT:  MERGE INTO foreign_table
		USING ft_target ON (foreign_table.id = ft_target.id)
		WHEN MATCHED THEN
			DELETE
		WHEN NOT MATCHED THEN
			INSERT (id, user_val) VALUES (ft_target.id, ft_target.user_val);
2023-11-25 04:52:05.471 UTC [1110044] ERROR:  0A000: MERGE command is not supported with combination of distributed/local tables yet
2023-11-25 04:52:05.471 UTC [1110044] LOCATION:  ErrorIfMergeHasUnsupportedTables, merge_planner.c:456
2023-11-25 04:52:05.471 UTC [1110044] STATEMENT:  MERGE INTO t1
		USING s1 ON (s1.id = t1.val) -- val is not a distribution column
		WHEN MATCHED AND s1.val = 0 THEN
			DELETE
		WHEN MATCHED THEN
			UPDATE SET val = t1.val + 1
		WHEN NOT MATCHED THEN
			INSERT (id, val) VALUES (s1.id, s1.val);
2023-11-25 04:52:05.486 UTC [1110044] ERROR:  0A000: non-IMMUTABLE functions are not yet supported in MERGE sql with distributed tables 
2023-11-25 04:52:05.486 UTC [1110044] LOCATION:  MergeQuerySupported, merge_planner.c:134
2023-11-25 04:52:05.486 UTC [1110044] STATEMENT:  MERGE INTO t1 USING (SELECT * FROM s1 WHERE true) s1 ON
	  t1.id = s1.id AND s1.id = 2
	   WHEN matched THEN
	 UPDATE SET id = s1.id, val = random();
2023-11-25 04:52:05.488 UTC [1110044] ERROR:  0A000: non-IMMUTABLE functions are not yet supported in MERGE sql with distributed tables 
2023-11-25 04:52:05.488 UTC [1110044] LOCATION:  MergeQuerySupported, merge_planner.c:134
2023-11-25 04:52:05.488 UTC [1110044] STATEMENT:  MERGE INTO t1
	USING s1 ON t1.id = s1.id
	WHEN NOT MATCHED THEN
		INSERT VALUES(s1.id, add_s(s1.val, 2));
2023-11-25 04:52:05.488 UTC [1110044] ERROR:  0A000: non-IMMUTABLE functions are not yet supported in MERGE sql with distributed tables 
2023-11-25 04:52:05.488 UTC [1110044] LOCATION:  MergeQuerySupported, merge_planner.c:134
2023-11-25 04:52:05.488 UTC [1110044] STATEMENT:  MERGE INTO t1
	USING s1 ON t1.id = s1.id AND t1.id = 2 AND (merge_when_and_write())
	WHEN MATCHED THEN
	        UPDATE SET val = t1.val + s1.val;
2023-11-25 04:52:05.488 UTC [1110044] ERROR:  0A000: non-IMMUTABLE functions are not yet supported in MERGE sql with distributed tables 
2023-11-25 04:52:05.488 UTC [1110044] LOCATION:  MergeQuerySupported, merge_planner.c:134
2023-11-25 04:52:05.488 UTC [1110044] STATEMENT:  MERGE INTO t1
	USING s1 ON t1.id = s1.id AND t1.id = 2
	WHEN MATCHED AND (merge_when_and_write()) THEN
	        UPDATE SET val = t1.val + s1.val;
2023-11-25 04:52:05.488 UTC [1110044] ERROR:  0A000: MERGE command is only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:52:05.488 UTC [1110044] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:52:05.488 UTC [1110044] STATEMENT:  MERGE INTO t1
		USING (SELECT * FROM s1) sub ON (sub.val = t1.id) -- sub.val is not a distribution column
		WHEN MATCHED AND sub.val = 0 THEN
			DELETE
		WHEN MATCHED THEN
			UPDATE SET val = t1.val + 1
		WHEN NOT MATCHED THEN
			INSERT (id, val) VALUES (sub.id, sub.val);
2023-11-25 04:52:05.488 UTC [1110044] ERROR:  0A000: MERGE command is only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:52:05.488 UTC [1110044] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:52:05.488 UTC [1110044] STATEMENT:  WITH s1_res AS (
		SELECT * FROM s1
	)
	MERGE INTO t1
		USING s1_res ON (s1_res.val = t1.id)
		WHEN MATCHED AND s1_res.val = 0 THEN
			DELETE
		WHEN MATCHED THEN
			UPDATE SET val = t1.val + 1
		WHEN NOT MATCHED THEN
			INSERT (id, val) VALUES (s1_res.id, s1_res.val);
2023-11-25 04:52:05.489 UTC [1110044] ERROR:  0A000: MERGE command is only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:52:05.489 UTC [1110044] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:52:05.489 UTC [1110044] STATEMENT:  WITH s1_res AS (
		SELECT * FROM s1
	)
	MERGE INTO t1
		USING s1_res ON (TRUE)
		WHEN MATCHED AND s1_res.val = 0 THEN
			DELETE
		WHEN MATCHED THEN
			UPDATE SET val = t1.val + 1
		WHEN NOT MATCHED THEN
			INSERT (id, val) VALUES (s1_res.id, s1_res.val);
2023-11-25 04:52:05.489 UTC [1110044] ERROR:  0A000: MERGE command is only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:52:05.489 UTC [1110044] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:52:05.489 UTC [1110044] STATEMENT:  WITH s1_res AS (
	     SELECT * FROM s1
	 )
	 MERGE INTO t1 USING s1_res ON (s1_res.id = t1.val)
	 WHEN MATCHED THEN DELETE
		WHEN NOT MATCHED THEN
			INSERT (id, val) VALUES (s1_res.id, s1_res.val);
2023-11-25 04:52:05.512 UTC [1110044] ERROR:  0A000: MERGE command is not supported on reference tables yet
2023-11-25 04:52:05.512 UTC [1110044] LOCATION:  CheckIfRTETypeIsUnsupported, merge_planner.c:330
2023-11-25 04:52:05.512 UTC [1110044] STATEMENT:  MERGE INTO t1
		USING s1 ON (s1.id = t1.id)
		WHEN MATCHED AND s1.val = 0 THEN
			DELETE
		WHEN MATCHED THEN
			UPDATE SET val = t1.val + 1
		WHEN NOT MATCHED THEN
			INSERT (id, val) VALUES (s1.id, s1.val);
2023-11-25 04:52:05.529 UTC [1110044] ERROR:  0A000: MERGE command is not supported with combination of distributed/local tables yet
2023-11-25 04:52:05.529 UTC [1110044] LOCATION:  ErrorIfMergeHasUnsupportedTables, merge_planner.c:456
2023-11-25 04:52:05.529 UTC [1110044] STATEMENT:  MERGE INTO t1
		USING s1 ON (s1.id = t1.id)
		WHEN MATCHED AND s1.val = 0 THEN
			DELETE
		WHEN MATCHED THEN
			UPDATE SET val = t1.val + 1
		WHEN NOT MATCHED THEN
			INSERT (id, val) VALUES (s1.id, s1.val);
2023-11-25 04:52:05.529 UTC [1110044] ERROR:  0A000: MERGE command is not supported with combination of distributed/local tables yet
2023-11-25 04:52:05.529 UTC [1110044] LOCATION:  ErrorIfMergeHasUnsupportedTables, merge_planner.c:456
2023-11-25 04:52:05.529 UTC [1110044] STATEMENT:  MERGE INTO t1
		USING (SELECT * FROM s1) sub ON (sub.id = t1.id)
		WHEN MATCHED AND sub.val = 0 THEN
			DELETE
		WHEN MATCHED THEN
			UPDATE SET val = t1.val + 1
		WHEN NOT MATCHED THEN
			INSERT (id, val) VALUES (sub.id, sub.val);
2023-11-25 04:52:05.540 UTC [1110044] ERROR:  0A000: MERGE command is not supported with combination of distributed/local tables yet
2023-11-25 04:52:05.540 UTC [1110044] LOCATION:  ErrorIfMergeHasUnsupportedTables, merge_planner.c:456
2023-11-25 04:52:05.540 UTC [1110044] STATEMENT:  MERGE INTO t1
		USING (SELECT s1.id, pg.val FROM s1, pg) sub ON (sub.id = t1.id)
		WHEN MATCHED AND sub.val = 0 THEN
			DELETE
		WHEN MATCHED THEN
			UPDATE SET val = t1.val + 1
		WHEN NOT MATCHED THEN
			INSERT (id, val) VALUES (sub.id, sub.val);
2023-11-25 04:52:05.540 UTC [1110044] ERROR:  0A000: MERGE command is not supported with combination of distributed/local tables yet
2023-11-25 04:52:05.540 UTC [1110044] LOCATION:  ErrorIfMergeHasUnsupportedTables, merge_planner.c:456
2023-11-25 04:52:05.540 UTC [1110044] STATEMENT:  WITH pg_res AS (
		SELECT * FROM pg
	)
	MERGE INTO t1
		USING (SELECT s1.id, pg_res.val FROM s1, pg_res) sub ON (sub.id = t1.id)
		WHEN MATCHED AND sub.val = 0 THEN
			DELETE
		WHEN MATCHED THEN
			UPDATE SET val = t1.val + 1
		WHEN NOT MATCHED THEN
			INSERT (id, val) VALUES (sub.id, sub.val);
2023-11-25 04:52:05.556 UTC [1110044] ERROR:  21000: MERGE command cannot affect row a second time
2023-11-25 04:52:05.556 UTC [1110044] HINT:  Ensure that not more than one source row matches any one target row.
2023-11-25 04:52:05.556 UTC [1110044] LOCATION:  ExecMergeMatched, nodeModifyTable.c:2932
2023-11-25 04:52:05.556 UTC [1110044] STATEMENT:  MERGE INTO t1
		USING s1 ON (s1.id = t1.id)
		WHEN MATCHED AND s1.val = 0 THEN
			DELETE
		WHEN MATCHED THEN
			UPDATE SET val = t1.val + 1
		WHEN NOT MATCHED THEN
			INSERT (id, val) VALUES (s1.id, s1.val);
2023-11-25 04:52:05.556 UTC [1110044] ERROR:  0A000: cannot execute MERGE on relation "mv_source"
2023-11-25 04:52:05.556 UTC [1110044] DETAIL:  This operation is not supported for materialized views.
2023-11-25 04:52:05.556 UTC [1110044] LOCATION:  transformMergeStmt, parse_merge.c:180
2023-11-25 04:52:05.556 UTC [1110044] STATEMENT:  MERGE INTO mv_source
	USING mv_target
	ON mv_source.id = mv_target.id
	WHEN MATCHED THEN
	    DO NOTHING
	WHEN NOT MATCHED THEN
	    INSERT VALUES(mv_source.id, mv_source.val);
2023-11-25 04:52:05.569 UTC [1110044] ERROR:  0A000: For MERGE command, all the distributed tables must be colocated
2023-11-25 04:52:05.569 UTC [1110044] LOCATION:  ErrorIfDistTablesNotColocated, merge_planner.c:277
2023-11-25 04:52:05.569 UTC [1110044] STATEMENT:  MERGE INTO dist_target
	USING dist_source
	ON dist_target.id = dist_source.id
	WHEN MATCHED THEN
	UPDATE SET val = dist_source.val
	WHEN NOT MATCHED THEN
	INSERT VALUES(dist_source.id, dist_source.val);
2023-11-25 04:52:05.573 UTC [1110044] ERROR:  0A000: MERGE command is only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 04:52:05.573 UTC [1110044] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 04:52:05.573 UTC [1110044] STATEMENT:  MERGE INTO dist_target
	USING dist_colocated
	ON dist_target.id = dist_colocated.val -- val is not the distribution column
	WHEN MATCHED THEN
	UPDATE SET val = dist_colocated.val
	WHEN NOT MATCHED THEN
	INSERT VALUES(dist_colocated.id, dist_colocated.val);
2023-11-25 04:52:05.573 UTC [1110044] ERROR:  0A000: For MERGE command, both the source and target must be distributed
2023-11-25 04:52:05.573 UTC [1110044] LOCATION:  ErrorIfDistTablesNotColocated, merge_planner.c:269
2023-11-25 04:52:05.573 UTC [1110044] STATEMENT:  MERGE INTO dist_target
	USING (SELECT 100 id) AS source
	ON dist_target.id = source.id AND dist_target.val = 'const'
	WHEN MATCHED THEN
	UPDATE SET val = 'source'
	WHEN NOT MATCHED THEN
	INSERT VALUES(source.id, 'source');
2023-11-25 04:52:05.586 UTC [1110044] ERROR:  0A000: For MERGE command, all the distributed tables must be colocated, for append/range distribution, colocation is not supported
2023-11-25 04:52:05.586 UTC [1110044] HINT:  Consider using hash distribution instead
2023-11-25 04:52:05.586 UTC [1110044] LOCATION:  CheckIfRTETypeIsUnsupported, merge_planner.c:339
2023-11-25 04:52:05.586 UTC [1110044] STATEMENT:  MERGE INTO dist_target
	USING dist_source
	ON dist_target.id = dist_source.id
	WHEN MATCHED THEN
	UPDATE SET val = dist_source.val
	WHEN NOT MATCHED THEN
	INSERT VALUES(dist_source.id, dist_source.val);
2023-11-25 04:52:05.589 UTC [1110044] ERROR:  0A000: For MERGE command, all the distributed tables must be colocated, for append/range distribution, colocation is not supported
2023-11-25 04:52:05.589 UTC [1110044] HINT:  Consider using hash distribution instead
2023-11-25 04:52:05.589 UTC [1110044] LOCATION:  CheckIfRTETypeIsUnsupported, merge_planner.c:339
2023-11-25 04:52:05.589 UTC [1110044] STATEMENT:  MERGE INTO dist_target
	USING dist_source
	ON dist_target.id = dist_source.id
	WHEN MATCHED THEN
	UPDATE SET val = dist_source.val
	WHEN NOT MATCHED THEN
	INSERT VALUES(dist_source.id, dist_source.val);
2023-11-25 04:52:05.600 UTC [1110044] ERROR:  0A000: For MERGE command, all the distributed tables must be colocated, for append/range distribution, colocation is not supported
2023-11-25 04:52:05.600 UTC [1110044] HINT:  Consider using hash distribution instead
2023-11-25 04:52:05.600 UTC [1110044] LOCATION:  CheckIfRTETypeIsUnsupported, merge_planner.c:339
2023-11-25 04:52:05.600 UTC [1110044] STATEMENT:  MERGE INTO dist_target
	USING dist_source
	ON dist_target.id = dist_source.id
	WHEN MATCHED THEN
	UPDATE SET val = dist_source.val
	WHEN NOT MATCHED THEN
	INSERT VALUES(dist_source.id, dist_source.val);
2023-11-25 04:52:05.606 UTC [1110044] ERROR:  0A000: For MERGE command, all the distributed tables must be colocated, for append/range distribution, colocation is not supported
2023-11-25 04:52:05.606 UTC [1110044] HINT:  Consider using hash distribution instead
2023-11-25 04:52:05.606 UTC [1110044] LOCATION:  CheckIfRTETypeIsUnsupported, merge_planner.c:339
2023-11-25 04:52:05.606 UTC [1110044] STATEMENT:  MERGE INTO dist_target
	USING dist_source
	ON dist_target.id = dist_source.id
	WHEN MATCHED THEN
	UPDATE SET val = dist_source.val
	WHEN NOT MATCHED THEN
	INSERT VALUES(dist_source.id, dist_source.val);
2023-11-25 04:52:06.014 UTC [1110134] ERROR:  42601: syntax error at or near "RANDOMWORD" at character 21
2023-11-25 04:52:06.014 UTC [1110134] LOCATION:  scanner_yyerror, scan.l:1188
2023-11-25 04:52:06.014 UTC [1110134] STATEMENT:  MERGE INTO target t RANDOMWORD
	USING source AS s
	ON t.tid = s.sid
	WHEN MATCHED THEN
		UPDATE SET balance = 0;
2023-11-25 04:52:06.014 UTC [1110134] ERROR:  42601: syntax error at or near "INSERT" at character 75
2023-11-25 04:52:06.014 UTC [1110134] LOCATION:  scanner_yyerror, scan.l:1188
2023-11-25 04:52:06.014 UTC [1110134] STATEMENT:  MERGE INTO target t
	USING source AS s
	ON t.tid = s.sid
	WHEN MATCHED THEN
		INSERT DEFAULT VALUES;
2023-11-25 04:52:06.014 UTC [1110134] ERROR:  42601: syntax error at or near "INTO" at character 86
2023-11-25 04:52:06.014 UTC [1110134] LOCATION:  scanner_yyerror, scan.l:1188
2023-11-25 04:52:06.014 UTC [1110134] STATEMENT:  MERGE INTO target t
	USING source AS s
	ON t.tid = s.sid
	WHEN NOT MATCHED THEN
		INSERT INTO target DEFAULT VALUES;
2023-11-25 04:52:06.014 UTC [1110134] ERROR:  42601: syntax error at or near "," at character 98
2023-11-25 04:52:06.014 UTC [1110134] LOCATION:  scanner_yyerror, scan.l:1188
2023-11-25 04:52:06.014 UTC [1110134] STATEMENT:  MERGE INTO target t
	USING source AS s
	ON t.tid = s.sid
	WHEN NOT MATCHED THEN
		INSERT VALUES (1,1), (2,2);
2023-11-25 04:52:06.014 UTC [1110134] ERROR:  42601: syntax error at or near "SELECT" at character 86
2023-11-25 04:52:06.014 UTC [1110134] LOCATION:  scanner_yyerror, scan.l:1188
2023-11-25 04:52:06.014 UTC [1110134] STATEMENT:  MERGE INTO target t
	USING source AS s
	ON t.tid = s.sid
	WHEN NOT MATCHED THEN
		INSERT SELECT (1, 1);
2023-11-25 04:52:06.014 UTC [1110134] ERROR:  42601: syntax error at or near "UPDATE" at character 79
2023-11-25 04:52:06.014 UTC [1110134] LOCATION:  scanner_yyerror, scan.l:1188
2023-11-25 04:52:06.014 UTC [1110134] STATEMENT:  MERGE INTO target t
	USING source AS s
	ON t.tid = s.sid
	WHEN NOT MATCHED THEN
		UPDATE SET balance = 0;
2023-11-25 04:52:06.015 UTC [1110134] ERROR:  42601: syntax error at or near "target" at character 82
2023-11-25 04:52:06.015 UTC [1110134] LOCATION:  scanner_yyerror, scan.l:1188
2023-11-25 04:52:06.015 UTC [1110134] STATEMENT:  MERGE INTO target t
	USING source AS s
	ON t.tid = s.sid
	WHEN MATCHED THEN
		UPDATE target SET balance = 0;
2023-11-25 04:52:06.015 UTC [1110134] ERROR:  42712: name "target" specified more than once
2023-11-25 04:52:06.015 UTC [1110134] DETAIL:  The name is used both as MERGE target table and data source.
2023-11-25 04:52:06.015 UTC [1110134] LOCATION:  transformMergeStmt, parse_merge.c:206
2023-11-25 04:52:06.015 UTC [1110134] STATEMENT:  MERGE INTO target
	USING target
	ON tid = tid
	WHEN MATCHED THEN DO NOTHING;
2023-11-25 04:52:06.015 UTC [1110134] ERROR:  0A000: MERGE not supported in WITH query at character 6
2023-11-25 04:52:06.015 UTC [1110134] LOCATION:  transformWithClause, parse_cte.c:131
2023-11-25 04:52:06.015 UTC [1110134] STATEMENT:  WITH foo AS (
	  MERGE INTO target USING source ON (true)
	  WHEN MATCHED THEN DELETE
	) SELECT * FROM foo;
2023-11-25 04:52:06.015 UTC [1110134] ERROR:  0A000: MERGE not supported in COPY
2023-11-25 04:52:06.015 UTC [1110134] LOCATION:  DoCopy, copy.c:281
2023-11-25 04:52:06.015 UTC [1110134] STATEMENT:  COPY (
	  MERGE INTO target USING source ON (true)
	  WHEN MATCHED THEN DELETE
	) TO stdout;
2023-11-25 04:52:06.021 UTC [1110134] ERROR:  0A000: cannot execute MERGE on relation "tv"
2023-11-25 04:52:06.021 UTC [1110134] DETAIL:  This operation is not supported for views.
2023-11-25 04:52:06.021 UTC [1110134] LOCATION:  transformMergeStmt, parse_merge.c:180
2023-11-25 04:52:06.021 UTC [1110134] STATEMENT:  MERGE INTO tv t
	USING source s
	ON t.tid = s.sid
	WHEN NOT MATCHED THEN
		INSERT DEFAULT VALUES;
2023-11-25 04:52:06.027 UTC [1110134] ERROR:  0A000: cannot execute MERGE on relation "mv"
2023-11-25 04:52:06.027 UTC [1110134] DETAIL:  This operation is not supported for materialized views.
2023-11-25 04:52:06.027 UTC [1110134] LOCATION:  transformMergeStmt, parse_merge.c:180
2023-11-25 04:52:06.027 UTC [1110134] STATEMENT:  MERGE INTO mv t
	USING source s
	ON t.tid = s.sid
	WHEN NOT MATCHED THEN
		INSERT DEFAULT VALUES;
2023-11-25 04:52:06.027 UTC [1110134] ERROR:  42501: permission denied for table source2
2023-11-25 04:52:06.027 UTC [1110134] LOCATION:  aclcheck_error, aclchk.c:3650
2023-11-25 04:52:06.027 UTC [1110134] STATEMENT:  MERGE INTO target
	USING source2
	ON target.tid = source2.sid
	WHEN MATCHED THEN
		UPDATE SET balance = 0;
2023-11-25 04:52:06.028 UTC [1110134] ERROR:  42501: permission denied for table target
2023-11-25 04:52:06.028 UTC [1110134] LOCATION:  aclcheck_error, aclchk.c:3650
2023-11-25 04:52:06.028 UTC [1110134] STATEMENT:  MERGE INTO target
	USING source2
	ON target.tid = source2.sid
	WHEN MATCHED THEN
		UPDATE SET balance = 0;
2023-11-25 04:52:06.033 UTC [1110134] ERROR:  42501: permission denied for table target2
2023-11-25 04:52:06.033 UTC [1110134] LOCATION:  aclcheck_error, aclchk.c:3650
2023-11-25 04:52:06.033 UTC [1110134] STATEMENT:  MERGE INTO target2
	USING source
	ON target2.tid = source.sid
	WHEN MATCHED THEN
		DELETE;
2023-11-25 04:52:06.033 UTC [1110134] ERROR:  42501: permission denied for table target2
2023-11-25 04:52:06.033 UTC [1110134] LOCATION:  aclcheck_error, aclchk.c:3650
2023-11-25 04:52:06.033 UTC [1110134] STATEMENT:  MERGE INTO target2
	USING source
	ON target2.tid = source.sid
	WHEN NOT MATCHED THEN
		INSERT DEFAULT VALUES;
2023-11-25 04:52:06.034 UTC [1110134] ERROR:  42P01: invalid reference to FROM-clause entry for table "t" at character 55
2023-11-25 04:52:06.034 UTC [1110134] HINT:  There is an entry for table "t", but it cannot be referenced from this part of the query.
2023-11-25 04:52:06.034 UTC [1110134] LOCATION:  errorMissingRTE, parse_relation.c:3597
2023-11-25 04:52:06.034 UTC [1110134] STATEMENT:  MERGE INTO target t
	USING (SELECT * FROM source WHERE t.tid > sid) s
	ON t.tid = s.sid
	WHEN NOT MATCHED THEN
		INSERT DEFAULT VALUES;
2023-11-25 04:52:06.048 UTC [1110134] ERROR:  21000: MERGE command cannot affect row a second time
2023-11-25 04:52:06.048 UTC [1110134] HINT:  Ensure that not more than one source row matches any one target row.
2023-11-25 04:52:06.048 UTC [1110134] LOCATION:  ExecMergeMatched, nodeModifyTable.c:2932
2023-11-25 04:52:06.048 UTC [1110134] STATEMENT:  MERGE INTO target t
	USING source AS s
	ON t.tid = s.sid
	WHEN MATCHED THEN
		UPDATE SET balance = 0;
2023-11-25 04:52:06.049 UTC [1110134] ERROR:  21000: MERGE command cannot affect row a second time
2023-11-25 04:52:06.049 UTC [1110134] HINT:  Ensure that not more than one source row matches any one target row.
2023-11-25 04:52:06.049 UTC [1110134] LOCATION:  ExecMergeMatched, nodeModifyTable.c:2932
2023-11-25 04:52:06.049 UTC [1110134] STATEMENT:  MERGE INTO target t
	USING source AS s
	ON t.tid = s.sid
	WHEN MATCHED THEN
		DELETE;
2023-11-25 04:52:06.050 UTC [1110134] ERROR:  23505: duplicate key value violates unique constraint "targetidx_4001000"
2023-11-25 04:52:06.050 UTC [1110134] DETAIL:  Key (tid)=(4) already exists.
2023-11-25 04:52:06.050 UTC [1110134] LOCATION:  _bt_check_unique, nbtinsert.c:664
2023-11-25 04:52:06.050 UTC [1110134] STATEMENT:  MERGE INTO target t
	USING source AS s
	ON t.tid = s.sid
	WHEN NOT MATCHED THEN
	  INSERT VALUES (4, NULL);
2023-11-25 04:52:06.050 UTC [1110134] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 04:52:06.050 UTC [1110134] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 04:52:06.050 UTC [1110134] STATEMENT:  SELECT * FROM target ORDER BY tid;
2023-11-25 04:52:06.056 UTC [1110134] ERROR:  42P01: invalid reference to FROM-clause entry for table "t" at character 109
2023-11-25 04:52:06.056 UTC [1110134] HINT:  There is an entry for table "t", but it cannot be referenced from this part of the query.
2023-11-25 04:52:06.056 UTC [1110134] LOCATION:  errorMissingRTE, parse_relation.c:3597
2023-11-25 04:52:06.056 UTC [1110134] STATEMENT:  MERGE INTO target t
	USING source AS s
	ON t.tid = s.sid
	WHEN NOT MATCHED THEN
		INSERT (tid, balance) VALUES (t.tid, s.delta);
2023-11-25 04:52:06.056 UTC [1110134] ERROR:  42P01: invalid reference to FROM-clause entry for table "t" at character 109
2023-11-25 04:52:06.056 UTC [1110134] HINT:  There is an entry for table "t", but it cannot be referenced from this part of the query.
2023-11-25 04:52:06.056 UTC [1110134] LOCATION:  errorMissingRTE, parse_relation.c:3597
2023-11-25 04:52:06.056 UTC [1110134] STATEMENT:  MERGE INTO target t
	USING source AS s
	ON (SELECT true)
	WHEN NOT MATCHED THEN
		INSERT (tid, balance) VALUES (t.tid, s.delta);
2023-11-25 04:52:06.056 UTC [1110134] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 04:52:06.056 UTC [1110134] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 04:52:06.056 UTC [1110134] STATEMENT:  SELECT * FROM target ORDER BY tid;
2023-11-25 04:52:06.057 UTC [1110134] ERROR:  42601: unreachable WHEN clause specified after unconditional WHEN clause
2023-11-25 04:52:06.057 UTC [1110134] LOCATION:  transformMergeStmt, parse_merge.c:159
2023-11-25 04:52:06.057 UTC [1110134] STATEMENT:  MERGE INTO target t
	USING source AS s
	ON t.tid = s.sid
	WHEN MATCHED THEN /* Terminal WHEN clause for MATCHED */
		DELETE
	WHEN MATCHED AND s.delta > 0 THEN
		UPDATE SET balance = t.balance - s.delta;
2023-11-25 04:52:06.066 UTC [1110134] ERROR:  42P01: invalid reference to FROM-clause entry for table "t" at character 80
2023-11-25 04:52:06.066 UTC [1110134] HINT:  There is an entry for table "t", but it cannot be referenced from this part of the query.
2023-11-25 04:52:06.066 UTC [1110134] LOCATION:  errorMissingRTE, parse_relation.c:3597
2023-11-25 04:52:06.066 UTC [1110134] STATEMENT:  MERGE INTO wq_target t
	USING wq_source s ON t.tid = s.sid
	WHEN NOT MATCHED AND t.balance = 100 THEN
		INSERT (tid) VALUES (s.sid);
2023-11-25 04:52:06.066 UTC [1110134] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 04:52:06.066 UTC [1110134] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 04:52:06.066 UTC [1110134] STATEMENT:  SELECT * FROM wq_target;
2023-11-25 04:52:06.069 UTC [1110134] ERROR:  42P10: cannot use system column "xmin" in MERGE WHEN condition at character 76
2023-11-25 04:52:06.069 UTC [1110134] LOCATION:  scanNSItemForColumn, parse_relation.c:709
2023-11-25 04:52:06.069 UTC [1110134] STATEMENT:  MERGE INTO wq_target t
	USING wq_source s ON t.tid = s.sid
	WHEN MATCHED AND t.xmin = t.xmax THEN
		UPDATE SET balance = t.balance + s.balance;
2023-11-25 04:52:06.072 UTC [1110134] ERROR:  0A000: non-IMMUTABLE functions are not yet supported in MERGE sql with distributed tables 
2023-11-25 04:52:06.072 UTC [1110134] LOCATION:  MergeQuerySupported, merge_planner.c:134
2023-11-25 04:52:06.072 UTC [1110134] STATEMENT:  MERGE INTO wq_target t
	USING wq_source s ON t.tid = s.sid
	WHEN MATCHED AND (merge_when_and_write()) THEN
		UPDATE SET balance = t.balance + s.balance;
2023-11-25 04:52:06.072 UTC [1110134] ERROR:  0A000: non-IMMUTABLE functions are not yet supported in MERGE sql with distributed tables 
2023-11-25 04:52:06.072 UTC [1110134] LOCATION:  MergeQuerySupported, merge_planner.c:134
2023-11-25 04:52:06.072 UTC [1110134] STATEMENT:  MERGE INTO wq_target t
	USING wq_source s ON t.tid = s.sid AND (merge_when_and_write())
	WHEN MATCHED THEN
		UPDATE SET balance = t.balance + s.balance;
2023-11-25 04:52:06.120 UTC [1110134] ERROR:  42702: column reference "balance" is ambiguous at character 98
2023-11-25 04:52:06.120 UTC [1110134] LOCATION:  colNameToVar, parse_relation.c:898
2023-11-25 04:52:06.120 UTC [1110134] STATEMENT:  MERGE INTO sq_target
	USING v
	ON tid = sid
	WHEN MATCHED AND tid > 2 THEN
	    UPDATE SET balance = balance + delta
	WHEN NOT MATCHED THEN
		INSERT (balance, tid) VALUES (balance + delta, sid)
	WHEN MATCHED AND tid < 2 THEN
		DELETE;
2023-11-25 04:52:06.121 UTC [1110134] ERROR:  42601: syntax error at or near "RETURNING" at character 231
2023-11-25 04:52:06.121 UTC [1110134] LOCATION:  scanner_yyerror, scan.l:1188
2023-11-25 04:52:06.121 UTC [1110134] STATEMENT:  MERGE INTO sq_target t
	USING v
	ON tid = sid
	WHEN MATCHED AND tid > 2 THEN
	    UPDATE SET balance = t.balance + delta
	WHEN NOT MATCHED THEN
		INSERT (balance, tid) VALUES (balance + delta, sid)
	WHEN MATCHED AND tid < 2 THEN
		DELETE
	RETURNING *;
2023-11-25 04:52:06.473 UTC [1094384] LOG:  00000: received SIGHUP, reloading configuration files
2023-11-25 04:52:06.473 UTC [1094384] LOCATION:  SIGHUP_handler, postmaster.c:2769
2023-11-25 04:52:06.474 UTC [1094384] LOG:  00000: parameter "citus.max_cached_conns_per_worker" changed to "0"
2023-11-25 04:52:06.474 UTC [1094384] LOCATION:  ProcessConfigFileInternal, guc-file.l:453
2023-11-25 04:52:06.474 UTC [1094384] LOG:  00000: parameter "citus.distributed_deadlock_detection_factor" changed to "-1"
2023-11-25 04:52:06.474 UTC [1094384] LOCATION:  ProcessConfigFileInternal, guc-file.l:453
2023-11-25 04:52:06.474 UTC [1094384] LOG:  00000: received SIGHUP, reloading configuration files
2023-11-25 04:52:06.474 UTC [1094384] LOCATION:  SIGHUP_handler, postmaster.c:2769
2023-11-25 04:52:06.474 UTC [1094384] LOG:  00000: parameter "citus.recover_2pc_interval" changed to "1ms"
2023-11-25 04:52:06.474 UTC [1094384] LOCATION:  ProcessConfigFileInternal, guc-file.l:453
2023-11-25 04:52:06.575 UTC [1094384] LOG:  00000: received SIGHUP, reloading configuration files
2023-11-25 04:52:06.575 UTC [1094384] LOCATION:  SIGHUP_handler, postmaster.c:2769
2023-11-25 04:52:06.576 UTC [1094384] LOG:  00000: parameter "citus.recover_2pc_interval" changed to "-1"
2023-11-25 04:52:06.576 UTC [1094384] LOCATION:  ProcessConfigFileInternal, guc-file.l:453
2023-11-25 04:52:06.813 UTC [1094384] LOG:  00000: received SIGHUP, reloading configuration files
2023-11-25 04:52:06.813 UTC [1094384] LOCATION:  SIGHUP_handler, postmaster.c:2769
2023-11-25 04:52:06.813 UTC [1094384] LOG:  00000: parameter "citus.distributed_deadlock_detection_factor" removed from configuration file, reset to default
2023-11-25 04:52:06.813 UTC [1094384] LOCATION:  ProcessConfigFileInternal, guc-file.l:388
2023-11-25 04:52:06.813 UTC [1094384] LOG:  00000: parameter "citus.max_cached_conns_per_worker" removed from configuration file, reset to default
2023-11-25 04:52:06.813 UTC [1094384] LOCATION:  ProcessConfigFileInternal, guc-file.l:388
2023-11-25 04:52:06.813 UTC [1094384] LOG:  00000: parameter "citus.recover_2pc_interval" removed from configuration file, reset to default
2023-11-25 04:52:06.813 UTC [1094384] LOCATION:  ProcessConfigFileInternal, guc-file.l:388
2023-11-25 04:52:06.956 UTC [1110322] ERROR:  0A000: cannot complete operation on generated_identities.smallint_identity_column with smallint/int identity column
2023-11-25 04:52:06.956 UTC [1110322] HINT:  Use bigint identity column instead.
2023-11-25 04:52:06.956 UTC [1110322] LOCATION:  ErrorIfTableHasUnsupportedIdentityColumn, table.c:3997
2023-11-25 04:52:06.956 UTC [1110322] STATEMENT:  SELECT create_distributed_table('smallint_identity_column', 'a');
2023-11-25 04:52:07.101 UTC [1110322] ERROR:  0A000: cannot complete operation on generated_identities.smallint_identity_column with smallint/int identity column
2023-11-25 04:52:07.101 UTC [1110322] HINT:  Use bigint identity column instead.
2023-11-25 04:52:07.101 UTC [1110322] LOCATION:  ErrorIfTableHasUnsupportedIdentityColumn, table.c:3997
2023-11-25 04:52:07.101 UTC [1110322] STATEMENT:  SELECT create_distributed_table_concurrently('smallint_identity_column', 'a');
2023-11-25 04:52:07.101 UTC [1110322] ERROR:  0A000: cannot complete operation on a table with identity column
2023-11-25 04:52:07.101 UTC [1110322] LOCATION:  ErrorIfTableHasIdentityColumn, table.c:4026
2023-11-25 04:52:07.101 UTC [1110322] STATEMENT:  SELECT create_reference_table('smallint_identity_column');
2023-11-25 04:52:07.109 UTC [1110322] ERROR:  0A000: cannot complete operation on generated_identities.int_identity_column with smallint/int identity column
2023-11-25 04:52:07.109 UTC [1110322] HINT:  Use bigint identity column instead.
2023-11-25 04:52:07.109 UTC [1110322] LOCATION:  ErrorIfTableHasUnsupportedIdentityColumn, table.c:3997
2023-11-25 04:52:07.109 UTC [1110322] STATEMENT:  SELECT create_distributed_table('int_identity_column', 'a');
2023-11-25 04:52:07.127 UTC [1110322] ERROR:  0A000: cannot complete operation on generated_identities.int_identity_column with smallint/int identity column
2023-11-25 04:52:07.127 UTC [1110322] HINT:  Use bigint identity column instead.
2023-11-25 04:52:07.127 UTC [1110322] LOCATION:  ErrorIfTableHasUnsupportedIdentityColumn, table.c:3997
2023-11-25 04:52:07.127 UTC [1110322] STATEMENT:  SELECT create_distributed_table_concurrently('int_identity_column', 'a');
2023-11-25 04:52:07.127 UTC [1110322] ERROR:  0A000: cannot complete operation on a table with identity column
2023-11-25 04:52:07.127 UTC [1110322] LOCATION:  ErrorIfTableHasIdentityColumn, table.c:4026
2023-11-25 04:52:07.127 UTC [1110322] STATEMENT:  SELECT create_reference_table('int_identity_column');
2023-11-25 04:52:07.211 UTC [1110369] ERROR:  0A000: cannot complete operation on a table with identity column
2023-11-25 04:52:07.211 UTC [1110369] LOCATION:  ErrorIfTableHasIdentityColumn, table.c:4026
2023-11-25 04:52:07.211 UTC [1110369] STATEMENT:  SELECT alter_distributed_table('bigint_identity_column', 'b');
2023-11-25 04:52:07.211 UTC [1110369] ERROR:  0A000: cannot complete operation on a table with identity column
2023-11-25 04:52:07.211 UTC [1110369] LOCATION:  ErrorIfTableHasIdentityColumn, table.c:4026
2023-11-25 04:52:07.211 UTC [1110369] STATEMENT:  SELECT undistribute_table('bigint_identity_column');
2023-11-25 04:52:07.323 UTC [1110386] ERROR:  0A000: alter table command is currently unsupported
2023-11-25 04:52:07.323 UTC [1110386] DETAIL:  Only ADD|DROP COLUMN, SET|DROP NOT NULL, SET|DROP DEFAULT, ADD|DROP|VALIDATE CONSTRAINT, SET (), RESET (), ENABLE|DISABLE|NO FORCE|FORCE ROW LEVEL SECURITY, ATTACH|DETACH PARTITION and TYPE subcommands are supported.
2023-11-25 04:52:07.323 UTC [1110386] LOCATION:  ErrorIfUnsupportedAlterTableStmt, table.c:3506
2023-11-25 04:52:07.323 UTC [1110386] STATEMENT:  ALTER TABLE partitioned_table ALTER COLUMN g ADD GENERATED ALWAYS AS IDENTITY;
2023-11-25 04:52:07.323 UTC [1110386] ERROR:  XX000: cannot execute ALTER COLUMN command involving identity column
2023-11-25 04:52:07.323 UTC [1110386] LOCATION:  ErrorIfUnsupportedAlterTableStmt, table.c:3306
2023-11-25 04:52:07.323 UTC [1110386] STATEMENT:  ALTER TABLE partitioned_table ALTER COLUMN b TYPE int;
2023-11-25 04:52:07.404 UTC [1110401] ERROR:  42501: must be owner of table color
2023-11-25 04:52:07.404 UTC [1110401] LOCATION:  aclcheck_error, aclchk.c:3788
2023-11-25 04:52:07.404 UTC [1110401] STATEMENT:  SELECT create_distributed_table('color', 'color_id');
2023-11-25 04:52:07.433 UTC [1110401] LOG:  00000: performing non-blocking create_distributed_table_concurrently 
2023-11-25 04:52:07.433 UTC [1110401] LOCATION:  SplitShard, shard_split.c:519
2023-11-25 04:52:07.433 UTC [1110401] STATEMENT:  SELECT create_distributed_table_concurrently('color', 'color_id');
2023-11-25 04:52:07.437 UTC [1110401] LOG:  00000: creating child shards for create_distributed_table_concurrently
2023-11-25 04:52:07.437 UTC [1110401] LOCATION:  NonBlockingShardSplit, shard_split.c:1430
2023-11-25 04:52:07.437 UTC [1110401] STATEMENT:  SELECT create_distributed_table_concurrently('color', 'color_id');
2023-11-25 04:52:07.462 UTC [1110401] LOG:  00000: creating replication artifacts (publications, replication slots, subscriptions for create_distributed_table_concurrently
2023-11-25 04:52:07.462 UTC [1110401] LOCATION:  NonBlockingShardSplit, shard_split.c:1462
2023-11-25 04:52:07.462 UTC [1110401] STATEMENT:  SELECT create_distributed_table_concurrently('color', 'color_id');
2023-11-25 04:52:07.473 UTC [1110411] LOG:  00000: Initializing CDC decoder
2023-11-25 04:52:07.473 UTC [1110411] LOCATION:  _PG_output_plugin_init, cdc_decoder.c:82
2023-11-25 04:52:07.473 UTC [1110411] STATEMENT:  CREATE_REPLICATION_SLOT citus_shard_split_slot_16_10_24 LOGICAL citus EXPORT_SNAPSHOT;
2023-11-25 04:52:07.473 UTC [1110411] LOG:  00000: logical decoding found consistent point at 0/7C615C8
2023-11-25 04:52:07.473 UTC [1110411] DETAIL:  There are no running transactions.
2023-11-25 04:52:07.473 UTC [1110411] LOCATION:  SnapBuildFindSnapshot, snapbuild.c:1366
2023-11-25 04:52:07.473 UTC [1110411] STATEMENT:  CREATE_REPLICATION_SLOT citus_shard_split_slot_16_10_24 LOGICAL citus EXPORT_SNAPSHOT;
2023-11-25 04:52:07.473 UTC [1110411] LOG:  00000: exported logical decoding snapshot: "00000007-00000821-1" with 0 transaction IDs
2023-11-25 04:52:07.473 UTC [1110411] LOCATION:  SnapBuildExportSnapshot, snapbuild.c:687
2023-11-25 04:52:07.473 UTC [1110411] STATEMENT:  CREATE_REPLICATION_SLOT citus_shard_split_slot_16_10_24 LOGICAL citus EXPORT_SNAPSHOT;
2023-11-25 04:52:07.481 UTC [1110410] LOG:  00000: Initializing CDC decoder
2023-11-25 04:52:07.481 UTC [1110410] LOCATION:  _PG_output_plugin_init, cdc_decoder.c:82
2023-11-25 04:52:07.481 UTC [1110410] STATEMENT:  SELECT pg_catalog.pg_copy_logical_replication_slot('citus_shard_split_slot_16_10_24', 'citus_shard_split_slot_34_10_24')
2023-11-25 04:52:07.503 UTC [1110401] LOG:  00000: performing copy for create_distributed_table_concurrently
2023-11-25 04:52:07.503 UTC [1110401] LOCATION:  NonBlockingShardSplit, shard_split.c:1534
2023-11-25 04:52:07.503 UTC [1110401] STATEMENT:  SELECT create_distributed_table_concurrently('color', 'color_id');
2023-11-25 04:52:07.503 UTC [1110414] LOG:  00000: performing copy from shard generated_identities.color_363176 to [generated_identities.color_363177 (nodeId: 16), generated_identities.color_363178 (nodeId: 34), generated_identities.color_363179 (nodeId: 16), generated_identities.color_363180 (nodeId: 34)]
2023-11-25 04:52:07.503 UTC [1110414] LOCATION:  worker_split_copy, worker_split_copy_udf.c:107
2023-11-25 04:52:07.503 UTC [1110414] STATEMENT:  SELECT pg_catalog.worker_split_copy(363176, 'color_id', ARRAY[ROW(363177, -2147483648, -1073741825, 16)::pg_catalog.split_copy_info,ROW(363178, -1073741824, -1, 34)::pg_catalog.split_copy_info,ROW(363179, 0, 1073741823, 16)::pg_catalog.split_copy_info,ROW(363180, 1073741824, 2147483647, 34)::pg_catalog.split_copy_info]);
2023-11-25 04:52:07.504 UTC [1110401] LOG:  00000: replicating changes for create_distributed_table_concurrently
2023-11-25 04:52:07.504 UTC [1110401] LOCATION:  NonBlockingShardSplit, shard_split.c:1541
2023-11-25 04:52:07.504 UTC [1110401] STATEMENT:  SELECT create_distributed_table_concurrently('color', 'color_id');
2023-11-25 04:52:07.510 UTC [1110419] LOG:  00000: Initializing CDC decoder
2023-11-25 04:52:07.510 UTC [1110419] LOCATION:  _PG_output_plugin_init, cdc_decoder.c:82
2023-11-25 04:52:07.510 UTC [1110419] STATEMENT:  START_REPLICATION SLOT "citus_shard_split_slot_34_10_24" LOGICAL 0/0 (proto_version '3', publication_names '"citus_shard_split_publication_34_10_24"', binary 'true')
2023-11-25 04:52:07.510 UTC [1110420] LOG:  00000: Initializing CDC decoder
2023-11-25 04:52:07.510 UTC [1110420] LOCATION:  _PG_output_plugin_init, cdc_decoder.c:82
2023-11-25 04:52:07.510 UTC [1110420] STATEMENT:  START_REPLICATION SLOT "citus_shard_split_slot_16_10_24" LOGICAL 0/0 (proto_version '3', publication_names '"citus_shard_split_publication_16_10_24"', binary 'true')
2023-11-25 04:52:07.511 UTC [1110420] LOG:  00000: starting logical decoding for slot "citus_shard_split_slot_16_10_24"
2023-11-25 04:52:07.511 UTC [1110420] DETAIL:  Streaming transactions committing after 0/7C61600, reading WAL from 0/7C615C8.
2023-11-25 04:52:07.511 UTC [1110420] LOCATION:  CreateDecodingContext, logical.c:569
2023-11-25 04:52:07.511 UTC [1110420] STATEMENT:  START_REPLICATION SLOT "citus_shard_split_slot_16_10_24" LOGICAL 0/0 (proto_version '3', publication_names '"citus_shard_split_publication_16_10_24"', binary 'true')
2023-11-25 04:52:07.511 UTC [1110419] LOG:  00000: starting logical decoding for slot "citus_shard_split_slot_34_10_24"
2023-11-25 04:52:07.511 UTC [1110419] DETAIL:  Streaming transactions committing after 0/7C61600, reading WAL from 0/7C615C8.
2023-11-25 04:52:07.511 UTC [1110419] LOCATION:  CreateDecodingContext, logical.c:569
2023-11-25 04:52:07.511 UTC [1110419] STATEMENT:  START_REPLICATION SLOT "citus_shard_split_slot_34_10_24" LOGICAL 0/0 (proto_version '3', publication_names '"citus_shard_split_publication_34_10_24"', binary 'true')
2023-11-25 04:52:07.511 UTC [1110420] LOG:  00000: logical decoding found consistent point at 0/7C615C8
2023-11-25 04:52:07.511 UTC [1110420] DETAIL:  There are no running transactions.
2023-11-25 04:52:07.511 UTC [1110420] LOCATION:  SnapBuildFindSnapshot, snapbuild.c:1366
2023-11-25 04:52:07.511 UTC [1110420] STATEMENT:  START_REPLICATION SLOT "citus_shard_split_slot_16_10_24" LOGICAL 0/0 (proto_version '3', publication_names '"citus_shard_split_publication_16_10_24"', binary 'true')
2023-11-25 04:52:07.511 UTC [1110419] LOG:  00000: logical decoding found consistent point at 0/7C615C8
2023-11-25 04:52:07.511 UTC [1110419] DETAIL:  There are no running transactions.
2023-11-25 04:52:07.511 UTC [1110419] LOCATION:  SnapBuildFindSnapshot, snapbuild.c:1366
2023-11-25 04:52:07.511 UTC [1110419] STATEMENT:  START_REPLICATION SLOT "citus_shard_split_slot_34_10_24" LOGICAL 0/0 (proto_version '3', publication_names '"citus_shard_split_publication_34_10_24"', binary 'true')
2023-11-25 04:52:08.507 UTC [1110401] LOG:  00000: The LSN of the target subscriptions on node localhost:57638 have caught up with the source LSN 
2023-11-25 04:52:08.507 UTC [1110401] LOCATION:  WaitForGroupedLogicalRepTargetsToCatchUp, multi_logical_replication.c:1842
2023-11-25 04:52:08.507 UTC [1110401] STATEMENT:  SELECT create_distributed_table_concurrently('color', 'color_id');
2023-11-25 04:52:08.508 UTC [1110401] LOG:  00000: The LSN of the target subscriptions on node localhost:57637 have caught up with the source LSN 
2023-11-25 04:52:08.508 UTC [1110401] LOCATION:  WaitForGroupedLogicalRepTargetsToCatchUp, multi_logical_replication.c:1842
2023-11-25 04:52:08.508 UTC [1110401] STATEMENT:  SELECT create_distributed_table_concurrently('color', 'color_id');
2023-11-25 04:52:08.532 UTC [1110401] LOG:  00000: The LSN of the target subscriptions on node localhost:57638 have caught up with the source LSN 
2023-11-25 04:52:08.532 UTC [1110401] LOCATION:  WaitForGroupedLogicalRepTargetsToCatchUp, multi_logical_replication.c:1842
2023-11-25 04:52:08.532 UTC [1110401] STATEMENT:  SELECT create_distributed_table_concurrently('color', 'color_id');
2023-11-25 04:52:08.533 UTC [1110401] LOG:  00000: The LSN of the target subscriptions on node localhost:57637 have caught up with the source LSN 
2023-11-25 04:52:08.533 UTC [1110401] LOCATION:  WaitForGroupedLogicalRepTargetsToCatchUp, multi_logical_replication.c:1842
2023-11-25 04:52:08.533 UTC [1110401] STATEMENT:  SELECT create_distributed_table_concurrently('color', 'color_id');
2023-11-25 04:52:08.533 UTC [1110401] LOG:  00000: The LSN of the target subscriptions on node localhost:57638 have caught up with the source LSN 
2023-11-25 04:52:08.533 UTC [1110401] LOCATION:  WaitForGroupedLogicalRepTargetsToCatchUp, multi_logical_replication.c:1842
2023-11-25 04:52:08.533 UTC [1110401] STATEMENT:  SELECT create_distributed_table_concurrently('color', 'color_id');
2023-11-25 04:52:08.534 UTC [1110401] LOG:  00000: The LSN of the target subscriptions on node localhost:57637 have caught up with the source LSN 
2023-11-25 04:52:08.534 UTC [1110401] LOCATION:  WaitForGroupedLogicalRepTargetsToCatchUp, multi_logical_replication.c:1842
2023-11-25 04:52:08.534 UTC [1110401] STATEMENT:  SELECT create_distributed_table_concurrently('color', 'color_id');
2023-11-25 04:52:08.534 UTC [1110401] LOG:  00000: marking deferred cleanup of source shard(s) for create_distributed_table_concurrently
2023-11-25 04:52:08.534 UTC [1110401] LOCATION:  NonBlockingShardSplit, shard_split.c:1560
2023-11-25 04:52:08.534 UTC [1110401] STATEMENT:  SELECT create_distributed_table_concurrently('color', 'color_id');
2023-11-25 04:52:08.536 UTC [1110401] LOG:  00000: creating foreign key constraints (if any) for create_distributed_table_concurrently
2023-11-25 04:52:08.536 UTC [1110401] LOCATION:  NonBlockingShardSplit, shard_split.c:1610
2023-11-25 04:52:08.536 UTC [1110401] STATEMENT:  SELECT create_distributed_table_concurrently('color', 'color_id');
2023-11-25 04:52:08.662 UTC [1110434] ERROR:  0A000: cannot execute ADD COLUMN commands involving identity columns when metadata is synchronized to workers
2023-11-25 04:52:08.662 UTC [1110434] LOCATION:  ErrorIfUnsupportedAlterTableStmt, table.c:3182
2023-11-25 04:52:08.662 UTC [1110434] STATEMENT:  ALTER TABLE color ADD COLUMN color_id BIGINT GENERATED ALWAYS AS IDENTITY;
2023-11-25 04:52:08.663 UTC [1110434] ERROR:  XX000: Altering a distributed sequence is currently not supported.
2023-11-25 04:52:08.663 UTC [1110434] LOCATION:  PreprocessAlterSequenceStmt, sequence.c:464
2023-11-25 04:52:08.663 UTC [1110434] STATEMENT:  ALTER SEQUENCE color_color_id_seq RESTART WITH 1000;
2023-11-25 04:52:08.663 UTC [1110434] ERROR:  428C9: cannot insert a non-DEFAULT value into column "color_id"
2023-11-25 04:52:08.663 UTC [1110434] DETAIL:  Column "color_id" is an identity column defined as GENERATED ALWAYS.
2023-11-25 04:52:08.663 UTC [1110434] HINT:  Use OVERRIDING SYSTEM VALUE to override.
2023-11-25 04:52:08.663 UTC [1110434] LOCATION:  rewriteTargetListIU, rewriteHandler.c:884
2023-11-25 04:52:08.663 UTC [1110434] STATEMENT:  INSERT INTO color(color_id, color_name) VALUES (1, 'Red');
2023-11-25 04:52:08.663 UTC [1110434] ERROR:  428C9: cannot insert a non-DEFAULT value into column "color_id"
2023-11-25 04:52:08.663 UTC [1110434] DETAIL:  Column "color_id" is an identity column defined as GENERATED ALWAYS.
2023-11-25 04:52:08.663 UTC [1110434] HINT:  Use OVERRIDING SYSTEM VALUE to override.
2023-11-25 04:52:08.663 UTC [1110434] LOCATION:  rewriteTargetListIU, rewriteHandler.c:884
2023-11-25 04:52:08.663 UTC [1110434] STATEMENT:  INSERT INTO color(color_id, color_name) VALUES (NULL, 'Red');
2023-11-25 04:52:08.668 UTC [1110434] ERROR:  23505: duplicate key value violates unique constraint "color_color_id_key_12400000"
2023-11-25 04:52:08.668 UTC [1110434] DETAIL:  Key (color_id)=(1) already exists.
2023-11-25 04:52:08.668 UTC [1110434] CONTEXT:  while executing command on localhost:57637
2023-11-25 04:52:08.668 UTC [1110434] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 04:52:08.668 UTC [1110434] STATEMENT:  INSERT INTO color(color_id, color_name) OVERRIDING SYSTEM VALUE VALUES (1, 'Red');
2023-11-25 04:52:08.669 UTC [1110434] ERROR:  428C9: column "color_id" can only be updated to DEFAULT
2023-11-25 04:52:08.669 UTC [1110434] DETAIL:  Column "color_id" is an identity column defined as GENERATED ALWAYS.
2023-11-25 04:52:08.669 UTC [1110434] LOCATION:  rewriteTargetListIU, rewriteHandler.c:950
2023-11-25 04:52:08.669 UTC [1110434] STATEMENT:  UPDATE color SET color_id = NULL;
2023-11-25 04:52:08.669 UTC [1110434] ERROR:  428C9: column "color_id" can only be updated to DEFAULT
2023-11-25 04:52:08.669 UTC [1110434] DETAIL:  Column "color_id" is an identity column defined as GENERATED ALWAYS.
2023-11-25 04:52:08.669 UTC [1110434] LOCATION:  rewriteTargetListIU, rewriteHandler.c:950
2023-11-25 04:52:08.669 UTC [1110434] STATEMENT:  UPDATE color SET color_id = 1;
2023-11-25 04:52:08.913 UTC [1110457] LOG:  00000: starting maintenance daemon on database 33270 user 10
2023-11-25 04:52:08.913 UTC [1110457] CONTEXT:  Citus maintenance daemon for database 33270 user 10
2023-11-25 04:52:08.913 UTC [1110457] LOCATION:  CitusMaintenanceDaemonMain, maintenanced.c:373
2023-11-25 04:52:09.035 UTC [1094385] LOG:  00000: checkpoint starting: immediate force wait
2023-11-25 04:52:09.035 UTC [1094385] LOCATION:  LogCheckpointStart, xlog.c:6089
2023-11-25 04:52:09.057 UTC [1094385] LOG:  00000: checkpoint complete: wrote 2553 buffers (15.6%); 0 WAL file(s) added, 0 removed, 4 recycled; write=0.011 s, sync=0.001 s, total=0.023 s; sync files=0, longest=0.000 s, average=0.000 s; distance=61702 kB, estimate=61702 kB
2023-11-25 04:52:09.057 UTC [1094385] LOCATION:  LogCheckpointEnd, xlog.c:6170
2023-11-25 04:52:09.065 UTC [1094385] LOG:  00000: checkpoint starting: immediate force wait
2023-11-25 04:52:09.065 UTC [1094385] LOCATION:  LogCheckpointStart, xlog.c:6089
2023-11-25 04:52:09.066 UTC [1094385] LOG:  00000: checkpoint complete: wrote 2 buffers (0.0%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0.001 s, sync=0.001 s, total=0.001 s; sync files=0, longest=0.000 s, average=0.000 s; distance=1 kB, estimate=55532 kB
2023-11-25 04:52:09.066 UTC [1094385] LOCATION:  LogCheckpointEnd, xlog.c:6170
2023-11-25 04:52:10.072 UTC [1094384] LOG:  00000: received fast shutdown request
2023-11-25 04:52:10.072 UTC [1094384] LOCATION:  pmdie, postmaster.c:2904
2023-11-25 04:52:10.072 UTC [1094384] LOG:  00000: aborting any active transactions
2023-11-25 04:52:10.072 UTC [1094384] LOCATION:  pmdie, postmaster.c:2922
2023-11-25 04:52:10.073 UTC [1094384] LOG:  00000: background worker "logical replication launcher" (PID 1094390) exited with exit code 1
2023-11-25 04:52:10.073 UTC [1094384] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 04:52:10.074 UTC [1094385] LOG:  00000: shutting down
2023-11-25 04:52:10.074 UTC [1094385] LOCATION:  ShutdownXLOG, xlog.c:6039
2023-11-25 04:52:10.074 UTC [1094385] LOG:  00000: checkpoint starting: shutdown immediate
2023-11-25 04:52:10.074 UTC [1094385] LOCATION:  LogCheckpointStart, xlog.c:6089
2023-11-25 04:52:10.074 UTC [1094385] LOG:  00000: checkpoint complete: wrote 11 buffers (0.1%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0.001 s, sync=0.001 s, total=0.001 s; sync files=0, longest=0.000 s, average=0.000 s; distance=43 kB, estimate=49983 kB
2023-11-25 04:52:10.074 UTC [1094385] LOCATION:  LogCheckpointEnd, xlog.c:6170
2023-11-25 04:52:10.086 UTC [1094384] LOG:  00000: database system is shut down
2023-11-25 04:52:10.086 UTC [1094384] LOCATION:  UnlinkLockFiles, miscinit.c:977
