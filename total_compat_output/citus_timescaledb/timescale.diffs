diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/alter.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/alter.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/alter.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/alter.out	2023-11-25 05:27:03.397140770 +0000
@@ -1,793 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
--- Set this variable to avoid using a hard-coded path each time query
--- results are compared
-\set QUERY_RESULT_TEST_EQUAL_RELPATH 'include/query_result_test_equal.sql'
--- DROP a table's column before making it a hypertable
-CREATE TABLE alter_before(id serial, time timestamp, temp float, colorid integer, notes text, notes_2 text);
-ALTER TABLE alter_before DROP COLUMN id;
-ALTER TABLE alter_before ALTER COLUMN temp SET (n_distinct = 10);
-ALTER TABLE alter_before ALTER COLUMN colorid SET (n_distinct = 11);
-ALTER TABLE alter_before ALTER COLUMN colorid RESET (n_distinct);
-ALTER TABLE alter_before ALTER COLUMN temp SET STATISTICS 100;
-ALTER TABLE alter_before ALTER COLUMN notes SET STORAGE EXTERNAL;
-SELECT create_hypertable('alter_before', 'time', chunk_time_interval => 2628000000000);
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-     create_hypertable     
----------------------------
- (1,public,alter_before,t)
-(1 row)
-
-INSERT INTO alter_before VALUES ('2017-03-22T09:18:22', 23.5, 1);
-SELECT * FROM alter_before;
-           time           | temp | colorid | notes | notes_2 
---------------------------+------+---------+-------+---------
- Wed Mar 22 09:18:22 2017 | 23.5 |       1 |       | 
-(1 row)
-
--- Show that deleted column is marked as dropped and that attnums are
--- now different for the root table and the chunk
-SELECT c.relname, a.attname, a.attnum, a.attoptions, a.attstattarget, a.attstorage FROM pg_attribute a, pg_class c
-WHERE a.attrelid = c.oid
-AND (c.relname LIKE '_hyper_1%_chunk' OR c.relname = 'alter_before')
-AND a.attnum > 0
-ORDER BY c.relname, a.attnum;
-     relname      |           attname            | attnum |   attoptions    | attstattarget | attstorage 
-------------------+------------------------------+--------+-----------------+---------------+------------
- _hyper_1_1_chunk | time                         |      1 |                 |            -1 | p
- _hyper_1_1_chunk | temp                         |      2 | {n_distinct=10} |           100 | p
- _hyper_1_1_chunk | colorid                      |      3 |                 |            -1 | p
- _hyper_1_1_chunk | notes                        |      4 |                 |            -1 | e
- _hyper_1_1_chunk | notes_2                      |      5 |                 |            -1 | x
- alter_before     | ........pg.dropped.1........ |      1 |                 |             0 | p
- alter_before     | time                         |      2 |                 |            -1 | p
- alter_before     | temp                         |      3 | {n_distinct=10} |           100 | p
- alter_before     | colorid                      |      4 |                 |            -1 | p
- alter_before     | notes                        |      5 |                 |            -1 | e
- alter_before     | notes_2                      |      6 |                 |            -1 | x
-(11 rows)
-
--- DROP a table's column after making it a hypertable and having data
-CREATE TABLE alter_after(id serial, time timestamp, temp float, colorid integer, notes text, notes_2 text);
-SELECT create_hypertable('alter_after', 'time', chunk_time_interval => 2628000000000);
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-    create_hypertable     
---------------------------
- (2,public,alter_after,t)
-(1 row)
-
--- Create first chunk
-INSERT INTO alter_after (time, temp, colorid) VALUES ('2017-03-22T09:18:22', 23.5, 1);
-ALTER TABLE alter_after DROP COLUMN id;
-ALTER TABLE alter_after ALTER COLUMN temp SET (n_distinct = 10);
-ALTER TABLE alter_after ALTER COLUMN colorid SET (n_distinct = 11);
-ALTER TABLE alter_after ALTER COLUMN colorid RESET (n_distinct);
-ALTER TABLE alter_after ALTER COLUMN colorid SET STATISTICS 101;
-ALTER TABLE alter_after ALTER COLUMN notes_2 SET STORAGE EXTERNAL;
--- Creating new chunks after dropping a column should work just fine
-INSERT INTO alter_after VALUES ('2017-03-22T09:18:23', 21.5, 1),
-                               ('2017-05-22T09:18:22', 36.2, 2),
-                               ('2017-05-22T09:18:23', 15.2, 2);
--- Make sure tuple conversion also works with COPY
-\COPY alter_after FROM 'data/alter.tsv' NULL AS '';
--- Data should look OK
-SELECT * FROM alter_after;
-           time           | temp | colorid | notes | notes_2 
---------------------------+------+---------+-------+---------
- Wed Mar 22 09:18:22 2017 | 23.5 |       1 |       | 
- Wed Mar 22 09:18:23 2017 | 21.5 |       1 |       | 
- Mon May 22 09:18:22 2017 | 36.2 |       2 |       | 
- Mon May 22 09:18:23 2017 | 15.2 |       2 |       | 
- Tue Aug 22 09:19:22 2017 | 21.4 |       3 | nr1   | n2r1
- Wed Aug 23 09:20:17 2017 | 31.5 |       2 | nr2   | n2r2
-(6 rows)
-
--- Show that attnums are different for chunks created after DROP
--- column
-SELECT c.relname, a.attname, a.attnum FROM pg_attribute a, pg_class c
-WHERE a.attrelid = c.oid
-AND (c.relname LIKE '_hyper_2%_chunk' OR c.relname = 'alter_after')
-AND a.attnum > 0
-ORDER BY c.relname, a.attnum;
-     relname      |           attname            | attnum 
-------------------+------------------------------+--------
- _hyper_2_2_chunk | ........pg.dropped.1........ |      1
- _hyper_2_2_chunk | time                         |      2
- _hyper_2_2_chunk | temp                         |      3
- _hyper_2_2_chunk | colorid                      |      4
- _hyper_2_2_chunk | notes                        |      5
- _hyper_2_2_chunk | notes_2                      |      6
- _hyper_2_3_chunk | time                         |      1
- _hyper_2_3_chunk | temp                         |      2
- _hyper_2_3_chunk | colorid                      |      3
- _hyper_2_3_chunk | notes                        |      4
- _hyper_2_3_chunk | notes_2                      |      5
- _hyper_2_4_chunk | time                         |      1
- _hyper_2_4_chunk | temp                         |      2
- _hyper_2_4_chunk | colorid                      |      3
- _hyper_2_4_chunk | notes                        |      4
- _hyper_2_4_chunk | notes_2                      |      5
- alter_after      | ........pg.dropped.1........ |      1
- alter_after      | time                         |      2
- alter_after      | temp                         |      3
- alter_after      | colorid                      |      4
- alter_after      | notes                        |      5
- alter_after      | notes_2                      |      6
-(22 rows)
-
--- Add an ID column again
-ALTER TABLE alter_after ADD COLUMN id serial;
-INSERT INTO alter_after (time, temp, colorid) VALUES ('2017-08-22T09:19:14', 12.5, 3);
---test thing that we are allowed to do on chunks
-ALTER TABLE  _timescaledb_internal._hyper_2_3_chunk ALTER COLUMN temp RESET (n_distinct);
-ALTER TABLE  _timescaledb_internal._hyper_2_4_chunk ALTER COLUMN temp SET (n_distinct = 20);
-ALTER TABLE  _timescaledb_internal._hyper_2_4_chunk ALTER COLUMN temp SET STATISTICS 201;
-ALTER TABLE  _timescaledb_internal._hyper_2_4_chunk ALTER COLUMN notes SET STORAGE EXTERNAL;
-SELECT c.relname, a.attname, a.attnum, a.attoptions, a.attstattarget, a.attstorage FROM pg_attribute a, pg_class c
-WHERE a.attrelid = c.oid
-AND (c.relname LIKE '_hyper_2%_chunk' OR c.relname = 'alter_after')
-AND a.attnum > 0
-ORDER BY c.relname, a.attnum;
-     relname      |           attname            | attnum |   attoptions    | attstattarget | attstorage 
-------------------+------------------------------+--------+-----------------+---------------+------------
- _hyper_2_2_chunk | ........pg.dropped.1........ |      1 |                 |             0 | p
- _hyper_2_2_chunk | time                         |      2 |                 |            -1 | p
- _hyper_2_2_chunk | temp                         |      3 | {n_distinct=10} |            -1 | p
- _hyper_2_2_chunk | colorid                      |      4 |                 |           101 | p
- _hyper_2_2_chunk | notes                        |      5 |                 |            -1 | x
- _hyper_2_2_chunk | notes_2                      |      6 |                 |            -1 | e
- _hyper_2_2_chunk | id                           |      7 |                 |            -1 | p
- _hyper_2_3_chunk | time                         |      1 |                 |            -1 | p
- _hyper_2_3_chunk | temp                         |      2 |                 |            -1 | p
- _hyper_2_3_chunk | colorid                      |      3 |                 |           101 | p
- _hyper_2_3_chunk | notes                        |      4 |                 |            -1 | x
- _hyper_2_3_chunk | notes_2                      |      5 |                 |            -1 | e
- _hyper_2_3_chunk | id                           |      6 |                 |            -1 | p
- _hyper_2_4_chunk | time                         |      1 |                 |            -1 | p
- _hyper_2_4_chunk | temp                         |      2 | {n_distinct=20} |           201 | p
- _hyper_2_4_chunk | colorid                      |      3 |                 |           101 | p
- _hyper_2_4_chunk | notes                        |      4 |                 |            -1 | e
- _hyper_2_4_chunk | notes_2                      |      5 |                 |            -1 | e
- _hyper_2_4_chunk | id                           |      6 |                 |            -1 | p
- alter_after      | ........pg.dropped.1........ |      1 |                 |             0 | p
- alter_after      | time                         |      2 |                 |            -1 | p
- alter_after      | temp                         |      3 | {n_distinct=10} |            -1 | p
- alter_after      | colorid                      |      4 |                 |           101 | p
- alter_after      | notes                        |      5 |                 |            -1 | x
- alter_after      | notes_2                      |      6 |                 |            -1 | e
- alter_after      | id                           |      7 |                 |            -1 | p
-(26 rows)
-
-SELECT * FROM alter_after;
-           time           | temp | colorid | notes | notes_2 | id 
---------------------------+------+---------+-------+---------+----
- Wed Mar 22 09:18:22 2017 | 23.5 |       1 |       |         |  1
- Wed Mar 22 09:18:23 2017 | 21.5 |       1 |       |         |  2
- Mon May 22 09:18:22 2017 | 36.2 |       2 |       |         |  3
- Mon May 22 09:18:23 2017 | 15.2 |       2 |       |         |  4
- Tue Aug 22 09:19:22 2017 | 21.4 |       3 | nr1   | n2r1    |  5
- Wed Aug 23 09:20:17 2017 | 31.5 |       2 | nr2   | n2r2    |  6
- Tue Aug 22 09:19:14 2017 | 12.5 |       3 |       |         |  7
-(7 rows)
-
--- test setting reloptions
-ALTER TABLE  _timescaledb_internal._hyper_2_3_chunk SET (parallel_workers=2);
-ALTER TABLE  _timescaledb_internal._hyper_2_4_chunk SET (parallel_workers=4);
-ALTER TABLE  _timescaledb_internal._hyper_2_4_chunk RESET (parallel_workers);
-SELECT relname, reloptions FROM pg_class WHERE relname IN ('_hyper_2_3_chunk','_hyper_2_4_chunk');
-     relname      |      reloptions      
-------------------+----------------------
- _hyper_2_3_chunk | {parallel_workers=2}
- _hyper_2_4_chunk | 
-(2 rows)
-
--- Need superuser to ALTER chunks in _timescaledb_internal schema
-\c :TEST_DBNAME :ROLE_SUPERUSER
-SELECT * FROM _timescaledb_catalog.chunk WHERE id = 2;
- id | hypertable_id |      schema_name      |    table_name    | compressed_chunk_id | dropped | status | osm_chunk 
-----+---------------+-----------------------+------------------+---------------------+---------+--------+-----------
-  2 |             2 | _timescaledb_internal | _hyper_2_2_chunk |                     | f       |      0 | f
-(1 row)
-
--- Rename chunk
-ALTER TABLE _timescaledb_internal._hyper_2_2_chunk RENAME TO new_chunk_name;
-SELECT * FROM _timescaledb_catalog.chunk WHERE id = 2;
- id | hypertable_id |      schema_name      |   table_name   | compressed_chunk_id | dropped | status | osm_chunk 
-----+---------------+-----------------------+----------------+---------------------+---------+--------+-----------
-  2 |             2 | _timescaledb_internal | new_chunk_name |                     | f       |      0 | f
-(1 row)
-
--- Set schema
-ALTER TABLE _timescaledb_internal.new_chunk_name SET SCHEMA public;
-SELECT * FROM _timescaledb_catalog.chunk WHERE id = 2;
- id | hypertable_id | schema_name |   table_name   | compressed_chunk_id | dropped | status | osm_chunk 
-----+---------------+-------------+----------------+---------------------+---------+--------+-----------
-  2 |             2 | public      | new_chunk_name |                     | f       |      0 | f
-(1 row)
-
--- Test that we cannot rename chunk columns
-\set ON_ERROR_STOP 0
-ALTER TABLE public.new_chunk_name RENAME COLUMN time TO newtime;
-ERROR:  cannot rename column "time" of hypertable chunk "new_chunk_name"
-\set ON_ERROR_STOP 1
--- Test that we can set tablespace of a hypertable
-\c :TEST_DBNAME :ROLE_SUPERUSER
-SET client_min_messages = ERROR;
-DROP TABLESPACE IF EXISTS tablespace1;
-DROP TABLESPACE IF EXISTS tablespace2;
-SET client_min_messages = NOTICE;
---test hypertable with tables space
-CREATE TABLESPACE tablespace1 OWNER :ROLE_DEFAULT_PERM_USER LOCATION :TEST_TABLESPACE1_PATH;
-CREATE TABLESPACE tablespace2 OWNER :ROLE_DEFAULT_PERM_USER LOCATION :TEST_TABLESPACE2_PATH;
-\c :TEST_DBNAME :ROLE_DEFAULT_PERM_USER
--- Test that we can directly change chunk tablespace
-ALTER TABLE public.new_chunk_name SET TABLESPACE tablespace1;
-SELECT tablespace FROM pg_tables WHERE tablename = 'new_chunk_name';
- tablespace  
--------------
- tablespace1
-(1 row)
-
--- drop all tables to make checking the tests below easier
-DROP TABLE alter_before;
-DROP TABLE alter_after;
--- should return 0 rows
-SELECT tablename, tablespace FROM pg_tables
-WHERE tablename = 'hyper_in_space' OR tablename LIKE '\_hyper\__\__\_chunk' ORDER BY tablename;
- tablename | tablespace 
------------+------------
-(0 rows)
-
-CREATE TABLE hyper_in_space(time bigint, temp float, device int);
-SELECT create_hypertable('hyper_in_space', 'time', 'device', 4, chunk_time_interval=>1);
-NOTICE:  adding not-null constraint to column "time"
-      create_hypertable      
------------------------------
- (3,public,hyper_in_space,t)
-(1 row)
-
-INSERT INTO hyper_in_space(time, temp, device) VALUES (1, 20, 1);
-INSERT INTO hyper_in_space(time, temp, device) VALUES (3, 21, 2);
-INSERT INTO hyper_in_space(time, temp, device) VALUES (5, 23, 1);
-SELECT tablename FROM pg_tables WHERE tablespace = 'tablespace1' ORDER BY tablename;
- tablename 
------------
-(0 rows)
-
-SET default_tablespace = tablespace1;
--- should be inserted in tablespace1 which is now default
-INSERT INTO hyper_in_space(time, temp, device) VALUES (11, 24, 3);
-SELECT tablename, tablespace FROM pg_tables
-WHERE tablename = 'hyper_in_space' OR tablename LIKE '\_hyper\__\__\_chunk' ORDER BY tablename;
-    tablename     | tablespace  
-------------------+-------------
- _hyper_3_5_chunk | 
- _hyper_3_6_chunk | 
- _hyper_3_7_chunk | 
- _hyper_3_8_chunk | tablespace1
- hyper_in_space   | 
-(5 rows)
-
-SET default_tablespace TO DEFAULT;
-ALTER TABLE hyper_in_space SET TABLESPACE tablespace1;
-SELECT tablename FROM pg_tables WHERE tablespace = 'tablespace1' ORDER BY tablename;
-    tablename     
-------------------
- _hyper_3_5_chunk
- _hyper_3_6_chunk
- _hyper_3_7_chunk
- _hyper_3_8_chunk
- hyper_in_space
-(5 rows)
-
--- should be inserted in an existing chunk in the new tablespace,
--- no new chunks
-INSERT INTO hyper_in_space(time, temp, device) VALUES (5, 27, 1);
--- the new chunk should be create in the new tablespace
-INSERT INTO hyper_in_space(time, temp, device) VALUES (8, 24, 2);
-SELECT tablename, tablespace FROM pg_tables
-WHERE tablename = 'hyper_in_space' OR tablename LIKE '\_hyper\__\__\_chunk' ORDER BY tablename;
-    tablename     | tablespace  
-------------------+-------------
- _hyper_3_5_chunk | tablespace1
- _hyper_3_6_chunk | tablespace1
- _hyper_3_7_chunk | tablespace1
- _hyper_3_8_chunk | tablespace1
- _hyper_3_9_chunk | tablespace1
- hyper_in_space   | tablespace1
-(6 rows)
-
--- should not fail (unlike attach_tablespace)
-ALTER TABLE hyper_in_space SET TABLESPACE tablespace1;
-\set ON_ERROR_STOP 0
--- not an empty tablespace
-DROP TABLESPACE tablespace1;
-ERROR:  tablespace "tablespace1" is still attached to 1 hypertables
-\set ON_ERROR_STOP 1
--- show_chunks and drop_chunks output should be the same
-\set QUERY1 'SELECT show_chunks(\'hyper_in_space\', 22)::NAME'
-\set QUERY2 'SELECT drop_chunks(\'hyper_in_space\', 22)::NAME'
-\set ECHO errors
- Different Rows | Total Rows from Query 1 | Total Rows from Query 2 
-----------------+-------------------------+-------------------------
-              0 |                       5 |                       5
-(1 row)
-
-SELECT tablename, tablespace FROM pg_tables WHERE tablespace = 'tablespace1' ORDER BY tablename;
-   tablename    | tablespace  
-----------------+-------------
- hyper_in_space | tablespace1
-(1 row)
-
-\set ON_ERROR_STOP 0
--- should not be able to drop tablespace if a hypertable depends on it
--- even when there are no chunks
-DROP TABLESPACE tablespace1;
-ERROR:  tablespace "tablespace1" is still attached to 1 hypertables
-\set ON_ERROR_STOP 1
-DROP TABLE hyper_in_space;
-CREATE TABLE hyper_in_space(time bigint, temp float, device int) TABLESPACE tablespace1;
-SELECT create_hypertable('hyper_in_space', 'time', 'device', 4, chunk_time_interval=>1);
-NOTICE:  adding not-null constraint to column "time"
-      create_hypertable      
------------------------------
- (4,public,hyper_in_space,t)
-(1 row)
-
-INSERT INTO hyper_in_space(time, temp, device) VALUES (1, 20, 1);
-INSERT INTO hyper_in_space(time, temp, device) VALUES (3, 21, 2);
-INSERT INTO hyper_in_space(time, temp, device) VALUES (5, 23, 1);
-SELECT tablename, tablespace FROM pg_tables
-WHERE tablename = 'hyper_in_space' OR tablename ~ '_hyper_\d+_\d+_chunk' ORDER BY tablename;
-     tablename     | tablespace  
--------------------+-------------
- _hyper_4_10_chunk | tablespace1
- _hyper_4_11_chunk | tablespace1
- _hyper_4_12_chunk | tablespace1
- hyper_in_space    | tablespace1
-(4 rows)
-
-SELECT attach_tablespace('tablespace2', 'hyper_in_space');
- attach_tablespace 
--------------------
- 
-(1 row)
-
-\set ON_ERROR_STOP 0
--- should fail as >1 tablespaces are attached
-ALTER TABLE hyper_in_space SET TABLESPACE tablespace1;
-ERROR:  cannot set new tablespace when multiple tablespaces are attached to hypertable "hyper_in_space"
-\set ON_ERROR_STOP 1
-SELECT detach_tablespace('tablespace2', 'hyper_in_space');
- detach_tablespace 
--------------------
-                 1
-(1 row)
-
-SELECT * FROM _timescaledb_catalog.tablespace;
- id | hypertable_id | tablespace_name 
-----+---------------+-----------------
-  3 |             4 | tablespace1
-(1 row)
-
--- make sure when using ALTER TABLE, table spaces are not accumulated
--- as in case of attach_tablespace
--- should have one result
-SELECT * FROM _timescaledb_catalog.tablespace;
- id | hypertable_id | tablespace_name 
-----+---------------+-----------------
-  3 |             4 | tablespace1
-(1 row)
-
-ALTER TABLE hyper_in_space SET TABLESPACE tablespace2;
--- should have one result
-SELECT * FROM _timescaledb_catalog.tablespace;
- id | hypertable_id | tablespace_name 
-----+---------------+-----------------
-  5 |             4 | tablespace2
-(1 row)
-
-ALTER TABLE hyper_in_space SET TABLESPACE tablespace1;
--- should have one result, (same as the first in the block)
-SELECT * FROM _timescaledb_catalog.tablespace;
- id | hypertable_id | tablespace_name 
-----+---------------+-----------------
-  6 |             4 | tablespace1
-(1 row)
-
-SELECT tablename, tablespace FROM pg_tables
-WHERE tablename = 'hyper_in_space' OR tablename ~ '_hyper_\d+_\d+_chunk' ORDER BY tablename;
-     tablename     | tablespace  
--------------------+-------------
- _hyper_4_10_chunk | tablespace1
- _hyper_4_11_chunk | tablespace1
- _hyper_4_12_chunk | tablespace1
- hyper_in_space    | tablespace1
-(4 rows)
-
--- attach tb2 <-> ALTER SET tb1 <-> detach tb1 should work
-SELECT detach_tablespace('tablespace1', 'hyper_in_space');
- detach_tablespace 
--------------------
-                 1
-(1 row)
-
-INSERT INTO hyper_in_space(time, temp, device) VALUES (5, 23, 1);
-INSERT INTO hyper_in_space(time, temp, device) VALUES (7, 23, 1);
--- Since we have detached tablespace1 the new chunk should not be
--- placed there.
-SELECT tablename, tablespace FROM pg_tables
-WHERE tablename = 'hyper_in_space' OR tablename ~ '_hyper_\d+_\d+_chunk' ORDER BY tablename;
-     tablename     | tablespace  
--------------------+-------------
- _hyper_4_10_chunk | tablespace1
- _hyper_4_11_chunk | tablespace1
- _hyper_4_12_chunk | tablespace1
- _hyper_4_13_chunk | 
- hyper_in_space    | 
-(5 rows)
-
-SELECT * FROM _timescaledb_catalog.tablespace;
- id | hypertable_id | tablespace_name 
-----+---------------+-----------------
-(0 rows)
-
--- tablespace functions should handle the default tablespace just as they do others
-SELECT attach_tablespace('pg_default', 'hyper_in_space');
- attach_tablespace 
--------------------
- 
-(1 row)
-
-SELECT attach_tablespace('tablespace2', 'hyper_in_space');
- attach_tablespace 
--------------------
- 
-(1 row)
-
-SELECT tablename, tablespace FROM pg_tables
-WHERE tablename = 'hyper_in_space' OR tablename ~ '_hyper_\d+_\d+_chunk' ORDER BY tablename;
-     tablename     | tablespace  
--------------------+-------------
- _hyper_4_10_chunk | tablespace1
- _hyper_4_11_chunk | tablespace1
- _hyper_4_12_chunk | tablespace1
- _hyper_4_13_chunk | 
- hyper_in_space    | tablespace2
-(5 rows)
-
-SELECT * FROM _timescaledb_catalog.tablespace;
- id | hypertable_id | tablespace_name 
-----+---------------+-----------------
-  7 |             4 | pg_default
-  8 |             4 | tablespace2
-(2 rows)
-
-INSERT INTO hyper_in_space(time, temp, device) VALUES (12, 22, 1);
-INSERT INTO hyper_in_space(time, temp, device) VALUES (13, 23, 3);
-SELECT tablename, tablespace FROM pg_tables
-WHERE tablename = 'hyper_in_space' OR tablename ~ '_hyper_\d+_\d+_chunk' ORDER BY tablename;
-     tablename     | tablespace  
--------------------+-------------
- _hyper_4_10_chunk | tablespace1
- _hyper_4_11_chunk | tablespace1
- _hyper_4_12_chunk | tablespace1
- _hyper_4_13_chunk | 
- _hyper_4_14_chunk | 
- _hyper_4_15_chunk | tablespace2
- hyper_in_space    | tablespace2
-(7 rows)
-
-SELECT detach_tablespace('pg_default', 'hyper_in_space');
- detach_tablespace 
--------------------
-                 1
-(1 row)
-
-ALTER TABLE hyper_in_space SET TABLESPACE pg_default;
-SELECT tablename, tablespace FROM pg_tables
-WHERE tablename = 'hyper_in_space' OR tablename ~ '_hyper_\d+_\d+_chunk' ORDER BY tablename;
-     tablename     | tablespace 
--------------------+------------
- _hyper_4_10_chunk | 
- _hyper_4_11_chunk | 
- _hyper_4_12_chunk | 
- _hyper_4_13_chunk | 
- _hyper_4_14_chunk | 
- _hyper_4_15_chunk | 
- hyper_in_space    | 
-(7 rows)
-
-SELECT detach_tablespace('pg_default', 'hyper_in_space');
- detach_tablespace 
--------------------
-                 1
-(1 row)
-
-DROP TABLE hyper_in_space;
--- test altering tablespace on index, issue #903
-CREATE TABLE series(
-  time timestamptz not null,
-  device int,
-  value float,
-  CONSTRAINT series_pk PRIMARY KEY (time, device) USING INDEX TABLESPACE tablespace1);
-SELECT create_hypertable('series', 'time', create_default_indexes => FALSE);
-  create_hypertable  
----------------------
- (5,public,series,t)
-(1 row)
-
-INSERT INTO series VALUES ('2019-04-21 10:12', 1, 1.01);
-CREATE INDEX series_value ON series (value, time) TABLESPACE tablespace2;
-SELECT schemaname, tablename, indexname, tablespace
-FROM pg_indexes
-WHERE indexname LIKE '%series%'
-ORDER BY indexname;
-      schemaname       |     tablename     |           indexname            | tablespace  
------------------------+-------------------+--------------------------------+-------------
- _timescaledb_internal | _hyper_5_16_chunk | 16_1_series_pk                 | tablespace1
- _timescaledb_internal | _hyper_5_16_chunk | _hyper_5_16_chunk_series_value | tablespace2
- public                | series            | series_pk                      | tablespace1
- public                | series            | series_value                   | tablespace2
-(4 rows)
-
-ALTER INDEX series_pk SET TABLESPACE tablespace2;
-CREATE INDEX ON series (time) TABLESPACE tablespace1;
-ALTER INDEX series_value SET TABLESPACE pg_default;
-INSERT INTO series VALUES ('2019-04-29 10:12', 2, 1.31);
-SELECT schemaname, tablename, indexname, tablespace
-FROM pg_indexes
-WHERE indexname LIKE '%series%'
-ORDER BY indexname;
-      schemaname       |     tablename     |             indexname             | tablespace  
------------------------+-------------------+-----------------------------------+-------------
- _timescaledb_internal | _hyper_5_16_chunk | 16_1_series_pk                    | tablespace2
- _timescaledb_internal | _hyper_5_17_chunk | 17_2_series_pk                    | tablespace2
- _timescaledb_internal | _hyper_5_16_chunk | _hyper_5_16_chunk_series_time_idx | tablespace1
- _timescaledb_internal | _hyper_5_16_chunk | _hyper_5_16_chunk_series_value    | 
- _timescaledb_internal | _hyper_5_17_chunk | _hyper_5_17_chunk_series_time_idx | tablespace1
- _timescaledb_internal | _hyper_5_17_chunk | _hyper_5_17_chunk_series_value    | 
- public                | series            | series_pk                         | tablespace2
- public                | series            | series_time_idx                   | tablespace1
- public                | series            | series_value                      | 
-(9 rows)
-
-DROP TABLE series;
-DROP TABLESPACE tablespace1;
-DROP TABLESPACE tablespace2;
--- Make sure we handle ALTER SCHEMA RENAME for hypertable schemas
-\c :TEST_DBNAME :ROLE_SUPERUSER
-CREATE SCHEMA IF NOT EXISTS original_name;
-CREATE TABLE original_name.my_table (
-  date timestamp with time zone NOT NULL,
-  quantity double precision
-);
-SELECT create_hypertable('original_name.my_table','date');
-      create_hypertable       
-------------------------------
- (6,original_name,my_table,t)
-(1 row)
-
-INSERT INTO original_name.my_table (date, quantity) VALUES ('2018-07-04T21:00:00+00:00', 8);
-ALTER SCHEMA original_name RENAME TO new_name;
-DROP TABLE new_name.my_table;
-DROP SCHEMA new_name;
--- Now make sure schema is renamed for multiple hypertables, but not hypertables not in the schema
-CREATE SCHEMA IF NOT EXISTS original_name;
-CREATE TABLE original_name.my_table (
-  date timestamp with time zone NOT NULL,
-  quantity double precision
-);
-CREATE TABLE original_name.my_table2 (
-  date timestamp with time zone NOT NULL,
-  quantity double precision
-);
-CREATE TABLE regular_table (
-  date timestamp with time zone NOT NULL,
-  quantity double precision
-);
-SELECT create_hypertable('original_name.my_table','date');
-      create_hypertable       
-------------------------------
- (7,original_name,my_table,t)
-(1 row)
-
-SELECT create_hypertable('original_name.my_table2','date');
-       create_hypertable       
--------------------------------
- (8,original_name,my_table2,t)
-(1 row)
-
-SELECT create_hypertable('regular_table','date');
-     create_hypertable      
-----------------------------
- (9,public,regular_table,t)
-(1 row)
-
-INSERT INTO original_name.my_table (date, quantity) VALUES ('2018-07-04T21:00:00+00:00', 8);
-INSERT INTO original_name.my_table2 (date, quantity) VALUES ('2018-07-04T21:00:00+00:00', 8);
-INSERT INTO regular_table (date, quantity) VALUES ('2018-07-04T21:00:00+00:00', 8);
-ALTER SCHEMA original_name RENAME TO new_name;
-DROP TABLE new_name.my_table;
-DROP TABLE new_name.my_table2;
-DROP TABLE regular_table;
-DROP SCHEMA new_name;
--- These tables should also drop when we drop the whole schema
-CREATE SCHEMA IF NOT EXISTS original_name;
-CREATE TABLE original_name.my_table (
-  date timestamp with time zone NOT NULL,
-  quantity double precision
-);
-CREATE TABLE original_name.my_table2 (
-  date timestamp with time zone NOT NULL,
-  quantity double precision
-);
-SELECT create_hypertable('original_name.my_table','date');
-       create_hypertable       
--------------------------------
- (10,original_name,my_table,t)
-(1 row)
-
-SELECT create_hypertable('original_name.my_table2','date');
-       create_hypertable        
---------------------------------
- (11,original_name,my_table2,t)
-(1 row)
-
-INSERT INTO original_name.my_table (date, quantity) VALUES ('2018-07-04T21:00:00+00:00', 8);
-INSERT INTO original_name.my_table2 (date, quantity) VALUES ('2018-07-04T21:00:00+00:00', 8);
-ALTER SCHEMA original_name RENAME TO new_name;
-DROP SCHEMA new_name CASCADE;
-NOTICE:  drop cascades to 4 other objects
-\dt new_name.*;
-      List of relations
- Schema | Name | Type | Owner 
---------+------+------+-------
-(0 rows)
-
--- Make sure we can't rename internal schemas
-\set ON_ERROR_STOP 0
-ALTER SCHEMA _timescaledb_internal RENAME TO my_new_schema_name;
-ERROR:  cannot rename schemas used by the TimescaleDB extension
-ALTER SCHEMA _timescaledb_catalog RENAME TO my_new_schema_name;
-ERROR:  cannot rename schemas used by the TimescaleDB extension
-ALTER SCHEMA _timescaledb_cache RENAME TO my_new_schema_name;
-ERROR:  cannot rename schemas used by the TimescaleDB extension
-ALTER SCHEMA _timescaledb_config RENAME TO my_new_schema_name;
-ERROR:  cannot rename schemas used by the TimescaleDB extension
-\set ON_ERROR_STOP 1
--- Make sure we can rename associated schemas
-CREATE TABLE my_table (
-  date timestamp with time zone NOT NULL,
-  quantity double precision
-);
-SELECT create_hypertable('my_table','date', associated_schema_name => 'my_associated_schema');
-   create_hypertable    
-------------------------
- (12,public,my_table,t)
-(1 row)
-
-INSERT INTO my_table (date, quantity) VALUES ('2018-07-04T21:00:00+00:00', 8);
-ALTER SCHEMA my_associated_schema RENAME TO new_associated_schema;
-INSERT INTO my_table (date, quantity) VALUES ('2018-08-10T23:00:00+00:00', 20);
--- Make sure the schema name is changed in both catalog tables
-SELECT * from _timescaledb_catalog.hypertable;
- id | schema_name | table_name | associated_schema_name | associated_table_prefix | num_dimensions | chunk_sizing_func_schema |  chunk_sizing_func_name  | chunk_target_size | compression_state | compressed_hypertable_id | replication_factor 
-----+-------------+------------+------------------------+-------------------------+----------------+--------------------------+--------------------------+-------------------+-------------------+--------------------------+--------------------
- 12 | public      | my_table   | new_associated_schema  | _hyper_12               |              1 | _timescaledb_internal    | calculate_chunk_interval |                 0 |                 0 |                          |                   
-(1 row)
-
-SELECT * from _timescaledb_catalog.chunk;
- id | hypertable_id |      schema_name      |     table_name     | compressed_chunk_id | dropped | status | osm_chunk 
-----+---------------+-----------------------+--------------------+---------------------+---------+--------+-----------
- 24 |            12 | new_associated_schema | _hyper_12_24_chunk |                     | f       |      0 | f
- 25 |            12 | new_associated_schema | _hyper_12_25_chunk |                     | f       |      0 | f
-(2 rows)
-
-DROP TABLE my_table;
--- test renaming unique constraints/indexes
-CREATE TABLE t_hypertable ( id INTEGER NOT NULL, time TIMESTAMPTZ NOT NULL, value FLOAT NOT NULL CHECK (value > 0), UNIQUE(id, time));
-SELECT create_hypertable('t_hypertable', 'time');
-     create_hypertable      
-----------------------------
- (13,public,t_hypertable,t)
-(1 row)
-
-INSERT INTO t_hypertable AS h VALUES ( 1, '2020-01-01 00:00:00', 3.2) ON CONFLICT (id, time) DO UPDATE SET value = h.value + EXCLUDED.value;
-INSERT INTO t_hypertable AS h VALUES ( 1, '2021-01-01 00:00:00', 3.2) ON CONFLICT (id, time) DO UPDATE SET value = h.value + EXCLUDED.value;
-BEGIN;
-ALTER INDEX t_hypertable_id_time_key RENAME TO t_new_constraint;
--- chunk_index and chunk_constraint should both have updated constraint names
-SELECT hypertable_index_name, index_name from _timescaledb_catalog.chunk_index WHERE hypertable_index_name = 't_new_constraint' ORDER BY 1,2;
- hypertable_index_name |             index_name              
------------------------+-------------------------------------
- t_new_constraint      | _hyper_13_26_chunk_t_new_constraint
- t_new_constraint      | _hyper_13_27_chunk_t_new_constraint
-(2 rows)
-
-SELECT hypertable_constraint_name, constraint_name from _timescaledb_catalog.chunk_constraint WHERE hypertable_constraint_name = 't_new_constraint' ORDER BY 1,2;
- hypertable_constraint_name |           constraint_name           
-----------------------------+-------------------------------------
- t_new_constraint           | _hyper_13_26_chunk_t_new_constraint
- t_new_constraint           | _hyper_13_27_chunk_t_new_constraint
-(2 rows)
-
-INSERT INTO t_hypertable AS h VALUES ( 1, '2020-01-01 00:01:00', 3.2) ON CONFLICT (id, time) DO UPDATE SET value = h.value + EXCLUDED.value;
-ROLLBACK;
-BEGIN;
-ALTER TABLE t_hypertable RENAME CONSTRAINT t_hypertable_id_time_key TO t_new_constraint;
--- chunk_index and chunk_constraint should both have updated constraint names
-SELECT hypertable_index_name, index_name from _timescaledb_catalog.chunk_index WHERE hypertable_index_name = 't_new_constraint' ORDER BY 1,2;
- hypertable_index_name |      index_name       
------------------------+-----------------------
- t_new_constraint      | 26_5_t_new_constraint
- t_new_constraint      | 27_6_t_new_constraint
-(2 rows)
-
-SELECT hypertable_constraint_name, constraint_name from _timescaledb_catalog.chunk_constraint WHERE hypertable_constraint_name = 't_new_constraint' ORDER BY 1,2;
- hypertable_constraint_name |    constraint_name    
-----------------------------+-----------------------
- t_new_constraint           | 26_5_t_new_constraint
- t_new_constraint           | 27_6_t_new_constraint
-(2 rows)
-
-INSERT INTO t_hypertable AS h VALUES ( 1, '2020-01-01 00:01:00', 3.2) ON CONFLICT (id, time) DO UPDATE SET value = h.value + EXCLUDED.value;
-ROLLBACK;
--- predicate reconstruction when attnos are different in hypertable and chunk
-CREATE TABLE p_hypertable (a integer not null, b integer, c integer);
-SELECT create_hypertable('p_hypertable', 'a', chunk_time_interval => int '3');
-     create_hypertable      
-----------------------------
- (14,public,p_hypertable,t)
-(1 row)
-
-BEGIN;
-ALTER TABLE p_hypertable DROP COLUMN b, ADD COLUMN d boolean;
-CREATE INDEX idx_ht ON p_hypertable(a, c) WHERE d = FALSE;
-END;
-INSERT INTO p_hypertable(a, c, d) VALUES (1, 1, FALSE);
-\d+ _timescaledb_internal._hyper_14_28_chunk
-                     Table "_timescaledb_internal._hyper_14_28_chunk"
- Column |  Type   | Collation | Nullable | Default | Storage | Stats target | Description 
---------+---------+-----------+----------+---------+---------+--------------+-------------
- a      | integer |           | not null |         | plain   |              | 
- c      | integer |           |          |         | plain   |              | 
- d      | boolean |           |          |         | plain   |              | 
-Indexes:
-    "_hyper_14_28_chunk_idx_ht" btree (a, c) WHERE NOT d
-    "_hyper_14_28_chunk_p_hypertable_a_idx" btree (a DESC)
-Check constraints:
-    "constraint_34" CHECK (a >= 0 AND a < 3)
-Inherits: p_hypertable
-
-DROP TABLE p_hypertable;
--- check none of our hooks interact badly with normal alter view handling
-CREATE VIEW v1 AS SELECT random();
-\set ON_ERROR_STOP 0
--- should error with unrecognized parameter
-ALTER VIEW v1 SET (autovacuum_enabled = false);
-ERROR:  unrecognized parameter "autovacuum_enabled"
-\set ON_ERROR_STOP 1
--- issue 4474
--- test hypertable with non-default statistics target
--- and chunk creation triggered by non-owner
-CREATE ROLE role_4474;
-CREATE TABLE i4474(time timestamptz NOT NULL);
-SELECT table_name FROM public.create_hypertable( 'i4474', 'time');
- table_name 
-------------
- i4474
-(1 row)
-
-GRANT SELECT, INSERT on i4474 TO role_4474;
--- create chunk as owner
-INSERT INTO i4474 SELECT '2020-01-01';
--- set statistics
-ALTER TABLE i4474 ALTER COLUMN time SET statistics 10;
--- create chunk as non-owner
-SET ROLE role_4474;
-INSERT INTO i4474 SELECT '2021-01-01';
-RESET ROLE;
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/alternate_users.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/alternate_users.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/alternate_users.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/alternate_users.out	2023-11-25 05:27:08.457126164 +0000
@@ -1,530 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-\ir include/insert_single.sql
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-CREATE TABLE PUBLIC."one_Partition" (
-  "timeCustom" BIGINT NOT NULL,
-  device_id TEXT NOT NULL,
-  series_0 DOUBLE PRECISION NULL,
-  series_1 DOUBLE PRECISION NULL,
-  series_2 DOUBLE PRECISION NULL,
-  series_bool BOOLEAN NULL
-);
-CREATE INDEX ON PUBLIC."one_Partition" (device_id, "timeCustom" DESC NULLS LAST) WHERE device_id IS NOT NULL;
-CREATE INDEX ON PUBLIC."one_Partition" ("timeCustom" DESC NULLS LAST, series_0) WHERE series_0 IS NOT NULL;
-CREATE INDEX ON PUBLIC."one_Partition" ("timeCustom" DESC NULLS LAST, series_1)  WHERE series_1 IS NOT NULL;
-CREATE INDEX ON PUBLIC."one_Partition" ("timeCustom" DESC NULLS LAST, series_2) WHERE series_2 IS NOT NULL;
-CREATE INDEX ON PUBLIC."one_Partition" ("timeCustom" DESC NULLS LAST, series_bool) WHERE series_bool IS NOT NULL;
-\c :DBNAME :ROLE_SUPERUSER
-CREATE SCHEMA "one_Partition" AUTHORIZATION :ROLE_DEFAULT_PERM_USER;
-\c :DBNAME :ROLE_DEFAULT_PERM_USER;
-SELECT * FROM create_hypertable('"public"."one_Partition"', 'timeCustom', associated_schema_name=>'one_Partition', chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
- hypertable_id | schema_name |  table_name   | created 
----------------+-------------+---------------+---------
-             1 | public      | one_Partition | t
-(1 row)
-
---output command tags
-\set QUIET off
-BEGIN;
-BEGIN
-\COPY "one_Partition" FROM 'data/ds1_dev1_1.tsv' NULL AS '';
-COPY 7
-COMMIT;
-COMMIT
-INSERT INTO "one_Partition"("timeCustom", device_id, series_0, series_1) VALUES
-(1257987600000000000, 'dev1', 1.5, 1),
-(1257987600000000000, 'dev1', 1.5, 2),
-(1257894000000000000, 'dev2', 1.5, 1),
-(1257894002000000000, 'dev1', 2.5, 3);
-INSERT 0 4
-INSERT INTO "one_Partition"("timeCustom", device_id, series_0, series_1) VALUES
-(1257894000000000000, 'dev2', 1.5, 2);
-INSERT 0 1
-\set QUIET on
-\c :TEST_DBNAME :ROLE_SUPERUSER
--- make sure tablespace1 exists
--- since there is no CREATE TABLESPACE IF EXISTS we drop with if exists and recreate
-SET client_min_messages TO error;
-DROP TABLESPACE IF EXISTS tablespace1;
-RESET client_min_messages;
-CREATE TABLESPACE tablespace1 OWNER :ROLE_DEFAULT_PERM_USER LOCATION :TEST_TABLESPACE1_PATH;
---needed for ddl ops:
-CREATE SCHEMA IF NOT EXISTS "customSchema" AUTHORIZATION :ROLE_DEFAULT_PERM_USER_2;
---needed for ROLE_DEFAULT_PERM_USER_2 to write to the 'one_Partition' schema which
---is owned by ROLE_DEFAULT_PERM_USER
-GRANT CREATE ON SCHEMA "one_Partition" TO :ROLE_DEFAULT_PERM_USER_2;
---test creating and using schema as non-superuser
-\c :TEST_DBNAME :ROLE_DEFAULT_PERM_USER_2
-\dt
-                 List of relations
- Schema |     Name      | Type  |       Owner       
---------+---------------+-------+-------------------
- public | one_Partition | table | default_perm_user
-(1 row)
-
-\set ON_ERROR_STOP 0
-SELECT * FROM "one_Partition";
-ERROR:  permission denied for table one_Partition
-SELECT set_chunk_time_interval('"one_Partition"', 1::bigint);
-ERROR:  must be owner of hypertable "one_Partition"
-select add_dimension('"one_Partition"', 'device_id', 2);
-ERROR:  must be owner of hypertable "one_Partition"
-select attach_tablespace('tablespace1', '"one_Partition"');
-ERROR:  must be owner of hypertable "one_Partition"
-\set ON_ERROR_STOP 1
-CREATE TABLE "1dim"(time timestamp, temp float);
-SELECT create_hypertable('"1dim"', 'time');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
- create_hypertable 
--------------------
- (2,public,1dim,t)
-(1 row)
-
-INSERT INTO "1dim" VALUES('2017-01-20T09:00:01', 22.5);
-INSERT INTO "1dim" VALUES('2017-01-20T09:00:21', 21.2);
-INSERT INTO "1dim" VALUES('2017-01-20T09:00:47', 25.1);
-SELECT * FROM "1dim";
-           time           | temp 
---------------------------+------
- Fri Jan 20 09:00:01 2017 | 22.5
- Fri Jan 20 09:00:21 2017 | 21.2
- Fri Jan 20 09:00:47 2017 | 25.1
-(3 rows)
-
-\ir include/ddl_ops_1.sql
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-CREATE TABLE PUBLIC."Hypertable_1" (
-  time BIGINT NOT NULL,
-  "Device_id" TEXT NOT NULL,
-  temp_c int NOT NULL DEFAULT -1,
-  humidity numeric NULL DEFAULT 0,
-  sensor_1 NUMERIC NULL DEFAULT 1,
-  sensor_2 NUMERIC NOT NULL DEFAULT 1,
-  sensor_3 NUMERIC NOT NULL DEFAULT 1,
-  sensor_4 NUMERIC NOT NULL DEFAULT 1
-);
-CREATE INDEX ON PUBLIC."Hypertable_1" (time, "Device_id");
-CREATE TABLE "customSchema"."Hypertable_1" (
-  time BIGINT NOT NULL,
-  "Device_id" TEXT NOT NULL,
-  temp_c int NOT NULL DEFAULT -1,
-  humidity numeric NULL DEFAULT 0,
-  sensor_1 NUMERIC NULL DEFAULT 1,
-  sensor_2 NUMERIC NOT NULL DEFAULT 1,
-  sensor_3 NUMERIC NOT NULL DEFAULT 1,
-  sensor_4 NUMERIC NOT NULL DEFAULT 1
-);
-CREATE INDEX ON "customSchema"."Hypertable_1" (time, "Device_id");
-SELECT * FROM create_hypertable('"public"."Hypertable_1"', 'time', 'Device_id', 1, chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
- hypertable_id | schema_name |  table_name  | created 
----------------+-------------+--------------+---------
-             3 | public      | Hypertable_1 | t
-(1 row)
-
-SELECT * FROM create_hypertable('"customSchema"."Hypertable_1"', 'time', NULL, 1, chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
- hypertable_id | schema_name  |  table_name  | created 
----------------+--------------+--------------+---------
-             4 | customSchema | Hypertable_1 | t
-(1 row)
-
-SELECT * FROM _timescaledb_catalog.hypertable;
- id | schema_name  |  table_name   | associated_schema_name | associated_table_prefix | num_dimensions | chunk_sizing_func_schema |  chunk_sizing_func_name  | chunk_target_size | compression_state | compressed_hypertable_id | replication_factor 
-----+--------------+---------------+------------------------+-------------------------+----------------+--------------------------+--------------------------+-------------------+-------------------+--------------------------+--------------------
-  1 | public       | one_Partition | one_Partition          | _hyper_1                |              1 | _timescaledb_internal    | calculate_chunk_interval |                 0 |                 0 |                          |                   
-  2 | public       | 1dim          | _timescaledb_internal  | _hyper_2                |              1 | _timescaledb_internal    | calculate_chunk_interval |                 0 |                 0 |                          |                   
-  3 | public       | Hypertable_1  | _timescaledb_internal  | _hyper_3                |              2 | _timescaledb_internal    | calculate_chunk_interval |                 0 |                 0 |                          |                   
-  4 | customSchema | Hypertable_1  | _timescaledb_internal  | _hyper_4                |              1 | _timescaledb_internal    | calculate_chunk_interval |                 0 |                 0 |                          |                   
-(4 rows)
-
-CREATE INDEX ON PUBLIC."Hypertable_1" (time, "temp_c");
-CREATE INDEX "ind_humidity" ON PUBLIC."Hypertable_1" (time, "humidity");
-CREATE INDEX "ind_sensor_1" ON PUBLIC."Hypertable_1" (time, "sensor_1");
-INSERT INTO PUBLIC."Hypertable_1"(time, "Device_id", temp_c, humidity, sensor_1, sensor_2, sensor_3, sensor_4)
-VALUES(1257894000000000000, 'dev1', 30, 70, 1, 2, 3, 100);
-CREATE UNIQUE INDEX "Unique1" ON PUBLIC."Hypertable_1" (time, "Device_id");
-CREATE UNIQUE INDEX "Unique1" ON "customSchema"."Hypertable_1" (time);
-INSERT INTO "customSchema"."Hypertable_1"(time, "Device_id", temp_c, humidity, sensor_1, sensor_2, sensor_3, sensor_4)
-VALUES(1257894000000000000, 'dev1', 30, 70, 1, 2, 3, 100);
-INSERT INTO "customSchema"."Hypertable_1"(time, "Device_id", temp_c, humidity, sensor_1, sensor_2, sensor_3, sensor_4)
-VALUES(1257894000000000001, 'dev1', 30, 70, 1, 2, 3, 100);
-SELECT * FROM _timescaledb_catalog.chunk_index ORDER BY hypertable_id, hypertable_index_name, chunk_id;
- chunk_id |                        index_name                         | hypertable_id |          hypertable_index_name           
-----------+-----------------------------------------------------------+---------------+------------------------------------------
-        1 | _hyper_1_1_chunk_one_Partition_device_id_timeCustom_idx   |             1 | one_Partition_device_id_timeCustom_idx
-        2 | _hyper_1_2_chunk_one_Partition_device_id_timeCustom_idx   |             1 | one_Partition_device_id_timeCustom_idx
-        3 | _hyper_1_3_chunk_one_Partition_device_id_timeCustom_idx   |             1 | one_Partition_device_id_timeCustom_idx
-        1 | _hyper_1_1_chunk_one_Partition_timeCustom_idx             |             1 | one_Partition_timeCustom_idx
-        2 | _hyper_1_2_chunk_one_Partition_timeCustom_idx             |             1 | one_Partition_timeCustom_idx
-        3 | _hyper_1_3_chunk_one_Partition_timeCustom_idx             |             1 | one_Partition_timeCustom_idx
-        1 | _hyper_1_1_chunk_one_Partition_timeCustom_series_0_idx    |             1 | one_Partition_timeCustom_series_0_idx
-        2 | _hyper_1_2_chunk_one_Partition_timeCustom_series_0_idx    |             1 | one_Partition_timeCustom_series_0_idx
-        3 | _hyper_1_3_chunk_one_Partition_timeCustom_series_0_idx    |             1 | one_Partition_timeCustom_series_0_idx
-        1 | _hyper_1_1_chunk_one_Partition_timeCustom_series_1_idx    |             1 | one_Partition_timeCustom_series_1_idx
-        2 | _hyper_1_2_chunk_one_Partition_timeCustom_series_1_idx    |             1 | one_Partition_timeCustom_series_1_idx
-        3 | _hyper_1_3_chunk_one_Partition_timeCustom_series_1_idx    |             1 | one_Partition_timeCustom_series_1_idx
-        1 | _hyper_1_1_chunk_one_Partition_timeCustom_series_2_idx    |             1 | one_Partition_timeCustom_series_2_idx
-        2 | _hyper_1_2_chunk_one_Partition_timeCustom_series_2_idx    |             1 | one_Partition_timeCustom_series_2_idx
-        3 | _hyper_1_3_chunk_one_Partition_timeCustom_series_2_idx    |             1 | one_Partition_timeCustom_series_2_idx
-        1 | _hyper_1_1_chunk_one_Partition_timeCustom_series_bool_idx |             1 | one_Partition_timeCustom_series_bool_idx
-        2 | _hyper_1_2_chunk_one_Partition_timeCustom_series_bool_idx |             1 | one_Partition_timeCustom_series_bool_idx
-        3 | _hyper_1_3_chunk_one_Partition_timeCustom_series_bool_idx |             1 | one_Partition_timeCustom_series_bool_idx
-        4 | _hyper_2_4_chunk_1dim_time_idx                            |             2 | 1dim_time_idx
-        5 | _hyper_3_5_chunk_Hypertable_1_Device_id_time_idx          |             3 | Hypertable_1_Device_id_time_idx
-        5 | _hyper_3_5_chunk_Hypertable_1_time_Device_id_idx          |             3 | Hypertable_1_time_Device_id_idx
-        5 | _hyper_3_5_chunk_Hypertable_1_time_idx                    |             3 | Hypertable_1_time_idx
-        5 | _hyper_3_5_chunk_Hypertable_1_time_temp_c_idx             |             3 | Hypertable_1_time_temp_c_idx
-        5 | _hyper_3_5_chunk_Unique1                                  |             3 | Unique1
-        5 | _hyper_3_5_chunk_ind_humidity                             |             3 | ind_humidity
-        5 | _hyper_3_5_chunk_ind_sensor_1                             |             3 | ind_sensor_1
-        6 | _hyper_4_6_chunk_Hypertable_1_time_Device_id_idx          |             4 | Hypertable_1_time_Device_id_idx
-        6 | _hyper_4_6_chunk_Hypertable_1_time_idx                    |             4 | Hypertable_1_time_idx
-        6 | _hyper_4_6_chunk_Unique1                                  |             4 | Unique1
-(29 rows)
-
---expect error cases
-\set ON_ERROR_STOP 0
-INSERT INTO "customSchema"."Hypertable_1"(time, "Device_id", temp_c, humidity, sensor_1, sensor_2, sensor_3, sensor_4)
-VALUES(1257894000000000000, 'dev1', 31, 71, 72, 4, 1, 102);
-psql:include/ddl_ops_1.sql:56: ERROR:  duplicate key value violates unique constraint "_hyper_4_6_chunk_Unique1"
-CREATE UNIQUE INDEX "Unique2" ON PUBLIC."Hypertable_1" ("Device_id");
-psql:include/ddl_ops_1.sql:57: ERROR:  cannot create a unique index without the column "time" (used in partitioning)
-CREATE UNIQUE INDEX "Unique2" ON PUBLIC."Hypertable_1" (time);
-psql:include/ddl_ops_1.sql:58: ERROR:  cannot create a unique index without the column "Device_id" (used in partitioning)
-CREATE UNIQUE INDEX "Unique2" ON PUBLIC."Hypertable_1" (sensor_1);
-psql:include/ddl_ops_1.sql:59: ERROR:  cannot create a unique index without the column "time" (used in partitioning)
-UPDATE ONLY PUBLIC."Hypertable_1" SET time = 0 WHERE TRUE;
-DELETE FROM ONLY PUBLIC."Hypertable_1" WHERE "Device_id" = 'dev1';
-\set ON_ERROR_STOP 1
-CREATE TABLE my_ht (time BIGINT, val integer);
-SELECT * FROM create_hypertable('my_ht', 'time', chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
-psql:include/ddl_ops_1.sql:66: NOTICE:  adding not-null constraint to column "time"
- hypertable_id | schema_name | table_name | created 
----------------+-------------+------------+---------
-             5 | public      | my_ht      | t
-(1 row)
-
-ALTER TABLE my_ht ADD COLUMN val2 integer;
-SELECT * FROM test.show_columns('my_ht');
- Column |  Type   | NotNull 
---------+---------+---------
- time   | bigint  | t
- val    | integer | f
- val2   | integer | f
-(3 rows)
-
--- Should error when adding again
-\set ON_ERROR_STOP 0
-ALTER TABLE my_ht ADD COLUMN val2 integer;
-psql:include/ddl_ops_1.sql:72: ERROR:  column "val2" of relation "my_ht" already exists
-\set ON_ERROR_STOP 1
--- Should create
-ALTER TABLE my_ht ADD COLUMN IF NOT EXISTS val3 integer;
-SELECT * FROM test.show_columns('my_ht');
- Column |  Type   | NotNull 
---------+---------+---------
- time   | bigint  | t
- val    | integer | f
- val2   | integer | f
- val3   | integer | f
-(4 rows)
-
--- Should skip and not error
-ALTER TABLE my_ht ADD COLUMN IF NOT EXISTS val3 integer;
-psql:include/ddl_ops_1.sql:80: NOTICE:  column "val3" of relation "my_ht" already exists, skipping
-SELECT * FROM test.show_columns('my_ht');
- Column |  Type   | NotNull 
---------+---------+---------
- time   | bigint  | t
- val    | integer | f
- val2   | integer | f
- val3   | integer | f
-(4 rows)
-
--- Should drop
-ALTER TABLE my_ht DROP COLUMN IF EXISTS val3;
-SELECT * FROM test.show_columns('my_ht');
- Column |  Type   | NotNull 
---------+---------+---------
- time   | bigint  | t
- val    | integer | f
- val2   | integer | f
-(3 rows)
-
--- Should skip and not error
-ALTER TABLE my_ht DROP COLUMN IF EXISTS val3;
-psql:include/ddl_ops_1.sql:88: NOTICE:  column "val3" of relation "my_ht" does not exist, skipping
-SELECT * FROM test.show_columns('my_ht');
- Column |  Type   | NotNull 
---------+---------+---------
- time   | bigint  | t
- val    | integer | f
- val2   | integer | f
-(3 rows)
-
---Test default index creation on create_hypertable().
---Make sure that we do not duplicate indexes that already exists
---
---No existing indexes: both time and space-time indexes created
-BEGIN;
-CREATE TABLE PUBLIC."Hypertable_1_with_default_index_enabled" (
-  "Time" BIGINT NOT NULL,
-  "Device_id" TEXT NOT NULL,
-  sensor_1 NUMERIC NULL DEFAULT 1
-);
-SELECT * FROM create_hypertable('"public"."Hypertable_1_with_default_index_enabled"', 'Time', 'Device_id', 1, chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
- hypertable_id | schema_name |               table_name                | created 
----------------+-------------+-----------------------------------------+---------
-             6 | public      | Hypertable_1_with_default_index_enabled | t
-(1 row)
-
-SELECT * FROM test.show_indexes('"Hypertable_1_with_default_index_enabled"');
-                            Index                             |     Columns      | Expr | Unique | Primary | Exclusion | Tablespace 
---------------------------------------------------------------+------------------+------+--------+---------+-----------+------------
- "Hypertable_1_with_default_index_enabled_Device_id_Time_idx" | {Device_id,Time} |      | f      | f       | f         | 
- "Hypertable_1_with_default_index_enabled_Time_idx"           | {Time}           |      | f      | f       | f         | 
-(2 rows)
-
-ROLLBACK;
---Space index exists: only time index created
-BEGIN;
-CREATE TABLE PUBLIC."Hypertable_1_with_default_index_enabled" (
-  "Time" BIGINT NOT NULL,
-  "Device_id" TEXT NOT NULL,
-  sensor_1 NUMERIC NULL DEFAULT 1
-);
-CREATE INDEX ON PUBLIC."Hypertable_1_with_default_index_enabled" ("Device_id", "Time" DESC);
-SELECT * FROM create_hypertable('"public"."Hypertable_1_with_default_index_enabled"', 'Time', 'Device_id', 1, chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
- hypertable_id | schema_name |               table_name                | created 
----------------+-------------+-----------------------------------------+---------
-             7 | public      | Hypertable_1_with_default_index_enabled | t
-(1 row)
-
-SELECT * FROM test.show_indexes('"Hypertable_1_with_default_index_enabled"');
-                            Index                             |     Columns      | Expr | Unique | Primary | Exclusion | Tablespace 
---------------------------------------------------------------+------------------+------+--------+---------+-----------+------------
- "Hypertable_1_with_default_index_enabled_Device_id_Time_idx" | {Device_id,Time} |      | f      | f       | f         | 
- "Hypertable_1_with_default_index_enabled_Time_idx"           | {Time}           |      | f      | f       | f         | 
-(2 rows)
-
-ROLLBACK;
---Time index exists, only partition index created
-BEGIN;
-CREATE TABLE PUBLIC."Hypertable_1_with_default_index_enabled" (
-  "Time" BIGINT NOT NULL,
-  "Device_id" TEXT NOT NULL,
-  sensor_1 NUMERIC NULL DEFAULT 1
-);
-CREATE INDEX ON PUBLIC."Hypertable_1_with_default_index_enabled" ("Time" DESC);
-SELECT * FROM create_hypertable('"public"."Hypertable_1_with_default_index_enabled"', 'Time', 'Device_id', 1, chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
- hypertable_id | schema_name |               table_name                | created 
----------------+-------------+-----------------------------------------+---------
-             8 | public      | Hypertable_1_with_default_index_enabled | t
-(1 row)
-
-SELECT * FROM test.show_indexes('"Hypertable_1_with_default_index_enabled"');
-                            Index                             |     Columns      | Expr | Unique | Primary | Exclusion | Tablespace 
---------------------------------------------------------------+------------------+------+--------+---------+-----------+------------
- "Hypertable_1_with_default_index_enabled_Device_id_Time_idx" | {Device_id,Time} |      | f      | f       | f         | 
- "Hypertable_1_with_default_index_enabled_Time_idx"           | {Time}           |      | f      | f       | f         | 
-(2 rows)
-
-ROLLBACK;
---No space partitioning, only time index created
-BEGIN;
-CREATE TABLE PUBLIC."Hypertable_1_with_default_index_enabled" (
-  "Time" BIGINT NOT NULL,
-  "Device_id" TEXT NOT NULL,
-  sensor_1 NUMERIC NULL DEFAULT 1
-);
-SELECT * FROM create_hypertable('"public"."Hypertable_1_with_default_index_enabled"', 'Time', chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
- hypertable_id | schema_name |               table_name                | created 
----------------+-------------+-----------------------------------------+---------
-             9 | public      | Hypertable_1_with_default_index_enabled | t
-(1 row)
-
-SELECT * FROM test.show_indexes('"Hypertable_1_with_default_index_enabled"');
-                       Index                        | Columns | Expr | Unique | Primary | Exclusion | Tablespace 
-----------------------------------------------------+---------+------+--------+---------+-----------+------------
- "Hypertable_1_with_default_index_enabled_Time_idx" | {Time}  |      | f      | f       | f         | 
-(1 row)
-
-ROLLBACK;
---Disable index creation: no default indexes created
-BEGIN;
-CREATE TABLE PUBLIC."Hypertable_1_with_default_index_enabled" (
-  "Time" BIGINT NOT NULL,
-  "Device_id" TEXT NOT NULL,
-  sensor_1 NUMERIC NULL DEFAULT 1
-);
-SELECT * FROM create_hypertable('"public"."Hypertable_1_with_default_index_enabled"', 'Time', 'Device_id', 1, create_default_indexes=>FALSE, chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
- hypertable_id | schema_name |               table_name                | created 
----------------+-------------+-----------------------------------------+---------
-            10 | public      | Hypertable_1_with_default_index_enabled | t
-(1 row)
-
-SELECT * FROM test.show_indexes('"Hypertable_1_with_default_index_enabled"');
- Index | Columns | Expr | Unique | Primary | Exclusion | Tablespace 
--------+---------+------+--------+---------+-----------+------------
-(0 rows)
-
-ROLLBACK;
-\ir include/ddl_ops_2.sql
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-ALTER TABLE PUBLIC."Hypertable_1" ADD COLUMN temp_f INTEGER NOT NULL DEFAULT 31;
-ALTER TABLE PUBLIC."Hypertable_1" DROP COLUMN temp_c;
-ALTER TABLE PUBLIC."Hypertable_1" DROP COLUMN sensor_4;
-ALTER TABLE PUBLIC."Hypertable_1" ALTER COLUMN humidity SET DEFAULT 100;
-ALTER TABLE PUBLIC."Hypertable_1" ALTER COLUMN sensor_1 DROP DEFAULT;
-ALTER TABLE PUBLIC."Hypertable_1" ALTER COLUMN sensor_2 SET DEFAULT NULL;
-ALTER TABLE PUBLIC."Hypertable_1" ALTER COLUMN sensor_1 SET NOT NULL;
-ALTER TABLE PUBLIC."Hypertable_1" ALTER COLUMN sensor_2 DROP NOT NULL;
-ALTER TABLE PUBLIC."Hypertable_1" RENAME COLUMN sensor_2 TO sensor_2_renamed;
-ALTER TABLE PUBLIC."Hypertable_1" RENAME COLUMN sensor_3 TO sensor_3_renamed;
-DROP INDEX "ind_sensor_1";
-CREATE OR REPLACE FUNCTION empty_trigger_func()
-    RETURNS TRIGGER LANGUAGE PLPGSQL AS
-$BODY$
-BEGIN
-END
-$BODY$;
-CREATE TRIGGER test_trigger BEFORE UPDATE OR DELETE ON PUBLIC."Hypertable_1"
-FOR EACH STATEMENT EXECUTE FUNCTION empty_trigger_func();
-ALTER TABLE PUBLIC."Hypertable_1" ALTER COLUMN sensor_2_renamed SET DATA TYPE int;
-ALTER INDEX "ind_humidity" RENAME TO "ind_humdity2";
--- Change should be reflected here
-SELECT * FROM _timescaledb_catalog.chunk_index;
- chunk_id |                        index_name                         | hypertable_id |          hypertable_index_name           
-----------+-----------------------------------------------------------+---------------+------------------------------------------
-        1 | _hyper_1_1_chunk_one_Partition_device_id_timeCustom_idx   |             1 | one_Partition_device_id_timeCustom_idx
-        1 | _hyper_1_1_chunk_one_Partition_timeCustom_series_0_idx    |             1 | one_Partition_timeCustom_series_0_idx
-        1 | _hyper_1_1_chunk_one_Partition_timeCustom_series_1_idx    |             1 | one_Partition_timeCustom_series_1_idx
-        1 | _hyper_1_1_chunk_one_Partition_timeCustom_series_2_idx    |             1 | one_Partition_timeCustom_series_2_idx
-        1 | _hyper_1_1_chunk_one_Partition_timeCustom_series_bool_idx |             1 | one_Partition_timeCustom_series_bool_idx
-        1 | _hyper_1_1_chunk_one_Partition_timeCustom_idx             |             1 | one_Partition_timeCustom_idx
-        2 | _hyper_1_2_chunk_one_Partition_device_id_timeCustom_idx   |             1 | one_Partition_device_id_timeCustom_idx
-        2 | _hyper_1_2_chunk_one_Partition_timeCustom_series_0_idx    |             1 | one_Partition_timeCustom_series_0_idx
-        2 | _hyper_1_2_chunk_one_Partition_timeCustom_series_1_idx    |             1 | one_Partition_timeCustom_series_1_idx
-        2 | _hyper_1_2_chunk_one_Partition_timeCustom_series_2_idx    |             1 | one_Partition_timeCustom_series_2_idx
-        2 | _hyper_1_2_chunk_one_Partition_timeCustom_series_bool_idx |             1 | one_Partition_timeCustom_series_bool_idx
-        2 | _hyper_1_2_chunk_one_Partition_timeCustom_idx             |             1 | one_Partition_timeCustom_idx
-        3 | _hyper_1_3_chunk_one_Partition_device_id_timeCustom_idx   |             1 | one_Partition_device_id_timeCustom_idx
-        3 | _hyper_1_3_chunk_one_Partition_timeCustom_series_0_idx    |             1 | one_Partition_timeCustom_series_0_idx
-        3 | _hyper_1_3_chunk_one_Partition_timeCustom_series_1_idx    |             1 | one_Partition_timeCustom_series_1_idx
-        3 | _hyper_1_3_chunk_one_Partition_timeCustom_series_2_idx    |             1 | one_Partition_timeCustom_series_2_idx
-        3 | _hyper_1_3_chunk_one_Partition_timeCustom_series_bool_idx |             1 | one_Partition_timeCustom_series_bool_idx
-        3 | _hyper_1_3_chunk_one_Partition_timeCustom_idx             |             1 | one_Partition_timeCustom_idx
-        4 | _hyper_2_4_chunk_1dim_time_idx                            |             2 | 1dim_time_idx
-        5 | _hyper_3_5_chunk_Hypertable_1_time_Device_id_idx          |             3 | Hypertable_1_time_Device_id_idx
-        5 | _hyper_3_5_chunk_Hypertable_1_time_idx                    |             3 | Hypertable_1_time_idx
-        5 | _hyper_3_5_chunk_Hypertable_1_Device_id_time_idx          |             3 | Hypertable_1_Device_id_time_idx
-        5 | _hyper_3_5_chunk_Unique1                                  |             3 | Unique1
-        6 | _hyper_4_6_chunk_Hypertable_1_time_Device_id_idx          |             4 | Hypertable_1_time_Device_id_idx
-        6 | _hyper_4_6_chunk_Hypertable_1_time_idx                    |             4 | Hypertable_1_time_idx
-        6 | _hyper_4_6_chunk_Unique1                                  |             4 | Unique1
-        5 | _hyper_3_5_chunk_ind_humdity2                             |             3 | ind_humdity2
-(27 rows)
-
---create column with same name as previously renamed one
-ALTER TABLE PUBLIC."Hypertable_1" ADD COLUMN sensor_3 BIGINT NOT NULL DEFAULT 131;
---create column with same name as previously dropped one
-ALTER TABLE PUBLIC."Hypertable_1" ADD COLUMN sensor_4 BIGINT NOT NULL DEFAULT 131;
---test proper denials for all security definer functions:
-\c :TEST_DBNAME :ROLE_SUPERUSER
-CREATE TABLE plain_table_su (time timestamp, temp float);
-CREATE TABLE hypertable_su (time timestamp, temp float);
-SELECT create_hypertable('hypertable_su', 'time');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-      create_hypertable      
------------------------------
- (11,public,hypertable_su,t)
-(1 row)
-
-CREATE INDEX "ind_1" ON hypertable_su (time);
-INSERT INTO hypertable_su VALUES('2017-01-20T09:00:01', 22.5);
-\c :TEST_DBNAME :ROLE_DEFAULT_PERM_USER_2
---all of the following should produce errors
-\set ON_ERROR_STOP 0
-SELECT create_hypertable('plain_table_su', 'time');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-ERROR:  must be owner of hypertable "plain_table_su"
-CREATE INDEX ON plain_table_su (time, temp);
-ERROR:  must be owner of table plain_table_su
-CREATE INDEX ON hypertable_su (time, temp);
-ERROR:  must be owner of hypertable "hypertable_su"
-DROP INDEX "ind_1";
-ERROR:  must be owner of index ind_1
-ALTER INDEX "ind_1" RENAME TO "ind_2";
-ERROR:  must be owner of index ind_1
-\set ON_ERROR_STOP 1
---test that I can't do anything to a non-owned hypertable.
-\set ON_ERROR_STOP 0
-CREATE INDEX ON hypertable_su (time, temp);
-ERROR:  must be owner of hypertable "hypertable_su"
-SELECT * FROM hypertable_su;
-ERROR:  permission denied for table hypertable_su
-INSERT INTO hypertable_su VALUES('2017-01-20T09:00:01', 22.5);
-ERROR:  permission denied for table hypertable_su
-ALTER TABLE hypertable_su ADD COLUMN val2 integer;
-ERROR:  must be owner of table hypertable_su
-\set ON_ERROR_STOP 1
---grant read permissions
-\c :TEST_DBNAME :ROLE_SUPERUSER
-GRANT SELECT ON hypertable_su TO :ROLE_DEFAULT_PERM_USER_2;
-\c :TEST_DBNAME :ROLE_DEFAULT_PERM_USER_2
-SELECT * FROM hypertable_su;
-           time           | temp 
---------------------------+------
- Fri Jan 20 09:00:01 2017 | 22.5
-(1 row)
-
-\set ON_ERROR_STOP 0
-CREATE INDEX ON hypertable_su (time, temp);
-ERROR:  must be owner of hypertable "hypertable_su"
-INSERT INTO hypertable_su VALUES('2017-01-20T09:00:01', 22.5);
-ERROR:  permission denied for table hypertable_su
-ALTER TABLE hypertable_su ADD COLUMN val2 integer;
-ERROR:  must be owner of table hypertable_su
-\set ON_ERROR_STOP 1
---grant read, insert permissions
-\c :TEST_DBNAME :ROLE_SUPERUSER
-GRANT SELECT, INSERT ON hypertable_su TO :ROLE_DEFAULT_PERM_USER_2;
-\c :TEST_DBNAME :ROLE_DEFAULT_PERM_USER_2
-INSERT INTO hypertable_su VALUES('2017-01-20T09:00:01', 22.5);
-SELECT * FROM hypertable_su;
-           time           | temp 
---------------------------+------
- Fri Jan 20 09:00:01 2017 | 22.5
- Fri Jan 20 09:00:01 2017 | 22.5
-(2 rows)
-
-\set ON_ERROR_STOP 0
-CREATE INDEX ON hypertable_su (time, temp);
-ERROR:  must be owner of hypertable "hypertable_su"
-ALTER TABLE hypertable_su ADD COLUMN val2 integer;
-ERROR:  must be owner of table hypertable_su
-\set ON_ERROR_STOP 1
---change owner
-\c :TEST_DBNAME :ROLE_SUPERUSER
-ALTER TABLE hypertable_su OWNER TO :ROLE_DEFAULT_PERM_USER_2;
-\c :TEST_DBNAME :ROLE_DEFAULT_PERM_USER_2
-INSERT INTO hypertable_su VALUES('2017-01-20T09:00:01', 22.5);
-SELECT * FROM hypertable_su;
-           time           | temp 
---------------------------+------
- Fri Jan 20 09:00:01 2017 | 22.5
- Fri Jan 20 09:00:01 2017 | 22.5
- Fri Jan 20 09:00:01 2017 | 22.5
-(3 rows)
-
-CREATE INDEX ON hypertable_su (time, temp);
-ALTER TABLE hypertable_su ADD COLUMN val2 integer;
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/chunk_utils.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/chunk_utils.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/chunk_utils.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/chunk_utils.out	2023-11-25 05:27:13.513111553 +0000
@@ -1,1468 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
--- Set this variable to avoid using a hard-coded path each time query
--- results are compared
-\set QUERY_RESULT_TEST_EQUAL_RELPATH 'include/query_result_test_equal.sql'
-CREATE OR REPLACE FUNCTION dimension_get_time(
-    hypertable_id INT
-)
-    RETURNS _timescaledb_catalog.dimension LANGUAGE SQL STABLE AS
-$BODY$
-    SELECT *
-    FROM _timescaledb_catalog.dimension d
-    WHERE d.hypertable_id = dimension_get_time.hypertable_id AND
-          d.interval_length IS NOT NULL
-$BODY$;
-CREATE TABLE PUBLIC.drop_chunk_test1(time bigint, temp float8, device_id text);
-CREATE TABLE PUBLIC.drop_chunk_test2(time bigint, temp float8, device_id text);
-CREATE TABLE PUBLIC.drop_chunk_test3(time bigint, temp float8, device_id text);
-CREATE INDEX ON drop_chunk_test1(time DESC);
--- show_chunks() without specifying a table is not allowed
-\set ON_ERROR_STOP 0
-SELECT show_chunks(NULL);
-ERROR:  invalid hypertable or continuous aggregate
-\set ON_ERROR_STOP 1
-SELECT create_hypertable('public.drop_chunk_test1', 'time', chunk_time_interval => 1, create_default_indexes=>false);
-NOTICE:  adding not-null constraint to column "time"
-       create_hypertable       
--------------------------------
- (1,public,drop_chunk_test1,t)
-(1 row)
-
-SELECT create_hypertable('public.drop_chunk_test2', 'time', chunk_time_interval => 1, create_default_indexes=>false);
-NOTICE:  adding not-null constraint to column "time"
-       create_hypertable       
--------------------------------
- (2,public,drop_chunk_test2,t)
-(1 row)
-
-SELECT create_hypertable('public.drop_chunk_test3', 'time', chunk_time_interval => 1, create_default_indexes=>false);
-NOTICE:  adding not-null constraint to column "time"
-       create_hypertable       
--------------------------------
- (3,public,drop_chunk_test3,t)
-(1 row)
-
--- Add space dimensions to ensure chunks share dimension slices
-SELECT add_dimension('public.drop_chunk_test1', 'device_id', 2);
-              add_dimension              
------------------------------------------
- (4,public,drop_chunk_test1,device_id,t)
-(1 row)
-
-SELECT add_dimension('public.drop_chunk_test2', 'device_id', 2);
-              add_dimension              
------------------------------------------
- (5,public,drop_chunk_test2,device_id,t)
-(1 row)
-
-SELECT add_dimension('public.drop_chunk_test3', 'device_id', 2);
-              add_dimension              
------------------------------------------
- (6,public,drop_chunk_test3,device_id,t)
-(1 row)
-
-SELECT c.id AS chunk_id, c.hypertable_id, c.schema_name AS chunk_schema, c.table_name AS chunk_table, ds.range_start, ds.range_end
-FROM _timescaledb_catalog.chunk c
-INNER JOIN _timescaledb_catalog.hypertable h ON (c.hypertable_id = h.id)
-INNER JOIN  dimension_get_time(h.id) time_dimension ON(true)
-INNER JOIN  _timescaledb_catalog.dimension_slice ds ON (ds.dimension_id = time_dimension.id)
-INNER JOIN  _timescaledb_catalog.chunk_constraint cc ON (cc.dimension_slice_id = ds.id AND cc.chunk_id = c.id)
-WHERE h.schema_name = 'public' AND (h.table_name = 'drop_chunk_test1' OR h.table_name = 'drop_chunk_test2')
-ORDER BY c.id;
- chunk_id | hypertable_id | chunk_schema | chunk_table | range_start | range_end 
-----------+---------------+--------------+-------------+-------------+-----------
-(0 rows)
-
-\dt "_timescaledb_internal"._hyper*
-      List of relations
- Schema | Name | Type | Owner 
---------+------+------+-------
-(0 rows)
-
-SELECT  _timescaledb_internal.get_partition_for_key('dev1'::text);
- get_partition_for_key 
------------------------
-            1129986420
-(1 row)
-
-SELECT  _timescaledb_internal.get_partition_for_key('dev7'::varchar(5));
- get_partition_for_key 
------------------------
-             449729092
-(1 row)
-
-INSERT INTO PUBLIC.drop_chunk_test1 VALUES(1, 1.0, 'dev1');
-INSERT INTO PUBLIC.drop_chunk_test1 VALUES(2, 2.0, 'dev1');
-INSERT INTO PUBLIC.drop_chunk_test1 VALUES(3, 3.0, 'dev1');
-INSERT INTO PUBLIC.drop_chunk_test1 VALUES(4, 4.0, 'dev7');
-INSERT INTO PUBLIC.drop_chunk_test1 VALUES(5, 5.0, 'dev7');
-INSERT INTO PUBLIC.drop_chunk_test1 VALUES(6, 6.0, 'dev7');
-INSERT INTO PUBLIC.drop_chunk_test2 VALUES(1, 1.0, 'dev1');
-INSERT INTO PUBLIC.drop_chunk_test2 VALUES(2, 2.0, 'dev1');
-INSERT INTO PUBLIC.drop_chunk_test2 VALUES(3, 3.0, 'dev1');
-INSERT INTO PUBLIC.drop_chunk_test2 VALUES(4, 4.0, 'dev7');
-INSERT INTO PUBLIC.drop_chunk_test2 VALUES(5, 5.0, 'dev7');
-INSERT INTO PUBLIC.drop_chunk_test2 VALUES(6, 6.0, 'dev7');
-INSERT INTO PUBLIC.drop_chunk_test3 VALUES(1, 1.0, 'dev1');
-INSERT INTO PUBLIC.drop_chunk_test3 VALUES(2, 2.0, 'dev1');
-INSERT INTO PUBLIC.drop_chunk_test3 VALUES(3, 3.0, 'dev1');
-INSERT INTO PUBLIC.drop_chunk_test3 VALUES(4, 4.0, 'dev7');
-INSERT INTO PUBLIC.drop_chunk_test3 VALUES(5, 5.0, 'dev7');
-INSERT INTO PUBLIC.drop_chunk_test3 VALUES(6, 6.0, 'dev7');
-SELECT c.id AS chunk_id, c.hypertable_id, c.schema_name AS chunk_schema, c.table_name AS chunk_table, ds.range_start, ds.range_end
-FROM _timescaledb_catalog.chunk c
-INNER JOIN _timescaledb_catalog.hypertable h ON (c.hypertable_id = h.id)
-INNER JOIN  dimension_get_time(h.id) time_dimension ON(true)
-INNER JOIN  _timescaledb_catalog.dimension_slice ds ON (ds.dimension_id = time_dimension.id)
-INNER JOIN  _timescaledb_catalog.chunk_constraint cc ON (cc.dimension_slice_id = ds.id AND cc.chunk_id = c.id)
-WHERE h.schema_name = 'public' AND (h.table_name = 'drop_chunk_test1' OR h.table_name = 'drop_chunk_test2')
-ORDER BY c.id;
- chunk_id | hypertable_id |     chunk_schema      |    chunk_table    | range_start | range_end 
-----------+---------------+-----------------------+-------------------+-------------+-----------
-        1 |             1 | _timescaledb_internal | _hyper_1_1_chunk  |           1 |         2
-        2 |             1 | _timescaledb_internal | _hyper_1_2_chunk  |           2 |         3
-        3 |             1 | _timescaledb_internal | _hyper_1_3_chunk  |           3 |         4
-        4 |             1 | _timescaledb_internal | _hyper_1_4_chunk  |           4 |         5
-        5 |             1 | _timescaledb_internal | _hyper_1_5_chunk  |           5 |         6
-        6 |             1 | _timescaledb_internal | _hyper_1_6_chunk  |           6 |         7
-        7 |             2 | _timescaledb_internal | _hyper_2_7_chunk  |           1 |         2
-        8 |             2 | _timescaledb_internal | _hyper_2_8_chunk  |           2 |         3
-        9 |             2 | _timescaledb_internal | _hyper_2_9_chunk  |           3 |         4
-       10 |             2 | _timescaledb_internal | _hyper_2_10_chunk |           4 |         5
-       11 |             2 | _timescaledb_internal | _hyper_2_11_chunk |           5 |         6
-       12 |             2 | _timescaledb_internal | _hyper_2_12_chunk |           6 |         7
-(12 rows)
-
-\dt "_timescaledb_internal"._hyper*
-                           List of relations
-        Schema         |       Name        | Type  |       Owner       
------------------------+-------------------+-------+-------------------
- _timescaledb_internal | _hyper_1_1_chunk  | table | default_perm_user
- _timescaledb_internal | _hyper_1_2_chunk  | table | default_perm_user
- _timescaledb_internal | _hyper_1_3_chunk  | table | default_perm_user
- _timescaledb_internal | _hyper_1_4_chunk  | table | default_perm_user
- _timescaledb_internal | _hyper_1_5_chunk  | table | default_perm_user
- _timescaledb_internal | _hyper_1_6_chunk  | table | default_perm_user
- _timescaledb_internal | _hyper_2_10_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_2_11_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_2_12_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_2_7_chunk  | table | default_perm_user
- _timescaledb_internal | _hyper_2_8_chunk  | table | default_perm_user
- _timescaledb_internal | _hyper_2_9_chunk  | table | default_perm_user
- _timescaledb_internal | _hyper_3_13_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_3_14_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_3_15_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_3_16_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_3_17_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_3_18_chunk | table | default_perm_user
-(18 rows)
-
--- next two calls of show_chunks should give same set of chunks as above when combined
-SELECT show_chunks('drop_chunk_test1');
-              show_chunks               
-----------------------------------------
- _timescaledb_internal._hyper_1_1_chunk
- _timescaledb_internal._hyper_1_2_chunk
- _timescaledb_internal._hyper_1_3_chunk
- _timescaledb_internal._hyper_1_4_chunk
- _timescaledb_internal._hyper_1_5_chunk
- _timescaledb_internal._hyper_1_6_chunk
-(6 rows)
-
-SELECT * FROM show_chunks('drop_chunk_test2');
-               show_chunks               
------------------------------------------
- _timescaledb_internal._hyper_2_7_chunk
- _timescaledb_internal._hyper_2_8_chunk
- _timescaledb_internal._hyper_2_9_chunk
- _timescaledb_internal._hyper_2_10_chunk
- _timescaledb_internal._hyper_2_11_chunk
- _timescaledb_internal._hyper_2_12_chunk
-(6 rows)
-
-CREATE VIEW dependent_view AS SELECT * FROM _timescaledb_internal._hyper_1_1_chunk;
-\set ON_ERROR_STOP 0
-SELECT drop_chunks('drop_chunk_test1');
-ERROR:  invalid time range for dropping chunks
-SELECT drop_chunks('drop_chunk_test1', older_than => 2);
-ERROR:  cannot drop table _timescaledb_internal._hyper_1_1_chunk because other objects depend on it
-SELECT drop_chunks('drop_chunk_test1', older_than => NULL::interval);
-ERROR:  invalid time range for dropping chunks
-SELECT drop_chunks('drop_chunk_test1', older_than => NULL::int);
-ERROR:  invalid time range for dropping chunks
-DROP VIEW dependent_view;
--- should error because wrong time type
-SELECT drop_chunks('drop_chunk_test1', older_than => now());
-ERROR:  invalid time argument type "timestamp with time zone"
-SELECT show_chunks('drop_chunk_test3', now());
-ERROR:  invalid time argument type "timestamp with time zone"
--- should error because of wrong relative order of time constraints
-SELECT show_chunks('drop_chunk_test1', older_than=>3, newer_than=>4);
-ERROR:  invalid time range
--- Should error because NULL was used for the table name.
-SELECT drop_chunks(NULL, older_than => 3);
-ERROR:  invalid hypertable or continuous aggregate
--- should error because there is no relation with that OID.
-SELECT drop_chunks(3533, older_than => 3);
-ERROR:  invalid hypertable or continuous aggregate
-\set ON_ERROR_STOP 1
--- show created constraints and dimension slices for each chunk
-SELECT c.table_name, cc.constraint_name, ds.id AS dimension_slice_id, ds.range_start, ds.range_end
-FROM _timescaledb_catalog.chunk c
-INNER JOIN _timescaledb_catalog.chunk_constraint cc ON (c.id = cc.chunk_id)
-FULL OUTER JOIN _timescaledb_catalog.dimension_slice ds ON (ds.id = cc.dimension_slice_id)
-ORDER BY c.id;
-    table_name     | constraint_name | dimension_slice_id |     range_start      |      range_end      
--------------------+-----------------+--------------------+----------------------+---------------------
- _hyper_1_1_chunk  | constraint_1    |                  1 |                    1 |                   2
- _hyper_1_1_chunk  | constraint_2    |                  2 |           1073741823 | 9223372036854775807
- _hyper_1_2_chunk  | constraint_3    |                  3 |                    2 |                   3
- _hyper_1_2_chunk  | constraint_2    |                  2 |           1073741823 | 9223372036854775807
- _hyper_1_3_chunk  | constraint_4    |                  4 |                    3 |                   4
- _hyper_1_3_chunk  | constraint_2    |                  2 |           1073741823 | 9223372036854775807
- _hyper_1_4_chunk  | constraint_5    |                  5 |                    4 |                   5
- _hyper_1_4_chunk  | constraint_6    |                  6 | -9223372036854775808 |          1073741823
- _hyper_1_5_chunk  | constraint_7    |                  7 |                    5 |                   6
- _hyper_1_5_chunk  | constraint_6    |                  6 | -9223372036854775808 |          1073741823
- _hyper_1_6_chunk  | constraint_8    |                  8 |                    6 |                   7
- _hyper_1_6_chunk  | constraint_6    |                  6 | -9223372036854775808 |          1073741823
- _hyper_2_7_chunk  | constraint_9    |                  9 |                    1 |                   2
- _hyper_2_7_chunk  | constraint_10   |                 10 |           1073741823 | 9223372036854775807
- _hyper_2_8_chunk  | constraint_11   |                 11 |                    2 |                   3
- _hyper_2_8_chunk  | constraint_10   |                 10 |           1073741823 | 9223372036854775807
- _hyper_2_9_chunk  | constraint_12   |                 12 |                    3 |                   4
- _hyper_2_9_chunk  | constraint_10   |                 10 |           1073741823 | 9223372036854775807
- _hyper_2_10_chunk | constraint_13   |                 13 |                    4 |                   5
- _hyper_2_10_chunk | constraint_14   |                 14 | -9223372036854775808 |          1073741823
- _hyper_2_11_chunk | constraint_15   |                 15 |                    5 |                   6
- _hyper_2_11_chunk | constraint_14   |                 14 | -9223372036854775808 |          1073741823
- _hyper_2_12_chunk | constraint_16   |                 16 |                    6 |                   7
- _hyper_2_12_chunk | constraint_14   |                 14 | -9223372036854775808 |          1073741823
- _hyper_3_13_chunk | constraint_17   |                 17 |                    1 |                   2
- _hyper_3_13_chunk | constraint_18   |                 18 |           1073741823 | 9223372036854775807
- _hyper_3_14_chunk | constraint_19   |                 19 |                    2 |                   3
- _hyper_3_14_chunk | constraint_18   |                 18 |           1073741823 | 9223372036854775807
- _hyper_3_15_chunk | constraint_20   |                 20 |                    3 |                   4
- _hyper_3_15_chunk | constraint_18   |                 18 |           1073741823 | 9223372036854775807
- _hyper_3_16_chunk | constraint_21   |                 21 |                    4 |                   5
- _hyper_3_16_chunk | constraint_22   |                 22 | -9223372036854775808 |          1073741823
- _hyper_3_17_chunk | constraint_23   |                 23 |                    5 |                   6
- _hyper_3_17_chunk | constraint_22   |                 22 | -9223372036854775808 |          1073741823
- _hyper_3_18_chunk | constraint_24   |                 24 |                    6 |                   7
- _hyper_3_18_chunk | constraint_22   |                 22 | -9223372036854775808 |          1073741823
-(36 rows)
-
-SELECT * FROM _timescaledb_catalog.dimension_slice ORDER BY id;
- id | dimension_id |     range_start      |      range_end      
-----+--------------+----------------------+---------------------
-  1 |            1 |                    1 |                   2
-  2 |            4 |           1073741823 | 9223372036854775807
-  3 |            1 |                    2 |                   3
-  4 |            1 |                    3 |                   4
-  5 |            1 |                    4 |                   5
-  6 |            4 | -9223372036854775808 |          1073741823
-  7 |            1 |                    5 |                   6
-  8 |            1 |                    6 |                   7
-  9 |            2 |                    1 |                   2
- 10 |            5 |           1073741823 | 9223372036854775807
- 11 |            2 |                    2 |                   3
- 12 |            2 |                    3 |                   4
- 13 |            2 |                    4 |                   5
- 14 |            5 | -9223372036854775808 |          1073741823
- 15 |            2 |                    5 |                   6
- 16 |            2 |                    6 |                   7
- 17 |            3 |                    1 |                   2
- 18 |            6 |           1073741823 | 9223372036854775807
- 19 |            3 |                    2 |                   3
- 20 |            3 |                    3 |                   4
- 21 |            3 |                    4 |                   5
- 22 |            6 | -9223372036854775808 |          1073741823
- 23 |            3 |                    5 |                   6
- 24 |            3 |                    6 |                   7
-(24 rows)
-
--- Test that truncating chunks works
-SELECT count(*) FROM _timescaledb_internal._hyper_2_7_chunk;
- count 
--------
-     1
-(1 row)
-
-TRUNCATE TABLE _timescaledb_internal._hyper_2_7_chunk;
-SELECT count(*) FROM _timescaledb_internal._hyper_2_7_chunk;
- count 
--------
-     0
-(1 row)
-
--- Drop one chunk "manually" and verify that dimension slices and
--- constraints are cleaned up. Each chunk has two constraints and two
--- dimension slices. Both constraints should be deleted, but only one
--- slice should be deleted since the space-dimension slice is shared
--- with other chunks in the same hypertable
-DROP TABLE _timescaledb_internal._hyper_2_7_chunk;
--- Two constraints deleted compared to above
-SELECT c.table_name, cc.constraint_name, ds.id AS dimension_slice_id, ds.range_start, ds.range_end
-FROM _timescaledb_catalog.chunk c
-INNER JOIN _timescaledb_catalog.chunk_constraint cc ON (c.id = cc.chunk_id)
-FULL OUTER JOIN _timescaledb_catalog.dimension_slice ds ON (ds.id = cc.dimension_slice_id)
-ORDER BY c.id;
-    table_name     | constraint_name | dimension_slice_id |     range_start      |      range_end      
--------------------+-----------------+--------------------+----------------------+---------------------
- _hyper_1_1_chunk  | constraint_1    |                  1 |                    1 |                   2
- _hyper_1_1_chunk  | constraint_2    |                  2 |           1073741823 | 9223372036854775807
- _hyper_1_2_chunk  | constraint_3    |                  3 |                    2 |                   3
- _hyper_1_2_chunk  | constraint_2    |                  2 |           1073741823 | 9223372036854775807
- _hyper_1_3_chunk  | constraint_4    |                  4 |                    3 |                   4
- _hyper_1_3_chunk  | constraint_2    |                  2 |           1073741823 | 9223372036854775807
- _hyper_1_4_chunk  | constraint_5    |                  5 |                    4 |                   5
- _hyper_1_4_chunk  | constraint_6    |                  6 | -9223372036854775808 |          1073741823
- _hyper_1_5_chunk  | constraint_7    |                  7 |                    5 |                   6
- _hyper_1_5_chunk  | constraint_6    |                  6 | -9223372036854775808 |          1073741823
- _hyper_1_6_chunk  | constraint_8    |                  8 |                    6 |                   7
- _hyper_1_6_chunk  | constraint_6    |                  6 | -9223372036854775808 |          1073741823
- _hyper_2_8_chunk  | constraint_11   |                 11 |                    2 |                   3
- _hyper_2_8_chunk  | constraint_10   |                 10 |           1073741823 | 9223372036854775807
- _hyper_2_9_chunk  | constraint_12   |                 12 |                    3 |                   4
- _hyper_2_9_chunk  | constraint_10   |                 10 |           1073741823 | 9223372036854775807
- _hyper_2_10_chunk | constraint_13   |                 13 |                    4 |                   5
- _hyper_2_10_chunk | constraint_14   |                 14 | -9223372036854775808 |          1073741823
- _hyper_2_11_chunk | constraint_15   |                 15 |                    5 |                   6
- _hyper_2_11_chunk | constraint_14   |                 14 | -9223372036854775808 |          1073741823
- _hyper_2_12_chunk | constraint_16   |                 16 |                    6 |                   7
- _hyper_2_12_chunk | constraint_14   |                 14 | -9223372036854775808 |          1073741823
- _hyper_3_13_chunk | constraint_17   |                 17 |                    1 |                   2
- _hyper_3_13_chunk | constraint_18   |                 18 |           1073741823 | 9223372036854775807
- _hyper_3_14_chunk | constraint_19   |                 19 |                    2 |                   3
- _hyper_3_14_chunk | constraint_18   |                 18 |           1073741823 | 9223372036854775807
- _hyper_3_15_chunk | constraint_20   |                 20 |                    3 |                   4
- _hyper_3_15_chunk | constraint_18   |                 18 |           1073741823 | 9223372036854775807
- _hyper_3_16_chunk | constraint_21   |                 21 |                    4 |                   5
- _hyper_3_16_chunk | constraint_22   |                 22 | -9223372036854775808 |          1073741823
- _hyper_3_17_chunk | constraint_23   |                 23 |                    5 |                   6
- _hyper_3_17_chunk | constraint_22   |                 22 | -9223372036854775808 |          1073741823
- _hyper_3_18_chunk | constraint_24   |                 24 |                    6 |                   7
- _hyper_3_18_chunk | constraint_22   |                 22 | -9223372036854775808 |          1073741823
-(34 rows)
-
--- Only one dimension slice deleted
-SELECT * FROM _timescaledb_catalog.dimension_slice ORDER BY id;
- id | dimension_id |     range_start      |      range_end      
-----+--------------+----------------------+---------------------
-  1 |            1 |                    1 |                   2
-  2 |            4 |           1073741823 | 9223372036854775807
-  3 |            1 |                    2 |                   3
-  4 |            1 |                    3 |                   4
-  5 |            1 |                    4 |                   5
-  6 |            4 | -9223372036854775808 |          1073741823
-  7 |            1 |                    5 |                   6
-  8 |            1 |                    6 |                   7
- 10 |            5 |           1073741823 | 9223372036854775807
- 11 |            2 |                    2 |                   3
- 12 |            2 |                    3 |                   4
- 13 |            2 |                    4 |                   5
- 14 |            5 | -9223372036854775808 |          1073741823
- 15 |            2 |                    5 |                   6
- 16 |            2 |                    6 |                   7
- 17 |            3 |                    1 |                   2
- 18 |            6 |           1073741823 | 9223372036854775807
- 19 |            3 |                    2 |                   3
- 20 |            3 |                    3 |                   4
- 21 |            3 |                    4 |                   5
- 22 |            6 | -9223372036854775808 |          1073741823
- 23 |            3 |                    5 |                   6
- 24 |            3 |                    6 |                   7
-(23 rows)
-
--- We drop all chunks older than timestamp 2 in all hypertable. This
--- is added only to avoid making the diff for this commit larger than
--- necessary and make reviews easier.
-SELECT drop_chunks(format('%1$I.%2$I', schema_name, table_name)::regclass, older_than => 2)
-  FROM _timescaledb_catalog.hypertable;
-               drop_chunks               
------------------------------------------
- _timescaledb_internal._hyper_1_1_chunk
- _timescaledb_internal._hyper_3_13_chunk
-(2 rows)
-
-SELECT c.table_name, cc.constraint_name, ds.id AS dimension_slice_id, ds.range_start, ds.range_end
-FROM _timescaledb_catalog.chunk c
-INNER JOIN _timescaledb_catalog.chunk_constraint cc ON (c.id = cc.chunk_id)
-FULL OUTER JOIN _timescaledb_catalog.dimension_slice ds ON (ds.id = cc.dimension_slice_id)
-ORDER BY c.id;
-    table_name     | constraint_name | dimension_slice_id |     range_start      |      range_end      
--------------------+-----------------+--------------------+----------------------+---------------------
- _hyper_1_2_chunk  | constraint_3    |                  3 |                    2 |                   3
- _hyper_1_2_chunk  | constraint_2    |                  2 |           1073741823 | 9223372036854775807
- _hyper_1_3_chunk  | constraint_4    |                  4 |                    3 |                   4
- _hyper_1_3_chunk  | constraint_2    |                  2 |           1073741823 | 9223372036854775807
- _hyper_1_4_chunk  | constraint_5    |                  5 |                    4 |                   5
- _hyper_1_4_chunk  | constraint_6    |                  6 | -9223372036854775808 |          1073741823
- _hyper_1_5_chunk  | constraint_7    |                  7 |                    5 |                   6
- _hyper_1_5_chunk  | constraint_6    |                  6 | -9223372036854775808 |          1073741823
- _hyper_1_6_chunk  | constraint_8    |                  8 |                    6 |                   7
- _hyper_1_6_chunk  | constraint_6    |                  6 | -9223372036854775808 |          1073741823
- _hyper_2_8_chunk  | constraint_11   |                 11 |                    2 |                   3
- _hyper_2_8_chunk  | constraint_10   |                 10 |           1073741823 | 9223372036854775807
- _hyper_2_9_chunk  | constraint_12   |                 12 |                    3 |                   4
- _hyper_2_9_chunk  | constraint_10   |                 10 |           1073741823 | 9223372036854775807
- _hyper_2_10_chunk | constraint_13   |                 13 |                    4 |                   5
- _hyper_2_10_chunk | constraint_14   |                 14 | -9223372036854775808 |          1073741823
- _hyper_2_11_chunk | constraint_15   |                 15 |                    5 |                   6
- _hyper_2_11_chunk | constraint_14   |                 14 | -9223372036854775808 |          1073741823
- _hyper_2_12_chunk | constraint_16   |                 16 |                    6 |                   7
- _hyper_2_12_chunk | constraint_14   |                 14 | -9223372036854775808 |          1073741823
- _hyper_3_14_chunk | constraint_19   |                 19 |                    2 |                   3
- _hyper_3_14_chunk | constraint_18   |                 18 |           1073741823 | 9223372036854775807
- _hyper_3_15_chunk | constraint_20   |                 20 |                    3 |                   4
- _hyper_3_15_chunk | constraint_18   |                 18 |           1073741823 | 9223372036854775807
- _hyper_3_16_chunk | constraint_21   |                 21 |                    4 |                   5
- _hyper_3_16_chunk | constraint_22   |                 22 | -9223372036854775808 |          1073741823
- _hyper_3_17_chunk | constraint_23   |                 23 |                    5 |                   6
- _hyper_3_17_chunk | constraint_22   |                 22 | -9223372036854775808 |          1073741823
- _hyper_3_18_chunk | constraint_24   |                 24 |                    6 |                   7
- _hyper_3_18_chunk | constraint_22   |                 22 | -9223372036854775808 |          1073741823
-(30 rows)
-
-SELECT * FROM _timescaledb_catalog.dimension_slice ORDER BY id;
- id | dimension_id |     range_start      |      range_end      
-----+--------------+----------------------+---------------------
-  2 |            4 |           1073741823 | 9223372036854775807
-  3 |            1 |                    2 |                   3
-  4 |            1 |                    3 |                   4
-  5 |            1 |                    4 |                   5
-  6 |            4 | -9223372036854775808 |          1073741823
-  7 |            1 |                    5 |                   6
-  8 |            1 |                    6 |                   7
- 10 |            5 |           1073741823 | 9223372036854775807
- 11 |            2 |                    2 |                   3
- 12 |            2 |                    3 |                   4
- 13 |            2 |                    4 |                   5
- 14 |            5 | -9223372036854775808 |          1073741823
- 15 |            2 |                    5 |                   6
- 16 |            2 |                    6 |                   7
- 18 |            6 |           1073741823 | 9223372036854775807
- 19 |            3 |                    2 |                   3
- 20 |            3 |                    3 |                   4
- 21 |            3 |                    4 |                   5
- 22 |            6 | -9223372036854775808 |          1073741823
- 23 |            3 |                    5 |                   6
- 24 |            3 |                    6 |                   7
-(21 rows)
-
-SELECT c.id AS chunk_id, c.hypertable_id, c.schema_name AS chunk_schema, c.table_name AS chunk_table, ds.range_start, ds.range_end
-FROM _timescaledb_catalog.chunk c
-INNER JOIN _timescaledb_catalog.hypertable h ON (c.hypertable_id = h.id)
-INNER JOIN  dimension_get_time(h.id) time_dimension ON(true)
-INNER JOIN  _timescaledb_catalog.dimension_slice ds ON (ds.dimension_id = time_dimension.id)
-INNER JOIN  _timescaledb_catalog.chunk_constraint cc ON (cc.dimension_slice_id = ds.id AND cc.chunk_id = c.id)
-WHERE h.schema_name = 'public' AND (h.table_name = 'drop_chunk_test1' OR h.table_name = 'drop_chunk_test2')
-ORDER BY c.id;
- chunk_id | hypertable_id |     chunk_schema      |    chunk_table    | range_start | range_end 
-----------+---------------+-----------------------+-------------------+-------------+-----------
-        2 |             1 | _timescaledb_internal | _hyper_1_2_chunk  |           2 |         3
-        3 |             1 | _timescaledb_internal | _hyper_1_3_chunk  |           3 |         4
-        4 |             1 | _timescaledb_internal | _hyper_1_4_chunk  |           4 |         5
-        5 |             1 | _timescaledb_internal | _hyper_1_5_chunk  |           5 |         6
-        6 |             1 | _timescaledb_internal | _hyper_1_6_chunk  |           6 |         7
-        8 |             2 | _timescaledb_internal | _hyper_2_8_chunk  |           2 |         3
-        9 |             2 | _timescaledb_internal | _hyper_2_9_chunk  |           3 |         4
-       10 |             2 | _timescaledb_internal | _hyper_2_10_chunk |           4 |         5
-       11 |             2 | _timescaledb_internal | _hyper_2_11_chunk |           5 |         6
-       12 |             2 | _timescaledb_internal | _hyper_2_12_chunk |           6 |         7
-(10 rows)
-
--- next two calls of show_chunks should give same set of chunks as above when combined
-SELECT show_chunks('drop_chunk_test1');
-              show_chunks               
-----------------------------------------
- _timescaledb_internal._hyper_1_2_chunk
- _timescaledb_internal._hyper_1_3_chunk
- _timescaledb_internal._hyper_1_4_chunk
- _timescaledb_internal._hyper_1_5_chunk
- _timescaledb_internal._hyper_1_6_chunk
-(5 rows)
-
-SELECT * FROM show_chunks('drop_chunk_test2');
-               show_chunks               
------------------------------------------
- _timescaledb_internal._hyper_2_8_chunk
- _timescaledb_internal._hyper_2_9_chunk
- _timescaledb_internal._hyper_2_10_chunk
- _timescaledb_internal._hyper_2_11_chunk
- _timescaledb_internal._hyper_2_12_chunk
-(5 rows)
-
-\dt "_timescaledb_internal"._hyper*
-                           List of relations
-        Schema         |       Name        | Type  |       Owner       
------------------------+-------------------+-------+-------------------
- _timescaledb_internal | _hyper_1_2_chunk  | table | default_perm_user
- _timescaledb_internal | _hyper_1_3_chunk  | table | default_perm_user
- _timescaledb_internal | _hyper_1_4_chunk  | table | default_perm_user
- _timescaledb_internal | _hyper_1_5_chunk  | table | default_perm_user
- _timescaledb_internal | _hyper_1_6_chunk  | table | default_perm_user
- _timescaledb_internal | _hyper_2_10_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_2_11_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_2_12_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_2_8_chunk  | table | default_perm_user
- _timescaledb_internal | _hyper_2_9_chunk  | table | default_perm_user
- _timescaledb_internal | _hyper_3_14_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_3_15_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_3_16_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_3_17_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_3_18_chunk | table | default_perm_user
-(15 rows)
-
--- show_chunks and drop_chunks output should be the same
-\set QUERY1 'SELECT show_chunks(\'drop_chunk_test1\', older_than => 3)::NAME'
-\set QUERY2 'SELECT drop_chunks(\'drop_chunk_test1\', older_than => 3)::NAME'
-\set ECHO errors
- Different Rows | Total Rows from Query 1 | Total Rows from Query 2 
-----------------+-------------------------+-------------------------
-              0 |                       1 |                       1
-(1 row)
-
-SELECT c.id AS chunk_id, c.hypertable_id, c.schema_name AS chunk_schema, c.table_name AS chunk_table, ds.range_start, ds.range_end
-FROM _timescaledb_catalog.chunk c
-INNER JOIN _timescaledb_catalog.hypertable h ON (c.hypertable_id = h.id)
-INNER JOIN  dimension_get_time(h.id) time_dimension ON(true)
-INNER JOIN  _timescaledb_catalog.dimension_slice ds ON (ds.dimension_id = time_dimension.id)
-INNER JOIN  _timescaledb_catalog.chunk_constraint cc ON (cc.dimension_slice_id = ds.id AND cc.chunk_id = c.id)
-WHERE h.schema_name = 'public' AND (h.table_name = 'drop_chunk_test1' OR h.table_name = 'drop_chunk_test2')
-ORDER BY c.id;
- chunk_id | hypertable_id |     chunk_schema      |    chunk_table    | range_start | range_end 
-----------+---------------+-----------------------+-------------------+-------------+-----------
-        3 |             1 | _timescaledb_internal | _hyper_1_3_chunk  |           3 |         4
-        4 |             1 | _timescaledb_internal | _hyper_1_4_chunk  |           4 |         5
-        5 |             1 | _timescaledb_internal | _hyper_1_5_chunk  |           5 |         6
-        6 |             1 | _timescaledb_internal | _hyper_1_6_chunk  |           6 |         7
-        8 |             2 | _timescaledb_internal | _hyper_2_8_chunk  |           2 |         3
-        9 |             2 | _timescaledb_internal | _hyper_2_9_chunk  |           3 |         4
-       10 |             2 | _timescaledb_internal | _hyper_2_10_chunk |           4 |         5
-       11 |             2 | _timescaledb_internal | _hyper_2_11_chunk |           5 |         6
-       12 |             2 | _timescaledb_internal | _hyper_2_12_chunk |           6 |         7
-(9 rows)
-
-\dt "_timescaledb_internal".*
-                             List of relations
-        Schema         |          Name          | Type  |       Owner       
------------------------+------------------------+-------+-------------------
- _timescaledb_internal | _hyper_1_3_chunk       | table | default_perm_user
- _timescaledb_internal | _hyper_1_4_chunk       | table | default_perm_user
- _timescaledb_internal | _hyper_1_5_chunk       | table | default_perm_user
- _timescaledb_internal | _hyper_1_6_chunk       | table | default_perm_user
- _timescaledb_internal | _hyper_2_10_chunk      | table | default_perm_user
- _timescaledb_internal | _hyper_2_11_chunk      | table | default_perm_user
- _timescaledb_internal | _hyper_2_12_chunk      | table | default_perm_user
- _timescaledb_internal | _hyper_2_8_chunk       | table | default_perm_user
- _timescaledb_internal | _hyper_2_9_chunk       | table | default_perm_user
- _timescaledb_internal | _hyper_3_14_chunk      | table | default_perm_user
- _timescaledb_internal | _hyper_3_15_chunk      | table | default_perm_user
- _timescaledb_internal | _hyper_3_16_chunk      | table | default_perm_user
- _timescaledb_internal | _hyper_3_17_chunk      | table | default_perm_user
- _timescaledb_internal | _hyper_3_18_chunk      | table | default_perm_user
- _timescaledb_internal | bgw_job_stat           | table | super_user
- _timescaledb_internal | bgw_policy_chunk_stats | table | super_user
- _timescaledb_internal | job_errors             | table | super_user
-(17 rows)
-
--- next two calls of show_chunks should give same set of chunks as above when combined
-SELECT show_chunks('drop_chunk_test1');
-              show_chunks               
-----------------------------------------
- _timescaledb_internal._hyper_1_3_chunk
- _timescaledb_internal._hyper_1_4_chunk
- _timescaledb_internal._hyper_1_5_chunk
- _timescaledb_internal._hyper_1_6_chunk
-(4 rows)
-
-SELECT * FROM show_chunks('drop_chunk_test2');
-               show_chunks               
------------------------------------------
- _timescaledb_internal._hyper_2_8_chunk
- _timescaledb_internal._hyper_2_9_chunk
- _timescaledb_internal._hyper_2_10_chunk
- _timescaledb_internal._hyper_2_11_chunk
- _timescaledb_internal._hyper_2_12_chunk
-(5 rows)
-
--- 2,147,483,647 is the largest int so this tests that BIGINTs work
--- show_chunks and drop_chunks output should be the same
-\set QUERY1 'SELECT show_chunks(\'drop_chunk_test3\', older_than => 2147483648)::NAME'
-\set QUERY2 'SELECT drop_chunks(\'drop_chunk_test3\', older_than => 2147483648)::NAME'
-\set ECHO errors
- Different Rows | Total Rows from Query 1 | Total Rows from Query 2 
-----------------+-------------------------+-------------------------
-              0 |                       5 |                       5
-(1 row)
-
-SELECT c.id AS chunk_id, c.hypertable_id, c.schema_name AS chunk_schema, c.table_name AS chunk_table, ds.range_start, ds.range_end
-FROM _timescaledb_catalog.chunk c
-INNER JOIN _timescaledb_catalog.hypertable h ON (c.hypertable_id = h.id)
-INNER JOIN  dimension_get_time(h.id) time_dimension ON(true)
-INNER JOIN  _timescaledb_catalog.dimension_slice ds ON (ds.dimension_id = time_dimension.id)
-INNER JOIN  _timescaledb_catalog.chunk_constraint cc ON (cc.dimension_slice_id = ds.id AND cc.chunk_id = c.id)
-WHERE h.schema_name = 'public' AND (h.table_name = 'drop_chunk_test1' OR h.table_name = 'drop_chunk_test2' OR h.table_name = 'drop_chunk_test3')
-ORDER BY c.id;
- chunk_id | hypertable_id |     chunk_schema      |    chunk_table    | range_start | range_end 
-----------+---------------+-----------------------+-------------------+-------------+-----------
-        3 |             1 | _timescaledb_internal | _hyper_1_3_chunk  |           3 |         4
-        4 |             1 | _timescaledb_internal | _hyper_1_4_chunk  |           4 |         5
-        5 |             1 | _timescaledb_internal | _hyper_1_5_chunk  |           5 |         6
-        6 |             1 | _timescaledb_internal | _hyper_1_6_chunk  |           6 |         7
-        8 |             2 | _timescaledb_internal | _hyper_2_8_chunk  |           2 |         3
-        9 |             2 | _timescaledb_internal | _hyper_2_9_chunk  |           3 |         4
-       10 |             2 | _timescaledb_internal | _hyper_2_10_chunk |           4 |         5
-       11 |             2 | _timescaledb_internal | _hyper_2_11_chunk |           5 |         6
-       12 |             2 | _timescaledb_internal | _hyper_2_12_chunk |           6 |         7
-(9 rows)
-
-\dt "_timescaledb_internal"._hyper*
-                           List of relations
-        Schema         |       Name        | Type  |       Owner       
------------------------+-------------------+-------+-------------------
- _timescaledb_internal | _hyper_1_3_chunk  | table | default_perm_user
- _timescaledb_internal | _hyper_1_4_chunk  | table | default_perm_user
- _timescaledb_internal | _hyper_1_5_chunk  | table | default_perm_user
- _timescaledb_internal | _hyper_1_6_chunk  | table | default_perm_user
- _timescaledb_internal | _hyper_2_10_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_2_11_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_2_12_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_2_8_chunk  | table | default_perm_user
- _timescaledb_internal | _hyper_2_9_chunk  | table | default_perm_user
-(9 rows)
-
-\set ON_ERROR_STOP 0
--- should error because no hypertable
-SELECT drop_chunks('drop_chunk_test4', older_than => 5);
-ERROR:  relation "drop_chunk_test4" does not exist at character 20
-SELECT show_chunks('drop_chunk_test4');
-ERROR:  relation "drop_chunk_test4" does not exist at character 20
-SELECT show_chunks('drop_chunk_test4', 5);
-ERROR:  relation "drop_chunk_test4" does not exist at character 20
-\set ON_ERROR_STOP 1
-DROP TABLE _timescaledb_internal._hyper_1_6_chunk;
-SELECT c.id AS chunk_id, c.hypertable_id, c.schema_name AS chunk_schema, c.table_name AS chunk_table, ds.range_start, ds.range_end
-FROM _timescaledb_catalog.chunk c
-INNER JOIN _timescaledb_catalog.hypertable h ON (c.hypertable_id = h.id)
-INNER JOIN  dimension_get_time(h.id) time_dimension ON(true)
-INNER JOIN  _timescaledb_catalog.dimension_slice ds ON (ds.dimension_id = time_dimension.id)
-INNER JOIN  _timescaledb_catalog.chunk_constraint cc ON (cc.dimension_slice_id = ds.id AND cc.chunk_id = c.id)
-WHERE h.schema_name = 'public' AND (h.table_name = 'drop_chunk_test1' OR h.table_name = 'drop_chunk_test2')
-ORDER BY c.id;
- chunk_id | hypertable_id |     chunk_schema      |    chunk_table    | range_start | range_end 
-----------+---------------+-----------------------+-------------------+-------------+-----------
-        3 |             1 | _timescaledb_internal | _hyper_1_3_chunk  |           3 |         4
-        4 |             1 | _timescaledb_internal | _hyper_1_4_chunk  |           4 |         5
-        5 |             1 | _timescaledb_internal | _hyper_1_5_chunk  |           5 |         6
-        8 |             2 | _timescaledb_internal | _hyper_2_8_chunk  |           2 |         3
-        9 |             2 | _timescaledb_internal | _hyper_2_9_chunk  |           3 |         4
-       10 |             2 | _timescaledb_internal | _hyper_2_10_chunk |           4 |         5
-       11 |             2 | _timescaledb_internal | _hyper_2_11_chunk |           5 |         6
-       12 |             2 | _timescaledb_internal | _hyper_2_12_chunk |           6 |         7
-(8 rows)
-
-\dt "_timescaledb_internal"._hyper*
-                           List of relations
-        Schema         |       Name        | Type  |       Owner       
------------------------+-------------------+-------+-------------------
- _timescaledb_internal | _hyper_1_3_chunk  | table | default_perm_user
- _timescaledb_internal | _hyper_1_4_chunk  | table | default_perm_user
- _timescaledb_internal | _hyper_1_5_chunk  | table | default_perm_user
- _timescaledb_internal | _hyper_2_10_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_2_11_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_2_12_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_2_8_chunk  | table | default_perm_user
- _timescaledb_internal | _hyper_2_9_chunk  | table | default_perm_user
-(8 rows)
-
--- newer_than tests
--- show_chunks and drop_chunks output should be the same
-\set QUERY1 'SELECT show_chunks(\'drop_chunk_test1\', newer_than=>5)::NAME'
-\set QUERY2 'SELECT drop_chunks(\'drop_chunk_test1\', newer_than=>5, verbose => true)::NAME'
-\set ECHO errors
-psql:include/query_result_test_equal.sql:16: INFO:  dropping chunk _timescaledb_internal._hyper_1_5_chunk
- Different Rows | Total Rows from Query 1 | Total Rows from Query 2 
-----------------+-------------------------+-------------------------
-              0 |                       1 |                       1
-(1 row)
-
-SELECT c.id AS chunk_id, c.hypertable_id, c.schema_name AS chunk_schema, c.table_name AS chunk_table, ds.range_start, ds.range_end
-FROM _timescaledb_catalog.chunk c
-INNER JOIN _timescaledb_catalog.hypertable h ON (c.hypertable_id = h.id)
-INNER JOIN  dimension_get_time(h.id) time_dimension ON(true)
-INNER JOIN  _timescaledb_catalog.dimension_slice ds ON (ds.dimension_id = time_dimension.id)
-INNER JOIN  _timescaledb_catalog.chunk_constraint cc ON (cc.dimension_slice_id = ds.id AND cc.chunk_id = c.id)
-WHERE h.schema_name = 'public' AND (h.table_name = 'drop_chunk_test1')
-ORDER BY c.id;
- chunk_id | hypertable_id |     chunk_schema      |   chunk_table    | range_start | range_end 
-----------+---------------+-----------------------+------------------+-------------+-----------
-        3 |             1 | _timescaledb_internal | _hyper_1_3_chunk |           3 |         4
-        4 |             1 | _timescaledb_internal | _hyper_1_4_chunk |           4 |         5
-(2 rows)
-
-SELECT show_chunks('drop_chunk_test1');
-              show_chunks               
-----------------------------------------
- _timescaledb_internal._hyper_1_3_chunk
- _timescaledb_internal._hyper_1_4_chunk
-(2 rows)
-
-\dt "_timescaledb_internal"._hyper*
-                           List of relations
-        Schema         |       Name        | Type  |       Owner       
------------------------+-------------------+-------+-------------------
- _timescaledb_internal | _hyper_1_3_chunk  | table | default_perm_user
- _timescaledb_internal | _hyper_1_4_chunk  | table | default_perm_user
- _timescaledb_internal | _hyper_2_10_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_2_11_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_2_12_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_2_8_chunk  | table | default_perm_user
- _timescaledb_internal | _hyper_2_9_chunk  | table | default_perm_user
-(7 rows)
-
--- show_chunks and drop_chunks output should be the same
-\set QUERY1 'SELECT show_chunks(\'drop_chunk_test1\', older_than=>4, newer_than=>3)::NAME'
-\set QUERY2 'SELECT drop_chunks(\'drop_chunk_test1\', older_than=>4, newer_than=>3)::NAME'
-\set ECHO errors
- Different Rows | Total Rows from Query 1 | Total Rows from Query 2 
-----------------+-------------------------+-------------------------
-              0 |                       1 |                       1
-(1 row)
-
-SELECT c.id AS chunk_id, c.hypertable_id, c.schema_name AS chunk_schema, c.table_name AS chunk_table, ds.range_start, ds.range_end
-FROM _timescaledb_catalog.chunk c
-INNER JOIN _timescaledb_catalog.hypertable h ON (c.hypertable_id = h.id)
-INNER JOIN  dimension_get_time(h.id) time_dimension ON(true)
-INNER JOIN  _timescaledb_catalog.dimension_slice ds ON (ds.dimension_id = time_dimension.id)
-INNER JOIN  _timescaledb_catalog.chunk_constraint cc ON (cc.dimension_slice_id = ds.id AND cc.chunk_id = c.id)
-WHERE h.schema_name = 'public' AND (h.table_name = 'drop_chunk_test1')
-ORDER BY c.id;
- chunk_id | hypertable_id |     chunk_schema      |   chunk_table    | range_start | range_end 
-----------+---------------+-----------------------+------------------+-------------+-----------
-        4 |             1 | _timescaledb_internal | _hyper_1_4_chunk |           4 |         5
-(1 row)
-
--- the call of show_chunks should give same set of chunks as above
-SELECT show_chunks('drop_chunk_test1');
-              show_chunks               
-----------------------------------------
- _timescaledb_internal._hyper_1_4_chunk
-(1 row)
-
-SELECT c.id AS chunk_id, c.hypertable_id, c.schema_name AS chunk_schema, c.table_name AS chunk_table, ds.range_start, ds.range_end
-FROM _timescaledb_catalog.chunk c
-INNER JOIN _timescaledb_catalog.hypertable h ON (c.hypertable_id = h.id)
-INNER JOIN  dimension_get_time(h.id) time_dimension ON(true)
-INNER JOIN  _timescaledb_catalog.dimension_slice ds ON (ds.dimension_id = time_dimension.id)
-INNER JOIN  _timescaledb_catalog.chunk_constraint cc ON (cc.dimension_slice_id = ds.id AND cc.chunk_id = c.id)
-WHERE h.schema_name = 'public'
-ORDER BY c.id;
- chunk_id | hypertable_id |     chunk_schema      |    chunk_table    | range_start | range_end 
-----------+---------------+-----------------------+-------------------+-------------+-----------
-        4 |             1 | _timescaledb_internal | _hyper_1_4_chunk  |           4 |         5
-        8 |             2 | _timescaledb_internal | _hyper_2_8_chunk  |           2 |         3
-        9 |             2 | _timescaledb_internal | _hyper_2_9_chunk  |           3 |         4
-       10 |             2 | _timescaledb_internal | _hyper_2_10_chunk |           4 |         5
-       11 |             2 | _timescaledb_internal | _hyper_2_11_chunk |           5 |         6
-       12 |             2 | _timescaledb_internal | _hyper_2_12_chunk |           6 |         7
-(6 rows)
-
-SELECT drop_chunks(format('%1$I.%2$I', schema_name, table_name)::regclass, older_than => 5, newer_than => 4)
-  FROM _timescaledb_catalog.hypertable WHERE schema_name = 'public';
-               drop_chunks               
------------------------------------------
- _timescaledb_internal._hyper_1_4_chunk
- _timescaledb_internal._hyper_2_10_chunk
-(2 rows)
-
-CREATE TABLE PUBLIC.drop_chunk_test_ts(time timestamp, temp float8, device_id text);
-SELECT create_hypertable('public.drop_chunk_test_ts', 'time', chunk_time_interval => interval '1 minute', create_default_indexes=>false);
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-        create_hypertable        
----------------------------------
- (4,public,drop_chunk_test_ts,t)
-(1 row)
-
-CREATE TABLE PUBLIC.drop_chunk_test_tstz(time timestamptz, temp float8, device_id text);
-SELECT create_hypertable('public.drop_chunk_test_tstz', 'time', chunk_time_interval => interval '1 minute', create_default_indexes=>false);
-NOTICE:  adding not-null constraint to column "time"
-         create_hypertable         
------------------------------------
- (5,public,drop_chunk_test_tstz,t)
-(1 row)
-
-SET timezone = '+1';
-INSERT INTO PUBLIC.drop_chunk_test_ts VALUES(now()-INTERVAL '5 minutes', 1.0, 'dev1');
-INSERT INTO PUBLIC.drop_chunk_test_ts VALUES(now()+INTERVAL '5 minutes', 1.0, 'dev1');
-INSERT INTO PUBLIC.drop_chunk_test_tstz VALUES(now()-INTERVAL '5 minutes', 1.0, 'dev1');
-INSERT INTO PUBLIC.drop_chunk_test_tstz VALUES(now()+INTERVAL '5 minutes', 1.0, 'dev1');
-SELECT * FROM test.show_subtables('drop_chunk_test_ts');
-                  Child                  | Tablespace 
------------------------------------------+------------
- _timescaledb_internal._hyper_4_19_chunk | 
- _timescaledb_internal._hyper_4_20_chunk | 
-(2 rows)
-
-SELECT * FROM test.show_subtables('drop_chunk_test_tstz');
-                  Child                  | Tablespace 
------------------------------------------+------------
- _timescaledb_internal._hyper_5_21_chunk | 
- _timescaledb_internal._hyper_5_22_chunk | 
-(2 rows)
-
-BEGIN;
-    SELECT show_chunks('drop_chunk_test_ts');
-               show_chunks               
------------------------------------------
- _timescaledb_internal._hyper_4_19_chunk
- _timescaledb_internal._hyper_4_20_chunk
-(2 rows)
-
-    SELECT show_chunks('drop_chunk_test_ts', now()::timestamp-interval '1 minute');
-               show_chunks               
------------------------------------------
- _timescaledb_internal._hyper_4_19_chunk
-(1 row)
-
--- show_chunks and drop_chunks output should be the same
-    \set QUERY1 'SELECT show_chunks(\'drop_chunk_test_ts\', newer_than => interval \'1 minute\')::NAME'
-    \set QUERY2 'SELECT drop_chunks(\'drop_chunk_test_ts\', newer_than => interval \'1 minute\')::NAME'
-    \set ECHO errors
- Different Rows | Total Rows from Query 1 | Total Rows from Query 2 
-----------------+-------------------------+-------------------------
-              0 |                       1 |                       1
-(1 row)
-
-    \set QUERY1 'SELECT show_chunks(\'drop_chunk_test_ts\', older_than => interval \'6 minute\')::NAME'
-    \set QUERY2 'SELECT drop_chunks(\'drop_chunk_test_ts\', older_than => interval \'6 minute\')::NAME'
-    \set ECHO errors
- Different Rows | Total Rows from Query 1 | Total Rows from Query 2 
-----------------+-------------------------+-------------------------
-              0 |                       0 |                       0
-(1 row)
-
-    SELECT * FROM test.show_subtables('drop_chunk_test_ts');
-                  Child                  | Tablespace 
------------------------------------------+------------
- _timescaledb_internal._hyper_4_19_chunk | 
-(1 row)
-
-    \set QUERY1 'SELECT show_chunks(\'drop_chunk_test_ts\', older_than => interval \'1 minute\')::NAME'
-    \set QUERY2 'SELECT drop_chunks(\'drop_chunk_test_ts\', interval \'1 minute\')::NAME'
-    \set ECHO errors
- Different Rows | Total Rows from Query 1 | Total Rows from Query 2 
-----------------+-------------------------+-------------------------
-              0 |                       1 |                       1
-(1 row)
-
-    SELECT * FROM test.show_subtables('drop_chunk_test_ts');
- Child | Tablespace 
--------+------------
-(0 rows)
-
-    SELECT show_chunks('drop_chunk_test_tstz');
-               show_chunks               
------------------------------------------
- _timescaledb_internal._hyper_5_21_chunk
- _timescaledb_internal._hyper_5_22_chunk
-(2 rows)
-
-    SELECT show_chunks('drop_chunk_test_tstz', older_than => now() - interval '1 minute', newer_than => now() - interval '6 minute');
-               show_chunks               
------------------------------------------
- _timescaledb_internal._hyper_5_21_chunk
-(1 row)
-
-    SELECT show_chunks('drop_chunk_test_tstz', newer_than => now() - interval '1 minute');
-               show_chunks               
------------------------------------------
- _timescaledb_internal._hyper_5_22_chunk
-(1 row)
-
-    SELECT show_chunks('drop_chunk_test_tstz', older_than => now() - interval '1 minute');
-               show_chunks               
------------------------------------------
- _timescaledb_internal._hyper_5_21_chunk
-(1 row)
-
-    \set QUERY1 'SELECT show_chunks(older_than => interval \'1 minute\', relation => \'drop_chunk_test_tstz\')::NAME'
-    \set QUERY2 'SELECT drop_chunks(\'drop_chunk_test_tstz\', interval \'1 minute\')::NAME'
-    \set ECHO errors
- Different Rows | Total Rows from Query 1 | Total Rows from Query 2 
-----------------+-------------------------+-------------------------
-              0 |                       1 |                       1
-(1 row)
-
-    SELECT * FROM test.show_subtables('drop_chunk_test_tstz');
-                  Child                  | Tablespace 
------------------------------------------+------------
- _timescaledb_internal._hyper_5_22_chunk | 
-(1 row)
-
-ROLLBACK;
-BEGIN;
--- show_chunks and drop_chunks output should be the same
-    \set QUERY1 'SELECT show_chunks(\'drop_chunk_test_ts\', newer_than => interval \'6 minute\')::NAME'
-    \set QUERY2 'SELECT drop_chunks(\'drop_chunk_test_ts\', newer_than => interval \'6 minute\')::NAME'
-    \set ECHO errors
- Different Rows | Total Rows from Query 1 | Total Rows from Query 2 
-----------------+-------------------------+-------------------------
-              0 |                       2 |                       2
-(1 row)
-
-    SELECT * FROM test.show_subtables('drop_chunk_test_ts');
- Child | Tablespace 
--------+------------
-(0 rows)
-
-ROLLBACK;
-BEGIN;
--- show_chunks and drop_chunks output should be the same
-    \set QUERY1 'SELECT show_chunks(\'drop_chunk_test_ts\', older_than => interval \'1 minute\')::NAME'
-    \set QUERY2 'SELECT drop_chunks(\'drop_chunk_test_ts\', older_than => interval \'1 minute\')::NAME'
-    \set ECHO errors
- Different Rows | Total Rows from Query 1 | Total Rows from Query 2 
-----------------+-------------------------+-------------------------
-              0 |                       1 |                       1
-(1 row)
-
-    SELECT * FROM test.show_subtables('drop_chunk_test_ts');
-                  Child                  | Tablespace 
------------------------------------------+------------
- _timescaledb_internal._hyper_4_20_chunk | 
-(1 row)
-
-    \set QUERY1 'SELECT show_chunks(\'drop_chunk_test_tstz\', older_than => interval \'1 minute\')::NAME'
-    \set QUERY2 'SELECT drop_chunks(\'drop_chunk_test_tstz\', older_than => interval \'1 minute\')::NAME'
-    \set ECHO errors
- Different Rows | Total Rows from Query 1 | Total Rows from Query 2 
-----------------+-------------------------+-------------------------
-              0 |                       1 |                       1
-(1 row)
-
-    SELECT * FROM test.show_subtables('drop_chunk_test_tstz');
-                  Child                  | Tablespace 
------------------------------------------+------------
- _timescaledb_internal._hyper_5_22_chunk | 
-(1 row)
-
-ROLLBACK;
-BEGIN;
--- show_chunks and drop_chunks output should be the same
-    \set QUERY1 'SELECT show_chunks(\'drop_chunk_test_ts\', older_than => now()::timestamp-interval \'1 minute\')::NAME'
-    \set QUERY2 'SELECT drop_chunks(\'drop_chunk_test_ts\', older_than => now()::timestamp-interval \'1 minute\')::NAME'
-    \set ECHO errors
- Different Rows | Total Rows from Query 1 | Total Rows from Query 2 
-----------------+-------------------------+-------------------------
-              0 |                       1 |                       1
-(1 row)
-
-    SELECT * FROM test.show_subtables('drop_chunk_test_ts');
-                  Child                  | Tablespace 
------------------------------------------+------------
- _timescaledb_internal._hyper_4_20_chunk | 
-(1 row)
-
-    \set QUERY1 'SELECT show_chunks(\'drop_chunk_test_tstz\', older_than => now()-interval \'1 minute\')::NAME'
-    \set QUERY2 'SELECT drop_chunks(\'drop_chunk_test_tstz\', older_than => now()-interval \'1 minute\')::NAME'
-    \set ECHO errors
- Different Rows | Total Rows from Query 1 | Total Rows from Query 2 
-----------------+-------------------------+-------------------------
-              0 |                       1 |                       1
-(1 row)
-
-    SELECT * FROM test.show_subtables('drop_chunk_test_tstz');
-                  Child                  | Tablespace 
------------------------------------------+------------
- _timescaledb_internal._hyper_5_22_chunk | 
-(1 row)
-
-ROLLBACK;
-\dt "_timescaledb_internal"._hyper*
-                           List of relations
-        Schema         |       Name        | Type  |       Owner       
------------------------+-------------------+-------+-------------------
- _timescaledb_internal | _hyper_2_11_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_2_12_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_2_8_chunk  | table | default_perm_user
- _timescaledb_internal | _hyper_2_9_chunk  | table | default_perm_user
- _timescaledb_internal | _hyper_4_19_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_4_20_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_5_21_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_5_22_chunk | table | default_perm_user
-(8 rows)
-
-\set ON_ERROR_STOP 0
-SELECT drop_chunks(interval '1 minute');
-ERROR:  function drop_chunks(interval) does not exist at character 8
-SELECT drop_chunks('drop_chunk_test3', interval '1 minute');
-ERROR:  can only use an INTERVAL for TIMESTAMP, TIMESTAMPTZ, and DATE types
-SELECT drop_chunks('drop_chunk_test_ts', (now()-interval '1 minute'));
-ERROR:  invalid time argument type "timestamp with time zone"
-SELECT drop_chunks('drop_chunk_test3', verbose => true);
-ERROR:  invalid time range for dropping chunks
-\set ON_ERROR_STOP 1
-\dt "_timescaledb_internal"._hyper*
-                           List of relations
-        Schema         |       Name        | Type  |       Owner       
------------------------+-------------------+-------+-------------------
- _timescaledb_internal | _hyper_2_11_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_2_12_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_2_8_chunk  | table | default_perm_user
- _timescaledb_internal | _hyper_2_9_chunk  | table | default_perm_user
- _timescaledb_internal | _hyper_4_19_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_4_20_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_5_21_chunk | table | default_perm_user
- _timescaledb_internal | _hyper_5_22_chunk | table | default_perm_user
-(8 rows)
-
-CREATE TABLE PUBLIC.drop_chunk_test_date(time date, temp float8, device_id text);
-SELECT create_hypertable('public.drop_chunk_test_date', 'time', chunk_time_interval => interval '1 day', create_default_indexes=>false);
-NOTICE:  adding not-null constraint to column "time"
-         create_hypertable         
------------------------------------
- (6,public,drop_chunk_test_date,t)
-(1 row)
-
-SET timezone = '+100';
-INSERT INTO PUBLIC.drop_chunk_test_date VALUES(now()-INTERVAL '2 day', 1.0, 'dev1');
-BEGIN;
--- show_chunks and drop_chunks output should be the same
-    \set QUERY1 'SELECT show_chunks(\'drop_chunk_test_date\', older_than => interval \'1 day\')::NAME'
-    \set QUERY2 'SELECT drop_chunks(\'drop_chunk_test_date\', older_than => interval \'1 day\')::NAME'
-    \set ECHO errors
- Different Rows | Total Rows from Query 1 | Total Rows from Query 2 
-----------------+-------------------------+-------------------------
-              0 |                       1 |                       1
-(1 row)
-
-    SELECT * FROM test.show_subtables('drop_chunk_test_date');
- Child | Tablespace 
--------+------------
-(0 rows)
-
-ROLLBACK;
-BEGIN;
--- show_chunks and drop_chunks output should be the same
-    \set QUERY1 'SELECT show_chunks(\'drop_chunk_test_date\', older_than => (now()-interval \'1 day\')::date)::NAME'
-    \set QUERY2 'SELECT drop_chunks(\'drop_chunk_test_date\', older_than => (now()-interval \'1 day\')::date)::NAME'
-    \set ECHO errors
- Different Rows | Total Rows from Query 1 | Total Rows from Query 2 
-----------------+-------------------------+-------------------------
-              0 |                       1 |                       1
-(1 row)
-
-    SELECT * FROM test.show_subtables('drop_chunk_test_date');
- Child | Tablespace 
--------+------------
-(0 rows)
-
-ROLLBACK;
-SET timezone TO '-5';
-CREATE TABLE chunk_id_from_relid_test(time bigint, temp float8, device_id int);
-SELECT hypertable_id FROM create_hypertable('chunk_id_from_relid_test', 'time', chunk_time_interval => 10) \gset
-NOTICE:  adding not-null constraint to column "time"
-INSERT INTO chunk_id_from_relid_test VALUES (0, 1.1, 0), (0, 1.3, 11), (12, 2.0, 0), (12, 0.1, 11);
-SELECT _timescaledb_internal.chunk_id_from_relid(tableoid) FROM chunk_id_from_relid_test;
- chunk_id_from_relid 
----------------------
-                  24
-                  24
-                  25
-                  25
-(4 rows)
-
-DROP TABLE chunk_id_from_relid_test;
-CREATE TABLE chunk_id_from_relid_test(time bigint, temp float8, device_id int);
-SELECT hypertable_id FROM  create_hypertable('chunk_id_from_relid_test',
-    'time', chunk_time_interval => 10,
-    partitioning_column => 'device_id',
-    number_partitions => 3) \gset
-NOTICE:  adding not-null constraint to column "time"
-INSERT INTO chunk_id_from_relid_test VALUES (0, 1.1, 2), (0, 1.3, 11), (12, 2.0, 2), (12, 0.1, 11);
-SELECT _timescaledb_internal.chunk_id_from_relid(tableoid) FROM chunk_id_from_relid_test;
- chunk_id_from_relid 
----------------------
-                  26
-                  27
-                  28
-                  29
-(4 rows)
-
-\set ON_ERROR_STOP 0
-SELECT _timescaledb_internal.chunk_id_from_relid('pg_type'::regclass);
-ERROR:  chunk not found
-SELECT _timescaledb_internal.chunk_id_from_relid('chunk_id_from_relid_test'::regclass);
-ERROR:  chunk not found
--- test drop/show_chunks on custom partition types
-CREATE FUNCTION extract_time(a jsonb)
-RETURNS TIMESTAMPTZ
-LANGUAGE SQL
-AS $$ SELECT (a->>'time')::TIMESTAMPTZ $$ IMMUTABLE;
-CREATE TABLE test_weird_type(a jsonb);
-SELECT create_hypertable('test_weird_type', 'a',
-    time_partitioning_func=>'extract_time'::regproc,
-    chunk_time_interval=>'2 hours'::interval);
-NOTICE:  adding not-null constraint to column "a"
-      create_hypertable       
-------------------------------
- (9,public,test_weird_type,t)
-(1 row)
-
-INSERT INTO test_weird_type VALUES ('{"time":"2019/06/06 1:00+0"}'), ('{"time":"2019/06/06 5:00+0"}');
-SELECT * FROM test.show_subtables('test_weird_type');
-                  Child                  | Tablespace 
------------------------------------------+------------
- _timescaledb_internal._hyper_9_30_chunk | 
- _timescaledb_internal._hyper_9_31_chunk | 
-(2 rows)
-
-SELECT show_chunks('test_weird_type', older_than=>'2019/06/06 4:00+0'::TIMESTAMPTZ);
-               show_chunks               
------------------------------------------
- _timescaledb_internal._hyper_9_30_chunk
-(1 row)
-
-SELECT show_chunks('test_weird_type', older_than=>'2019/06/06 10:00+0'::TIMESTAMPTZ);
-               show_chunks               
------------------------------------------
- _timescaledb_internal._hyper_9_30_chunk
- _timescaledb_internal._hyper_9_31_chunk
-(2 rows)
-
--- show_chunks and drop_chunks output should be the same
-\set QUERY1 'SELECT show_chunks(\'test_weird_type\', older_than => \'2019/06/06 5:00+0\'::TIMESTAMPTZ)::NAME'
-\set QUERY2 'SELECT drop_chunks(\'test_weird_type\', older_than => \'2019/06/06 5:00+0\'::TIMESTAMPTZ)::NAME'
-\set ECHO errors
- Different Rows | Total Rows from Query 1 | Total Rows from Query 2 
-----------------+-------------------------+-------------------------
-              0 |                       1 |                       1
-(1 row)
-
-SELECT * FROM test.show_subtables('test_weird_type');
-                  Child                  | Tablespace 
------------------------------------------+------------
- _timescaledb_internal._hyper_9_31_chunk | 
-(1 row)
-
-SELECT show_chunks('test_weird_type', older_than=>'2019/06/06 4:00+0'::TIMESTAMPTZ);
- show_chunks 
--------------
-(0 rows)
-
-SELECT show_chunks('test_weird_type', older_than=>'2019/06/06 10:00+0'::TIMESTAMPTZ);
-               show_chunks               
------------------------------------------
- _timescaledb_internal._hyper_9_31_chunk
-(1 row)
-
--- show_chunks and drop_chunks output should be the same
-\set QUERY1 'SELECT show_chunks(\'test_weird_type\', older_than => \'2019/06/06 6:00+0\'::TIMESTAMPTZ)::NAME'
-\set QUERY2 'SELECT drop_chunks(\'test_weird_type\', older_than => \'2019/06/06 6:00+0\'::TIMESTAMPTZ)::NAME'
-\set ECHO errors
- Different Rows | Total Rows from Query 1 | Total Rows from Query 2 
-----------------+-------------------------+-------------------------
-              0 |                       1 |                       1
-(1 row)
-
-SELECT * FROM test.show_subtables('test_weird_type');
- Child | Tablespace 
--------+------------
-(0 rows)
-
-SELECT show_chunks('test_weird_type', older_than=>'2019/06/06 10:00+0'::TIMESTAMPTZ);
- show_chunks 
--------------
-(0 rows)
-
-DROP TABLE test_weird_type;
-CREATE FUNCTION extract_int_time(a jsonb)
-RETURNS BIGINT
-LANGUAGE SQL
-AS $$ SELECT (a->>'time')::BIGINT $$ IMMUTABLE;
-CREATE TABLE test_weird_type_i(a jsonb);
-SELECT create_hypertable('test_weird_type_i', 'a',
-    time_partitioning_func=>'extract_int_time'::regproc,
-    chunk_time_interval=>5);
-NOTICE:  adding not-null constraint to column "a"
-        create_hypertable        
----------------------------------
- (10,public,test_weird_type_i,t)
-(1 row)
-
-INSERT INTO test_weird_type_i VALUES ('{"time":"0"}'), ('{"time":"5"}');
-SELECT * FROM test.show_subtables('test_weird_type_i');
-                  Child                   | Tablespace 
-------------------------------------------+------------
- _timescaledb_internal._hyper_10_32_chunk | 
- _timescaledb_internal._hyper_10_33_chunk | 
-(2 rows)
-
-SELECT show_chunks('test_weird_type_i', older_than=>5);
-               show_chunks                
-------------------------------------------
- _timescaledb_internal._hyper_10_32_chunk
-(1 row)
-
-SELECT show_chunks('test_weird_type_i', older_than=>10);
-               show_chunks                
-------------------------------------------
- _timescaledb_internal._hyper_10_32_chunk
- _timescaledb_internal._hyper_10_33_chunk
-(2 rows)
-
--- show_chunks and drop_chunks output should be the same
-\set QUERY1 'SELECT show_chunks(\'test_weird_type_i\', older_than=>5)::NAME'
-\set QUERY2 'SELECT drop_chunks(\'test_weird_type_i\', older_than => 5)::NAME'
-\set ECHO errors
- Different Rows | Total Rows from Query 1 | Total Rows from Query 2 
-----------------+-------------------------+-------------------------
-              0 |                       1 |                       1
-(1 row)
-
-SELECT * FROM test.show_subtables('test_weird_type_i');
-                  Child                   | Tablespace 
-------------------------------------------+------------
- _timescaledb_internal._hyper_10_33_chunk | 
-(1 row)
-
-SELECT show_chunks('test_weird_type_i', older_than=>5);
- show_chunks 
--------------
-(0 rows)
-
-SELECT show_chunks('test_weird_type_i', older_than=>10);
-               show_chunks                
-------------------------------------------
- _timescaledb_internal._hyper_10_33_chunk
-(1 row)
-
--- show_chunks and drop_chunks output should be the same
-\set QUERY1 'SELECT show_chunks(\'test_weird_type_i\', older_than=>10)::NAME'
-\set QUERY2 'SELECT drop_chunks(\'test_weird_type_i\', older_than => 10)::NAME'
-\set ECHO errors
- Different Rows | Total Rows from Query 1 | Total Rows from Query 2 
-----------------+-------------------------+-------------------------
-              0 |                       1 |                       1
-(1 row)
-
-SELECT * FROM test.show_subtables('test_weird_type_i');
- Child | Tablespace 
--------+------------
-(0 rows)
-
-SELECT show_chunks('test_weird_type_i', older_than=>10);
- show_chunks 
--------------
-(0 rows)
-
-DROP TABLE test_weird_type_i CASCADE;
-\c  :TEST_DBNAME :ROLE_SUPERUSER
-ALTER TABLE drop_chunk_test2 OWNER TO :ROLE_DEFAULT_PERM_USER_2;
---drop chunks 3 will have a chunk we a dependent object (a view)
---we create the dependent object now
-INSERT INTO PUBLIC.drop_chunk_test3 VALUES(1, 1.0, 'dev1');
-SELECT c.schema_name as chunk_schema, c.table_name as chunk_table
-FROM _timescaledb_catalog.chunk c
-INNER JOIN _timescaledb_catalog.hypertable h ON (c.hypertable_id = h.id)
-WHERE h.schema_name = 'public' AND h.table_name = 'drop_chunk_test3'
-ORDER BY c.id \gset
-create view dependent_view as SELECT * FROM :"chunk_schema".:"chunk_table";
-create view dependent_view2 as SELECT * FROM :"chunk_schema".:"chunk_table";
-ALTER TABLE drop_chunk_test3 OWNER TO :ROLE_DEFAULT_PERM_USER_2;
-\c  :TEST_DBNAME :ROLE_DEFAULT_PERM_USER_2
-\set ON_ERROR_STOP 0
-SELECT drop_chunks('drop_chunk_test1', older_than=>4, newer_than=>3);
-ERROR:  must be owner of hypertable "drop_chunk_test1"
---works with modified owner tables
--- show_chunks and drop_chunks output should be the same
-\set QUERY1 'SELECT show_chunks(\'drop_chunk_test2\', older_than=>4, newer_than=>3)::NAME'
-\set QUERY2 'SELECT drop_chunks(\'drop_chunk_test2\', older_than=>4, newer_than=>3)::NAME'
-\set ECHO errors
- Different Rows | Total Rows from Query 1 | Total Rows from Query 2 
-----------------+-------------------------+-------------------------
-              0 |                       1 |                       1
-(1 row)
-
-\set VERBOSITY default
---this fails because there are dependent objects
-SELECT drop_chunks('drop_chunk_test3', older_than=>100);
-ERROR:  cannot drop table _timescaledb_internal._hyper_3_34_chunk because other objects depend on it
-DETAIL:  view dependent_view depends on table _timescaledb_internal._hyper_3_34_chunk
-view dependent_view2 depends on table _timescaledb_internal._hyper_3_34_chunk
-HINT:  Use DROP ... to drop the dependent objects.
-\set VERBOSITY terse
-\c  :TEST_DBNAME :ROLE_SUPERUSER
-DROP VIEW dependent_view;
-DROP VIEW dependent_view2;
-\c  :TEST_DBNAME :ROLE_DEFAULT_PERM_USER_2
-\set ON_ERROR_STOP 1
---drop chunks from hypertable with same name in different schema
--- order of schema in search_path matters --
-\c :TEST_DBNAME :ROLE_SUPERUSER
-drop table chunk_id_from_relid_test;
-drop table drop_chunk_test1;
-drop table drop_chunk_test2;
-drop table drop_chunk_test3;
-CREATE SCHEMA try_schema;
-CREATE SCHEMA test1;
-CREATE SCHEMA test2;
-CREATE SCHEMA test3;
-GRANT CREATE ON SCHEMA try_schema, test1, test2, test3 TO :ROLE_DEFAULT_PERM_USER;
-GRANT USAGE ON SCHEMA try_schema, test1, test2, test3 TO :ROLE_DEFAULT_PERM_USER;
-SET ROLE :ROLE_DEFAULT_PERM_USER;
-CREATE TABLE try_schema.drop_chunk_test_date(time date, temp float8, device_id text);
-SELECT create_hypertable('try_schema.drop_chunk_test_date', 'time', chunk_time_interval => interval '1 day', create_default_indexes=>false);
-NOTICE:  adding not-null constraint to column "time"
-           create_hypertable            
-----------------------------------------
- (11,try_schema,drop_chunk_test_date,t)
-(1 row)
-
-INSERT INTO public.drop_chunk_test_date VALUES( '2020-01-10', 100, 'hello');
-INSERT INTO try_schema.drop_chunk_test_date VALUES( '2020-01-10', 100, 'hello');
-set search_path to try_schema, test1, test2, test3, public;
-SELECT show_chunks('public.drop_chunk_test_date', older_than=>'1 day'::interval);
-               show_chunks               
------------------------------------------
- _timescaledb_internal._hyper_6_35_chunk
-(1 row)
-
-SELECT show_chunks('try_schema.drop_chunk_test_date', older_than=>'1 day'::interval);
-               show_chunks                
-------------------------------------------
- _timescaledb_internal._hyper_11_36_chunk
-(1 row)
-
-SELECT drop_chunks('drop_chunk_test_date', older_than=> '1 day'::interval);
-               drop_chunks                
-------------------------------------------
- _timescaledb_internal._hyper_11_36_chunk
-(1 row)
-
--- test drop chunks across two tables within the same schema
-CREATE TABLE test1.hyper1 (time bigint, temp float);
-CREATE TABLE test1.hyper2 (time bigint, temp float);
-SELECT create_hypertable('test1.hyper1', 'time', chunk_time_interval => 10);
-NOTICE:  adding not-null constraint to column "time"
-  create_hypertable  
----------------------
- (12,test1,hyper1,t)
-(1 row)
-
-SELECT create_hypertable('test1.hyper2', 'time', chunk_time_interval => 10);
-NOTICE:  adding not-null constraint to column "time"
-  create_hypertable  
----------------------
- (13,test1,hyper2,t)
-(1 row)
-
-INSERT INTO test1.hyper1 VALUES (10, 0.5);
-INSERT INTO test1.hyper2 VALUES (10, 0.7);
-SELECT show_chunks('test1.hyper1');
-               show_chunks                
-------------------------------------------
- _timescaledb_internal._hyper_12_37_chunk
-(1 row)
-
-SELECT show_chunks('test1.hyper2');
-               show_chunks                
-------------------------------------------
- _timescaledb_internal._hyper_13_38_chunk
-(1 row)
-
--- test drop chunks for given table name across all schemas
-CREATE TABLE test2.hyperx (time bigint, temp float);
-CREATE TABLE test3.hyperx (time bigint, temp float);
-SELECT create_hypertable('test2.hyperx', 'time', chunk_time_interval => 10);
-NOTICE:  adding not-null constraint to column "time"
-  create_hypertable  
----------------------
- (14,test2,hyperx,t)
-(1 row)
-
-SELECT create_hypertable('test3.hyperx', 'time', chunk_time_interval => 10);
-NOTICE:  adding not-null constraint to column "time"
-  create_hypertable  
----------------------
- (15,test3,hyperx,t)
-(1 row)
-
-INSERT INTO test2.hyperx VALUES (10, 0.5);
-INSERT INTO test3.hyperx VALUES (10, 0.7);
-SELECT show_chunks('test2.hyperx');
-               show_chunks                
-------------------------------------------
- _timescaledb_internal._hyper_14_39_chunk
-(1 row)
-
-SELECT show_chunks('test3.hyperx');
-               show_chunks                
-------------------------------------------
- _timescaledb_internal._hyper_15_40_chunk
-(1 row)
-
--- This will only drop from one of the tables since the one that is
--- first in the search path will hide the other one.
-SELECT drop_chunks('hyperx', older_than => 100);
-               drop_chunks                
-------------------------------------------
- _timescaledb_internal._hyper_14_39_chunk
-(1 row)
-
-SELECT show_chunks('test2.hyperx');
- show_chunks 
--------------
-(0 rows)
-
-SELECT show_chunks('test3.hyperx');
-               show_chunks                
-------------------------------------------
- _timescaledb_internal._hyper_15_40_chunk
-(1 row)
-
--- Check CTAS behavior when internal ALTER TABLE gets fired
-CREATE TABLE PUBLIC.drop_chunk_test4(time bigint, temp float8, device_id text);
-CREATE TABLE drop_chunks_table_id AS SELECT hypertable_id
-      FROM create_hypertable('public.drop_chunk_test4', 'time', chunk_time_interval => 1);
-NOTICE:  adding not-null constraint to column "time"
--- TEST for internal api that drops a single chunk
--- this drops the table and removes entry from the catalog.
--- does not affect any materialized cagg data
-INSERT INTO test1.hyper1 VALUES (20, 0.5);
-SELECT chunk_schema as "CHSCHEMA",  chunk_name as "CHNAME"
-FROM timescaledb_information.chunks
-WHERE hypertable_name = 'hyper1' and hypertable_schema = 'test1'
-ORDER BY chunk_name ;
-       CHSCHEMA        |       CHNAME       
------------------------+--------------------
- _timescaledb_internal | _hyper_12_37_chunk
- _timescaledb_internal | _hyper_12_41_chunk
-(2 rows)
-
---drop one of the chunks
-SELECT chunk_schema || '.' || chunk_name as "CHNAME"
-FROM timescaledb_information.chunks
-WHERE hypertable_name = 'hyper1' and hypertable_schema = 'test1'
-ORDER BY chunk_name LIMIT 1
-\gset
-SELECT  _timescaledb_internal.drop_chunk(:'CHNAME');
- drop_chunk 
-------------
- t
-(1 row)
-
-SELECT chunk_schema as "CHSCHEMA",  chunk_name as "CHNAME"
-FROM timescaledb_information.chunks
-WHERE hypertable_name = 'hyper1' and hypertable_schema = 'test1'
-ORDER BY chunk_name ;
-       CHSCHEMA        |       CHNAME       
------------------------+--------------------
- _timescaledb_internal | _hyper_12_41_chunk
-(1 row)
-
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/index.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/index.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/index.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/index.out	2023-11-25 05:27:18.573096913 +0000
@@ -1,728 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-CREATE TABLE index_test(time timestamptz, temp float);
-SELECT create_hypertable('index_test', 'time');
-NOTICE:  adding not-null constraint to column "time"
-    create_hypertable    
--------------------------
- (1,public,index_test,t)
-(1 row)
-
--- Default indexes created
-SELECT * FROM test.show_indexes('index_test');
-        Index        | Columns | Expr | Unique | Primary | Exclusion | Tablespace 
----------------------+---------+------+--------+---------+-----------+------------
- index_test_time_idx | {time}  |      | f      | f       | f         | 
-(1 row)
-
-DROP TABLE index_test;
-CREATE TABLE index_test(time timestamptz, device integer, temp float);
--- Create index before create_hypertable()
-CREATE UNIQUE INDEX index_test_time_idx ON index_test (time);
-\set ON_ERROR_STOP 0
--- Creating a hypertable from a table with an index that doesn't cover
--- all partitioning columns should fail
-SELECT create_hypertable('index_test', 'time', 'device', 2);
-NOTICE:  adding not-null constraint to column "time"
-ERROR:  cannot create a unique index without the column "device" (used in partitioning)
-\set ON_ERROR_STOP 1
--- Partitioning on only time should work
-SELECT create_hypertable('index_test', 'time');
-NOTICE:  adding not-null constraint to column "time"
-    create_hypertable    
--------------------------
- (3,public,index_test,t)
-(1 row)
-
-INSERT INTO index_test VALUES ('2017-01-20T09:00:01', 1, 17.5);
--- Check that index is also created on chunk
-SELECT * FROM test.show_indexes('index_test');
-        Index        | Columns | Expr | Unique | Primary | Exclusion | Tablespace 
----------------------+---------+------+--------+---------+-----------+------------
- index_test_time_idx | {time}  |      | t      | f       | f         | 
-(1 row)
-
-SELECT * FROM test.show_indexesp('_timescaledb_internal._hyper%_chunk');
-                 Table                  |                           Index                            | Columns | Expr | Unique | Primary | Exclusion | Tablespace 
-----------------------------------------+------------------------------------------------------------+---------+------+--------+---------+-----------+------------
- _timescaledb_internal._hyper_3_1_chunk | _timescaledb_internal._hyper_3_1_chunk_index_test_time_idx | {time}  |      | t      | f       | f         | 
-(1 row)
-
-SELECT * FROM _timescaledb_catalog.chunk_index ORDER BY index_name, hypertable_index_name;
- chunk_id |              index_name              | hypertable_id | hypertable_index_name 
-----------+--------------------------------------+---------------+-----------------------
-        1 | _hyper_3_1_chunk_index_test_time_idx |             3 | index_test_time_idx
-(1 row)
-
--- Create another chunk
-INSERT INTO index_test VALUES ('2017-05-20T09:00:01', 3, 17.5);
-SELECT * FROM test.show_indexesp('_timescaledb_internal._hyper%_chunk');
-                 Table                  |                           Index                            | Columns | Expr | Unique | Primary | Exclusion | Tablespace 
-----------------------------------------+------------------------------------------------------------+---------+------+--------+---------+-----------+------------
- _timescaledb_internal._hyper_3_1_chunk | _timescaledb_internal._hyper_3_1_chunk_index_test_time_idx | {time}  |      | t      | f       | f         | 
- _timescaledb_internal._hyper_3_2_chunk | _timescaledb_internal._hyper_3_2_chunk_index_test_time_idx | {time}  |      | t      | f       | f         | 
-(2 rows)
-
-SELECT * FROM _timescaledb_catalog.chunk_index ORDER BY index_name, hypertable_index_name;
- chunk_id |              index_name              | hypertable_id | hypertable_index_name 
-----------+--------------------------------------+---------------+-----------------------
-        1 | _hyper_3_1_chunk_index_test_time_idx |             3 | index_test_time_idx
-        2 | _hyper_3_2_chunk_index_test_time_idx |             3 | index_test_time_idx
-(2 rows)
-
--- Delete the index on only one chunk
-DROP INDEX _timescaledb_internal._hyper_3_1_chunk_index_test_time_idx;
-SELECT * FROM test.show_indexesp('_timescaledb_internal._hyper%_chunk');
-                 Table                  |                           Index                            | Columns | Expr | Unique | Primary | Exclusion | Tablespace 
-----------------------------------------+------------------------------------------------------------+---------+------+--------+---------+-----------+------------
- _timescaledb_internal._hyper_3_2_chunk | _timescaledb_internal._hyper_3_2_chunk_index_test_time_idx | {time}  |      | t      | f       | f         | 
-(1 row)
-
-SELECT * FROM _timescaledb_catalog.chunk_index ORDER BY index_name, hypertable_index_name;
- chunk_id |              index_name              | hypertable_id | hypertable_index_name 
-----------+--------------------------------------+---------------+-----------------------
-        2 | _hyper_3_2_chunk_index_test_time_idx |             3 | index_test_time_idx
-(1 row)
-
--- Recreate table with new partitioning
-DROP TABLE index_test;
-CREATE TABLE index_test(id serial, time timestamptz, device integer, temp float);
-SELECT * FROM test.show_columns('index_test');
- Column |           Type           | NotNull 
---------+--------------------------+---------
- id     | integer                  | t
- time   | timestamp with time zone | f
- device | integer                  | f
- temp   | double precision         | f
-(4 rows)
-
--- Test that we can handle difference in attnos across hypertable and
--- chunks by dropping the ID column
-ALTER TABLE index_test DROP COLUMN id;
-SELECT * FROM test.show_columns('index_test');
- Column |           Type           | NotNull 
---------+--------------------------+---------
- time   | timestamp with time zone | f
- device | integer                  | f
- temp   | double precision         | f
-(3 rows)
-
--- No pre-existing UNIQUE index, so partitioning on two columns should work
-SELECT create_hypertable('index_test', 'time', 'device', 2);
-NOTICE:  adding not-null constraint to column "time"
-    create_hypertable    
--------------------------
- (4,public,index_test,t)
-(1 row)
-
-INSERT INTO index_test VALUES ('2017-01-20T09:00:01', 1, 17.5);
-\set ON_ERROR_STOP 0
--- Create unique index without all partitioning columns should fail
-CREATE UNIQUE INDEX index_test_time_device_idx ON index_test (time);
-ERROR:  cannot create a unique index without the column "device" (used in partitioning)
-\set ON_ERROR_STOP 1
-CREATE UNIQUE INDEX index_test_time_device_idx ON index_test (time, device);
--- Regular index need not cover all partitioning columns
-CREATE INDEX ON index_test (time, temp);
--- Create another chunk
-INSERT INTO index_test VALUES ('2017-04-20T09:00:01', 1, 17.5);
--- New index should have been recursed to chunks
-SELECT * FROM test.show_indexes('index_test');
-           Index            |    Columns    | Expr | Unique | Primary | Exclusion | Tablespace 
-----------------------------+---------------+------+--------+---------+-----------+------------
- index_test_device_time_idx | {device,time} |      | f      | f       | f         | 
- index_test_time_device_idx | {time,device} |      | t      | f       | f         | 
- index_test_time_idx        | {time}        |      | f      | f       | f         | 
- index_test_time_temp_idx   | {time,temp}   |      | f      | f       | f         | 
-(4 rows)
-
-SELECT * FROM test.show_indexesp('_timescaledb_internal._hyper%_chunk');
-                 Table                  |                               Index                               |    Columns    | Expr | Unique | Primary | Exclusion | Tablespace 
-----------------------------------------+-------------------------------------------------------------------+---------------+------+--------+---------+-----------+------------
- _timescaledb_internal._hyper_4_3_chunk | _timescaledb_internal._hyper_4_3_chunk_index_test_time_idx        | {time}        |      | f      | f       | f         | 
- _timescaledb_internal._hyper_4_3_chunk | _timescaledb_internal._hyper_4_3_chunk_index_test_device_time_idx | {device,time} |      | f      | f       | f         | 
- _timescaledb_internal._hyper_4_3_chunk | _timescaledb_internal._hyper_4_3_chunk_index_test_time_device_idx | {time,device} |      | t      | f       | f         | 
- _timescaledb_internal._hyper_4_3_chunk | _timescaledb_internal._hyper_4_3_chunk_index_test_time_temp_idx   | {time,temp}   |      | f      | f       | f         | 
- _timescaledb_internal._hyper_4_4_chunk | _timescaledb_internal._hyper_4_4_chunk_index_test_time_idx        | {time}        |      | f      | f       | f         | 
- _timescaledb_internal._hyper_4_4_chunk | _timescaledb_internal._hyper_4_4_chunk_index_test_device_time_idx | {device,time} |      | f      | f       | f         | 
- _timescaledb_internal._hyper_4_4_chunk | _timescaledb_internal._hyper_4_4_chunk_index_test_time_device_idx | {time,device} |      | t      | f       | f         | 
- _timescaledb_internal._hyper_4_4_chunk | _timescaledb_internal._hyper_4_4_chunk_index_test_time_temp_idx   | {time,temp}   |      | f      | f       | f         | 
-(8 rows)
-
-SELECT * FROM _timescaledb_catalog.chunk_index ORDER BY index_name, hypertable_index_name;
- chunk_id |                 index_name                  | hypertable_id |   hypertable_index_name    
-----------+---------------------------------------------+---------------+----------------------------
-        3 | _hyper_4_3_chunk_index_test_device_time_idx |             4 | index_test_device_time_idx
-        3 | _hyper_4_3_chunk_index_test_time_device_idx |             4 | index_test_time_device_idx
-        3 | _hyper_4_3_chunk_index_test_time_idx        |             4 | index_test_time_idx
-        3 | _hyper_4_3_chunk_index_test_time_temp_idx   |             4 | index_test_time_temp_idx
-        4 | _hyper_4_4_chunk_index_test_device_time_idx |             4 | index_test_device_time_idx
-        4 | _hyper_4_4_chunk_index_test_time_device_idx |             4 | index_test_time_device_idx
-        4 | _hyper_4_4_chunk_index_test_time_idx        |             4 | index_test_time_idx
-        4 | _hyper_4_4_chunk_index_test_time_temp_idx   |             4 | index_test_time_temp_idx
-(8 rows)
-
-ALTER INDEX index_test_time_idx RENAME TO index_test_time_idx2;
--- Metadata and index should have changed name
-SELECT * FROM test.show_indexes('index_test');
-           Index            |    Columns    | Expr | Unique | Primary | Exclusion | Tablespace 
-----------------------------+---------------+------+--------+---------+-----------+------------
- index_test_device_time_idx | {device,time} |      | f      | f       | f         | 
- index_test_time_device_idx | {time,device} |      | t      | f       | f         | 
- index_test_time_idx2       | {time}        |      | f      | f       | f         | 
- index_test_time_temp_idx   | {time,temp}   |      | f      | f       | f         | 
-(4 rows)
-
-SELECT * FROM test.show_indexesp('_timescaledb_internal._hyper%_chunk');
-                 Table                  |                               Index                               |    Columns    | Expr | Unique | Primary | Exclusion | Tablespace 
-----------------------------------------+-------------------------------------------------------------------+---------------+------+--------+---------+-----------+------------
- _timescaledb_internal._hyper_4_3_chunk | _timescaledb_internal._hyper_4_3_chunk_index_test_time_idx2       | {time}        |      | f      | f       | f         | 
- _timescaledb_internal._hyper_4_3_chunk | _timescaledb_internal._hyper_4_3_chunk_index_test_device_time_idx | {device,time} |      | f      | f       | f         | 
- _timescaledb_internal._hyper_4_3_chunk | _timescaledb_internal._hyper_4_3_chunk_index_test_time_device_idx | {time,device} |      | t      | f       | f         | 
- _timescaledb_internal._hyper_4_3_chunk | _timescaledb_internal._hyper_4_3_chunk_index_test_time_temp_idx   | {time,temp}   |      | f      | f       | f         | 
- _timescaledb_internal._hyper_4_4_chunk | _timescaledb_internal._hyper_4_4_chunk_index_test_time_idx2       | {time}        |      | f      | f       | f         | 
- _timescaledb_internal._hyper_4_4_chunk | _timescaledb_internal._hyper_4_4_chunk_index_test_device_time_idx | {device,time} |      | f      | f       | f         | 
- _timescaledb_internal._hyper_4_4_chunk | _timescaledb_internal._hyper_4_4_chunk_index_test_time_device_idx | {time,device} |      | t      | f       | f         | 
- _timescaledb_internal._hyper_4_4_chunk | _timescaledb_internal._hyper_4_4_chunk_index_test_time_temp_idx   | {time,temp}   |      | f      | f       | f         | 
-(8 rows)
-
-SELECT * FROM _timescaledb_catalog.chunk_index ORDER BY index_name, hypertable_index_name;
- chunk_id |                 index_name                  | hypertable_id |   hypertable_index_name    
-----------+---------------------------------------------+---------------+----------------------------
-        3 | _hyper_4_3_chunk_index_test_device_time_idx |             4 | index_test_device_time_idx
-        3 | _hyper_4_3_chunk_index_test_time_device_idx |             4 | index_test_time_device_idx
-        3 | _hyper_4_3_chunk_index_test_time_idx2       |             4 | index_test_time_idx2
-        3 | _hyper_4_3_chunk_index_test_time_temp_idx   |             4 | index_test_time_temp_idx
-        4 | _hyper_4_4_chunk_index_test_device_time_idx |             4 | index_test_device_time_idx
-        4 | _hyper_4_4_chunk_index_test_time_device_idx |             4 | index_test_time_device_idx
-        4 | _hyper_4_4_chunk_index_test_time_idx2       |             4 | index_test_time_idx2
-        4 | _hyper_4_4_chunk_index_test_time_temp_idx   |             4 | index_test_time_temp_idx
-(8 rows)
-
-DROP INDEX index_test_time_idx2;
-DROP INDEX index_test_time_device_idx;
--- Index should have been dropped
-SELECT * FROM test.show_indexes('index_test');
-           Index            |    Columns    | Expr | Unique | Primary | Exclusion | Tablespace 
-----------------------------+---------------+------+--------+---------+-----------+------------
- index_test_device_time_idx | {device,time} |      | f      | f       | f         | 
- index_test_time_temp_idx   | {time,temp}   |      | f      | f       | f         | 
-(2 rows)
-
-SELECT * FROM test.show_indexesp('_timescaledb_internal._hyper%_chunk');
-                 Table                  |                               Index                               |    Columns    | Expr | Unique | Primary | Exclusion | Tablespace 
-----------------------------------------+-------------------------------------------------------------------+---------------+------+--------+---------+-----------+------------
- _timescaledb_internal._hyper_4_3_chunk | _timescaledb_internal._hyper_4_3_chunk_index_test_device_time_idx | {device,time} |      | f      | f       | f         | 
- _timescaledb_internal._hyper_4_3_chunk | _timescaledb_internal._hyper_4_3_chunk_index_test_time_temp_idx   | {time,temp}   |      | f      | f       | f         | 
- _timescaledb_internal._hyper_4_4_chunk | _timescaledb_internal._hyper_4_4_chunk_index_test_device_time_idx | {device,time} |      | f      | f       | f         | 
- _timescaledb_internal._hyper_4_4_chunk | _timescaledb_internal._hyper_4_4_chunk_index_test_time_temp_idx   | {time,temp}   |      | f      | f       | f         | 
-(4 rows)
-
-SELECT * FROM _timescaledb_catalog.chunk_index ORDER BY index_name, hypertable_index_name;
- chunk_id |                 index_name                  | hypertable_id |   hypertable_index_name    
-----------+---------------------------------------------+---------------+----------------------------
-        3 | _hyper_4_3_chunk_index_test_device_time_idx |             4 | index_test_device_time_idx
-        3 | _hyper_4_3_chunk_index_test_time_temp_idx   |             4 | index_test_time_temp_idx
-        4 | _hyper_4_4_chunk_index_test_device_time_idx |             4 | index_test_device_time_idx
-        4 | _hyper_4_4_chunk_index_test_time_temp_idx   |             4 | index_test_time_temp_idx
-(4 rows)
-
--- Create index with long name to see how this is handled on chunks
-CREATE INDEX a_hypertable_index_with_a_very_very_long_name_that_truncates ON index_test (time, temp);
-CREATE INDEX a_hypertable_index_with_a_very_very_long_name_that_truncates_2 ON index_test (time, temp);
-SELECT * FROM test.show_indexes('index_test');
-                             Index                              |    Columns    | Expr | Unique | Primary | Exclusion | Tablespace 
-----------------------------------------------------------------+---------------+------+--------+---------+-----------+------------
- a_hypertable_index_with_a_very_very_long_name_that_truncates   | {time,temp}   |      | f      | f       | f         | 
- a_hypertable_index_with_a_very_very_long_name_that_truncates_2 | {time,temp}   |      | f      | f       | f         | 
- index_test_device_time_idx                                     | {device,time} |      | f      | f       | f         | 
- index_test_time_temp_idx                                       | {time,temp}   |      | f      | f       | f         | 
-(4 rows)
-
-SELECT * FROM test.show_indexesp('_timescaledb_internal._hyper%_chunk');
-                 Table                  |                                         Index                                         |    Columns    | Expr | Unique | Primary | Exclusion | Tablespace 
-----------------------------------------+---------------------------------------------------------------------------------------+---------------+------+--------+---------+-----------+------------
- _timescaledb_internal._hyper_4_3_chunk | _timescaledb_internal._hyper_4_3_chunk_index_test_device_time_idx                     | {device,time} |      | f      | f       | f         | 
- _timescaledb_internal._hyper_4_3_chunk | _timescaledb_internal._hyper_4_3_chunk_index_test_time_temp_idx                       | {time,temp}   |      | f      | f       | f         | 
- _timescaledb_internal._hyper_4_3_chunk | _timescaledb_internal._hyper_4_3_chunk_a_hypertable_index_with_a_very_very_long_name_ | {time,temp}   |      | f      | f       | f         | 
- _timescaledb_internal._hyper_4_3_chunk | _timescaledb_internal._hyper_4_3_chunk_a_hypertable_index_with_a_very_very_long_nam_1 | {time,temp}   |      | f      | f       | f         | 
- _timescaledb_internal._hyper_4_4_chunk | _timescaledb_internal._hyper_4_4_chunk_index_test_device_time_idx                     | {device,time} |      | f      | f       | f         | 
- _timescaledb_internal._hyper_4_4_chunk | _timescaledb_internal._hyper_4_4_chunk_index_test_time_temp_idx                       | {time,temp}   |      | f      | f       | f         | 
- _timescaledb_internal._hyper_4_4_chunk | _timescaledb_internal._hyper_4_4_chunk_a_hypertable_index_with_a_very_very_long_name_ | {time,temp}   |      | f      | f       | f         | 
- _timescaledb_internal._hyper_4_4_chunk | _timescaledb_internal._hyper_4_4_chunk_a_hypertable_index_with_a_very_very_long_nam_1 | {time,temp}   |      | f      | f       | f         | 
-(8 rows)
-
-DROP INDEX a_hypertable_index_with_a_very_very_long_name_that_truncates;
-DROP INDEX a_hypertable_index_with_a_very_very_long_name_that_truncates_2;
-\set ON_ERROR_STOP 0
--- Create index CONCURRENTLY
-CREATE UNIQUE INDEX CONCURRENTLY index_test_time_device_idx ON index_test (time, device);
-ERROR:  hypertables do not support concurrent index creation
-\set ON_ERROR_STOP 1
--- Test tablespaces. Chunk indexes should end up in same tablespace as
--- main index.
-\c :TEST_DBNAME :ROLE_SUPERUSER
-SET client_min_messages = ERROR;
-DROP TABLESPACE IF EXISTS tablespace1;
-DROP TABLESPACE IF EXISTS tablespace2;
-SET client_min_messages = NOTICE;
-CREATE TABLESPACE tablespace1 OWNER :ROLE_DEFAULT_PERM_USER LOCATION :TEST_TABLESPACE1_PATH;
-\c :TEST_DBNAME :ROLE_DEFAULT_PERM_USER
-CREATE INDEX index_test_time_idx ON index_test (time) TABLESPACE tablespace1;
-SELECT * FROM test.show_indexes('index_test');
-           Index            |    Columns    | Expr | Unique | Primary | Exclusion | Tablespace  
-----------------------------+---------------+------+--------+---------+-----------+-------------
- index_test_device_time_idx | {device,time} |      | f      | f       | f         | 
- index_test_time_idx        | {time}        |      | f      | f       | f         | tablespace1
- index_test_time_temp_idx   | {time,temp}   |      | f      | f       | f         | 
-(3 rows)
-
-SELECT * FROM test.show_indexesp('_timescaledb_internal._hyper%_chunk');
-                 Table                  |                               Index                               |    Columns    | Expr | Unique | Primary | Exclusion | Tablespace  
-----------------------------------------+-------------------------------------------------------------------+---------------+------+--------+---------+-----------+-------------
- _timescaledb_internal._hyper_4_3_chunk | _timescaledb_internal._hyper_4_3_chunk_index_test_device_time_idx | {device,time} |      | f      | f       | f         | 
- _timescaledb_internal._hyper_4_3_chunk | _timescaledb_internal._hyper_4_3_chunk_index_test_time_temp_idx   | {time,temp}   |      | f      | f       | f         | 
- _timescaledb_internal._hyper_4_3_chunk | _timescaledb_internal._hyper_4_3_chunk_index_test_time_idx        | {time}        |      | f      | f       | f         | tablespace1
- _timescaledb_internal._hyper_4_4_chunk | _timescaledb_internal._hyper_4_4_chunk_index_test_device_time_idx | {device,time} |      | f      | f       | f         | 
- _timescaledb_internal._hyper_4_4_chunk | _timescaledb_internal._hyper_4_4_chunk_index_test_time_temp_idx   | {time,temp}   |      | f      | f       | f         | 
- _timescaledb_internal._hyper_4_4_chunk | _timescaledb_internal._hyper_4_4_chunk_index_test_time_idx        | {time}        |      | f      | f       | f         | tablespace1
-(6 rows)
-
-\c :TEST_DBNAME :ROLE_SUPERUSER
-CREATE TABLESPACE tablespace2 OWNER :ROLE_DEFAULT_PERM_USER LOCATION :TEST_TABLESPACE2_PATH;
-\c :TEST_DBNAME :ROLE_DEFAULT_PERM_USER
-ALTER INDEX index_test_time_idx SET TABLESPACE tablespace2;
-SELECT * FROM test.show_indexes('index_test');
-           Index            |    Columns    | Expr | Unique | Primary | Exclusion | Tablespace  
-----------------------------+---------------+------+--------+---------+-----------+-------------
- index_test_device_time_idx | {device,time} |      | f      | f       | f         | 
- index_test_time_idx        | {time}        |      | f      | f       | f         | tablespace2
- index_test_time_temp_idx   | {time,temp}   |      | f      | f       | f         | 
-(3 rows)
-
-SELECT * FROM test.show_indexesp('_timescaledb_internal._hyper%_chunk');
-                 Table                  |                               Index                               |    Columns    | Expr | Unique | Primary | Exclusion | Tablespace  
-----------------------------------------+-------------------------------------------------------------------+---------------+------+--------+---------+-----------+-------------
- _timescaledb_internal._hyper_4_3_chunk | _timescaledb_internal._hyper_4_3_chunk_index_test_device_time_idx | {device,time} |      | f      | f       | f         | 
- _timescaledb_internal._hyper_4_3_chunk | _timescaledb_internal._hyper_4_3_chunk_index_test_time_temp_idx   | {time,temp}   |      | f      | f       | f         | 
- _timescaledb_internal._hyper_4_3_chunk | _timescaledb_internal._hyper_4_3_chunk_index_test_time_idx        | {time}        |      | f      | f       | f         | tablespace2
- _timescaledb_internal._hyper_4_4_chunk | _timescaledb_internal._hyper_4_4_chunk_index_test_device_time_idx | {device,time} |      | f      | f       | f         | 
- _timescaledb_internal._hyper_4_4_chunk | _timescaledb_internal._hyper_4_4_chunk_index_test_time_temp_idx   | {time,temp}   |      | f      | f       | f         | 
- _timescaledb_internal._hyper_4_4_chunk | _timescaledb_internal._hyper_4_4_chunk_index_test_time_idx        | {time}        |      | f      | f       | f         | tablespace2
-(6 rows)
-
--- Add constraint index
-ALTER TABLE index_test ADD UNIQUE (time, device);
-SELECT * FROM test.show_indexes('index_test');
-           Index            |    Columns    | Expr | Unique | Primary | Exclusion | Tablespace  
-----------------------------+---------------+------+--------+---------+-----------+-------------
- index_test_device_time_idx | {device,time} |      | f      | f       | f         | 
- index_test_time_device_key | {time,device} |      | t      | f       | f         | 
- index_test_time_idx        | {time}        |      | f      | f       | f         | tablespace2
- index_test_time_temp_idx   | {time,temp}   |      | f      | f       | f         | 
-(4 rows)
-
-SELECT * FROM test.show_indexesp('_timescaledb_internal._hyper%_chunk');
-                 Table                  |                               Index                               |    Columns    | Expr | Unique | Primary | Exclusion | Tablespace  
-----------------------------------------+-------------------------------------------------------------------+---------------+------+--------+---------+-----------+-------------
- _timescaledb_internal._hyper_4_3_chunk | _timescaledb_internal._hyper_4_3_chunk_index_test_device_time_idx | {device,time} |      | f      | f       | f         | 
- _timescaledb_internal._hyper_4_3_chunk | _timescaledb_internal._hyper_4_3_chunk_index_test_time_temp_idx   | {time,temp}   |      | f      | f       | f         | 
- _timescaledb_internal._hyper_4_3_chunk | _timescaledb_internal._hyper_4_3_chunk_index_test_time_idx        | {time}        |      | f      | f       | f         | tablespace2
- _timescaledb_internal._hyper_4_3_chunk | _timescaledb_internal."3_1_index_test_time_device_key"            | {time,device} |      | t      | f       | f         | 
- _timescaledb_internal._hyper_4_4_chunk | _timescaledb_internal._hyper_4_4_chunk_index_test_device_time_idx | {device,time} |      | f      | f       | f         | 
- _timescaledb_internal._hyper_4_4_chunk | _timescaledb_internal._hyper_4_4_chunk_index_test_time_temp_idx   | {time,temp}   |      | f      | f       | f         | 
- _timescaledb_internal._hyper_4_4_chunk | _timescaledb_internal._hyper_4_4_chunk_index_test_time_idx        | {time}        |      | f      | f       | f         | tablespace2
- _timescaledb_internal._hyper_4_4_chunk | _timescaledb_internal."4_2_index_test_time_device_key"            | {time,device} |      | t      | f       | f         | 
-(8 rows)
-
--- Constraint indexes are added to chunk_index table.
-SELECT * FROM _timescaledb_catalog.chunk_index ORDER BY index_name, hypertable_index_name;
- chunk_id |                 index_name                  | hypertable_id |   hypertable_index_name    
-----------+---------------------------------------------+---------------+----------------------------
-        3 | 3_1_index_test_time_device_key              |             4 | index_test_time_device_key
-        4 | 4_2_index_test_time_device_key              |             4 | index_test_time_device_key
-        3 | _hyper_4_3_chunk_index_test_device_time_idx |             4 | index_test_device_time_idx
-        3 | _hyper_4_3_chunk_index_test_time_idx        |             4 | index_test_time_idx
-        3 | _hyper_4_3_chunk_index_test_time_temp_idx   |             4 | index_test_time_temp_idx
-        4 | _hyper_4_4_chunk_index_test_device_time_idx |             4 | index_test_device_time_idx
-        4 | _hyper_4_4_chunk_index_test_time_idx        |             4 | index_test_time_idx
-        4 | _hyper_4_4_chunk_index_test_time_temp_idx   |             4 | index_test_time_temp_idx
-(8 rows)
-
-SELECT * FROM _timescaledb_catalog.chunk_constraint;
- chunk_id | dimension_slice_id |        constraint_name         | hypertable_constraint_name 
-----------+--------------------+--------------------------------+----------------------------
-        3 |                  3 | constraint_3                   | 
-        3 |                  4 | constraint_4                   | 
-        4 |                  5 | constraint_5                   | 
-        4 |                  4 | constraint_4                   | 
-        3 |                    | 3_1_index_test_time_device_key | index_test_time_device_key
-        4 |                    | 4_2_index_test_time_device_key | index_test_time_device_key
-(6 rows)
-
-DROP TABLE index_test;
--- Metadata removed
-SELECT * FROM _timescaledb_catalog.chunk_index ORDER BY index_name, hypertable_index_name;
- chunk_id | index_name | hypertable_id | hypertable_index_name 
-----------+------------+---------------+-----------------------
-(0 rows)
-
--- Create table in a tablespace
-CREATE TABLE index_test(time timestamptz, temp float, device int) TABLESPACE tablespace1;
--- Default indexes should be in the table's tablespace
-SELECT create_hypertable('index_test', 'time');
-NOTICE:  adding not-null constraint to column "time"
-    create_hypertable    
--------------------------
- (5,public,index_test,t)
-(1 row)
-
--- Explicitly defining an index tablespace should work and propagate
--- to chunks
-CREATE INDEX ON index_test (time, device) TABLESPACE tablespace2;
--- New indexes without explicit tablespaces should use the default
--- tablespace
-CREATE INDEX ON index_test (device);
--- Create chunk
-INSERT INTO index_test VALUES ('2017-01-20T09:00:01', 17.5);
--- Check that the tablespaces of chunk indexes match the tablespace of
--- the main index
-SELECT * FROM test.show_indexes('index_test');
-           Index            |    Columns    | Expr | Unique | Primary | Exclusion | Tablespace  
-----------------------------+---------------+------+--------+---------+-----------+-------------
- index_test_device_idx      | {device}      |      | f      | f       | f         | 
- index_test_time_device_idx | {time,device} |      | f      | f       | f         | tablespace2
- index_test_time_idx        | {time}        |      | f      | f       | f         | tablespace1
-(3 rows)
-
-SELECT * FROM test.show_indexesp('_timescaledb_internal._hyper%_chunk');
-                 Table                  |                               Index                               |    Columns    | Expr | Unique | Primary | Exclusion | Tablespace  
-----------------------------------------+-------------------------------------------------------------------+---------------+------+--------+---------+-----------+-------------
- _timescaledb_internal._hyper_5_5_chunk | _timescaledb_internal._hyper_5_5_chunk_index_test_time_idx        | {time}        |      | f      | f       | f         | tablespace1
- _timescaledb_internal._hyper_5_5_chunk | _timescaledb_internal._hyper_5_5_chunk_index_test_time_device_idx | {time,device} |      | f      | f       | f         | tablespace2
- _timescaledb_internal._hyper_5_5_chunk | _timescaledb_internal._hyper_5_5_chunk_index_test_device_idx      | {device}      |      | f      | f       | f         | tablespace1
-(3 rows)
-
--- Creating a new index should propagate to existing chunks, including
--- the given tablespace
-CREATE INDEX ON index_test (time, temp) TABLESPACE tablespace2;
-SELECT * FROM test.show_indexes('index_test');
-           Index            |    Columns    | Expr | Unique | Primary | Exclusion | Tablespace  
-----------------------------+---------------+------+--------+---------+-----------+-------------
- index_test_device_idx      | {device}      |      | f      | f       | f         | 
- index_test_time_device_idx | {time,device} |      | f      | f       | f         | tablespace2
- index_test_time_idx        | {time}        |      | f      | f       | f         | tablespace1
- index_test_time_temp_idx   | {time,temp}   |      | f      | f       | f         | tablespace2
-(4 rows)
-
-SELECT * FROM test.show_indexesp('_timescaledb_internal._hyper%_chunk');
-                 Table                  |                               Index                               |    Columns    | Expr | Unique | Primary | Exclusion | Tablespace  
-----------------------------------------+-------------------------------------------------------------------+---------------+------+--------+---------+-----------+-------------
- _timescaledb_internal._hyper_5_5_chunk | _timescaledb_internal._hyper_5_5_chunk_index_test_time_idx        | {time}        |      | f      | f       | f         | tablespace1
- _timescaledb_internal._hyper_5_5_chunk | _timescaledb_internal._hyper_5_5_chunk_index_test_time_device_idx | {time,device} |      | f      | f       | f         | tablespace2
- _timescaledb_internal._hyper_5_5_chunk | _timescaledb_internal._hyper_5_5_chunk_index_test_device_idx      | {device}      |      | f      | f       | f         | tablespace1
- _timescaledb_internal._hyper_5_5_chunk | _timescaledb_internal._hyper_5_5_chunk_index_test_time_temp_idx   | {time,temp}   |      | f      | f       | f         | tablespace2
-(4 rows)
-
--- Cleanup
-DROP TABLE index_test CASCADE;
-\c :TEST_DBNAME :ROLE_SUPERUSER
-DROP TABLESPACE tablespace1;
-DROP TABLESPACE tablespace2;
--- Test expression indexes
-CREATE TABLE index_expr_test(id serial, time timestamptz, temp float, meta jsonb);
--- Screw up the attribute numbers
-ALTER TABLE index_expr_test DROP COLUMN id;
-CREATE INDEX ON index_expr_test ((meta ->> 'field')) ;
-INSERT INTO index_expr_test VALUES ('2017-01-20T09:00:01', 17.5, '{"field": "value1"}');
-INSERT INTO index_expr_test VALUES ('2017-01-20T09:00:01', 17.5, '{"field": "value2"}');
-EXPLAIN (verbose, costs off)
-SELECT * FROM index_expr_test WHERE meta ->> 'field' = 'value1';
-                                QUERY PLAN                                 
----------------------------------------------------------------------------
- Index Scan using index_expr_test_expr_idx on public.index_expr_test
-   Output: "time", temp, meta
-   Index Cond: ((index_expr_test.meta ->> 'field'::text) = 'value1'::text)
-(3 rows)
-
-SELECT * FROM index_expr_test WHERE meta ->> 'field' = 'value1';
-             time             | temp |        meta         
-------------------------------+------+---------------------
- Fri Jan 20 09:00:01 2017 PST | 17.5 | {"field": "value1"}
-(1 row)
-
--- Test INDEX DROP error for multiple objects
-CREATE TABLE index_test(time timestamptz, temp float);
-CREATE UNIQUE INDEX index_test_idx ON index_test (time, temp);
-SELECT create_hypertable('index_test', 'time');
-NOTICE:  adding not-null constraint to column "time"
-    create_hypertable    
--------------------------
- (6,public,index_test,t)
-(1 row)
-
-CREATE TABLE index_test_2(time timestamptz, temp float);
-CREATE UNIQUE INDEX index_test_2_idx ON index_test_2 (time, temp);
-\set ON_ERROR_STOP 0
-DROP INDEX index_test_idx, index_test_2_idx;
-ERROR:  cannot drop a hypertable index along with other objects
-\set ON_ERROR_STOP 1
--- test expression index with dropped columns
-CREATE TABLE idx_expr_test(filler int, time timestamptz, meta text);
-SELECT table_name FROM create_hypertable('idx_expr_test', 'time');
-NOTICE:  adding not-null constraint to column "time"
-  table_name   
----------------
- idx_expr_test
-(1 row)
-
-ALTER TABLE idx_expr_test DROP COLUMN filler;
-CREATE INDEX tag_idx ON idx_expr_test(('foo'||meta));
-INSERT INTO idx_expr_test(time, meta) VALUES ('2000-01-01', 'bar');
-DROP TABLE idx_expr_test CASCADE;
--- test multicolumn expression index with dropped columns
-CREATE TABLE idx_expr_test(filler int, time timestamptz, t1 text, t2 text, t3 text);
-SELECT table_name FROM create_hypertable('idx_expr_test', 'time');
-NOTICE:  adding not-null constraint to column "time"
-  table_name   
----------------
- idx_expr_test
-(1 row)
-
-ALTER TABLE idx_expr_test DROP COLUMN filler;
-CREATE INDEX tag_idx ON idx_expr_test((t1||t2||t3));
-INSERT INTO idx_expr_test(time, t1, t2, t3) VALUES ('2000-01-01', 'foo', 'bar', 'baz');
-DROP TABLE idx_expr_test CASCADE;
--- test index with predicate and dropped columns
-CREATE TABLE idx_predicate_test(filler int, time timestamptz);
-SELECT table_name FROM create_hypertable('idx_predicate_test', 'time');
-NOTICE:  adding not-null constraint to column "time"
-     table_name     
---------------------
- idx_predicate_test
-(1 row)
-
-ALTER TABLE idx_predicate_test DROP COLUMN filler;
-ALTER TABLE idx_predicate_test ADD COLUMN b1 bool;
-CREATE INDEX idx_predicate_test_b1 ON idx_predicate_test(b1) WHERE b1=true;
-INSERT INTO idx_predicate_test VALUES ('2000-01-01',true);
-DROP TABLE idx_predicate_test;
--- test index with table references
-CREATE TABLE idx_tableref_test(time timestamptz);
-SELECT table_name FROM create_hypertable('idx_tableref_test', 'time');
-NOTICE:  adding not-null constraint to column "time"
-    table_name     
--------------------
- idx_tableref_test
-(1 row)
-
--- we use security definer to prevent function inlining
-CREATE OR REPLACE FUNCTION tableref_func(t idx_tableref_test) RETURNS timestamptz LANGUAGE SQL IMMUTABLE SECURITY DEFINER AS $f$ SELECT t.time; $f$;
--- try creating index with no existing chunks
-CREATE INDEX tableref_idx ON idx_tableref_test(tableref_func(idx_tableref_test));
--- insert data to trigger chunk creation
-INSERT INTO idx_tableref_test SELECT '2000-01-01';
-DROP INDEX tableref_idx;
--- try creating index on hypertable with existing chunks
-CREATE INDEX tableref_idx ON idx_tableref_test(tableref_func(idx_tableref_test));
--- test index creation with if not exists
-CREATE TABLE idx_exists(time timestamptz NOT NULL);
-SELECT table_name FROM create_hypertable('idx_exists', 'time');
- table_name 
-------------
- idx_exists
-(1 row)
-
--- should be skipped since this index was already created by create_hypertable
-CREATE INDEX IF NOT EXISTS idx_exists_time_idx ON idx_exists(time DESC);
-NOTICE:  relation "idx_exists_time_idx" already exists, skipping
--- should create index
-CREATE INDEX IF NOT EXISTS idx_exists_time_asc_idx ON idx_exists(time ASC);
--- should be skipped since it was created in previous command
-CREATE INDEX IF NOT EXISTS idx_exists_time_asc_idx ON idx_exists(time ASC);
-NOTICE:  relation "idx_exists_time_asc_idx" already exists, skipping
-DROP INDEX idx_exists_time_asc_idx;
-INSERT INTO idx_exists VALUES ('2000-01-01'),('2001-01-01');
--- should create index
-CREATE INDEX IF NOT EXISTS idx_exists_time_asc_idx ON idx_exists(time ASC);
--- should be skipped since it was created in previous command
-CREATE INDEX IF NOT EXISTS idx_exists_time_asc_idx ON idx_exists(time ASC);
-NOTICE:  relation "idx_exists_time_asc_idx" already exists, skipping
--- test reindex
-CREATE TABLE reindex_test(time timestamp, temp float, PRIMARY KEY(time, temp));
-CREATE UNIQUE INDEX reindex_test_time_unique_idx ON reindex_test(time);
--- create hypertable with three chunks
-SELECT create_hypertable('reindex_test', 'time', chunk_time_interval => 2628000000000);
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-     create_hypertable      
-----------------------------
- (12,public,reindex_test,t)
-(1 row)
-
-INSERT INTO reindex_test VALUES ('2017-01-20T09:00:01', 17.5),
-                                ('2017-01-21T09:00:01', 19.1),
-                                ('2017-04-20T09:00:01', 89.5),
-                                ('2017-04-21T09:00:01', 17.1),
-                                ('2017-06-20T09:00:01', 18.5),
-                                ('2017-06-21T09:00:01', 11.0);
-SELECT * FROM test.show_columns('reindex_test');
- Column |            Type             | NotNull 
---------+-----------------------------+---------
- time   | timestamp without time zone | t
- temp   | double precision            | t
-(2 rows)
-
-SELECT * FROM test.show_subtables('reindex_test');
-                  Child                   | Tablespace 
-------------------------------------------+------------
- _timescaledb_internal._hyper_12_12_chunk | 
- _timescaledb_internal._hyper_12_13_chunk | 
- _timescaledb_internal._hyper_12_14_chunk | 
- _timescaledb_internal._hyper_12_15_chunk | 
- _timescaledb_internal._hyper_12_16_chunk | 
-(5 rows)
-
--- show reindexing
-REINDEX (VERBOSE) TABLE reindex_test;
-INFO:  index "12_3_reindex_test_pkey" was reindexed
-INFO:  index "_hyper_12_12_chunk_reindex_test_time_unique_idx" was reindexed
-INFO:  index "13_4_reindex_test_pkey" was reindexed
-INFO:  index "_hyper_12_13_chunk_reindex_test_time_unique_idx" was reindexed
-INFO:  index "14_5_reindex_test_pkey" was reindexed
-INFO:  index "_hyper_12_14_chunk_reindex_test_time_unique_idx" was reindexed
-INFO:  index "15_6_reindex_test_pkey" was reindexed
-INFO:  index "_hyper_12_15_chunk_reindex_test_time_unique_idx" was reindexed
-INFO:  index "16_7_reindex_test_pkey" was reindexed
-INFO:  index "_hyper_12_16_chunk_reindex_test_time_unique_idx" was reindexed
-\set ON_ERROR_STOP 0
--- REINDEX TABLE CONCURRENTLY is not supported on PG11 (but blocked on PG12+)
-REINDEX TABLE CONCURRENTLY reindex_test;
-ERROR:  concurrent index creation on hypertables is not supported
--- this one currently doesn't recurse to chunks and instead gives an
--- error
-REINDEX (VERBOSE) INDEX reindex_test_time_unique_idx;
-ERROR:  reindexing of a specific index on a hypertable is unsupported
-\set ON_ERROR_STOP 1
--- show reindexing on a normal table
-CREATE TABLE reindex_norm(time timestamp, temp float);
-CREATE UNIQUE INDEX reindex_norm_time_unique_idx ON reindex_norm(time);
-INSERT INTO reindex_norm VALUES ('2017-01-20T09:00:01', 17.5),
-                                ('2017-01-21T09:00:01', 19.1),
-                                ('2017-04-20T09:00:01', 89.5),
-                                ('2017-04-21T09:00:01', 17.1),
-                                ('2017-06-20T09:00:01', 18.5),
-                                ('2017-06-21T09:00:01', 11.0);
-REINDEX (VERBOSE) TABLE reindex_norm;
-INFO:  index "reindex_norm_time_unique_idx" was reindexed
-REINDEX (VERBOSE) INDEX reindex_norm_time_unique_idx;
-INFO:  index "reindex_norm_time_unique_idx" was reindexed
-SELECT * FROM test.show_constraintsp('_timescaledb_internal._hyper_12%');
-                  Table                   |       Constraint       | Type |   Columns   |                     Index                      |                                                                     Expr                                                                     | Deferrable | Deferred | Validated 
-------------------------------------------+------------------------+------+-------------+------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------+------------+----------+-----------
- _timescaledb_internal._hyper_12_12_chunk | 12_3_reindex_test_pkey | p    | {time,temp} | _timescaledb_internal."12_3_reindex_test_pkey" |                                                                                                                                              | f          | f        | t
- _timescaledb_internal._hyper_12_12_chunk | constraint_13          | c    | {time}      | -                                              | (("time" >= 'Thu Jan 19 10:00:00 2017'::timestamp without time zone) AND ("time" < 'Sat Feb 18 20:00:00 2017'::timestamp without time zone)) | f          | f        | t
- _timescaledb_internal._hyper_12_13_chunk | 13_4_reindex_test_pkey | p    | {time,temp} | _timescaledb_internal."13_4_reindex_test_pkey" |                                                                                                                                              | f          | f        | t
- _timescaledb_internal._hyper_12_13_chunk | constraint_14          | c    | {time}      | -                                              | (("time" >= 'Tue Mar 21 06:00:00 2017'::timestamp without time zone) AND ("time" < 'Thu Apr 20 16:00:00 2017'::timestamp without time zone)) | f          | f        | t
- _timescaledb_internal._hyper_12_14_chunk | 14_5_reindex_test_pkey | p    | {time,temp} | _timescaledb_internal."14_5_reindex_test_pkey" |                                                                                                                                              | f          | f        | t
- _timescaledb_internal._hyper_12_14_chunk | constraint_15          | c    | {time}      | -                                              | (("time" >= 'Thu Apr 20 16:00:00 2017'::timestamp without time zone) AND ("time" < 'Sun May 21 02:00:00 2017'::timestamp without time zone)) | f          | f        | t
- _timescaledb_internal._hyper_12_15_chunk | 15_6_reindex_test_pkey | p    | {time,temp} | _timescaledb_internal."15_6_reindex_test_pkey" |                                                                                                                                              | f          | f        | t
- _timescaledb_internal._hyper_12_15_chunk | constraint_16          | c    | {time}      | -                                              | (("time" >= 'Sun May 21 02:00:00 2017'::timestamp without time zone) AND ("time" < 'Tue Jun 20 12:00:00 2017'::timestamp without time zone)) | f          | f        | t
- _timescaledb_internal._hyper_12_16_chunk | 16_7_reindex_test_pkey | p    | {time,temp} | _timescaledb_internal."16_7_reindex_test_pkey" |                                                                                                                                              | f          | f        | t
- _timescaledb_internal._hyper_12_16_chunk | constraint_17          | c    | {time}      | -                                              | (("time" >= 'Tue Jun 20 12:00:00 2017'::timestamp without time zone) AND ("time" < 'Thu Jul 20 22:00:00 2017'::timestamp without time zone)) | f          | f        | t
-(10 rows)
-
-SELECT * FROM reindex_norm;
-           time           | temp 
---------------------------+------
- Fri Jan 20 09:00:01 2017 | 17.5
- Sat Jan 21 09:00:01 2017 | 19.1
- Thu Apr 20 09:00:01 2017 | 89.5
- Fri Apr 21 09:00:01 2017 | 17.1
- Tue Jun 20 09:00:01 2017 | 18.5
- Wed Jun 21 09:00:01 2017 |   11
-(6 rows)
-
-SELECT * FROM test.show_indexes('_timescaledb_internal._hyper_12_12_chunk');
-                                 Index                                 |   Columns   | Expr | Unique | Primary | Exclusion | Tablespace 
------------------------------------------------------------------------+-------------+------+--------+---------+-----------+------------
- _timescaledb_internal."12_3_reindex_test_pkey"                        | {time,temp} |      | t      | t       | f         | 
- _timescaledb_internal._hyper_12_12_chunk_reindex_test_time_unique_idx | {time}      |      | t      | f       | f         | 
-(2 rows)
-
-SELECT chunk_index_clone::regclass::text
-FROM _timescaledb_internal.chunk_index_clone('_timescaledb_internal."12_3_reindex_test_pkey"'::regclass);
-                        chunk_index_clone                        
------------------------------------------------------------------
- _timescaledb_internal._hyper_12_12_chunk_12_3_reindex_test_pkey
-(1 row)
-
-SELECT * FROM test.show_indexes('_timescaledb_internal._hyper_12_12_chunk');
-                                 Index                                 |   Columns   | Expr | Unique | Primary | Exclusion | Tablespace 
------------------------------------------------------------------------+-------------+------+--------+---------+-----------+------------
- _timescaledb_internal."12_3_reindex_test_pkey"                        | {time,temp} |      | t      | t       | f         | 
- _timescaledb_internal._hyper_12_12_chunk_12_3_reindex_test_pkey       | {time,temp} |      | t      | t       | f         | 
- _timescaledb_internal._hyper_12_12_chunk_reindex_test_time_unique_idx | {time}      |      | t      | f       | f         | 
-(3 rows)
-
-SELECT * FROM _timescaledb_internal.chunk_index_replace('_timescaledb_internal."12_3_reindex_test_pkey"'::regclass, '_timescaledb_internal."_hyper_12_12_chunk_12_3_reindex_test_pkey"'::regclass);
- chunk_index_replace 
----------------------
- 
-(1 row)
-
-SELECT * FROM test.show_indexes('_timescaledb_internal._hyper_12_12_chunk');
-                                 Index                                 |   Columns   | Expr | Unique | Primary | Exclusion | Tablespace 
------------------------------------------------------------------------+-------------+------+--------+---------+-----------+------------
- _timescaledb_internal."12_3_reindex_test_pkey"                        | {time,temp} |      | t      | t       | f         | 
- _timescaledb_internal._hyper_12_12_chunk_reindex_test_time_unique_idx | {time}      |      | t      | f       | f         | 
-(2 rows)
-
-CREATE TABLE ht_dropped(time timestamptz, d0 int, d1 int, c0 int, c1 int, c2 int);
-SELECT create_hypertable('ht_dropped','time');
-NOTICE:  adding not-null constraint to column "time"
-    create_hypertable     
---------------------------
- (13,public,ht_dropped,t)
-(1 row)
-
-INSERT INTO ht_dropped(time,c0,c1,c2) SELECT '2000-01-01',1,2,3;
-ALTER TABLE ht_dropped DROP COLUMN d0;
-INSERT INTO ht_dropped(time,c0,c1,c2) SELECT '2001-01-01',1,2,3;
-ALTER TABLE ht_dropped DROP COLUMN d1;
-INSERT INTO ht_dropped(time,c0,c1,c2) SELECT '2002-01-01',1,2,3;
-CREATE INDEX ON ht_dropped(c0,c1,c2) WHERE c1 IS NOT NULL;
-CREATE INDEX ON ht_dropped(c0,c1,c2) WITH(timescaledb.transaction_per_chunk) WHERE c2 IS NOT NULL;
-SELECT
-  oid::TEXT AS "Chunk",
-  i.*
-FROM
-  (SELECT tableoid::REGCLASS FROM ht_dropped GROUP BY tableoid) ch (oid)
-  LEFT JOIN LATERAL ( SELECT * FROM test.show_indexes (ch.oid)) i ON TRUE
-ORDER BY
-  1, 2;
-                  Chunk                   |                               Index                               |  Columns   | Expr | Unique | Primary | Exclusion | Tablespace 
-------------------------------------------+-------------------------------------------------------------------+------------+------+--------+---------+-----------+------------
- _timescaledb_internal._hyper_13_17_chunk | _timescaledb_internal._hyper_13_17_chunk_ht_dropped_time_idx      | {time}     |      | f      | f       | f         | 
- _timescaledb_internal._hyper_13_17_chunk | _timescaledb_internal._hyper_13_17_chunk_ht_dropped_c0_c1_c2_idx  | {c0,c1,c2} |      | f      | f       | f         | 
- _timescaledb_internal._hyper_13_17_chunk | _timescaledb_internal._hyper_13_17_chunk_ht_dropped_c0_c1_c2_idx1 | {c0,c1,c2} |      | f      | f       | f         | 
- _timescaledb_internal._hyper_13_18_chunk | _timescaledb_internal._hyper_13_18_chunk_ht_dropped_time_idx      | {time}     |      | f      | f       | f         | 
- _timescaledb_internal._hyper_13_18_chunk | _timescaledb_internal._hyper_13_18_chunk_ht_dropped_c0_c1_c2_idx  | {c0,c1,c2} |      | f      | f       | f         | 
- _timescaledb_internal._hyper_13_18_chunk | _timescaledb_internal._hyper_13_18_chunk_ht_dropped_c0_c1_c2_idx1 | {c0,c1,c2} |      | f      | f       | f         | 
- _timescaledb_internal._hyper_13_19_chunk | _timescaledb_internal._hyper_13_19_chunk_ht_dropped_time_idx      | {time}     |      | f      | f       | f         | 
- _timescaledb_internal._hyper_13_19_chunk | _timescaledb_internal._hyper_13_19_chunk_ht_dropped_c0_c1_c2_idx  | {c0,c1,c2} |      | f      | f       | f         | 
- _timescaledb_internal._hyper_13_19_chunk | _timescaledb_internal._hyper_13_19_chunk_ht_dropped_c0_c1_c2_idx1 | {c0,c1,c2} |      | f      | f       | f         | 
-(9 rows)
-
--- #3056 check chunk index column name mapping
-CREATE TABLE i3056(c int, order_number int NOT NULL, date_created timestamptz NOT NULL);
-CREATE INDEX ON i3056(order_number) INCLUDE(order_number);
-CREATE INDEX ON i3056(date_created, (order_number % 5)) INCLUDE(order_number);
-SELECT table_name FROM create_hypertable('i3056', 'date_created');
- table_name 
-------------
- i3056
-(1 row)
-
-ALTER TABLE i3056 DROP COLUMN c;
-INSERT INTO i3056(order_number,date_created) VALUES (1, '2000-01-01');
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/pg_dump_unprivileged.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/pg_dump_unprivileged.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/pg_dump_unprivileged.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/pg_dump_unprivileged.out	2023-11-25 05:27:23.625082278 +0000
@@ -1,16 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-\c template1 :ROLE_SUPERUSER
-SET client_min_messages TO ERROR;
-CREATE EXTENSION IF NOT EXISTS timescaledb;
-RESET client_min_messages;
-CREATE USER dump_unprivileged CREATEDB;
-\c template1 dump_unprivileged
-CREATE database dump_unprivileged;
-\! utils/pg_dump_unprivileged.sh
-Database dumped successfully
-\c template1 :ROLE_SUPERUSER
-DROP EXTENSION timescaledb;
-DROP DATABASE dump_unprivileged;
-DROP USER dump_unprivileged;
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/tablespace.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/tablespace.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/tablespace.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/tablespace.out	2023-11-25 05:27:28.681067613 +0000
@@ -1,613 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-\set ON_ERROR_STOP 0
-\c :TEST_DBNAME :ROLE_SUPERUSER
-CREATE VIEW hypertable_tablespaces AS
-SELECT cls.relname AS hypertable,
-       (SELECT spcname FROM pg_tablespace WHERE oid = reltablespace) AS tablespace
-  FROM _timescaledb_catalog.hypertable,
-  LATERAL (SELECT * FROM pg_class WHERE oid = format('%I.%I', schema_name, table_name)::regclass) AS cls
-  ORDER BY hypertable, tablespace;
-GRANT SELECT ON hypertable_tablespaces TO PUBLIC;
---Test hypertable with tablespace. Tablespaces are cluster-wide, so we
---attach the test name as prefix to allow tests to be executed in
---parallel.
-CREATE TABLESPACE tablespace1 OWNER :ROLE_DEFAULT_PERM_USER LOCATION :TEST_TABLESPACE1_PATH;
-\c :TEST_DBNAME :ROLE_DEFAULT_PERM_USER
---assigning a tablespace via the main table should work
-CREATE TABLE tspace_2dim(time timestamp, temp float, device text) TABLESPACE tablespace1;
-SELECT create_hypertable('tspace_2dim', 'time', 'device', 2);
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-    create_hypertable     
---------------------------
- (1,public,tspace_2dim,t)
-(1 row)
-
-INSERT INTO tspace_2dim VALUES ('2017-01-20T09:00:01', 24.3, 'blue');
--- Tablespace for tspace_2dim should be set
-SELECT * FROM hypertable_tablespaces WHERE hypertable = 'tspace_2dim';
- hypertable  | tablespace  
--------------+-------------
- tspace_2dim | tablespace1
-(1 row)
-
-SELECT show_tablespaces('tspace_2dim');
- show_tablespaces 
-------------------
- tablespace1
-(1 row)
-
---verify that the table chunk has the correct tablespace
-SELECT relname, spcname FROM pg_class c
-INNER JOIN pg_tablespace t ON (c.reltablespace = t.oid)
-INNER JOIN _timescaledb_catalog.chunk ch ON (ch.table_name = c.relname);
-     relname      |   spcname   
-------------------+-------------
- _hyper_1_1_chunk | tablespace1
-(1 row)
-
---check some error conditions
-SELECT attach_tablespace(NULL,NULL);
-ERROR:  invalid tablespace name
-SELECT attach_tablespace('tablespace2', NULL);
-ERROR:  invalid hypertable
-SELECT attach_tablespace(NULL, 'tspace_2dim');
-ERROR:  invalid tablespace name
-SELECT attach_tablespace('none_existing_tablespace', 'tspace_2dim');
-ERROR:  tablespace "none_existing_tablespace" does not exist
-SELECT attach_tablespace('tablespace2', 'none_existing_table');
-ERROR:  relation "none_existing_table" does not exist at character 41
-SELECT detach_tablespace(NULL);
-ERROR:  invalid tablespace name
-SELECT detach_tablespaces(NULL);
-ERROR:  invalid argument
-SELECT show_tablespaces(NULL);
- show_tablespaces 
-------------------
-(0 rows)
-
---attach another tablespace without first creating it --> should generate error
-SELECT attach_tablespace('tablespace2', 'tspace_2dim');
-ERROR:  tablespace "tablespace2" does not exist
---attach the same tablespace twice to same table should also generate error
-SELECT attach_tablespace('tablespace1', 'tspace_2dim');
-ERROR:  tablespace "tablespace1" is already attached to hypertable "tspace_2dim"
---no error if if_not_attached is given
-SELECT attach_tablespace('tablespace1', 'tspace_2dim', if_not_attached => true);
-NOTICE:  tablespace "tablespace1" is already attached to hypertable "tspace_2dim", skipping
- attach_tablespace 
--------------------
- 
-(1 row)
-
-\c :TEST_DBNAME :ROLE_SUPERUSER
---Tablespaces are cluster-wide, so we attach the test name as prefix
---to allow tests to be executed in parallel.
-CREATE TABLESPACE tablespace2 OWNER :ROLE_DEFAULT_PERM_USER_2 LOCATION :TEST_TABLESPACE2_PATH;
-\c :TEST_DBNAME :ROLE_DEFAULT_PERM_USER_2
---attach without permissions on the table should fail
-SELECT attach_tablespace('tablespace2', 'tspace_2dim');
-ERROR:  must be owner of hypertable "tspace_2dim"
-\c :TEST_DBNAME :ROLE_DEFAULT_PERM_USER
---attach without permissions on the tablespace should also fail
-SELECT attach_tablespace('tablespace2', 'tspace_2dim');
-ERROR:  permission denied for tablespace "tablespace2" by table owner "default_perm_user"
-\c :TEST_DBNAME :ROLE_SUPERUSER
-GRANT :ROLE_DEFAULT_PERM_USER_2 TO :ROLE_DEFAULT_PERM_USER;
-\c :TEST_DBNAME :ROLE_DEFAULT_PERM_USER
---should work with permissions on both the table and the tablespace
-SELECT attach_tablespace('tablespace2', 'tspace_2dim');
- attach_tablespace 
--------------------
- 
-(1 row)
-
-SELECT * FROM _timescaledb_catalog.tablespace;
- id | hypertable_id | tablespace_name 
-----+---------------+-----------------
-  1 |             1 | tablespace1
-  2 |             1 | tablespace2
-(2 rows)
-
-SELECT * FROM show_tablespaces('tspace_2dim');
- show_tablespaces 
-------------------
- tablespace1
- tablespace2
-(2 rows)
-
---insert into another chunk
-INSERT INTO tspace_2dim VALUES ('2017-01-20T09:00:01', 24.3, 'brown');
-SELECT * FROM test.show_subtables('tspace_2dim');
-                 Child                  | Tablespace  
-----------------------------------------+-------------
- _timescaledb_internal._hyper_1_1_chunk | tablespace1
- _timescaledb_internal._hyper_1_2_chunk | tablespace2
-(2 rows)
-
---indexes should inherit the tablespace of their chunk
-SELECT * FROM test.show_indexesp('_timescaledb_internal._hyper%_chunk');
-                 Table                  |                               Index                                |    Columns    | Expr | Unique | Primary | Exclusion | Tablespace  
-----------------------------------------+--------------------------------------------------------------------+---------------+------+--------+---------+-----------+-------------
- _timescaledb_internal._hyper_1_1_chunk | _timescaledb_internal._hyper_1_1_chunk_tspace_2dim_time_idx        | {time}        |      | f      | f       | f         | tablespace1
- _timescaledb_internal._hyper_1_1_chunk | _timescaledb_internal._hyper_1_1_chunk_tspace_2dim_device_time_idx | {device,time} |      | f      | f       | f         | tablespace1
- _timescaledb_internal._hyper_1_2_chunk | _timescaledb_internal._hyper_1_2_chunk_tspace_2dim_time_idx        | {time}        |      | f      | f       | f         | tablespace1
- _timescaledb_internal._hyper_1_2_chunk | _timescaledb_internal._hyper_1_2_chunk_tspace_2dim_device_time_idx | {device,time} |      | f      | f       | f         | tablespace1
-(4 rows)
-
-\x
-SELECT * FROM timescaledb_information.hypertables
-ORDER BY hypertable_schema, hypertable_name;
--[ RECORD 1 ]-------+--------------------------
-hypertable_schema   | public
-hypertable_name     | tspace_2dim
-owner               | default_perm_user
-num_dimensions      | 2
-num_chunks          | 2
-compression_enabled | f
-is_distributed      | f
-replication_factor  | 
-data_nodes          | 
-tablespaces         | {tablespace1,tablespace2}
-
-SELECT * FROM timescaledb_information.chunks ORDER BY chunk_name;
--[ RECORD 1 ]----------+-----------------------------
-hypertable_schema      | public
-hypertable_name        | tspace_2dim
-chunk_schema           | _timescaledb_internal
-chunk_name             | _hyper_1_1_chunk
-primary_dimension      | time
-primary_dimension_type | timestamp without time zone
-range_start            | Wed Jan 18 16:00:00 2017 PST
-range_end              | Wed Jan 25 16:00:00 2017 PST
-range_start_integer    | 
-range_end_integer      | 
-is_compressed          | f
-chunk_tablespace       | tablespace1
-data_nodes             | 
--[ RECORD 2 ]----------+-----------------------------
-hypertable_schema      | public
-hypertable_name        | tspace_2dim
-chunk_schema           | _timescaledb_internal
-chunk_name             | _hyper_1_2_chunk
-primary_dimension      | time
-primary_dimension_type | timestamp without time zone
-range_start            | Wed Jan 18 16:00:00 2017 PST
-range_end              | Wed Jan 25 16:00:00 2017 PST
-range_start_integer    | 
-range_end_integer      | 
-is_compressed          | f
-chunk_tablespace       | tablespace2
-data_nodes             | 
-
-\x
---
-SET ROLE :ROLE_DEFAULT_PERM_USER_2;
-CREATE TABLE tspace_1dim(time timestamp, temp float, device text);
-SELECT create_hypertable('tspace_1dim', 'time');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-    create_hypertable     
---------------------------
- (2,public,tspace_1dim,t)
-(1 row)
-
---user doesn't have permission on tablespace1 --> error
-SELECT attach_tablespace('tablespace1', 'tspace_1dim');
-ERROR:  permission denied for tablespace "tablespace1" by table owner "default_perm_user_2"
---grant permission to tablespace1
-SET ROLE :ROLE_DEFAULT_PERM_USER;
-GRANT CREATE ON TABLESPACE tablespace1 TO :ROLE_DEFAULT_PERM_USER_2;
-SET ROLE :ROLE_DEFAULT_PERM_USER_2;
---should work fine now. Test SELECT INTO utility statements to ensure
---internal alter table function call works with event triggers.
-SELECT true INTO attached FROM attach_tablespace('tablespace1', 'tspace_1dim');
-SELECT attach_tablespace('tablespace2', 'tspace_1dim');
- attach_tablespace 
--------------------
- 
-(1 row)
-
--- Tablespace for tspace_1dim should be set and attached
-SELECT * FROM hypertable_tablespaces WHERE hypertable = 'tspace_1dim';
- hypertable  | tablespace  
--------------+-------------
- tspace_1dim | tablespace1
-(1 row)
-
-SELECT show_tablespaces('tspace_1dim');
- show_tablespaces 
-------------------
- tablespace1
- tablespace2
-(2 rows)
-
---trying to revoke permissions while attached should fail
-SET ROLE :ROLE_DEFAULT_PERM_USER;
-REVOKE CREATE ON TABLESPACE tablespace1 FROM :ROLE_DEFAULT_PERM_USER_2;
-ERROR:  cannot revoke privilege while tablespace "tablespace1" is attached to hypertable "tspace_1dim"
-REVOKE ALL ON TABLESPACE tablespace1 FROM :ROLE_DEFAULT_PERM_USER_2;
-ERROR:  cannot revoke privilege while tablespace "tablespace1" is attached to hypertable "tspace_1dim"
-SET ROLE :ROLE_DEFAULT_PERM_USER_2;
-SELECT * FROM _timescaledb_catalog.tablespace;
- id | hypertable_id | tablespace_name 
-----+---------------+-----------------
-  1 |             1 | tablespace1
-  2 |             1 | tablespace2
-  3 |             2 | tablespace1
-  4 |             2 | tablespace2
-(4 rows)
-
-INSERT INTO tspace_1dim VALUES ('2017-01-20T09:00:01', 24.3, 'blue');
-INSERT INTO tspace_1dim VALUES ('2017-03-20T09:00:01', 24.3, 'brown');
-SELECT * FROM test.show_subtablesp('tspace_%');
-   Parent    |                 Child                  | Tablespace  
--------------+----------------------------------------+-------------
- tspace_2dim | _timescaledb_internal._hyper_1_1_chunk | tablespace1
- tspace_2dim | _timescaledb_internal._hyper_1_2_chunk | tablespace2
- tspace_1dim | _timescaledb_internal._hyper_2_3_chunk | tablespace1
- tspace_1dim | _timescaledb_internal._hyper_2_4_chunk | tablespace2
-(4 rows)
-
---indexes should inherit the tablespace of their chunk, unless the
---parent index has a tablespace set, in which case the chunks'
---corresponding indexes are pinned to the parent index's
---tablespace. The parent index can have a tablespace set in two cases:
---(1) if explicitly set in CREATE INDEX, or (2) if the main table was
---created with a tablespace, because then default indexes will be
---created in that tablespace too.
-SELECT * FROM test.show_indexesp('_timescaledb_internal._hyper%_chunk');
-                 Table                  |                               Index                                |    Columns    | Expr | Unique | Primary | Exclusion | Tablespace  
-----------------------------------------+--------------------------------------------------------------------+---------------+------+--------+---------+-----------+-------------
- _timescaledb_internal._hyper_1_1_chunk | _timescaledb_internal._hyper_1_1_chunk_tspace_2dim_time_idx        | {time}        |      | f      | f       | f         | tablespace1
- _timescaledb_internal._hyper_1_1_chunk | _timescaledb_internal._hyper_1_1_chunk_tspace_2dim_device_time_idx | {device,time} |      | f      | f       | f         | tablespace1
- _timescaledb_internal._hyper_1_2_chunk | _timescaledb_internal._hyper_1_2_chunk_tspace_2dim_time_idx        | {time}        |      | f      | f       | f         | tablespace1
- _timescaledb_internal._hyper_1_2_chunk | _timescaledb_internal._hyper_1_2_chunk_tspace_2dim_device_time_idx | {device,time} |      | f      | f       | f         | tablespace1
- _timescaledb_internal._hyper_2_3_chunk | _timescaledb_internal._hyper_2_3_chunk_tspace_1dim_time_idx        | {time}        |      | f      | f       | f         | tablespace2
- _timescaledb_internal._hyper_2_4_chunk | _timescaledb_internal._hyper_2_4_chunk_tspace_1dim_time_idx        | {time}        |      | f      | f       | f         | tablespace1
-(6 rows)
-
---detach tablespace1 from tspace_2dim should fail due to lack of permissions
-SELECT detach_tablespace('tablespace1', 'tspace_2dim');
-ERROR:  must be owner of hypertable "tspace_2dim"
---detach tablespace1 from all tables. Should only detach from
---'tspace_1dim' (1 tablespace) due to lack of permissions
-SELECT * FROM hypertable_tablespaces;
- hypertable  | tablespace  
--------------+-------------
- tspace_1dim | tablespace1
- tspace_2dim | tablespace1
-(2 rows)
-
-SELECT * INTO detached FROM detach_tablespace('tablespace1');
-NOTICE:  tablespace "tablespace1" remains attached to 1 hypertable(s) due to lack of permissions
-SELECT * FROM detached;
- detach_tablespace 
--------------------
-                 1
-(1 row)
-
-SELECT * FROM _timescaledb_catalog.tablespace;
- id | hypertable_id | tablespace_name 
-----+---------------+-----------------
-  1 |             1 | tablespace1
-  2 |             1 | tablespace2
-  4 |             2 | tablespace2
-(3 rows)
-
-SELECT * FROM show_tablespaces('tspace_1dim');
- show_tablespaces 
-------------------
- tablespace2
-(1 row)
-
-SELECT * FROM show_tablespaces('tspace_2dim');
- show_tablespaces 
-------------------
- tablespace1
- tablespace2
-(2 rows)
-
-SELECT * FROM hypertable_tablespaces;
- hypertable  | tablespace  
--------------+-------------
- tspace_1dim | 
- tspace_2dim | tablespace1
-(2 rows)
-
---it should now be possible to revoke permissions on tablespace1
-SET ROLE :ROLE_DEFAULT_PERM_USER;
-REVOKE CREATE ON TABLESPACE tablespace1 FROM :ROLE_DEFAULT_PERM_USER_2;
-SET ROLE :ROLE_DEFAULT_PERM_USER_2;
---detach the other tablespace
-SELECT detach_tablespace('tablespace2', 'tspace_1dim');
- detach_tablespace 
--------------------
-                 1
-(1 row)
-
-SELECT * FROM _timescaledb_catalog.tablespace;
- id | hypertable_id | tablespace_name 
-----+---------------+-----------------
-  1 |             1 | tablespace1
-  2 |             1 | tablespace2
-(2 rows)
-
-SELECT * FROM show_tablespaces('tspace_1dim');
- show_tablespaces 
-------------------
-(0 rows)
-
-SELECT * FROM show_tablespaces('tspace_2dim');
- show_tablespaces 
-------------------
- tablespace1
- tablespace2
-(2 rows)
-
-SELECT * FROM hypertable_tablespaces;
- hypertable  | tablespace  
--------------+-------------
- tspace_1dim | 
- tspace_2dim | tablespace1
-(2 rows)
-
---detaching tablespace2 from a table without permissions should fail
-SELECT detach_tablespace('tablespace2', 'tspace_2dim');
-ERROR:  must be owner of hypertable "tspace_2dim"
-SELECT detach_tablespaces('tspace_2dim');
-ERROR:  must be owner of hypertable "tspace_2dim"
-\c :TEST_DBNAME :ROLE_SUPERUSER
--- PERM_USER_2 owns tablespace2, and PERM_USER owns the table
--- 'tspace_2dim', which has tablespace2 attached. Revoking PERM_USER_2
--- FROM PERM_USER should therefore fail
-REVOKE :ROLE_DEFAULT_PERM_USER_2 FROM :ROLE_DEFAULT_PERM_USER;
-ERROR:  cannot revoke privilege while tablespace "tablespace2" is attached to hypertable "tspace_2dim"
-SET ROLE :ROLE_DEFAULT_PERM_USER_2;
---set other user should make detach work
-SET ROLE :ROLE_DEFAULT_PERM_USER;
-SELECT * INTO detached_all FROM detach_tablespaces('tspace_2dim');
-SELECT * FROM detached_all;
- detach_tablespaces 
---------------------
-                  2
-(1 row)
-
-SELECT * FROM _timescaledb_catalog.tablespace;
- id | hypertable_id | tablespace_name 
-----+---------------+-----------------
-(0 rows)
-
-SELECT * FROM show_tablespaces('tspace_1dim');
- show_tablespaces 
-------------------
-(0 rows)
-
-SELECT * FROM show_tablespaces('tspace_2dim');
- show_tablespaces 
-------------------
-(0 rows)
-
-\c :TEST_DBNAME :ROLE_SUPERUSER
--- It should now be possible to revoke PERM_USER_2 from PERM_USER
--- since tablespace2 is no longer attched to tspace_2dim
-REVOKE :ROLE_DEFAULT_PERM_USER_2 FROM :ROLE_DEFAULT_PERM_USER;
-SET ROLE :ROLE_DEFAULT_PERM_USER;
---detaching twice should fail
-SELECT detach_tablespace('tablespace2', 'tspace_2dim');
-ERROR:  tablespace "tablespace2" is not attached to hypertable "tspace_2dim"
---adding if_attached should only generate notice
-SELECT detach_tablespace('tablespace2', 'tspace_2dim', if_attached => true);
-NOTICE:  tablespace "tablespace2" is not attached to hypertable "tspace_2dim", skipping
- detach_tablespace 
--------------------
-                 0
-(1 row)
-
---attach tablespaces again to verify that tablespaces are cleaned up
---when tables are dropped
-\c :TEST_DBNAME :ROLE_SUPERUSER
-SELECT attach_tablespace('tablespace2', 'tspace_1dim');
- attach_tablespace 
--------------------
- 
-(1 row)
-
-SELECT attach_tablespace('tablespace1', 'tspace_2dim');
- attach_tablespace 
--------------------
- 
-(1 row)
-
-SELECT * FROM _timescaledb_catalog.tablespace;
- id | hypertable_id | tablespace_name 
-----+---------------+-----------------
-  5 |             2 | tablespace2
-  6 |             1 | tablespace1
-(2 rows)
-
-DROP TABLE tspace_1dim;
-SELECT * FROM _timescaledb_catalog.tablespace;
- id | hypertable_id | tablespace_name 
-----+---------------+-----------------
-  6 |             1 | tablespace1
-(1 row)
-
-DROP TABLE tspace_2dim;
-SELECT * FROM _timescaledb_catalog.tablespace;
- id | hypertable_id | tablespace_name 
-----+---------------+-----------------
-(0 rows)
-
--- Create two tables and attach multiple tablespaces to them. Verify
--- that dropping a tablespace from multiple tables work as expected.
-CREATE TABLE tbl_1(time timestamp, temp float, device text);
-SELECT create_hypertable('tbl_1', 'time');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
- create_hypertable  
---------------------
- (3,public,tbl_1,t)
-(1 row)
-
-CREATE TABLE tbl_2(time timestamp, temp float, device text);
-SELECT create_hypertable('tbl_2', 'time');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
- create_hypertable  
---------------------
- (4,public,tbl_2,t)
-(1 row)
-
-CREATE TABLE tbl_3(time timestamp, temp float, device text);
-SELECT create_hypertable('tbl_3', 'time');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
- create_hypertable  
---------------------
- (5,public,tbl_3,t)
-(1 row)
-
-SELECT * FROM hypertable_tablespaces;
- hypertable | tablespace 
-------------+------------
- tbl_1      | 
- tbl_2      | 
- tbl_3      | 
-(3 rows)
-
-SELECT * FROM show_tablespaces('tbl_1');
- show_tablespaces 
-------------------
-(0 rows)
-
-SELECT * FROM show_tablespaces('tbl_2');
- show_tablespaces 
-------------------
-(0 rows)
-
-SELECT * FROM show_tablespaces('tbl_3');
- show_tablespaces 
-------------------
-(0 rows)
-
-SELECT attach_tablespace('tablespace1', 'tbl_1');
- attach_tablespace 
--------------------
- 
-(1 row)
-
-SELECT attach_tablespace('tablespace2', 'tbl_1');
- attach_tablespace 
--------------------
- 
-(1 row)
-
-SELECT attach_tablespace('tablespace2', 'tbl_2');
- attach_tablespace 
--------------------
- 
-(1 row)
-
-SELECT attach_tablespace('tablespace2', 'tbl_3');
- attach_tablespace 
--------------------
- 
-(1 row)
-
-SELECT * FROM hypertable_tablespaces;
- hypertable | tablespace  
-------------+-------------
- tbl_1      | tablespace1
- tbl_2      | tablespace2
- tbl_3      | tablespace2
-(3 rows)
-
-SELECT * FROM show_tablespaces('tbl_1');
- show_tablespaces 
-------------------
- tablespace1
- tablespace2
-(2 rows)
-
-SELECT * FROM show_tablespaces('tbl_2');
- show_tablespaces 
-------------------
- tablespace2
-(1 row)
-
-SELECT * FROM show_tablespaces('tbl_3');
- show_tablespaces 
-------------------
- tablespace2
-(1 row)
-
-SELECT detach_tablespace('tablespace2');
- detach_tablespace 
--------------------
-                 3
-(1 row)
-
-SELECT * FROM hypertable_tablespaces;
- hypertable | tablespace  
-------------+-------------
- tbl_1      | tablespace1
- tbl_2      | 
- tbl_3      | 
-(3 rows)
-
-SELECT * FROM show_tablespaces('tbl_1');
- show_tablespaces 
-------------------
- tablespace1
-(1 row)
-
-SELECT * FROM show_tablespaces('tbl_2');
- show_tablespaces 
-------------------
-(0 rows)
-
-SELECT * FROM show_tablespaces('tbl_3');
- show_tablespaces 
-------------------
-(0 rows)
-
-DROP TABLE tbl_1;
-DROP TABLE tbl_2;
-DROP TABLE tbl_3;
--- verify that one cannot DROP a tablespace while it is attached to a
--- hypertable
-CREATE TABLE tbl_1(time timestamp, temp float, device text);
-SELECT create_hypertable('tbl_1', 'time');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
- create_hypertable  
---------------------
- (6,public,tbl_1,t)
-(1 row)
-
-SELECT attach_tablespace('tablespace1', 'tbl_1');
- attach_tablespace 
--------------------
- 
-(1 row)
-
-SELECT * FROM show_tablespaces('tbl_1');
- show_tablespaces 
-------------------
- tablespace1
-(1 row)
-
-DROP TABLESPACE tablespace1;
-ERROR:  tablespace "tablespace1" is still attached to 1 hypertables
---after detaching we should now be able to drop the tablespace
-SELECT detach_tablespace('tablespace1', 'tbl_1');
- detach_tablespace 
--------------------
-                 1
-(1 row)
-
-DROP TABLESPACE tablespace1;
-DROP TABLESPACE tablespace2;
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/agg_bookends-15.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/agg_bookends-15.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/agg_bookends-15.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/agg_bookends-15.out	2023-11-25 05:27:33.757052875 +0000
@@ -1,1502 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-\set TEST_BASE_NAME agg_bookends
-SELECT format('include/%s_load.sql', :'TEST_BASE_NAME') as "TEST_LOAD_NAME",
-       format('include/%s_query.sql', :'TEST_BASE_NAME') as "TEST_QUERY_NAME",
-       format('%s/results/%s_results_optimized.out', :'TEST_OUTPUT_DIR', :'TEST_BASE_NAME') as "TEST_RESULTS_OPTIMIZED",
-       format('%s/results/%s_results_unoptimized.out', :'TEST_OUTPUT_DIR', :'TEST_BASE_NAME') as "TEST_RESULTS_UNOPTIMIZED"
-\gset
-SELECT format('\! diff -u  --label "Unoptimized result" --label "Optimized result" %s %s', :'TEST_RESULTS_UNOPTIMIZED', :'TEST_RESULTS_OPTIMIZED') as "DIFF_CMD"
-\gset
-\set PREFIX 'EXPLAIN (analyze, costs off, timing off, summary off)'
-\ir :TEST_LOAD_NAME
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-CREATE TABLE btest(time timestamp NOT NULL, time_alt timestamp, gp INTEGER, temp float, strid TEXT DEFAULT 'testing');
-SELECT schema_name, table_name, created FROM create_hypertable('btest', 'time');
-psql:include/agg_bookends_load.sql:6: WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-psql:include/agg_bookends_load.sql:6: WARNING:  column type "timestamp without time zone" used for "time_alt" does not follow best practices
- schema_name | table_name | created 
--------------+------------+---------
- public      | btest      | t
-(1 row)
-
-INSERT INTO btest VALUES('2017-01-20T09:00:01', '2017-01-20T10:00:00', 1, 22.5);
-INSERT INTO btest VALUES('2017-01-20T09:00:21', '2017-01-20T09:00:59', 1, 21.2);
-INSERT INTO btest VALUES('2017-01-20T09:00:47', '2017-01-20T09:00:58', 1, 25.1);
-INSERT INTO btest VALUES('2017-01-20T09:00:02', '2017-01-20T09:00:57', 2, 35.5);
-INSERT INTO btest VALUES('2017-01-20T09:00:21', '2017-01-20T09:00:56', 2, 30.2);
---TOASTED;
-INSERT INTO btest VALUES('2017-01-20T09:00:43', '2017-01-20T09:01:55', 2, 20.1, repeat('xyz', 1000000) );
-CREATE TABLE btest_numeric (time timestamp NOT NULL, quantity numeric);
-SELECT schema_name, table_name, created FROM create_hypertable('btest_numeric', 'time');
-psql:include/agg_bookends_load.sql:16: WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
- schema_name |  table_name   | created 
--------------+---------------+---------
- public      | btest_numeric | t
-(1 row)
-
-\ir :TEST_QUERY_NAME
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
--- canary for results diff
--- this should be only output of results diff
-SELECT setting, current_setting(setting) AS value from (VALUES ('timescaledb.enable_optimizations')) v(setting);
-             setting              | value 
-----------------------------------+-------
- timescaledb.enable_optimizations | on
-(1 row)
-
-:PREFIX SELECT time, gp, temp FROM btest ORDER BY time;
-                                              QUERY PLAN                                               
--------------------------------------------------------------------------------------------------------
- Index Scan Backward using _hyper_1_1_chunk_btest_time_idx on _hyper_1_1_chunk (actual rows=6 loops=1)
-(1 row)
-
-:PREFIX SELECT last(temp, time) FROM btest;
-                                                 QUERY PLAN                                                 
-------------------------------------------------------------------------------------------------------------
- Result (actual rows=1 loops=1)
-   InitPlan 1 (returns $0)
-     ->  Limit (actual rows=1 loops=1)
-           ->  Index Scan using _hyper_1_1_chunk_btest_time_idx on _hyper_1_1_chunk (actual rows=1 loops=1)
-                 Index Cond: ("time" IS NOT NULL)
-(5 rows)
-
-:PREFIX SELECT first(temp, time) FROM btest;
-                                                     QUERY PLAN                                                      
----------------------------------------------------------------------------------------------------------------------
- Result (actual rows=1 loops=1)
-   InitPlan 1 (returns $0)
-     ->  Limit (actual rows=1 loops=1)
-           ->  Index Scan Backward using _hyper_1_1_chunk_btest_time_idx on _hyper_1_1_chunk (actual rows=1 loops=1)
-                 Index Cond: ("time" IS NOT NULL)
-(5 rows)
-
-:PREFIX SELECT last(temp, time_alt) FROM btest;
-                         QUERY PLAN                         
-------------------------------------------------------------
- Aggregate (actual rows=1 loops=1)
-   ->  Seq Scan on _hyper_1_1_chunk (actual rows=6 loops=1)
-(2 rows)
-
-:PREFIX SELECT first(temp, time_alt) FROM btest;
-                         QUERY PLAN                         
-------------------------------------------------------------
- Aggregate (actual rows=1 loops=1)
-   ->  Seq Scan on _hyper_1_1_chunk (actual rows=6 loops=1)
-(2 rows)
-
-:PREFIX SELECT gp, last(temp, time) FROM btest GROUP BY gp ORDER BY gp;
-                            QUERY PLAN                            
-------------------------------------------------------------------
- Sort (actual rows=2 loops=1)
-   Sort Key: _hyper_1_1_chunk.gp
-   Sort Method: quicksort 
-   ->  HashAggregate (actual rows=2 loops=1)
-         Group Key: _hyper_1_1_chunk.gp
-         Batches: 1 
-         ->  Seq Scan on _hyper_1_1_chunk (actual rows=6 loops=1)
-(7 rows)
-
-:PREFIX SELECT gp, first(temp, time) FROM btest GROUP BY gp ORDER BY gp;
-                            QUERY PLAN                            
-------------------------------------------------------------------
- Sort (actual rows=2 loops=1)
-   Sort Key: _hyper_1_1_chunk.gp
-   Sort Method: quicksort 
-   ->  HashAggregate (actual rows=2 loops=1)
-         Group Key: _hyper_1_1_chunk.gp
-         Batches: 1 
-         ->  Seq Scan on _hyper_1_1_chunk (actual rows=6 loops=1)
-(7 rows)
-
---check whole row
-:PREFIX SELECT gp, first(btest, time) FROM btest GROUP BY gp ORDER BY gp;
-                            QUERY PLAN                            
-------------------------------------------------------------------
- Sort (actual rows=2 loops=1)
-   Sort Key: _hyper_1_1_chunk.gp
-   Sort Method: quicksort 
-   ->  HashAggregate (actual rows=2 loops=1)
-         Group Key: _hyper_1_1_chunk.gp
-         Batches: 1 
-         ->  Seq Scan on _hyper_1_1_chunk (actual rows=6 loops=1)
-(7 rows)
-
---check toasted col
-:PREFIX SELECT gp, left(last(strid, time), 10) FROM btest GROUP BY gp ORDER BY gp;
-                            QUERY PLAN                            
-------------------------------------------------------------------
- Sort (actual rows=2 loops=1)
-   Sort Key: _hyper_1_1_chunk.gp
-   Sort Method: quicksort 
-   ->  HashAggregate (actual rows=2 loops=1)
-         Group Key: _hyper_1_1_chunk.gp
-         Batches: 1 
-         ->  Seq Scan on _hyper_1_1_chunk (actual rows=6 loops=1)
-(7 rows)
-
-:PREFIX SELECT gp, last(temp, strid) FROM btest GROUP BY gp ORDER BY gp;
-                            QUERY PLAN                            
-------------------------------------------------------------------
- Sort (actual rows=2 loops=1)
-   Sort Key: _hyper_1_1_chunk.gp
-   Sort Method: quicksort 
-   ->  HashAggregate (actual rows=2 loops=1)
-         Group Key: _hyper_1_1_chunk.gp
-         Batches: 1 
-         ->  Seq Scan on _hyper_1_1_chunk (actual rows=6 loops=1)
-(7 rows)
-
-:PREFIX SELECT gp, last(strid, temp) FROM btest GROUP BY gp ORDER BY gp;
-                            QUERY PLAN                            
-------------------------------------------------------------------
- Sort (actual rows=2 loops=1)
-   Sort Key: _hyper_1_1_chunk.gp
-   Sort Method: quicksort 
-   ->  HashAggregate (actual rows=2 loops=1)
-         Group Key: _hyper_1_1_chunk.gp
-         Batches: 1 
-         ->  Seq Scan on _hyper_1_1_chunk (actual rows=6 loops=1)
-(7 rows)
-
-BEGIN;
---check null value as last element
-INSERT INTO btest VALUES('2018-01-20T09:00:43', '2017-01-20T09:00:55', 2, NULL);
-:PREFIX SELECT last(temp, time) FROM btest;
-                                                    QUERY PLAN                                                    
-------------------------------------------------------------------------------------------------------------------
- Result (actual rows=1 loops=1)
-   InitPlan 1 (returns $0)
-     ->  Limit (actual rows=1 loops=1)
-           ->  Custom Scan (ChunkAppend) on btest (actual rows=1 loops=1)
-                 Order: btest."time" DESC
-                 ->  Index Scan using _hyper_1_2_chunk_btest_time_idx on _hyper_1_2_chunk (actual rows=1 loops=1)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan using _hyper_1_1_chunk_btest_time_idx on _hyper_1_1_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-(9 rows)
-
---check non-null element "overrides" NULL because it comes after.
-INSERT INTO btest VALUES('2019-01-20T09:00:43', '2018-01-20T09:00:55', 2, 30.5);
-:PREFIX SELECT last(temp, time) FROM btest;
-                                                    QUERY PLAN                                                    
-------------------------------------------------------------------------------------------------------------------
- Result (actual rows=1 loops=1)
-   InitPlan 1 (returns $0)
-     ->  Limit (actual rows=1 loops=1)
-           ->  Custom Scan (ChunkAppend) on btest (actual rows=1 loops=1)
-                 Order: btest."time" DESC
-                 ->  Index Scan using _hyper_1_3_chunk_btest_time_idx on _hyper_1_3_chunk (actual rows=1 loops=1)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan using _hyper_1_2_chunk_btest_time_idx on _hyper_1_2_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan using _hyper_1_1_chunk_btest_time_idx on _hyper_1_1_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-(11 rows)
-
---check null cmp element is skipped
-INSERT INTO btest VALUES('2018-01-20T09:00:43', NULL, 2, 32.3);
-:PREFIX SELECT last(temp, time_alt) FROM btest;
-                            QUERY PLAN                            
-------------------------------------------------------------------
- Aggregate (actual rows=1 loops=1)
-   ->  Append (actual rows=9 loops=1)
-         ->  Seq Scan on _hyper_1_1_chunk (actual rows=6 loops=1)
-         ->  Seq Scan on _hyper_1_2_chunk (actual rows=2 loops=1)
-         ->  Seq Scan on _hyper_1_3_chunk (actual rows=1 loops=1)
-(5 rows)
-
--- fist returns NULL value
-:PREFIX SELECT first(temp, time_alt) FROM btest;
-                            QUERY PLAN                            
-------------------------------------------------------------------
- Aggregate (actual rows=1 loops=1)
-   ->  Append (actual rows=9 loops=1)
-         ->  Seq Scan on _hyper_1_1_chunk (actual rows=6 loops=1)
-         ->  Seq Scan on _hyper_1_2_chunk (actual rows=2 loops=1)
-         ->  Seq Scan on _hyper_1_3_chunk (actual rows=1 loops=1)
-(5 rows)
-
--- test first return non NULL value
-INSERT INTO btest VALUES('2016-01-20T09:00:00', '2016-01-20T09:00:00', 2, 36.5);
-:PREFIX SELECT first(temp, time_alt) FROM btest;
-                            QUERY PLAN                            
-------------------------------------------------------------------
- Aggregate (actual rows=1 loops=1)
-   ->  Append (actual rows=10 loops=1)
-         ->  Seq Scan on _hyper_1_1_chunk (actual rows=6 loops=1)
-         ->  Seq Scan on _hyper_1_2_chunk (actual rows=2 loops=1)
-         ->  Seq Scan on _hyper_1_3_chunk (actual rows=1 loops=1)
-         ->  Seq Scan on _hyper_1_4_chunk (actual rows=1 loops=1)
-(6 rows)
-
---check non null cmp element insert after null cmp
-INSERT INTO btest VALUES('2020-01-20T09:00:43', '2020-01-20T09:00:43', 2, 35.3);
-:PREFIX SELECT last(temp, time_alt) FROM btest;
-                            QUERY PLAN                            
-------------------------------------------------------------------
- Aggregate (actual rows=1 loops=1)
-   ->  Append (actual rows=11 loops=1)
-         ->  Seq Scan on _hyper_1_1_chunk (actual rows=6 loops=1)
-         ->  Seq Scan on _hyper_1_2_chunk (actual rows=2 loops=1)
-         ->  Seq Scan on _hyper_1_3_chunk (actual rows=1 loops=1)
-         ->  Seq Scan on _hyper_1_4_chunk (actual rows=1 loops=1)
-         ->  Seq Scan on _hyper_1_5_chunk (actual rows=1 loops=1)
-(7 rows)
-
-:PREFIX SELECT first(temp, time_alt) FROM btest;
-                            QUERY PLAN                            
-------------------------------------------------------------------
- Aggregate (actual rows=1 loops=1)
-   ->  Append (actual rows=11 loops=1)
-         ->  Seq Scan on _hyper_1_1_chunk (actual rows=6 loops=1)
-         ->  Seq Scan on _hyper_1_2_chunk (actual rows=2 loops=1)
-         ->  Seq Scan on _hyper_1_3_chunk (actual rows=1 loops=1)
-         ->  Seq Scan on _hyper_1_4_chunk (actual rows=1 loops=1)
-         ->  Seq Scan on _hyper_1_5_chunk (actual rows=1 loops=1)
-(7 rows)
-
---cmp nulls should be ignored and not present in groups
-:PREFIX SELECT gp, last(temp, time_alt) FROM btest GROUP BY gp ORDER BY gp;
-                               QUERY PLAN                               
-------------------------------------------------------------------------
- Sort (actual rows=2 loops=1)
-   Sort Key: _hyper_1_1_chunk.gp
-   Sort Method: quicksort 
-   ->  HashAggregate (actual rows=2 loops=1)
-         Group Key: _hyper_1_1_chunk.gp
-         Batches: 1 
-         ->  Append (actual rows=11 loops=1)
-               ->  Seq Scan on _hyper_1_1_chunk (actual rows=6 loops=1)
-               ->  Seq Scan on _hyper_1_2_chunk (actual rows=2 loops=1)
-               ->  Seq Scan on _hyper_1_3_chunk (actual rows=1 loops=1)
-               ->  Seq Scan on _hyper_1_4_chunk (actual rows=1 loops=1)
-               ->  Seq Scan on _hyper_1_5_chunk (actual rows=1 loops=1)
-(12 rows)
-
---Previously, some bugs were found with NULLS and numeric types, so test that
-INSERT INTO btest_numeric VALUES ('2019-01-20T09:00:43', NULL);
-:PREFIX SELECT last(quantity, time) FROM btest_numeric;
-                                                     QUERY PLAN                                                     
---------------------------------------------------------------------------------------------------------------------
- Result (actual rows=1 loops=1)
-   InitPlan 1 (returns $0)
-     ->  Limit (actual rows=1 loops=1)
-           ->  Index Scan using _hyper_2_6_chunk_btest_numeric_time_idx on _hyper_2_6_chunk (actual rows=1 loops=1)
-                 Index Cond: ("time" IS NOT NULL)
-(5 rows)
-
---check non-null element "overrides" NULL because it comes after.
-INSERT INTO btest_numeric VALUES('2020-01-20T09:00:43', 30.5);
-:PREFIX SELECT last(quantity, time) FROM btest_numeric;
-                                                        QUERY PLAN                                                        
---------------------------------------------------------------------------------------------------------------------------
- Result (actual rows=1 loops=1)
-   InitPlan 1 (returns $0)
-     ->  Limit (actual rows=1 loops=1)
-           ->  Custom Scan (ChunkAppend) on btest_numeric (actual rows=1 loops=1)
-                 Order: btest_numeric."time" DESC
-                 ->  Index Scan using _hyper_2_7_chunk_btest_numeric_time_idx on _hyper_2_7_chunk (actual rows=1 loops=1)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan using _hyper_2_6_chunk_btest_numeric_time_idx on _hyper_2_6_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-(9 rows)
-
--- do index scan for last
-:PREFIX SELECT last(temp, time) FROM btest;
-                                                    QUERY PLAN                                                    
-------------------------------------------------------------------------------------------------------------------
- Result (actual rows=1 loops=1)
-   InitPlan 1 (returns $0)
-     ->  Limit (actual rows=1 loops=1)
-           ->  Custom Scan (ChunkAppend) on btest (actual rows=1 loops=1)
-                 Order: btest."time" DESC
-                 ->  Index Scan using _hyper_1_5_chunk_btest_time_idx on _hyper_1_5_chunk (actual rows=1 loops=1)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan using _hyper_1_3_chunk_btest_time_idx on _hyper_1_3_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan using _hyper_1_2_chunk_btest_time_idx on _hyper_1_2_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan using _hyper_1_1_chunk_btest_time_idx on _hyper_1_1_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan using _hyper_1_4_chunk_btest_time_idx on _hyper_1_4_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-(15 rows)
-
--- do index scan for first
-:PREFIX SELECT first(temp, time) FROM btest;
-                                                        QUERY PLAN                                                         
----------------------------------------------------------------------------------------------------------------------------
- Result (actual rows=1 loops=1)
-   InitPlan 1 (returns $0)
-     ->  Limit (actual rows=1 loops=1)
-           ->  Custom Scan (ChunkAppend) on btest (actual rows=1 loops=1)
-                 Order: btest."time"
-                 ->  Index Scan Backward using _hyper_1_4_chunk_btest_time_idx on _hyper_1_4_chunk (actual rows=1 loops=1)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan Backward using _hyper_1_1_chunk_btest_time_idx on _hyper_1_1_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan Backward using _hyper_1_2_chunk_btest_time_idx on _hyper_1_2_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan Backward using _hyper_1_3_chunk_btest_time_idx on _hyper_1_3_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan Backward using _hyper_1_5_chunk_btest_time_idx on _hyper_1_5_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-(15 rows)
-
--- can't do index scan when ordering on non-index column
-:PREFIX SELECT first(temp, time_alt) FROM btest;
-                            QUERY PLAN                            
-------------------------------------------------------------------
- Aggregate (actual rows=1 loops=1)
-   ->  Append (actual rows=11 loops=1)
-         ->  Seq Scan on _hyper_1_1_chunk (actual rows=6 loops=1)
-         ->  Seq Scan on _hyper_1_2_chunk (actual rows=2 loops=1)
-         ->  Seq Scan on _hyper_1_3_chunk (actual rows=1 loops=1)
-         ->  Seq Scan on _hyper_1_4_chunk (actual rows=1 loops=1)
-         ->  Seq Scan on _hyper_1_5_chunk (actual rows=1 loops=1)
-(7 rows)
-
--- do index scan for subquery
-:PREFIX SELECT * FROM (SELECT last(temp, time) FROM btest) last;
-                                                    QUERY PLAN                                                    
-------------------------------------------------------------------------------------------------------------------
- Result (actual rows=1 loops=1)
-   InitPlan 1 (returns $0)
-     ->  Limit (actual rows=1 loops=1)
-           ->  Custom Scan (ChunkAppend) on btest (actual rows=1 loops=1)
-                 Order: btest."time" DESC
-                 ->  Index Scan using _hyper_1_5_chunk_btest_time_idx on _hyper_1_5_chunk (actual rows=1 loops=1)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan using _hyper_1_3_chunk_btest_time_idx on _hyper_1_3_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan using _hyper_1_2_chunk_btest_time_idx on _hyper_1_2_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan using _hyper_1_1_chunk_btest_time_idx on _hyper_1_1_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan using _hyper_1_4_chunk_btest_time_idx on _hyper_1_4_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-(15 rows)
-
--- can't do index scan when using group by
-:PREFIX SELECT last(temp, time) FROM btest GROUP BY gp ORDER BY gp;
-                               QUERY PLAN                               
-------------------------------------------------------------------------
- Sort (actual rows=2 loops=1)
-   Sort Key: _hyper_1_1_chunk.gp
-   Sort Method: quicksort 
-   ->  HashAggregate (actual rows=2 loops=1)
-         Group Key: _hyper_1_1_chunk.gp
-         Batches: 1 
-         ->  Append (actual rows=11 loops=1)
-               ->  Seq Scan on _hyper_1_1_chunk (actual rows=6 loops=1)
-               ->  Seq Scan on _hyper_1_2_chunk (actual rows=2 loops=1)
-               ->  Seq Scan on _hyper_1_3_chunk (actual rows=1 loops=1)
-               ->  Seq Scan on _hyper_1_4_chunk (actual rows=1 loops=1)
-               ->  Seq Scan on _hyper_1_5_chunk (actual rows=1 loops=1)
-(12 rows)
-
--- do index scan when agg function is used in CTE subquery
-:PREFIX WITH last_temp AS (SELECT last(temp, time) FROM btest) SELECT * from last_temp;
-                                                    QUERY PLAN                                                    
-------------------------------------------------------------------------------------------------------------------
- Result (actual rows=1 loops=1)
-   InitPlan 1 (returns $0)
-     ->  Limit (actual rows=1 loops=1)
-           ->  Custom Scan (ChunkAppend) on btest (actual rows=1 loops=1)
-                 Order: btest."time" DESC
-                 ->  Index Scan using _hyper_1_5_chunk_btest_time_idx on _hyper_1_5_chunk (actual rows=1 loops=1)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan using _hyper_1_3_chunk_btest_time_idx on _hyper_1_3_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan using _hyper_1_2_chunk_btest_time_idx on _hyper_1_2_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan using _hyper_1_1_chunk_btest_time_idx on _hyper_1_1_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan using _hyper_1_4_chunk_btest_time_idx on _hyper_1_4_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-(15 rows)
-
--- do index scan when using both FIRST and LAST aggregate functions
-:PREFIX SELECT first(temp, time), last(temp, time) FROM btest;
-                                                                  QUERY PLAN                                                                  
-----------------------------------------------------------------------------------------------------------------------------------------------
- Result (actual rows=1 loops=1)
-   InitPlan 1 (returns $1)
-     ->  Limit (actual rows=1 loops=1)
-           ->  Custom Scan (ChunkAppend) on btest (actual rows=1 loops=1)
-                 Order: btest."time" DESC
-                 ->  Index Scan using _hyper_1_5_chunk_btest_time_idx on _hyper_1_5_chunk (actual rows=1 loops=1)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan using _hyper_1_3_chunk_btest_time_idx on _hyper_1_3_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan using _hyper_1_2_chunk_btest_time_idx on _hyper_1_2_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan using _hyper_1_1_chunk_btest_time_idx on _hyper_1_1_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan using _hyper_1_4_chunk_btest_time_idx on _hyper_1_4_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-   InitPlan 2 (returns $0)
-     ->  Limit (actual rows=1 loops=1)
-           ->  Custom Scan (ChunkAppend) on btest btest_1 (actual rows=1 loops=1)
-                 Order: btest_1."time"
-                 ->  Index Scan Backward using _hyper_1_4_chunk_btest_time_idx on _hyper_1_4_chunk _hyper_1_4_chunk_1 (actual rows=1 loops=1)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan Backward using _hyper_1_1_chunk_btest_time_idx on _hyper_1_1_chunk _hyper_1_1_chunk_1 (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan Backward using _hyper_1_2_chunk_btest_time_idx on _hyper_1_2_chunk _hyper_1_2_chunk_1 (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan Backward using _hyper_1_3_chunk_btest_time_idx on _hyper_1_3_chunk _hyper_1_3_chunk_1 (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan Backward using _hyper_1_5_chunk_btest_time_idx on _hyper_1_5_chunk _hyper_1_5_chunk_1 (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-(29 rows)
-
--- verify results when using both FIRST and LAST
-:PREFIX SELECT first(temp, time), last(temp, time) FROM btest;
-                                                                  QUERY PLAN                                                                  
-----------------------------------------------------------------------------------------------------------------------------------------------
- Result (actual rows=1 loops=1)
-   InitPlan 1 (returns $1)
-     ->  Limit (actual rows=1 loops=1)
-           ->  Custom Scan (ChunkAppend) on btest (actual rows=1 loops=1)
-                 Order: btest."time" DESC
-                 ->  Index Scan using _hyper_1_5_chunk_btest_time_idx on _hyper_1_5_chunk (actual rows=1 loops=1)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan using _hyper_1_3_chunk_btest_time_idx on _hyper_1_3_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan using _hyper_1_2_chunk_btest_time_idx on _hyper_1_2_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan using _hyper_1_1_chunk_btest_time_idx on _hyper_1_1_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan using _hyper_1_4_chunk_btest_time_idx on _hyper_1_4_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-   InitPlan 2 (returns $0)
-     ->  Limit (actual rows=1 loops=1)
-           ->  Custom Scan (ChunkAppend) on btest btest_1 (actual rows=1 loops=1)
-                 Order: btest_1."time"
-                 ->  Index Scan Backward using _hyper_1_4_chunk_btest_time_idx on _hyper_1_4_chunk _hyper_1_4_chunk_1 (actual rows=1 loops=1)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan Backward using _hyper_1_1_chunk_btest_time_idx on _hyper_1_1_chunk _hyper_1_1_chunk_1 (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan Backward using _hyper_1_2_chunk_btest_time_idx on _hyper_1_2_chunk _hyper_1_2_chunk_1 (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan Backward using _hyper_1_3_chunk_btest_time_idx on _hyper_1_3_chunk _hyper_1_3_chunk_1 (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan Backward using _hyper_1_5_chunk_btest_time_idx on _hyper_1_5_chunk _hyper_1_5_chunk_1 (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-(29 rows)
-
--- do index scan when using WHERE
-:PREFIX SELECT last(temp, time) FROM btest WHERE time <= '2017-01-20T09:00:02';
-                                                            QUERY PLAN                                                            
-----------------------------------------------------------------------------------------------------------------------------------
- Result (actual rows=1 loops=1)
-   InitPlan 1 (returns $0)
-     ->  Limit (actual rows=1 loops=1)
-           ->  Custom Scan (ChunkAppend) on btest (actual rows=1 loops=1)
-                 Order: btest."time" DESC
-                 ->  Index Scan using _hyper_1_1_chunk_btest_time_idx on _hyper_1_1_chunk (actual rows=1 loops=1)
-                       Index Cond: (("time" IS NOT NULL) AND ("time" <= 'Fri Jan 20 09:00:02 2017'::timestamp without time zone))
-                 ->  Index Scan using _hyper_1_4_chunk_btest_time_idx on _hyper_1_4_chunk (never executed)
-                       Index Cond: (("time" IS NOT NULL) AND ("time" <= 'Fri Jan 20 09:00:02 2017'::timestamp without time zone))
-(9 rows)
-
--- can't do index scan for MAX and LAST combined (MinMax optimization fails when having different aggregate functions)
-:PREFIX SELECT max(time), last(temp, time) FROM btest;
-                            QUERY PLAN                            
-------------------------------------------------------------------
- Aggregate (actual rows=1 loops=1)
-   ->  Append (actual rows=11 loops=1)
-         ->  Seq Scan on _hyper_1_1_chunk (actual rows=6 loops=1)
-         ->  Seq Scan on _hyper_1_2_chunk (actual rows=2 loops=1)
-         ->  Seq Scan on _hyper_1_3_chunk (actual rows=1 loops=1)
-         ->  Seq Scan on _hyper_1_4_chunk (actual rows=1 loops=1)
-         ->  Seq Scan on _hyper_1_5_chunk (actual rows=1 loops=1)
-(7 rows)
-
--- can't do index scan when using FIRST/LAST in ORDER BY
-:PREFIX SELECT last(temp, time) FROM btest ORDER BY last(temp, time);
-                               QUERY PLAN                               
-------------------------------------------------------------------------
- Sort (actual rows=1 loops=1)
-   Sort Key: (last(_hyper_1_1_chunk.temp, _hyper_1_1_chunk."time"))
-   Sort Method: quicksort 
-   ->  Aggregate (actual rows=1 loops=1)
-         ->  Append (actual rows=11 loops=1)
-               ->  Seq Scan on _hyper_1_1_chunk (actual rows=6 loops=1)
-               ->  Seq Scan on _hyper_1_2_chunk (actual rows=2 loops=1)
-               ->  Seq Scan on _hyper_1_3_chunk (actual rows=1 loops=1)
-               ->  Seq Scan on _hyper_1_4_chunk (actual rows=1 loops=1)
-               ->  Seq Scan on _hyper_1_5_chunk (actual rows=1 loops=1)
-(10 rows)
-
--- do index scan
-:PREFIX SELECT last(temp, time) FROM btest WHERE temp < 30;
-                                                    QUERY PLAN                                                    
-------------------------------------------------------------------------------------------------------------------
- Result (actual rows=1 loops=1)
-   InitPlan 1 (returns $0)
-     ->  Limit (actual rows=1 loops=1)
-           ->  Custom Scan (ChunkAppend) on btest (actual rows=1 loops=1)
-                 Order: btest."time" DESC
-                 ->  Index Scan using _hyper_1_5_chunk_btest_time_idx on _hyper_1_5_chunk (actual rows=0 loops=1)
-                       Index Cond: ("time" IS NOT NULL)
-                       Filter: (temp < '30'::double precision)
-                       Rows Removed by Filter: 1
-                 ->  Index Scan using _hyper_1_3_chunk_btest_time_idx on _hyper_1_3_chunk (actual rows=0 loops=1)
-                       Index Cond: ("time" IS NOT NULL)
-                       Filter: (temp < '30'::double precision)
-                       Rows Removed by Filter: 1
-                 ->  Index Scan using _hyper_1_2_chunk_btest_time_idx on _hyper_1_2_chunk (actual rows=0 loops=1)
-                       Index Cond: ("time" IS NOT NULL)
-                       Filter: (temp < '30'::double precision)
-                       Rows Removed by Filter: 2
-                 ->  Index Scan using _hyper_1_1_chunk_btest_time_idx on _hyper_1_1_chunk (actual rows=1 loops=1)
-                       Index Cond: ("time" IS NOT NULL)
-                       Filter: (temp < '30'::double precision)
-                 ->  Index Scan using _hyper_1_4_chunk_btest_time_idx on _hyper_1_4_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-                       Filter: (temp < '30'::double precision)
-(23 rows)
-
--- SELECT first(temp, time) FROM btest WHERE time >= '2017-01-20 09:00:47';
--- do index scan
-:PREFIX SELECT first(temp, time) FROM btest WHERE time >= '2017-01-20 09:00:47';
-                                                            QUERY PLAN                                                            
-----------------------------------------------------------------------------------------------------------------------------------
- Result (actual rows=1 loops=1)
-   InitPlan 1 (returns $0)
-     ->  Limit (actual rows=1 loops=1)
-           ->  Custom Scan (ChunkAppend) on btest (actual rows=1 loops=1)
-                 Order: btest."time"
-                 ->  Index Scan Backward using _hyper_1_1_chunk_btest_time_idx on _hyper_1_1_chunk (actual rows=1 loops=1)
-                       Index Cond: (("time" IS NOT NULL) AND ("time" >= 'Fri Jan 20 09:00:47 2017'::timestamp without time zone))
-                 ->  Index Scan Backward using _hyper_1_2_chunk_btest_time_idx on _hyper_1_2_chunk (never executed)
-                       Index Cond: (("time" IS NOT NULL) AND ("time" >= 'Fri Jan 20 09:00:47 2017'::timestamp without time zone))
-                 ->  Index Scan Backward using _hyper_1_3_chunk_btest_time_idx on _hyper_1_3_chunk (never executed)
-                       Index Cond: (("time" IS NOT NULL) AND ("time" >= 'Fri Jan 20 09:00:47 2017'::timestamp without time zone))
-                 ->  Index Scan Backward using _hyper_1_5_chunk_btest_time_idx on _hyper_1_5_chunk (never executed)
-                       Index Cond: (("time" IS NOT NULL) AND ("time" >= 'Fri Jan 20 09:00:47 2017'::timestamp without time zone))
-(13 rows)
-
--- can't do index scan when using WINDOW function
-:PREFIX SELECT gp, last(temp, time) OVER (PARTITION BY gp) AS last FROM btest;
-                               QUERY PLAN                               
-------------------------------------------------------------------------
- WindowAgg (actual rows=11 loops=1)
-   ->  Sort (actual rows=11 loops=1)
-         Sort Key: _hyper_1_1_chunk.gp
-         Sort Method: quicksort 
-         ->  Append (actual rows=11 loops=1)
-               ->  Seq Scan on _hyper_1_1_chunk (actual rows=6 loops=1)
-               ->  Seq Scan on _hyper_1_2_chunk (actual rows=2 loops=1)
-               ->  Seq Scan on _hyper_1_3_chunk (actual rows=1 loops=1)
-               ->  Seq Scan on _hyper_1_4_chunk (actual rows=1 loops=1)
-               ->  Seq Scan on _hyper_1_5_chunk (actual rows=1 loops=1)
-(10 rows)
-
--- test constants
-:PREFIX SELECT first(100, 100) FROM btest;
-                                   QUERY PLAN                                   
---------------------------------------------------------------------------------
- Result (actual rows=1 loops=1)
-   InitPlan 1 (returns $0)
-     ->  Limit (actual rows=1 loops=1)
-           ->  Result (actual rows=1 loops=1)
-                 ->  Append (actual rows=1 loops=1)
-                       ->  Seq Scan on _hyper_1_1_chunk (actual rows=1 loops=1)
-                       ->  Seq Scan on _hyper_1_2_chunk (never executed)
-                       ->  Seq Scan on _hyper_1_3_chunk (never executed)
-                       ->  Seq Scan on _hyper_1_4_chunk (never executed)
-                       ->  Seq Scan on _hyper_1_5_chunk (never executed)
-(10 rows)
-
--- create an index so we can test optimization
-CREATE INDEX btest_time_alt_idx ON btest(time_alt);
-:PREFIX SELECT last(temp, time_alt) FROM btest;
-                                                          QUERY PLAN                                                           
--------------------------------------------------------------------------------------------------------------------------------
- Result (actual rows=1 loops=1)
-   InitPlan 1 (returns $0)
-     ->  Limit (actual rows=1 loops=1)
-           ->  Merge Append (actual rows=1 loops=1)
-                 Sort Key: _hyper_1_1_chunk.time_alt DESC
-                 ->  Index Scan Backward using _hyper_1_1_chunk_btest_time_alt_idx on _hyper_1_1_chunk (actual rows=1 loops=1)
-                       Index Cond: (time_alt IS NOT NULL)
-                 ->  Index Scan Backward using _hyper_1_2_chunk_btest_time_alt_idx on _hyper_1_2_chunk (actual rows=1 loops=1)
-                       Index Cond: (time_alt IS NOT NULL)
-                 ->  Index Scan Backward using _hyper_1_3_chunk_btest_time_alt_idx on _hyper_1_3_chunk (actual rows=1 loops=1)
-                       Index Cond: (time_alt IS NOT NULL)
-                 ->  Index Scan Backward using _hyper_1_4_chunk_btest_time_alt_idx on _hyper_1_4_chunk (actual rows=1 loops=1)
-                       Index Cond: (time_alt IS NOT NULL)
-                 ->  Index Scan Backward using _hyper_1_5_chunk_btest_time_alt_idx on _hyper_1_5_chunk (actual rows=1 loops=1)
-                       Index Cond: (time_alt IS NOT NULL)
-(15 rows)
-
---test nested FIRST/LAST - should optimize
-:PREFIX SELECT abs(last(temp, time)) FROM btest;
-                                                    QUERY PLAN                                                    
-------------------------------------------------------------------------------------------------------------------
- Result (actual rows=1 loops=1)
-   InitPlan 1 (returns $0)
-     ->  Limit (actual rows=1 loops=1)
-           ->  Custom Scan (ChunkAppend) on btest (actual rows=1 loops=1)
-                 Order: btest."time" DESC
-                 ->  Index Scan using _hyper_1_5_chunk_btest_time_idx on _hyper_1_5_chunk (actual rows=1 loops=1)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan using _hyper_1_3_chunk_btest_time_idx on _hyper_1_3_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan using _hyper_1_2_chunk_btest_time_idx on _hyper_1_2_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan using _hyper_1_1_chunk_btest_time_idx on _hyper_1_1_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan using _hyper_1_4_chunk_btest_time_idx on _hyper_1_4_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-(15 rows)
-
--- test nested FIRST/LAST in ORDER BY - no optimization possible
-:PREFIX SELECT abs(last(temp, time)) FROM btest ORDER BY abs(last(temp,time));
-                               QUERY PLAN                                
--------------------------------------------------------------------------
- Sort (actual rows=1 loops=1)
-   Sort Key: (abs(last(_hyper_1_1_chunk.temp, _hyper_1_1_chunk."time")))
-   Sort Method: quicksort 
-   ->  Aggregate (actual rows=1 loops=1)
-         ->  Append (actual rows=11 loops=1)
-               ->  Seq Scan on _hyper_1_1_chunk (actual rows=6 loops=1)
-               ->  Seq Scan on _hyper_1_2_chunk (actual rows=2 loops=1)
-               ->  Seq Scan on _hyper_1_3_chunk (actual rows=1 loops=1)
-               ->  Seq Scan on _hyper_1_4_chunk (actual rows=1 loops=1)
-               ->  Seq Scan on _hyper_1_5_chunk (actual rows=1 loops=1)
-(10 rows)
-
-ROLLBACK;
--- Test with NULL numeric values
-BEGIN;
-TRUNCATE btest_numeric;
--- Empty table
-:PREFIX SELECT first(btest_numeric, time) FROM btest_numeric;
-              QUERY PLAN              
---------------------------------------
- Aggregate (actual rows=1 loops=1)
-   ->  Result (actual rows=0 loops=1)
-         One-Time Filter: false
-(3 rows)
-
-:PREFIX SELECT last(btest_numeric, time) FROM btest_numeric;
-              QUERY PLAN              
---------------------------------------
- Aggregate (actual rows=1 loops=1)
-   ->  Result (actual rows=0 loops=1)
-         One-Time Filter: false
-(3 rows)
-
--- Only NULL values
-INSERT INTO btest_numeric VALUES('2018-01-20T09:00:43', NULL);
-INSERT INTO btest_numeric VALUES('2018-01-20T09:00:43', NULL);
-:PREFIX SELECT first(quantity, time) FROM btest_numeric;
-                                                         QUERY PLAN                                                          
------------------------------------------------------------------------------------------------------------------------------
- Result (actual rows=1 loops=1)
-   InitPlan 1 (returns $0)
-     ->  Limit (actual rows=1 loops=1)
-           ->  Index Scan Backward using _hyper_2_8_chunk_btest_numeric_time_idx on _hyper_2_8_chunk (actual rows=1 loops=1)
-                 Index Cond: ("time" IS NOT NULL)
-(5 rows)
-
-:PREFIX SELECT last(quantity, time) FROM btest_numeric;
-                                                     QUERY PLAN                                                     
---------------------------------------------------------------------------------------------------------------------
- Result (actual rows=1 loops=1)
-   InitPlan 1 (returns $0)
-     ->  Limit (actual rows=1 loops=1)
-           ->  Index Scan using _hyper_2_8_chunk_btest_numeric_time_idx on _hyper_2_8_chunk (actual rows=1 loops=1)
-                 Index Cond: ("time" IS NOT NULL)
-(5 rows)
-
-:PREFIX SELECT first(time, quantity) FROM btest_numeric;
-                         QUERY PLAN                         
-------------------------------------------------------------
- Aggregate (actual rows=1 loops=1)
-   ->  Seq Scan on _hyper_2_8_chunk (actual rows=2 loops=1)
-(2 rows)
-
-:PREFIX SELECT last(time, quantity) FROM btest_numeric;
-                         QUERY PLAN                         
-------------------------------------------------------------
- Aggregate (actual rows=1 loops=1)
-   ->  Seq Scan on _hyper_2_8_chunk (actual rows=2 loops=1)
-(2 rows)
-
--- NULL values followed by non-NULL values
-INSERT INTO btest_numeric VALUES('2019-01-20T09:00:43', 1);
-INSERT INTO btest_numeric VALUES('2019-01-20T09:00:43', 2);
-:PREFIX SELECT first(quantity, time) FROM btest_numeric;
-                                                            QUERY PLAN                                                             
------------------------------------------------------------------------------------------------------------------------------------
- Result (actual rows=1 loops=1)
-   InitPlan 1 (returns $0)
-     ->  Limit (actual rows=1 loops=1)
-           ->  Custom Scan (ChunkAppend) on btest_numeric (actual rows=1 loops=1)
-                 Order: btest_numeric."time"
-                 ->  Index Scan Backward using _hyper_2_8_chunk_btest_numeric_time_idx on _hyper_2_8_chunk (actual rows=1 loops=1)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan Backward using _hyper_2_9_chunk_btest_numeric_time_idx on _hyper_2_9_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-(9 rows)
-
-:PREFIX SELECT last(quantity, time) FROM btest_numeric;
-                                                        QUERY PLAN                                                        
---------------------------------------------------------------------------------------------------------------------------
- Result (actual rows=1 loops=1)
-   InitPlan 1 (returns $0)
-     ->  Limit (actual rows=1 loops=1)
-           ->  Custom Scan (ChunkAppend) on btest_numeric (actual rows=1 loops=1)
-                 Order: btest_numeric."time" DESC
-                 ->  Index Scan using _hyper_2_9_chunk_btest_numeric_time_idx on _hyper_2_9_chunk (actual rows=1 loops=1)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan using _hyper_2_8_chunk_btest_numeric_time_idx on _hyper_2_8_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-(9 rows)
-
-:PREFIX SELECT first(time, quantity) FROM btest_numeric;
-                            QUERY PLAN                            
-------------------------------------------------------------------
- Aggregate (actual rows=1 loops=1)
-   ->  Append (actual rows=4 loops=1)
-         ->  Seq Scan on _hyper_2_8_chunk (actual rows=2 loops=1)
-         ->  Seq Scan on _hyper_2_9_chunk (actual rows=2 loops=1)
-(4 rows)
-
-:PREFIX SELECT last(time, quantity) FROM btest_numeric;
-                            QUERY PLAN                            
-------------------------------------------------------------------
- Aggregate (actual rows=1 loops=1)
-   ->  Append (actual rows=4 loops=1)
-         ->  Seq Scan on _hyper_2_8_chunk (actual rows=2 loops=1)
-         ->  Seq Scan on _hyper_2_9_chunk (actual rows=2 loops=1)
-(4 rows)
-
-TRUNCATE btest_numeric;
--- non-NULL values followed by NULL values
-INSERT INTO btest_numeric VALUES('2019-01-20T09:00:43', 1);
-INSERT INTO btest_numeric VALUES('2019-01-20T09:00:43', 2);
-INSERT INTO btest_numeric VALUES('2018-01-20T09:00:43', NULL);
-INSERT INTO btest_numeric VALUES('2018-01-20T09:00:43', NULL);
-:PREFIX SELECT first(quantity, time) FROM btest_numeric;
-                                                             QUERY PLAN                                                              
--------------------------------------------------------------------------------------------------------------------------------------
- Result (actual rows=1 loops=1)
-   InitPlan 1 (returns $0)
-     ->  Limit (actual rows=1 loops=1)
-           ->  Custom Scan (ChunkAppend) on btest_numeric (actual rows=1 loops=1)
-                 Order: btest_numeric."time"
-                 ->  Index Scan Backward using _hyper_2_11_chunk_btest_numeric_time_idx on _hyper_2_11_chunk (actual rows=1 loops=1)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan Backward using _hyper_2_10_chunk_btest_numeric_time_idx on _hyper_2_10_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-(9 rows)
-
-:PREFIX SELECT last(quantity, time) FROM btest_numeric;
-                                                         QUERY PLAN                                                         
-----------------------------------------------------------------------------------------------------------------------------
- Result (actual rows=1 loops=1)
-   InitPlan 1 (returns $0)
-     ->  Limit (actual rows=1 loops=1)
-           ->  Custom Scan (ChunkAppend) on btest_numeric (actual rows=1 loops=1)
-                 Order: btest_numeric."time" DESC
-                 ->  Index Scan using _hyper_2_10_chunk_btest_numeric_time_idx on _hyper_2_10_chunk (actual rows=1 loops=1)
-                       Index Cond: ("time" IS NOT NULL)
-                 ->  Index Scan using _hyper_2_11_chunk_btest_numeric_time_idx on _hyper_2_11_chunk (never executed)
-                       Index Cond: ("time" IS NOT NULL)
-(9 rows)
-
-:PREFIX SELECT first(time, quantity) FROM btest_numeric;
-                            QUERY PLAN                             
--------------------------------------------------------------------
- Aggregate (actual rows=1 loops=1)
-   ->  Append (actual rows=4 loops=1)
-         ->  Seq Scan on _hyper_2_10_chunk (actual rows=2 loops=1)
-         ->  Seq Scan on _hyper_2_11_chunk (actual rows=2 loops=1)
-(4 rows)
-
-:PREFIX SELECT last(time, quantity) FROM btest_numeric;
-                            QUERY PLAN                             
--------------------------------------------------------------------
- Aggregate (actual rows=1 loops=1)
-   ->  Append (actual rows=4 loops=1)
-         ->  Seq Scan on _hyper_2_10_chunk (actual rows=2 loops=1)
-         ->  Seq Scan on _hyper_2_11_chunk (actual rows=2 loops=1)
-(4 rows)
-
-ROLLBACK;
--- we want test results as part of the output too to make sure we produce correct output
-\set PREFIX ''
-\ir :TEST_QUERY_NAME
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
--- canary for results diff
--- this should be only output of results diff
-SELECT setting, current_setting(setting) AS value from (VALUES ('timescaledb.enable_optimizations')) v(setting);
-             setting              | value 
-----------------------------------+-------
- timescaledb.enable_optimizations | on
-(1 row)
-
-:PREFIX SELECT time, gp, temp FROM btest ORDER BY time;
-           time           | gp | temp 
---------------------------+----+------
- Fri Jan 20 09:00:01 2017 |  1 | 22.5
- Fri Jan 20 09:00:02 2017 |  2 | 35.5
- Fri Jan 20 09:00:21 2017 |  1 | 21.2
- Fri Jan 20 09:00:21 2017 |  2 | 30.2
- Fri Jan 20 09:00:43 2017 |  2 | 20.1
- Fri Jan 20 09:00:47 2017 |  1 | 25.1
-(6 rows)
-
-:PREFIX SELECT last(temp, time) FROM btest;
- last 
-------
- 25.1
-(1 row)
-
-:PREFIX SELECT first(temp, time) FROM btest;
- first 
--------
-  22.5
-(1 row)
-
-:PREFIX SELECT last(temp, time_alt) FROM btest;
- last 
-------
- 22.5
-(1 row)
-
-:PREFIX SELECT first(temp, time_alt) FROM btest;
- first 
--------
-  30.2
-(1 row)
-
-:PREFIX SELECT gp, last(temp, time) FROM btest GROUP BY gp ORDER BY gp;
- gp | last 
-----+------
-  1 | 25.1
-  2 | 20.1
-(2 rows)
-
-:PREFIX SELECT gp, first(temp, time) FROM btest GROUP BY gp ORDER BY gp;
- gp | first 
-----+-------
-  1 |  22.5
-  2 |  35.5
-(2 rows)
-
---check whole row
-:PREFIX SELECT gp, first(btest, time) FROM btest GROUP BY gp ORDER BY gp;
- gp |                                 first                                  
-----+------------------------------------------------------------------------
-  1 | ("Fri Jan 20 09:00:01 2017","Fri Jan 20 10:00:00 2017",1,22.5,testing)
-  2 | ("Fri Jan 20 09:00:02 2017","Fri Jan 20 09:00:57 2017",2,35.5,testing)
-(2 rows)
-
---check toasted col
-:PREFIX SELECT gp, left(last(strid, time), 10) FROM btest GROUP BY gp ORDER BY gp;
- gp |    left    
-----+------------
-  1 | testing
-  2 | xyzxyzxyzx
-(2 rows)
-
-:PREFIX SELECT gp, last(temp, strid) FROM btest GROUP BY gp ORDER BY gp;
- gp | last 
-----+------
-  1 | 22.5
-  2 | 20.1
-(2 rows)
-
-:PREFIX SELECT gp, last(strid, temp) FROM btest GROUP BY gp ORDER BY gp;
- gp |  last   
-----+---------
-  1 | testing
-  2 | testing
-(2 rows)
-
-BEGIN;
---check null value as last element
-INSERT INTO btest VALUES('2018-01-20T09:00:43', '2017-01-20T09:00:55', 2, NULL);
-:PREFIX SELECT last(temp, time) FROM btest;
- last 
-------
-     
-(1 row)
-
---check non-null element "overrides" NULL because it comes after.
-INSERT INTO btest VALUES('2019-01-20T09:00:43', '2018-01-20T09:00:55', 2, 30.5);
-:PREFIX SELECT last(temp, time) FROM btest;
- last 
-------
- 30.5
-(1 row)
-
---check null cmp element is skipped
-INSERT INTO btest VALUES('2018-01-20T09:00:43', NULL, 2, 32.3);
-:PREFIX SELECT last(temp, time_alt) FROM btest;
- last 
-------
- 30.5
-(1 row)
-
--- fist returns NULL value
-:PREFIX SELECT first(temp, time_alt) FROM btest;
- first 
--------
-      
-(1 row)
-
--- test first return non NULL value
-INSERT INTO btest VALUES('2016-01-20T09:00:00', '2016-01-20T09:00:00', 2, 36.5);
-:PREFIX SELECT first(temp, time_alt) FROM btest;
- first 
--------
-  36.5
-(1 row)
-
---check non null cmp element insert after null cmp
-INSERT INTO btest VALUES('2020-01-20T09:00:43', '2020-01-20T09:00:43', 2, 35.3);
-:PREFIX SELECT last(temp, time_alt) FROM btest;
- last 
-------
- 35.3
-(1 row)
-
-:PREFIX SELECT first(temp, time_alt) FROM btest;
- first 
--------
-  36.5
-(1 row)
-
---cmp nulls should be ignored and not present in groups
-:PREFIX SELECT gp, last(temp, time_alt) FROM btest GROUP BY gp ORDER BY gp;
- gp | last 
-----+------
-  1 | 22.5
-  2 | 35.3
-(2 rows)
-
---Previously, some bugs were found with NULLS and numeric types, so test that
-INSERT INTO btest_numeric VALUES ('2019-01-20T09:00:43', NULL);
-:PREFIX SELECT last(quantity, time) FROM btest_numeric;
- last 
-------
-     
-(1 row)
-
---check non-null element "overrides" NULL because it comes after.
-INSERT INTO btest_numeric VALUES('2020-01-20T09:00:43', 30.5);
-:PREFIX SELECT last(quantity, time) FROM btest_numeric;
- last 
-------
- 30.5
-(1 row)
-
--- do index scan for last
-:PREFIX SELECT last(temp, time) FROM btest;
- last 
-------
- 35.3
-(1 row)
-
--- do index scan for first
-:PREFIX SELECT first(temp, time) FROM btest;
- first 
--------
-  36.5
-(1 row)
-
--- can't do index scan when ordering on non-index column
-:PREFIX SELECT first(temp, time_alt) FROM btest;
- first 
--------
-  36.5
-(1 row)
-
--- do index scan for subquery
-:PREFIX SELECT * FROM (SELECT last(temp, time) FROM btest) last;
- last 
-------
- 35.3
-(1 row)
-
--- can't do index scan when using group by
-:PREFIX SELECT last(temp, time) FROM btest GROUP BY gp ORDER BY gp;
- last 
-------
- 25.1
- 35.3
-(2 rows)
-
--- do index scan when agg function is used in CTE subquery
-:PREFIX WITH last_temp AS (SELECT last(temp, time) FROM btest) SELECT * from last_temp;
- last 
-------
- 35.3
-(1 row)
-
--- do index scan when using both FIRST and LAST aggregate functions
-:PREFIX SELECT first(temp, time), last(temp, time) FROM btest;
- first | last 
--------+------
-  36.5 | 35.3
-(1 row)
-
--- verify results when using both FIRST and LAST
-:PREFIX SELECT first(temp, time), last(temp, time) FROM btest;
- first | last 
--------+------
-  36.5 | 35.3
-(1 row)
-
--- do index scan when using WHERE
-:PREFIX SELECT last(temp, time) FROM btest WHERE time <= '2017-01-20T09:00:02';
- last 
-------
- 35.5
-(1 row)
-
--- can't do index scan for MAX and LAST combined (MinMax optimization fails when having different aggregate functions)
-:PREFIX SELECT max(time), last(temp, time) FROM btest;
-           max            | last 
---------------------------+------
- Mon Jan 20 09:00:43 2020 | 35.3
-(1 row)
-
--- can't do index scan when using FIRST/LAST in ORDER BY
-:PREFIX SELECT last(temp, time) FROM btest ORDER BY last(temp, time);
- last 
-------
- 35.3
-(1 row)
-
--- do index scan
-:PREFIX SELECT last(temp, time) FROM btest WHERE temp < 30;
- last 
-------
- 25.1
-(1 row)
-
--- SELECT first(temp, time) FROM btest WHERE time >= '2017-01-20 09:00:47';
--- do index scan
-:PREFIX SELECT first(temp, time) FROM btest WHERE time >= '2017-01-20 09:00:47';
- first 
--------
-  25.1
-(1 row)
-
--- can't do index scan when using WINDOW function
-:PREFIX SELECT gp, last(temp, time) OVER (PARTITION BY gp) AS last FROM btest;
- gp | last 
-----+------
-  1 | 25.1
-  1 | 25.1
-  1 | 25.1
-  2 | 35.3
-  2 | 35.3
-  2 | 35.3
-  2 | 35.3
-  2 | 35.3
-  2 | 35.3
-  2 | 35.3
-  2 | 35.3
-(11 rows)
-
--- test constants
-:PREFIX SELECT first(100, 100) FROM btest;
- first 
--------
-   100
-(1 row)
-
--- create an index so we can test optimization
-CREATE INDEX btest_time_alt_idx ON btest(time_alt);
-:PREFIX SELECT last(temp, time_alt) FROM btest;
- last 
-------
- 35.3
-(1 row)
-
---test nested FIRST/LAST - should optimize
-:PREFIX SELECT abs(last(temp, time)) FROM btest;
- abs  
-------
- 35.3
-(1 row)
-
--- test nested FIRST/LAST in ORDER BY - no optimization possible
-:PREFIX SELECT abs(last(temp, time)) FROM btest ORDER BY abs(last(temp,time));
- abs  
-------
- 35.3
-(1 row)
-
-ROLLBACK;
--- Test with NULL numeric values
-BEGIN;
-TRUNCATE btest_numeric;
--- Empty table
-:PREFIX SELECT first(btest_numeric, time) FROM btest_numeric;
- first 
--------
- 
-(1 row)
-
-:PREFIX SELECT last(btest_numeric, time) FROM btest_numeric;
- last 
-------
- 
-(1 row)
-
--- Only NULL values
-INSERT INTO btest_numeric VALUES('2018-01-20T09:00:43', NULL);
-INSERT INTO btest_numeric VALUES('2018-01-20T09:00:43', NULL);
-:PREFIX SELECT first(quantity, time) FROM btest_numeric;
- first 
--------
-      
-(1 row)
-
-:PREFIX SELECT last(quantity, time) FROM btest_numeric;
- last 
-------
-     
-(1 row)
-
-:PREFIX SELECT first(time, quantity) FROM btest_numeric;
- first 
--------
- 
-(1 row)
-
-:PREFIX SELECT last(time, quantity) FROM btest_numeric;
- last 
-------
- 
-(1 row)
-
--- NULL values followed by non-NULL values
-INSERT INTO btest_numeric VALUES('2019-01-20T09:00:43', 1);
-INSERT INTO btest_numeric VALUES('2019-01-20T09:00:43', 2);
-:PREFIX SELECT first(quantity, time) FROM btest_numeric;
- first 
--------
-      
-(1 row)
-
-:PREFIX SELECT last(quantity, time) FROM btest_numeric;
- last 
-------
-    1
-(1 row)
-
-:PREFIX SELECT first(time, quantity) FROM btest_numeric;
-          first           
---------------------------
- Sun Jan 20 09:00:43 2019
-(1 row)
-
-:PREFIX SELECT last(time, quantity) FROM btest_numeric;
-           last           
---------------------------
- Sun Jan 20 09:00:43 2019
-(1 row)
-
-TRUNCATE btest_numeric;
--- non-NULL values followed by NULL values
-INSERT INTO btest_numeric VALUES('2019-01-20T09:00:43', 1);
-INSERT INTO btest_numeric VALUES('2019-01-20T09:00:43', 2);
-INSERT INTO btest_numeric VALUES('2018-01-20T09:00:43', NULL);
-INSERT INTO btest_numeric VALUES('2018-01-20T09:00:43', NULL);
-:PREFIX SELECT first(quantity, time) FROM btest_numeric;
- first 
--------
-      
-(1 row)
-
-:PREFIX SELECT last(quantity, time) FROM btest_numeric;
- last 
-------
-    1
-(1 row)
-
-:PREFIX SELECT first(time, quantity) FROM btest_numeric;
-          first           
---------------------------
- Sun Jan 20 09:00:43 2019
-(1 row)
-
-:PREFIX SELECT last(time, quantity) FROM btest_numeric;
-           last           
---------------------------
- Sun Jan 20 09:00:43 2019
-(1 row)
-
-ROLLBACK;
--- diff results with optimizations disabled and enabled
-\o :TEST_RESULTS_UNOPTIMIZED
-SET timescaledb.enable_optimizations TO false;
-\ir :TEST_QUERY_NAME
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
--- canary for results diff
--- this should be only output of results diff
-SELECT setting, current_setting(setting) AS value from (VALUES ('timescaledb.enable_optimizations')) v(setting);
-:PREFIX SELECT time, gp, temp FROM btest ORDER BY time;
-:PREFIX SELECT last(temp, time) FROM btest;
-:PREFIX SELECT first(temp, time) FROM btest;
-:PREFIX SELECT last(temp, time_alt) FROM btest;
-:PREFIX SELECT first(temp, time_alt) FROM btest;
-:PREFIX SELECT gp, last(temp, time) FROM btest GROUP BY gp ORDER BY gp;
-:PREFIX SELECT gp, first(temp, time) FROM btest GROUP BY gp ORDER BY gp;
---check whole row
-:PREFIX SELECT gp, first(btest, time) FROM btest GROUP BY gp ORDER BY gp;
---check toasted col
-:PREFIX SELECT gp, left(last(strid, time), 10) FROM btest GROUP BY gp ORDER BY gp;
-:PREFIX SELECT gp, last(temp, strid) FROM btest GROUP BY gp ORDER BY gp;
-:PREFIX SELECT gp, last(strid, temp) FROM btest GROUP BY gp ORDER BY gp;
-BEGIN;
---check null value as last element
-INSERT INTO btest VALUES('2018-01-20T09:00:43', '2017-01-20T09:00:55', 2, NULL);
-:PREFIX SELECT last(temp, time) FROM btest;
---check non-null element "overrides" NULL because it comes after.
-INSERT INTO btest VALUES('2019-01-20T09:00:43', '2018-01-20T09:00:55', 2, 30.5);
-:PREFIX SELECT last(temp, time) FROM btest;
---check null cmp element is skipped
-INSERT INTO btest VALUES('2018-01-20T09:00:43', NULL, 2, 32.3);
-:PREFIX SELECT last(temp, time_alt) FROM btest;
--- fist returns NULL value
-:PREFIX SELECT first(temp, time_alt) FROM btest;
--- test first return non NULL value
-INSERT INTO btest VALUES('2016-01-20T09:00:00', '2016-01-20T09:00:00', 2, 36.5);
-:PREFIX SELECT first(temp, time_alt) FROM btest;
---check non null cmp element insert after null cmp
-INSERT INTO btest VALUES('2020-01-20T09:00:43', '2020-01-20T09:00:43', 2, 35.3);
-:PREFIX SELECT last(temp, time_alt) FROM btest;
-:PREFIX SELECT first(temp, time_alt) FROM btest;
---cmp nulls should be ignored and not present in groups
-:PREFIX SELECT gp, last(temp, time_alt) FROM btest GROUP BY gp ORDER BY gp;
---Previously, some bugs were found with NULLS and numeric types, so test that
-INSERT INTO btest_numeric VALUES ('2019-01-20T09:00:43', NULL);
-:PREFIX SELECT last(quantity, time) FROM btest_numeric;
---check non-null element "overrides" NULL because it comes after.
-INSERT INTO btest_numeric VALUES('2020-01-20T09:00:43', 30.5);
-:PREFIX SELECT last(quantity, time) FROM btest_numeric;
--- do index scan for last
-:PREFIX SELECT last(temp, time) FROM btest;
--- do index scan for first
-:PREFIX SELECT first(temp, time) FROM btest;
--- can't do index scan when ordering on non-index column
-:PREFIX SELECT first(temp, time_alt) FROM btest;
--- do index scan for subquery
-:PREFIX SELECT * FROM (SELECT last(temp, time) FROM btest) last;
--- can't do index scan when using group by
-:PREFIX SELECT last(temp, time) FROM btest GROUP BY gp ORDER BY gp;
--- do index scan when agg function is used in CTE subquery
-:PREFIX WITH last_temp AS (SELECT last(temp, time) FROM btest) SELECT * from last_temp;
--- do index scan when using both FIRST and LAST aggregate functions
-:PREFIX SELECT first(temp, time), last(temp, time) FROM btest;
--- verify results when using both FIRST and LAST
-:PREFIX SELECT first(temp, time), last(temp, time) FROM btest;
--- do index scan when using WHERE
-:PREFIX SELECT last(temp, time) FROM btest WHERE time <= '2017-01-20T09:00:02';
--- can't do index scan for MAX and LAST combined (MinMax optimization fails when having different aggregate functions)
-:PREFIX SELECT max(time), last(temp, time) FROM btest;
--- can't do index scan when using FIRST/LAST in ORDER BY
-:PREFIX SELECT last(temp, time) FROM btest ORDER BY last(temp, time);
--- do index scan
-:PREFIX SELECT last(temp, time) FROM btest WHERE temp < 30;
--- SELECT first(temp, time) FROM btest WHERE time >= '2017-01-20 09:00:47';
--- do index scan
-:PREFIX SELECT first(temp, time) FROM btest WHERE time >= '2017-01-20 09:00:47';
--- can't do index scan when using WINDOW function
-:PREFIX SELECT gp, last(temp, time) OVER (PARTITION BY gp) AS last FROM btest;
--- test constants
-:PREFIX SELECT first(100, 100) FROM btest;
--- create an index so we can test optimization
-CREATE INDEX btest_time_alt_idx ON btest(time_alt);
-:PREFIX SELECT last(temp, time_alt) FROM btest;
---test nested FIRST/LAST - should optimize
-:PREFIX SELECT abs(last(temp, time)) FROM btest;
--- test nested FIRST/LAST in ORDER BY - no optimization possible
-:PREFIX SELECT abs(last(temp, time)) FROM btest ORDER BY abs(last(temp,time));
-ROLLBACK;
--- Test with NULL numeric values
-BEGIN;
-TRUNCATE btest_numeric;
--- Empty table
-:PREFIX SELECT first(btest_numeric, time) FROM btest_numeric;
-:PREFIX SELECT last(btest_numeric, time) FROM btest_numeric;
--- Only NULL values
-INSERT INTO btest_numeric VALUES('2018-01-20T09:00:43', NULL);
-INSERT INTO btest_numeric VALUES('2018-01-20T09:00:43', NULL);
-:PREFIX SELECT first(quantity, time) FROM btest_numeric;
-:PREFIX SELECT last(quantity, time) FROM btest_numeric;
-:PREFIX SELECT first(time, quantity) FROM btest_numeric;
-:PREFIX SELECT last(time, quantity) FROM btest_numeric;
--- NULL values followed by non-NULL values
-INSERT INTO btest_numeric VALUES('2019-01-20T09:00:43', 1);
-INSERT INTO btest_numeric VALUES('2019-01-20T09:00:43', 2);
-:PREFIX SELECT first(quantity, time) FROM btest_numeric;
-:PREFIX SELECT last(quantity, time) FROM btest_numeric;
-:PREFIX SELECT first(time, quantity) FROM btest_numeric;
-:PREFIX SELECT last(time, quantity) FROM btest_numeric;
-TRUNCATE btest_numeric;
--- non-NULL values followed by NULL values
-INSERT INTO btest_numeric VALUES('2019-01-20T09:00:43', 1);
-INSERT INTO btest_numeric VALUES('2019-01-20T09:00:43', 2);
-INSERT INTO btest_numeric VALUES('2018-01-20T09:00:43', NULL);
-INSERT INTO btest_numeric VALUES('2018-01-20T09:00:43', NULL);
-:PREFIX SELECT first(quantity, time) FROM btest_numeric;
-:PREFIX SELECT last(quantity, time) FROM btest_numeric;
-:PREFIX SELECT first(time, quantity) FROM btest_numeric;
-:PREFIX SELECT last(time, quantity) FROM btest_numeric;
-ROLLBACK;
-\o
-\o :TEST_RESULTS_OPTIMIZED
-SET timescaledb.enable_optimizations TO true;
-\ir :TEST_QUERY_NAME
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
--- canary for results diff
--- this should be only output of results diff
-SELECT setting, current_setting(setting) AS value from (VALUES ('timescaledb.enable_optimizations')) v(setting);
-:PREFIX SELECT time, gp, temp FROM btest ORDER BY time;
-:PREFIX SELECT last(temp, time) FROM btest;
-:PREFIX SELECT first(temp, time) FROM btest;
-:PREFIX SELECT last(temp, time_alt) FROM btest;
-:PREFIX SELECT first(temp, time_alt) FROM btest;
-:PREFIX SELECT gp, last(temp, time) FROM btest GROUP BY gp ORDER BY gp;
-:PREFIX SELECT gp, first(temp, time) FROM btest GROUP BY gp ORDER BY gp;
---check whole row
-:PREFIX SELECT gp, first(btest, time) FROM btest GROUP BY gp ORDER BY gp;
---check toasted col
-:PREFIX SELECT gp, left(last(strid, time), 10) FROM btest GROUP BY gp ORDER BY gp;
-:PREFIX SELECT gp, last(temp, strid) FROM btest GROUP BY gp ORDER BY gp;
-:PREFIX SELECT gp, last(strid, temp) FROM btest GROUP BY gp ORDER BY gp;
-BEGIN;
---check null value as last element
-INSERT INTO btest VALUES('2018-01-20T09:00:43', '2017-01-20T09:00:55', 2, NULL);
-:PREFIX SELECT last(temp, time) FROM btest;
---check non-null element "overrides" NULL because it comes after.
-INSERT INTO btest VALUES('2019-01-20T09:00:43', '2018-01-20T09:00:55', 2, 30.5);
-:PREFIX SELECT last(temp, time) FROM btest;
---check null cmp element is skipped
-INSERT INTO btest VALUES('2018-01-20T09:00:43', NULL, 2, 32.3);
-:PREFIX SELECT last(temp, time_alt) FROM btest;
--- fist returns NULL value
-:PREFIX SELECT first(temp, time_alt) FROM btest;
--- test first return non NULL value
-INSERT INTO btest VALUES('2016-01-20T09:00:00', '2016-01-20T09:00:00', 2, 36.5);
-:PREFIX SELECT first(temp, time_alt) FROM btest;
---check non null cmp element insert after null cmp
-INSERT INTO btest VALUES('2020-01-20T09:00:43', '2020-01-20T09:00:43', 2, 35.3);
-:PREFIX SELECT last(temp, time_alt) FROM btest;
-:PREFIX SELECT first(temp, time_alt) FROM btest;
---cmp nulls should be ignored and not present in groups
-:PREFIX SELECT gp, last(temp, time_alt) FROM btest GROUP BY gp ORDER BY gp;
---Previously, some bugs were found with NULLS and numeric types, so test that
-INSERT INTO btest_numeric VALUES ('2019-01-20T09:00:43', NULL);
-:PREFIX SELECT last(quantity, time) FROM btest_numeric;
---check non-null element "overrides" NULL because it comes after.
-INSERT INTO btest_numeric VALUES('2020-01-20T09:00:43', 30.5);
-:PREFIX SELECT last(quantity, time) FROM btest_numeric;
--- do index scan for last
-:PREFIX SELECT last(temp, time) FROM btest;
--- do index scan for first
-:PREFIX SELECT first(temp, time) FROM btest;
--- can't do index scan when ordering on non-index column
-:PREFIX SELECT first(temp, time_alt) FROM btest;
--- do index scan for subquery
-:PREFIX SELECT * FROM (SELECT last(temp, time) FROM btest) last;
--- can't do index scan when using group by
-:PREFIX SELECT last(temp, time) FROM btest GROUP BY gp ORDER BY gp;
--- do index scan when agg function is used in CTE subquery
-:PREFIX WITH last_temp AS (SELECT last(temp, time) FROM btest) SELECT * from last_temp;
--- do index scan when using both FIRST and LAST aggregate functions
-:PREFIX SELECT first(temp, time), last(temp, time) FROM btest;
--- verify results when using both FIRST and LAST
-:PREFIX SELECT first(temp, time), last(temp, time) FROM btest;
--- do index scan when using WHERE
-:PREFIX SELECT last(temp, time) FROM btest WHERE time <= '2017-01-20T09:00:02';
--- can't do index scan for MAX and LAST combined (MinMax optimization fails when having different aggregate functions)
-:PREFIX SELECT max(time), last(temp, time) FROM btest;
--- can't do index scan when using FIRST/LAST in ORDER BY
-:PREFIX SELECT last(temp, time) FROM btest ORDER BY last(temp, time);
--- do index scan
-:PREFIX SELECT last(temp, time) FROM btest WHERE temp < 30;
--- SELECT first(temp, time) FROM btest WHERE time >= '2017-01-20 09:00:47';
--- do index scan
-:PREFIX SELECT first(temp, time) FROM btest WHERE time >= '2017-01-20 09:00:47';
--- can't do index scan when using WINDOW function
-:PREFIX SELECT gp, last(temp, time) OVER (PARTITION BY gp) AS last FROM btest;
--- test constants
-:PREFIX SELECT first(100, 100) FROM btest;
--- create an index so we can test optimization
-CREATE INDEX btest_time_alt_idx ON btest(time_alt);
-:PREFIX SELECT last(temp, time_alt) FROM btest;
---test nested FIRST/LAST - should optimize
-:PREFIX SELECT abs(last(temp, time)) FROM btest;
--- test nested FIRST/LAST in ORDER BY - no optimization possible
-:PREFIX SELECT abs(last(temp, time)) FROM btest ORDER BY abs(last(temp,time));
-ROLLBACK;
--- Test with NULL numeric values
-BEGIN;
-TRUNCATE btest_numeric;
--- Empty table
-:PREFIX SELECT first(btest_numeric, time) FROM btest_numeric;
-:PREFIX SELECT last(btest_numeric, time) FROM btest_numeric;
--- Only NULL values
-INSERT INTO btest_numeric VALUES('2018-01-20T09:00:43', NULL);
-INSERT INTO btest_numeric VALUES('2018-01-20T09:00:43', NULL);
-:PREFIX SELECT first(quantity, time) FROM btest_numeric;
-:PREFIX SELECT last(quantity, time) FROM btest_numeric;
-:PREFIX SELECT first(time, quantity) FROM btest_numeric;
-:PREFIX SELECT last(time, quantity) FROM btest_numeric;
--- NULL values followed by non-NULL values
-INSERT INTO btest_numeric VALUES('2019-01-20T09:00:43', 1);
-INSERT INTO btest_numeric VALUES('2019-01-20T09:00:43', 2);
-:PREFIX SELECT first(quantity, time) FROM btest_numeric;
-:PREFIX SELECT last(quantity, time) FROM btest_numeric;
-:PREFIX SELECT first(time, quantity) FROM btest_numeric;
-:PREFIX SELECT last(time, quantity) FROM btest_numeric;
-TRUNCATE btest_numeric;
--- non-NULL values followed by NULL values
-INSERT INTO btest_numeric VALUES('2019-01-20T09:00:43', 1);
-INSERT INTO btest_numeric VALUES('2019-01-20T09:00:43', 2);
-INSERT INTO btest_numeric VALUES('2018-01-20T09:00:43', NULL);
-INSERT INTO btest_numeric VALUES('2018-01-20T09:00:43', NULL);
-:PREFIX SELECT first(quantity, time) FROM btest_numeric;
-:PREFIX SELECT last(quantity, time) FROM btest_numeric;
-:PREFIX SELECT first(time, quantity) FROM btest_numeric;
-:PREFIX SELECT last(time, quantity) FROM btest_numeric;
-ROLLBACK;
-\o
-:DIFF_CMD
---- Unoptimized result
-+++ Optimized result
-@@ -1,6 +1,6 @@
-              setting              | value 
- ----------------------------------+-------
-- timescaledb.enable_optimizations | off
-+ timescaledb.enable_optimizations | on
- (1 row)
- 
-            time           | gp | temp 
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/append-15.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/append-15.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/append-15.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/append-15.out	2023-11-25 05:27:33.761052864 +0000
@@ -1,2416 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-\set TEST_BASE_NAME append
-SELECT format('include/%s_load.sql', :'TEST_BASE_NAME') as "TEST_LOAD_NAME",
-       format('include/%s_query.sql', :'TEST_BASE_NAME') as "TEST_QUERY_NAME",
-       format('%s/results/%s_results_optimized.out', :'TEST_OUTPUT_DIR', :'TEST_BASE_NAME') as "TEST_RESULTS_OPTIMIZED",
-       format('%s/results/%s_results_unoptimized.out', :'TEST_OUTPUT_DIR', :'TEST_BASE_NAME') as "TEST_RESULTS_UNOPTIMIZED"
-\gset
-SELECT format('\! diff -u --label "Unoptimized results" --label "Optimized results" %s %s', :'TEST_RESULTS_UNOPTIMIZED', :'TEST_RESULTS_OPTIMIZED') as "DIFF_CMD"
-\gset
-SET timescaledb.enable_now_constify TO false;
--- disable memoize node to make EXPLAIN output comparable between PG14 and previous versions
-SELECT CASE WHEN current_setting('server_version_num')::int/10000 >= 14 THEN set_config('enable_memoize','off',false) ELSE 'off' END AS enable_memoize;
- enable_memoize 
-----------------
- off
-(1 row)
-
-\set PREFIX 'EXPLAIN (analyze, costs off, timing off, summary off)'
-\ir :TEST_LOAD_NAME
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
--- create a now() function for repeatable testing that always returns
--- the same timestamp. It needs to be marked STABLE
-CREATE OR REPLACE FUNCTION now_s()
-RETURNS timestamptz LANGUAGE PLPGSQL STABLE AS
-$BODY$
-BEGIN
-    RAISE NOTICE 'Stable function now_s() called!';
-    RETURN '2017-08-22T10:00:00'::timestamptz;
-END;
-$BODY$;
-CREATE OR REPLACE FUNCTION now_i()
-RETURNS timestamptz LANGUAGE PLPGSQL IMMUTABLE AS
-$BODY$
-BEGIN
-    RAISE NOTICE 'Immutable function now_i() called!';
-    RETURN '2017-08-22T10:00:00'::timestamptz;
-END;
-$BODY$;
-CREATE OR REPLACE FUNCTION now_v()
-RETURNS timestamptz LANGUAGE PLPGSQL VOLATILE AS
-$BODY$
-BEGIN
-    RAISE NOTICE 'Volatile function now_v() called!';
-    RETURN '2017-08-22T10:00:00'::timestamptz;
-END;
-$BODY$;
-CREATE TABLE append_test(time timestamptz, temp float, colorid integer, attr jsonb);
-SELECT create_hypertable('append_test', 'time', chunk_time_interval => 2628000000000);
-psql:include/append_load.sql:35: NOTICE:  adding not-null constraint to column "time"
-    create_hypertable     
---------------------------
- (1,public,append_test,t)
-(1 row)
-
--- create three chunks
-INSERT INTO append_test VALUES ('2017-03-22T09:18:22', 23.5, 1, '{"a": 1, "b": 2}'),
-                               ('2017-03-22T09:18:23', 21.5, 1, '{"a": 1, "b": 2}'),
-                               ('2017-05-22T09:18:22', 36.2, 2, '{"c": 3, "b": 2}'),
-                               ('2017-05-22T09:18:23', 15.2, 2, '{"c": 3}'),
-                               ('2017-08-22T09:18:22', 34.1, 3, '{"c": 4}');
--- Create another hypertable to join with
-CREATE TABLE join_test(time timestamptz, temp float, colorid integer);
-SELECT create_hypertable('join_test', 'time', chunk_time_interval => 2628000000000);
-psql:include/append_load.sql:46: NOTICE:  adding not-null constraint to column "time"
-   create_hypertable    
-------------------------
- (2,public,join_test,t)
-(1 row)
-
-INSERT INTO join_test VALUES ('2017-01-22T09:18:22', 15.2, 1),
-                             ('2017-02-22T09:18:22', 24.5, 2),
-                             ('2017-08-22T09:18:22', 23.1, 3);
--- Create another table to join with which is not a hypertable.
-CREATE TABLE join_test_plain(time timestamptz, temp float, colorid integer, attr jsonb);
-INSERT INTO join_test_plain VALUES ('2017-01-22T09:18:22', 15.2, 1, '{"a": 1}'),
-                             ('2017-02-22T09:18:22', 24.5, 2, '{"b": 2}'),
-                             ('2017-08-22T09:18:22', 23.1, 3, '{"c": 3}');
--- create hypertable with DATE time dimension
-CREATE TABLE metrics_date(time DATE NOT NULL);
-SELECT create_hypertable('metrics_date','time');
-     create_hypertable     
----------------------------
- (3,public,metrics_date,t)
-(1 row)
-
-INSERT INTO metrics_date SELECT generate_series('2000-01-01'::date, '2000-02-01'::date, '5m'::interval);
-ANALYZE metrics_date;
--- create hypertable with TIMESTAMP time dimension
-CREATE TABLE metrics_timestamp(time TIMESTAMP NOT NULL);
-SELECT create_hypertable('metrics_timestamp','time');
-psql:include/append_load.sql:67: WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-       create_hypertable        
---------------------------------
- (4,public,metrics_timestamp,t)
-(1 row)
-
-INSERT INTO metrics_timestamp SELECT generate_series('2000-01-01'::date, '2000-02-01'::date, '5m'::interval);
-ANALYZE metrics_timestamp;
--- create hypertable with TIMESTAMPTZ time dimension
-CREATE TABLE metrics_timestamptz(time TIMESTAMPTZ NOT NULL, device_id INT NOT NULL);
-CREATE INDEX ON metrics_timestamptz(device_id,time);
-SELECT create_hypertable('metrics_timestamptz','time');
-        create_hypertable         
-----------------------------------
- (5,public,metrics_timestamptz,t)
-(1 row)
-
-INSERT INTO metrics_timestamptz SELECT generate_series('2000-01-01'::date, '2000-02-01'::date, '5m'::interval), 1;
-INSERT INTO metrics_timestamptz SELECT generate_series('2000-01-01'::date, '2000-02-01'::date, '5m'::interval), 2;
-INSERT INTO metrics_timestamptz SELECT generate_series('2000-01-01'::date, '2000-02-01'::date, '5m'::interval), 3;
-ANALYZE metrics_timestamptz;
--- create space partitioned hypertable
-CREATE TABLE metrics_space(time timestamptz NOT NULL, device_id int NOT NULL, v1 float, v2 float, v3 text);
-SELECT create_hypertable('metrics_space','time','device_id',3);
-     create_hypertable      
-----------------------------
- (6,public,metrics_space,t)
-(1 row)
-
-INSERT INTO metrics_space
-SELECT time, device_id, device_id + 0.25, device_id + 0.75, device_id
-FROM generate_series('2000-01-01'::timestamptz, '2000-01-14'::timestamptz, '5m'::interval) g1(time),
-  generate_series(1,10,1) g2(device_id)
-ORDER BY time, device_id;
-ANALYZE metrics_space;
--- test ChunkAppend projection #2661
-CREATE TABLE i2661 (
-  machine_id int4 NOT NULL,
-  "name" varchar(255) NOT NULL,
-  "timestamp" timestamptz NOT NULL,
-  "first" float4 NULL
-);
-SELECT create_hypertable('i2661', 'timestamp');
-psql:include/append_load.sql:99: WARNING:  column type "character varying" used for "name" does not follow best practices
- create_hypertable  
---------------------
- (7,public,i2661,t)
-(1 row)
-
-INSERT INTO i2661 SELECT 1, 'speed', generate_series('2019-12-31 00:00:00', '2020-01-10 00:00:00', '2m'::interval), 0;
-ANALYZE i2661;
-\ir :TEST_QUERY_NAME
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
--- canary for results diff
--- this should be the only output of the results diff
-SELECT setting, current_setting(setting) AS value from (VALUES ('timescaledb.enable_optimizations'),('timescaledb.enable_chunk_append')) v(setting);
-             setting              | value 
-----------------------------------+-------
- timescaledb.enable_optimizations | on
- timescaledb.enable_chunk_append  | on
-(2 rows)
-
--- query should exclude all chunks with optimization on
-:PREFIX
-SELECT * FROM append_test WHERE time > now_s() + '1 month'
-ORDER BY time DESC;
-psql:include/append_query.sql:12: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:12: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:12: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:12: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:12: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:12: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:12: NOTICE:  Stable function now_s() called!
-                            QUERY PLAN                            
-------------------------------------------------------------------
- Custom Scan (ChunkAppend) on append_test (actual rows=0 loops=1)
-   Order: append_test."time" DESC
-   Chunks excluded during startup: 3
-(3 rows)
-
---query should exclude all chunks and be a MergeAppend
-:PREFIX
-SELECT * FROM append_test WHERE time > now_s() + '1 month'
-ORDER BY time DESC limit 1;
-psql:include/append_query.sql:17: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:17: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:17: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:17: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:17: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:17: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:17: NOTICE:  Stable function now_s() called!
-                               QUERY PLAN                               
-------------------------------------------------------------------------
- Limit (actual rows=0 loops=1)
-   ->  Custom Scan (ChunkAppend) on append_test (actual rows=0 loops=1)
-         Order: append_test."time" DESC
-         Chunks excluded during startup: 3
-(4 rows)
-
--- when optimized, the plan should be a constraint-aware append and
--- cover only one chunk. It should be a backward index scan due to
--- descending index on time. Should also skip the main table, since it
--- cannot hold tuples
-:PREFIX
-SELECT * FROM append_test WHERE time > now_s() - interval '2 months';
-psql:include/append_query.sql:24: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:24: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:24: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:24: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:24: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:24: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:24: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:24: NOTICE:  Stable function now_s() called!
-                                                QUERY PLAN                                                
-----------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on append_test (actual rows=1 loops=1)
-   Chunks excluded during startup: 2
-   ->  Index Scan using _hyper_1_3_chunk_append_test_time_idx on _hyper_1_3_chunk (actual rows=1 loops=1)
-         Index Cond: ("time" > (now_s() - '@ 2 mons'::interval))
-(4 rows)
-
--- adding ORDER BY and LIMIT should turn the plan into an optimized
--- ordered append plan
-:PREFIX
-SELECT * FROM append_test WHERE time > now_s() - interval '2 months'
-ORDER BY time LIMIT 3;
-psql:include/append_query.sql:30: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:30: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:30: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:30: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:30: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:30: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:30: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:30: NOTICE:  Stable function now_s() called!
-                                                       QUERY PLAN                                                        
--------------------------------------------------------------------------------------------------------------------------
- Limit (actual rows=1 loops=1)
-   ->  Custom Scan (ChunkAppend) on append_test (actual rows=1 loops=1)
-         Order: append_test."time"
-         Chunks excluded during startup: 2
-         ->  Index Scan Backward using _hyper_1_3_chunk_append_test_time_idx on _hyper_1_3_chunk (actual rows=1 loops=1)
-               Index Cond: ("time" > (now_s() - '@ 2 mons'::interval))
-(6 rows)
-
--- no optimized plan for queries with restrictions that can be
--- constified at planning time. Regular planning-time constraint
--- exclusion should occur.
-:PREFIX
-SELECT * FROM append_test WHERE time > now_i() - interval '2 months'
-ORDER BY time;
-psql:include/append_query.sql:37: NOTICE:  Immutable function now_i() called!
-                                                    QUERY PLAN                                                     
--------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on append_test (actual rows=1 loops=1)
-   Order: append_test."time"
-   Chunks excluded during startup: 2
-   ->  Index Scan Backward using _hyper_1_3_chunk_append_test_time_idx on _hyper_1_3_chunk (actual rows=1 loops=1)
-         Index Cond: ("time" > ('Tue Aug 22 10:00:00 2017 PDT'::timestamp with time zone - '@ 2 mons'::interval))
-(5 rows)
-
--- currently, we cannot distinguish between stable and volatile
--- functions as far as applying our modified plan. However, volatile
--- function should not be pre-evaluated to constants, so no chunk
--- exclusion should occur.
-:PREFIX
-SELECT * FROM append_test WHERE time > now_v() - interval '2 months'
-ORDER BY time;
-psql:include/append_query.sql:45: NOTICE:  Volatile function now_v() called!
-psql:include/append_query.sql:45: NOTICE:  Volatile function now_v() called!
-psql:include/append_query.sql:45: NOTICE:  Volatile function now_v() called!
-psql:include/append_query.sql:45: NOTICE:  Volatile function now_v() called!
-psql:include/append_query.sql:45: NOTICE:  Volatile function now_v() called!
-                                                    QUERY PLAN                                                     
--------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on append_test (actual rows=1 loops=1)
-   Order: append_test."time"
-   Chunks excluded during startup: 0
-   ->  Index Scan Backward using _hyper_1_1_chunk_append_test_time_idx on _hyper_1_1_chunk (actual rows=0 loops=1)
-         Filter: ("time" > (now_v() - '@ 2 mons'::interval))
-         Rows Removed by Filter: 2
-   ->  Index Scan Backward using _hyper_1_2_chunk_append_test_time_idx on _hyper_1_2_chunk (actual rows=0 loops=1)
-         Filter: ("time" > (now_v() - '@ 2 mons'::interval))
-         Rows Removed by Filter: 2
-   ->  Index Scan Backward using _hyper_1_3_chunk_append_test_time_idx on _hyper_1_3_chunk (actual rows=1 loops=1)
-         Filter: ("time" > (now_v() - '@ 2 mons'::interval))
-(11 rows)
-
--- prepared statement output should be the same regardless of
--- optimizations
-PREPARE query_opt AS
-SELECT * FROM append_test WHERE time > now_s() - interval '2 months'
-ORDER BY time;
-:PREFIX EXECUTE query_opt;
-psql:include/append_query.sql:53: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:53: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:53: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:53: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:53: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:53: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:53: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:53: NOTICE:  Stable function now_s() called!
-                                                    QUERY PLAN                                                     
--------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on append_test (actual rows=1 loops=1)
-   Order: append_test."time"
-   Chunks excluded during startup: 2
-   ->  Index Scan Backward using _hyper_1_3_chunk_append_test_time_idx on _hyper_1_3_chunk (actual rows=1 loops=1)
-         Index Cond: ("time" > (now_s() - '@ 2 mons'::interval))
-(5 rows)
-
-DEALLOCATE query_opt;
--- aggregates should produce same output
-:PREFIX
-SELECT date_trunc('year', time) t, avg(temp) FROM append_test
-WHERE time > now_s() - interval '4 months'
-GROUP BY t
-ORDER BY t DESC;
-psql:include/append_query.sql:62: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:62: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:62: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:62: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:62: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:62: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:62: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:62: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:62: NOTICE:  Stable function now_s() called!
-                                                      QUERY PLAN                                                      
-----------------------------------------------------------------------------------------------------------------------
- GroupAggregate (actual rows=1 loops=1)
-   Group Key: (date_trunc('year'::text, append_test."time"))
-   ->  Result (actual rows=3 loops=1)
-         ->  Custom Scan (ChunkAppend) on append_test (actual rows=3 loops=1)
-               Order: date_trunc('year'::text, append_test."time") DESC
-               Chunks excluded during startup: 1
-               ->  Index Scan using _hyper_1_3_chunk_append_test_time_idx on _hyper_1_3_chunk (actual rows=1 loops=1)
-                     Index Cond: ("time" > (now_s() - '@ 4 mons'::interval))
-               ->  Index Scan using _hyper_1_2_chunk_append_test_time_idx on _hyper_1_2_chunk (actual rows=2 loops=1)
-                     Index Cond: ("time" > (now_s() - '@ 4 mons'::interval))
-(10 rows)
-
--- querying outside the time range should return nothing. This tests
--- that ConstraintAwareAppend can handle the case when an Append node
--- is turned into a Result node due to no children
-:PREFIX
-SELECT date_trunc('year', time) t, avg(temp)
-FROM append_test
-WHERE time < '2016-03-22'
-AND date_part('dow', time) between 1 and 5
-GROUP BY t
-ORDER BY t DESC;
-                        QUERY PLAN                         
------------------------------------------------------------
- GroupAggregate (actual rows=0 loops=1)
-   Group Key: (date_trunc('year'::text, "time"))
-   ->  Sort (actual rows=0 loops=1)
-         Sort Key: (date_trunc('year'::text, "time")) DESC
-         Sort Method: quicksort 
-         ->  Result (actual rows=0 loops=1)
-               One-Time Filter: false
-(7 rows)
-
--- a parameterized query can safely constify params, so won't be
--- optimized by constraint-aware append since regular constraint
--- exclusion works just fine
-PREPARE query_param AS
-SELECT * FROM append_test WHERE time > $1 ORDER BY time;
-:PREFIX
-EXECUTE query_param(now_s() - interval '2 months');
-psql:include/append_query.sql:82: NOTICE:  Stable function now_s() called!
-                                                 QUERY PLAN                                                  
--------------------------------------------------------------------------------------------------------------
- Index Scan Backward using _hyper_1_3_chunk_append_test_time_idx on _hyper_1_3_chunk (actual rows=1 loops=1)
-   Index Cond: ("time" > 'Thu Jun 22 10:00:00 2017 PDT'::timestamp with time zone)
-(2 rows)
-
-DEALLOCATE query_param;
---test with cte
-:PREFIX
-WITH data AS (
-    SELECT time_bucket(INTERVAL '30 day', TIME) AS btime, AVG(temp) AS VALUE
-    FROM append_test
-    WHERE
-        TIME > now_s() - INTERVAL '400 day'
-    AND colorid > 0
-    GROUP BY btime
-),
-period AS (
-    SELECT time_bucket(INTERVAL '30 day', TIME) AS btime
-      FROM  GENERATE_SERIES('2017-03-22T01:01:01', '2017-08-23T01:01:01', INTERVAL '30 day') TIME
-  )
-SELECT period.btime, VALUE
-    FROM period
-    LEFT JOIN DATA USING (btime)
-    ORDER BY period.btime;
-psql:include/append_query.sql:102: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:102: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:102: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:102: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:102: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:102: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:102: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:102: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:102: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:102: NOTICE:  Stable function now_s() called!
-                                                                      QUERY PLAN                                                                       
--------------------------------------------------------------------------------------------------------------------------------------------------------
- Sort (actual rows=6 loops=1)
-   Sort Key: (time_bucket('@ 30 days'::interval, "time"."time"))
-   Sort Method: quicksort 
-   ->  Hash Left Join (actual rows=6 loops=1)
-         Hash Cond: (time_bucket('@ 30 days'::interval, "time"."time") = data.btime)
-         ->  Function Scan on generate_series "time" (actual rows=6 loops=1)
-         ->  Hash (actual rows=3 loops=1)
-               Buckets: 1024  Batches: 1 
-               ->  Subquery Scan on data (actual rows=3 loops=1)
-                     ->  HashAggregate (actual rows=3 loops=1)
-                           Group Key: time_bucket('@ 30 days'::interval, append_test."time")
-                           Batches: 1 
-                           ->  Result (actual rows=5 loops=1)
-                                 ->  Custom Scan (ChunkAppend) on append_test (actual rows=5 loops=1)
-                                       Chunks excluded during startup: 0
-                                       ->  Index Scan Backward using _hyper_1_1_chunk_append_test_time_idx on _hyper_1_1_chunk (actual rows=2 loops=1)
-                                             Index Cond: ("time" > (now_s() - '@ 400 days'::interval))
-                                             Filter: (colorid > 0)
-                                       ->  Index Scan Backward using _hyper_1_2_chunk_append_test_time_idx on _hyper_1_2_chunk (actual rows=2 loops=1)
-                                             Index Cond: ("time" > (now_s() - '@ 400 days'::interval))
-                                             Filter: (colorid > 0)
-                                       ->  Index Scan Backward using _hyper_1_3_chunk_append_test_time_idx on _hyper_1_3_chunk (actual rows=1 loops=1)
-                                             Index Cond: ("time" > (now_s() - '@ 400 days'::interval))
-                                             Filter: (colorid > 0)
-(24 rows)
-
-WITH data AS (
-    SELECT time_bucket(INTERVAL '30 day', TIME) AS btime, AVG(temp) AS VALUE
-    FROM append_test
-    WHERE
-        TIME > now_s() - INTERVAL '400 day'
-    AND colorid > 0
-    GROUP BY btime
-),
-period AS (
-    SELECT time_bucket(INTERVAL '30 day', TIME) AS btime
-      FROM  GENERATE_SERIES('2017-03-22T01:01:01', '2017-08-23T01:01:01', INTERVAL '30 day') TIME
-  )
-SELECT period.btime, VALUE
-    FROM period
-    LEFT JOIN DATA USING (btime)
-    ORDER BY period.btime;
-psql:include/append_query.sql:119: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:119: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:119: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:119: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:119: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:119: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:119: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:119: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:119: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:119: NOTICE:  Stable function now_s() called!
-            btime             | value 
-------------------------------+-------
- Fri Mar 03 16:00:00 2017 PST |  22.5
- Sun Apr 02 17:00:00 2017 PDT |      
- Tue May 02 17:00:00 2017 PDT |  25.7
- Thu Jun 01 17:00:00 2017 PDT |      
- Sat Jul 01 17:00:00 2017 PDT |      
- Mon Jul 31 17:00:00 2017 PDT |  34.1
-(6 rows)
-
--- force nested loop join with no materialization. This tests that the
--- inner ConstraintAwareScan supports resetting its scan for every
--- iteration of the outer relation loop
-set enable_hashjoin = 'off';
-set enable_mergejoin = 'off';
-set enable_material = 'off';
-:PREFIX
-SELECT * FROM append_test a INNER JOIN join_test j ON (a.colorid = j.colorid)
-WHERE a.time > now_s() - interval '3 hours' AND j.time > now_s() - interval '3 hours';
-psql:include/append_query.sql:130: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:130: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:130: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:130: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:130: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:130: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:130: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:130: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:130: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:130: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:130: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:130: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:130: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:130: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:130: NOTICE:  Stable function now_s() called!
-psql:include/append_query.sql:130: NOTICE:  Stable function now_s() called!
-                                                     QUERY PLAN                                                     
---------------------------------------------------------------------------------------------------------------------
- Nested Loop (actual rows=1 loops=1)
-   Join Filter: (a.colorid = j.colorid)
-   ->  Custom Scan (ChunkAppend) on append_test a (actual rows=1 loops=1)
-         Chunks excluded during startup: 2
-         ->  Index Scan using _hyper_1_3_chunk_append_test_time_idx on _hyper_1_3_chunk a_1 (actual rows=1 loops=1)
-               Index Cond: ("time" > (now_s() - '@ 3 hours'::interval))
-   ->  Custom Scan (ChunkAppend) on join_test j (actual rows=1 loops=1)
-         Chunks excluded during startup: 2
-         ->  Index Scan using _hyper_2_6_chunk_join_test_time_idx on _hyper_2_6_chunk j_1 (actual rows=1 loops=1)
-               Index Cond: ("time" > (now_s() - '@ 3 hours'::interval))
-(10 rows)
-
-reset enable_hashjoin;
-reset enable_mergejoin;
-reset enable_material;
--- test constraint_exclusion with date time dimension and DATE/TIMESTAMP/TIMESTAMPTZ constraints
--- the queries should all have 3 chunks
-:PREFIX SELECT * FROM metrics_date WHERE time > '2000-01-15'::date ORDER BY time;
-                                                          QUERY PLAN                                                          
-------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_date (actual rows=4609 loops=1)
-   Order: metrics_date."time"
-   ->  Index Only Scan Backward using _hyper_3_9_chunk_metrics_date_time_idx on _hyper_3_9_chunk (actual rows=1152 loops=1)
-         Index Cond: ("time" > '01-15-2000'::date)
-         Heap Fetches: 1152
-   ->  Index Only Scan Backward using _hyper_3_10_chunk_metrics_date_time_idx on _hyper_3_10_chunk (actual rows=2016 loops=1)
-         Index Cond: ("time" > '01-15-2000'::date)
-         Heap Fetches: 2016
-   ->  Index Only Scan Backward using _hyper_3_11_chunk_metrics_date_time_idx on _hyper_3_11_chunk (actual rows=1441 loops=1)
-         Index Cond: ("time" > '01-15-2000'::date)
-         Heap Fetches: 1441
-(11 rows)
-
-:PREFIX SELECT * FROM metrics_date WHERE time > '2000-01-15'::timestamp ORDER BY time;
-                                                          QUERY PLAN                                                          
-------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_date (actual rows=4609 loops=1)
-   Order: metrics_date."time"
-   ->  Index Only Scan Backward using _hyper_3_9_chunk_metrics_date_time_idx on _hyper_3_9_chunk (actual rows=1152 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000'::timestamp without time zone)
-         Heap Fetches: 1152
-   ->  Index Only Scan Backward using _hyper_3_10_chunk_metrics_date_time_idx on _hyper_3_10_chunk (actual rows=2016 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000'::timestamp without time zone)
-         Heap Fetches: 2016
-   ->  Index Only Scan Backward using _hyper_3_11_chunk_metrics_date_time_idx on _hyper_3_11_chunk (actual rows=1441 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000'::timestamp without time zone)
-         Heap Fetches: 1441
-(11 rows)
-
-:PREFIX SELECT * FROM metrics_date WHERE time > '2000-01-15'::timestamptz ORDER BY time;
-                                                          QUERY PLAN                                                          
-------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_date (actual rows=4609 loops=1)
-   Order: metrics_date."time"
-   Chunks excluded during startup: 2
-   ->  Index Only Scan Backward using _hyper_3_9_chunk_metrics_date_time_idx on _hyper_3_9_chunk (actual rows=1152 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000 PST'::timestamp with time zone)
-         Heap Fetches: 1152
-   ->  Index Only Scan Backward using _hyper_3_10_chunk_metrics_date_time_idx on _hyper_3_10_chunk (actual rows=2016 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000 PST'::timestamp with time zone)
-         Heap Fetches: 2016
-   ->  Index Only Scan Backward using _hyper_3_11_chunk_metrics_date_time_idx on _hyper_3_11_chunk (actual rows=1441 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000 PST'::timestamp with time zone)
-         Heap Fetches: 1441
-(12 rows)
-
--- test Const OP Var
--- the queries should all have 3 chunks
-:PREFIX SELECT * FROM metrics_date WHERE '2000-01-15'::date < time ORDER BY time;
-                                                          QUERY PLAN                                                          
-------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_date (actual rows=4609 loops=1)
-   Order: metrics_date."time"
-   ->  Index Only Scan Backward using _hyper_3_9_chunk_metrics_date_time_idx on _hyper_3_9_chunk (actual rows=1152 loops=1)
-         Index Cond: ("time" > '01-15-2000'::date)
-         Heap Fetches: 1152
-   ->  Index Only Scan Backward using _hyper_3_10_chunk_metrics_date_time_idx on _hyper_3_10_chunk (actual rows=2016 loops=1)
-         Index Cond: ("time" > '01-15-2000'::date)
-         Heap Fetches: 2016
-   ->  Index Only Scan Backward using _hyper_3_11_chunk_metrics_date_time_idx on _hyper_3_11_chunk (actual rows=1441 loops=1)
-         Index Cond: ("time" > '01-15-2000'::date)
-         Heap Fetches: 1441
-(11 rows)
-
-:PREFIX SELECT * FROM metrics_date WHERE '2000-01-15'::timestamp < time ORDER BY time;
-                                                          QUERY PLAN                                                          
-------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_date (actual rows=4609 loops=1)
-   Order: metrics_date."time"
-   ->  Index Only Scan Backward using _hyper_3_9_chunk_metrics_date_time_idx on _hyper_3_9_chunk (actual rows=1152 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000'::timestamp without time zone)
-         Heap Fetches: 1152
-   ->  Index Only Scan Backward using _hyper_3_10_chunk_metrics_date_time_idx on _hyper_3_10_chunk (actual rows=2016 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000'::timestamp without time zone)
-         Heap Fetches: 2016
-   ->  Index Only Scan Backward using _hyper_3_11_chunk_metrics_date_time_idx on _hyper_3_11_chunk (actual rows=1441 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000'::timestamp without time zone)
-         Heap Fetches: 1441
-(11 rows)
-
-:PREFIX SELECT * FROM metrics_date WHERE '2000-01-15'::timestamptz < time ORDER BY time;
-                                                          QUERY PLAN                                                          
-------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_date (actual rows=4609 loops=1)
-   Order: metrics_date."time"
-   Chunks excluded during startup: 2
-   ->  Index Only Scan Backward using _hyper_3_9_chunk_metrics_date_time_idx on _hyper_3_9_chunk (actual rows=1152 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000 PST'::timestamp with time zone)
-         Heap Fetches: 1152
-   ->  Index Only Scan Backward using _hyper_3_10_chunk_metrics_date_time_idx on _hyper_3_10_chunk (actual rows=2016 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000 PST'::timestamp with time zone)
-         Heap Fetches: 2016
-   ->  Index Only Scan Backward using _hyper_3_11_chunk_metrics_date_time_idx on _hyper_3_11_chunk (actual rows=1441 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000 PST'::timestamp with time zone)
-         Heap Fetches: 1441
-(12 rows)
-
--- test 2 constraints
--- the queries should all have 2 chunks
-:PREFIX SELECT * FROM metrics_date WHERE time > '2000-01-15'::date AND time < '2000-01-21'::date ORDER BY time;
-                                                         QUERY PLAN                                                          
------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_date (actual rows=1440 loops=1)
-   Order: metrics_date."time"
-   ->  Index Only Scan Backward using _hyper_3_9_chunk_metrics_date_time_idx on _hyper_3_9_chunk (actual rows=1152 loops=1)
-         Index Cond: (("time" > '01-15-2000'::date) AND ("time" < '01-21-2000'::date))
-         Heap Fetches: 1152
-   ->  Index Only Scan Backward using _hyper_3_10_chunk_metrics_date_time_idx on _hyper_3_10_chunk (actual rows=288 loops=1)
-         Index Cond: (("time" > '01-15-2000'::date) AND ("time" < '01-21-2000'::date))
-         Heap Fetches: 288
-(8 rows)
-
-:PREFIX SELECT * FROM metrics_date WHERE time > '2000-01-15'::timestamp AND time < '2000-01-21'::timestamp ORDER BY time;
-                                                                           QUERY PLAN                                                                            
------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_date (actual rows=1440 loops=1)
-   Order: metrics_date."time"
-   ->  Index Only Scan Backward using _hyper_3_9_chunk_metrics_date_time_idx on _hyper_3_9_chunk (actual rows=1152 loops=1)
-         Index Cond: (("time" > 'Sat Jan 15 00:00:00 2000'::timestamp without time zone) AND ("time" < 'Fri Jan 21 00:00:00 2000'::timestamp without time zone))
-         Heap Fetches: 1152
-   ->  Index Only Scan Backward using _hyper_3_10_chunk_metrics_date_time_idx on _hyper_3_10_chunk (actual rows=288 loops=1)
-         Index Cond: (("time" > 'Sat Jan 15 00:00:00 2000'::timestamp without time zone) AND ("time" < 'Fri Jan 21 00:00:00 2000'::timestamp without time zone))
-         Heap Fetches: 288
-(8 rows)
-
-:PREFIX SELECT * FROM metrics_date WHERE time > '2000-01-15'::timestamptz AND time < '2000-01-21'::timestamptz ORDER BY time;
-                                                                            QUERY PLAN                                                                             
--------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_date (actual rows=1440 loops=1)
-   Order: metrics_date."time"
-   Chunks excluded during startup: 3
-   ->  Index Only Scan Backward using _hyper_3_9_chunk_metrics_date_time_idx on _hyper_3_9_chunk (actual rows=1152 loops=1)
-         Index Cond: (("time" > 'Sat Jan 15 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Fri Jan 21 00:00:00 2000 PST'::timestamp with time zone))
-         Heap Fetches: 1152
-   ->  Index Only Scan Backward using _hyper_3_10_chunk_metrics_date_time_idx on _hyper_3_10_chunk (actual rows=288 loops=1)
-         Index Cond: (("time" > 'Sat Jan 15 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Fri Jan 21 00:00:00 2000 PST'::timestamp with time zone))
-         Heap Fetches: 288
-(9 rows)
-
--- test constraint_exclusion with timestamp time dimension and DATE/TIMESTAMP/TIMESTAMPTZ constraints
--- the queries should all have 3 chunks
-:PREFIX SELECT * FROM metrics_timestamp WHERE time > '2000-01-15'::date ORDER BY time;
-                                                            QUERY PLAN                                                             
------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamp (actual rows=4896 loops=1)
-   Order: metrics_timestamp."time"
-   ->  Index Only Scan Backward using _hyper_4_14_chunk_metrics_timestamp_time_idx on _hyper_4_14_chunk (actual rows=1439 loops=1)
-         Index Cond: ("time" > '01-15-2000'::date)
-         Heap Fetches: 1439
-   ->  Index Only Scan Backward using _hyper_4_15_chunk_metrics_timestamp_time_idx on _hyper_4_15_chunk (actual rows=2016 loops=1)
-         Index Cond: ("time" > '01-15-2000'::date)
-         Heap Fetches: 2016
-   ->  Index Only Scan Backward using _hyper_4_16_chunk_metrics_timestamp_time_idx on _hyper_4_16_chunk (actual rows=1441 loops=1)
-         Index Cond: ("time" > '01-15-2000'::date)
-         Heap Fetches: 1441
-(11 rows)
-
-:PREFIX SELECT * FROM metrics_timestamp WHERE time > '2000-01-15'::timestamp ORDER BY time;
-                                                            QUERY PLAN                                                             
------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamp (actual rows=4896 loops=1)
-   Order: metrics_timestamp."time"
-   ->  Index Only Scan Backward using _hyper_4_14_chunk_metrics_timestamp_time_idx on _hyper_4_14_chunk (actual rows=1439 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000'::timestamp without time zone)
-         Heap Fetches: 1439
-   ->  Index Only Scan Backward using _hyper_4_15_chunk_metrics_timestamp_time_idx on _hyper_4_15_chunk (actual rows=2016 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000'::timestamp without time zone)
-         Heap Fetches: 2016
-   ->  Index Only Scan Backward using _hyper_4_16_chunk_metrics_timestamp_time_idx on _hyper_4_16_chunk (actual rows=1441 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000'::timestamp without time zone)
-         Heap Fetches: 1441
-(11 rows)
-
-:PREFIX SELECT * FROM metrics_timestamp WHERE time > '2000-01-15'::timestamptz ORDER BY time;
-                                                            QUERY PLAN                                                             
------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamp (actual rows=4896 loops=1)
-   Order: metrics_timestamp."time"
-   Chunks excluded during startup: 2
-   ->  Index Only Scan Backward using _hyper_4_14_chunk_metrics_timestamp_time_idx on _hyper_4_14_chunk (actual rows=1439 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000 PST'::timestamp with time zone)
-         Heap Fetches: 1439
-   ->  Index Only Scan Backward using _hyper_4_15_chunk_metrics_timestamp_time_idx on _hyper_4_15_chunk (actual rows=2016 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000 PST'::timestamp with time zone)
-         Heap Fetches: 2016
-   ->  Index Only Scan Backward using _hyper_4_16_chunk_metrics_timestamp_time_idx on _hyper_4_16_chunk (actual rows=1441 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000 PST'::timestamp with time zone)
-         Heap Fetches: 1441
-(12 rows)
-
--- test Const OP Var
--- the queries should all have 3 chunks
-:PREFIX SELECT * FROM metrics_timestamp WHERE '2000-01-15'::date < time ORDER BY time;
-                                                            QUERY PLAN                                                             
------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamp (actual rows=4896 loops=1)
-   Order: metrics_timestamp."time"
-   ->  Index Only Scan Backward using _hyper_4_14_chunk_metrics_timestamp_time_idx on _hyper_4_14_chunk (actual rows=1439 loops=1)
-         Index Cond: ("time" > '01-15-2000'::date)
-         Heap Fetches: 1439
-   ->  Index Only Scan Backward using _hyper_4_15_chunk_metrics_timestamp_time_idx on _hyper_4_15_chunk (actual rows=2016 loops=1)
-         Index Cond: ("time" > '01-15-2000'::date)
-         Heap Fetches: 2016
-   ->  Index Only Scan Backward using _hyper_4_16_chunk_metrics_timestamp_time_idx on _hyper_4_16_chunk (actual rows=1441 loops=1)
-         Index Cond: ("time" > '01-15-2000'::date)
-         Heap Fetches: 1441
-(11 rows)
-
-:PREFIX SELECT * FROM metrics_timestamp WHERE '2000-01-15'::timestamp < time ORDER BY time;
-                                                            QUERY PLAN                                                             
------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamp (actual rows=4896 loops=1)
-   Order: metrics_timestamp."time"
-   ->  Index Only Scan Backward using _hyper_4_14_chunk_metrics_timestamp_time_idx on _hyper_4_14_chunk (actual rows=1439 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000'::timestamp without time zone)
-         Heap Fetches: 1439
-   ->  Index Only Scan Backward using _hyper_4_15_chunk_metrics_timestamp_time_idx on _hyper_4_15_chunk (actual rows=2016 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000'::timestamp without time zone)
-         Heap Fetches: 2016
-   ->  Index Only Scan Backward using _hyper_4_16_chunk_metrics_timestamp_time_idx on _hyper_4_16_chunk (actual rows=1441 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000'::timestamp without time zone)
-         Heap Fetches: 1441
-(11 rows)
-
-:PREFIX SELECT * FROM metrics_timestamp WHERE '2000-01-15'::timestamptz < time ORDER BY time;
-                                                            QUERY PLAN                                                             
------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamp (actual rows=4896 loops=1)
-   Order: metrics_timestamp."time"
-   Chunks excluded during startup: 2
-   ->  Index Only Scan Backward using _hyper_4_14_chunk_metrics_timestamp_time_idx on _hyper_4_14_chunk (actual rows=1439 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000 PST'::timestamp with time zone)
-         Heap Fetches: 1439
-   ->  Index Only Scan Backward using _hyper_4_15_chunk_metrics_timestamp_time_idx on _hyper_4_15_chunk (actual rows=2016 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000 PST'::timestamp with time zone)
-         Heap Fetches: 2016
-   ->  Index Only Scan Backward using _hyper_4_16_chunk_metrics_timestamp_time_idx on _hyper_4_16_chunk (actual rows=1441 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000 PST'::timestamp with time zone)
-         Heap Fetches: 1441
-(12 rows)
-
--- test 2 constraints
--- the queries should all have 2 chunks
-:PREFIX SELECT * FROM metrics_timestamp WHERE time > '2000-01-15'::date AND time < '2000-01-21'::date ORDER BY time;
-                                                            QUERY PLAN                                                             
------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamp (actual rows=1727 loops=1)
-   Order: metrics_timestamp."time"
-   ->  Index Only Scan Backward using _hyper_4_14_chunk_metrics_timestamp_time_idx on _hyper_4_14_chunk (actual rows=1439 loops=1)
-         Index Cond: (("time" > '01-15-2000'::date) AND ("time" < '01-21-2000'::date))
-         Heap Fetches: 1439
-   ->  Index Only Scan Backward using _hyper_4_15_chunk_metrics_timestamp_time_idx on _hyper_4_15_chunk (actual rows=288 loops=1)
-         Index Cond: (("time" > '01-15-2000'::date) AND ("time" < '01-21-2000'::date))
-         Heap Fetches: 288
-(8 rows)
-
-:PREFIX SELECT * FROM metrics_timestamp WHERE time > '2000-01-15'::timestamp AND time < '2000-01-21'::timestamp ORDER BY time;
-                                                                           QUERY PLAN                                                                            
------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamp (actual rows=1727 loops=1)
-   Order: metrics_timestamp."time"
-   ->  Index Only Scan Backward using _hyper_4_14_chunk_metrics_timestamp_time_idx on _hyper_4_14_chunk (actual rows=1439 loops=1)
-         Index Cond: (("time" > 'Sat Jan 15 00:00:00 2000'::timestamp without time zone) AND ("time" < 'Fri Jan 21 00:00:00 2000'::timestamp without time zone))
-         Heap Fetches: 1439
-   ->  Index Only Scan Backward using _hyper_4_15_chunk_metrics_timestamp_time_idx on _hyper_4_15_chunk (actual rows=288 loops=1)
-         Index Cond: (("time" > 'Sat Jan 15 00:00:00 2000'::timestamp without time zone) AND ("time" < 'Fri Jan 21 00:00:00 2000'::timestamp without time zone))
-         Heap Fetches: 288
-(8 rows)
-
-:PREFIX SELECT * FROM metrics_timestamp WHERE time > '2000-01-15'::timestamptz AND time < '2000-01-21'::timestamptz ORDER BY time;
-                                                                            QUERY PLAN                                                                             
--------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamp (actual rows=1727 loops=1)
-   Order: metrics_timestamp."time"
-   Chunks excluded during startup: 3
-   ->  Index Only Scan Backward using _hyper_4_14_chunk_metrics_timestamp_time_idx on _hyper_4_14_chunk (actual rows=1439 loops=1)
-         Index Cond: (("time" > 'Sat Jan 15 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Fri Jan 21 00:00:00 2000 PST'::timestamp with time zone))
-         Heap Fetches: 1439
-   ->  Index Only Scan Backward using _hyper_4_15_chunk_metrics_timestamp_time_idx on _hyper_4_15_chunk (actual rows=288 loops=1)
-         Index Cond: (("time" > 'Sat Jan 15 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Fri Jan 21 00:00:00 2000 PST'::timestamp with time zone))
-         Heap Fetches: 288
-(9 rows)
-
--- test constraint_exclusion with timestamptz time dimension and DATE/TIMESTAMP/TIMESTAMPTZ constraints
--- the queries should all have 3 chunks
-:PREFIX SELECT time FROM metrics_timestamptz WHERE time > '2000-01-15'::date ORDER BY time;
-                                                             QUERY PLAN                                                              
--------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamptz (actual rows=14688 loops=1)
-   Order: metrics_timestamptz."time"
-   Chunks excluded during startup: 2
-   ->  Index Only Scan Backward using _hyper_5_19_chunk_metrics_timestamptz_time_idx on _hyper_5_19_chunk (actual rows=4029 loops=1)
-         Index Cond: ("time" > '01-15-2000'::date)
-         Heap Fetches: 4029
-   ->  Index Only Scan Backward using _hyper_5_20_chunk_metrics_timestamptz_time_idx on _hyper_5_20_chunk (actual rows=6048 loops=1)
-         Index Cond: ("time" > '01-15-2000'::date)
-         Heap Fetches: 6048
-   ->  Index Only Scan Backward using _hyper_5_21_chunk_metrics_timestamptz_time_idx on _hyper_5_21_chunk (actual rows=4611 loops=1)
-         Index Cond: ("time" > '01-15-2000'::date)
-         Heap Fetches: 4611
-(12 rows)
-
-:PREFIX SELECT time FROM metrics_timestamptz WHERE time > '2000-01-15'::timestamp ORDER BY time;
-                                                             QUERY PLAN                                                              
--------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamptz (actual rows=14688 loops=1)
-   Order: metrics_timestamptz."time"
-   Chunks excluded during startup: 2
-   ->  Index Only Scan Backward using _hyper_5_19_chunk_metrics_timestamptz_time_idx on _hyper_5_19_chunk (actual rows=4029 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000'::timestamp without time zone)
-         Heap Fetches: 4029
-   ->  Index Only Scan Backward using _hyper_5_20_chunk_metrics_timestamptz_time_idx on _hyper_5_20_chunk (actual rows=6048 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000'::timestamp without time zone)
-         Heap Fetches: 6048
-   ->  Index Only Scan Backward using _hyper_5_21_chunk_metrics_timestamptz_time_idx on _hyper_5_21_chunk (actual rows=4611 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000'::timestamp without time zone)
-         Heap Fetches: 4611
-(12 rows)
-
-:PREFIX SELECT time FROM metrics_timestamptz WHERE time > '2000-01-15'::timestamptz ORDER BY time;
-                                                             QUERY PLAN                                                              
--------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamptz (actual rows=14688 loops=1)
-   Order: metrics_timestamptz."time"
-   ->  Index Only Scan Backward using _hyper_5_19_chunk_metrics_timestamptz_time_idx on _hyper_5_19_chunk (actual rows=4029 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000 PST'::timestamp with time zone)
-         Heap Fetches: 4029
-   ->  Index Only Scan Backward using _hyper_5_20_chunk_metrics_timestamptz_time_idx on _hyper_5_20_chunk (actual rows=6048 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000 PST'::timestamp with time zone)
-         Heap Fetches: 6048
-   ->  Index Only Scan Backward using _hyper_5_21_chunk_metrics_timestamptz_time_idx on _hyper_5_21_chunk (actual rows=4611 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000 PST'::timestamp with time zone)
-         Heap Fetches: 4611
-(11 rows)
-
--- test Const OP Var
--- the queries should all have 3 chunks
-:PREFIX SELECT time FROM metrics_timestamptz WHERE '2000-01-15'::date < time ORDER BY time;
-                                                             QUERY PLAN                                                              
--------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamptz (actual rows=14688 loops=1)
-   Order: metrics_timestamptz."time"
-   Chunks excluded during startup: 2
-   ->  Index Only Scan Backward using _hyper_5_19_chunk_metrics_timestamptz_time_idx on _hyper_5_19_chunk (actual rows=4029 loops=1)
-         Index Cond: ("time" > '01-15-2000'::date)
-         Heap Fetches: 4029
-   ->  Index Only Scan Backward using _hyper_5_20_chunk_metrics_timestamptz_time_idx on _hyper_5_20_chunk (actual rows=6048 loops=1)
-         Index Cond: ("time" > '01-15-2000'::date)
-         Heap Fetches: 6048
-   ->  Index Only Scan Backward using _hyper_5_21_chunk_metrics_timestamptz_time_idx on _hyper_5_21_chunk (actual rows=4611 loops=1)
-         Index Cond: ("time" > '01-15-2000'::date)
-         Heap Fetches: 4611
-(12 rows)
-
-:PREFIX SELECT time FROM metrics_timestamptz WHERE '2000-01-15'::timestamp < time ORDER BY time;
-                                                             QUERY PLAN                                                              
--------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamptz (actual rows=14688 loops=1)
-   Order: metrics_timestamptz."time"
-   Chunks excluded during startup: 2
-   ->  Index Only Scan Backward using _hyper_5_19_chunk_metrics_timestamptz_time_idx on _hyper_5_19_chunk (actual rows=4029 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000'::timestamp without time zone)
-         Heap Fetches: 4029
-   ->  Index Only Scan Backward using _hyper_5_20_chunk_metrics_timestamptz_time_idx on _hyper_5_20_chunk (actual rows=6048 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000'::timestamp without time zone)
-         Heap Fetches: 6048
-   ->  Index Only Scan Backward using _hyper_5_21_chunk_metrics_timestamptz_time_idx on _hyper_5_21_chunk (actual rows=4611 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000'::timestamp without time zone)
-         Heap Fetches: 4611
-(12 rows)
-
-:PREFIX SELECT time FROM metrics_timestamptz WHERE '2000-01-15'::timestamptz < time ORDER BY time;
-                                                             QUERY PLAN                                                              
--------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamptz (actual rows=14688 loops=1)
-   Order: metrics_timestamptz."time"
-   ->  Index Only Scan Backward using _hyper_5_19_chunk_metrics_timestamptz_time_idx on _hyper_5_19_chunk (actual rows=4029 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000 PST'::timestamp with time zone)
-         Heap Fetches: 4029
-   ->  Index Only Scan Backward using _hyper_5_20_chunk_metrics_timestamptz_time_idx on _hyper_5_20_chunk (actual rows=6048 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000 PST'::timestamp with time zone)
-         Heap Fetches: 6048
-   ->  Index Only Scan Backward using _hyper_5_21_chunk_metrics_timestamptz_time_idx on _hyper_5_21_chunk (actual rows=4611 loops=1)
-         Index Cond: ("time" > 'Sat Jan 15 00:00:00 2000 PST'::timestamp with time zone)
-         Heap Fetches: 4611
-(11 rows)
-
--- test 2 constraints
--- the queries should all have 2 chunks
-:PREFIX SELECT time FROM metrics_timestamptz WHERE time > '2000-01-15'::date AND time < '2000-01-21'::date ORDER BY time;
-                                                             QUERY PLAN                                                              
--------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamptz (actual rows=5181 loops=1)
-   Order: metrics_timestamptz."time"
-   Chunks excluded during startup: 3
-   ->  Index Only Scan Backward using _hyper_5_19_chunk_metrics_timestamptz_time_idx on _hyper_5_19_chunk (actual rows=4029 loops=1)
-         Index Cond: (("time" > '01-15-2000'::date) AND ("time" < '01-21-2000'::date))
-         Heap Fetches: 4029
-   ->  Index Only Scan Backward using _hyper_5_20_chunk_metrics_timestamptz_time_idx on _hyper_5_20_chunk (actual rows=1152 loops=1)
-         Index Cond: (("time" > '01-15-2000'::date) AND ("time" < '01-21-2000'::date))
-         Heap Fetches: 1152
-(9 rows)
-
-:PREFIX SELECT time FROM metrics_timestamptz WHERE time > '2000-01-15'::timestamp AND time < '2000-01-21'::timestamp ORDER BY time;
-                                                                           QUERY PLAN                                                                            
------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamptz (actual rows=5181 loops=1)
-   Order: metrics_timestamptz."time"
-   Chunks excluded during startup: 3
-   ->  Index Only Scan Backward using _hyper_5_19_chunk_metrics_timestamptz_time_idx on _hyper_5_19_chunk (actual rows=4029 loops=1)
-         Index Cond: (("time" > 'Sat Jan 15 00:00:00 2000'::timestamp without time zone) AND ("time" < 'Fri Jan 21 00:00:00 2000'::timestamp without time zone))
-         Heap Fetches: 4029
-   ->  Index Only Scan Backward using _hyper_5_20_chunk_metrics_timestamptz_time_idx on _hyper_5_20_chunk (actual rows=1152 loops=1)
-         Index Cond: (("time" > 'Sat Jan 15 00:00:00 2000'::timestamp without time zone) AND ("time" < 'Fri Jan 21 00:00:00 2000'::timestamp without time zone))
-         Heap Fetches: 1152
-(9 rows)
-
-:PREFIX SELECT time FROM metrics_timestamptz WHERE time > '2000-01-15'::timestamptz AND time < '2000-01-21'::timestamptz ORDER BY time;
-                                                                            QUERY PLAN                                                                             
--------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamptz (actual rows=5181 loops=1)
-   Order: metrics_timestamptz."time"
-   ->  Index Only Scan Backward using _hyper_5_19_chunk_metrics_timestamptz_time_idx on _hyper_5_19_chunk (actual rows=4029 loops=1)
-         Index Cond: (("time" > 'Sat Jan 15 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Fri Jan 21 00:00:00 2000 PST'::timestamp with time zone))
-         Heap Fetches: 4029
-   ->  Index Only Scan Backward using _hyper_5_20_chunk_metrics_timestamptz_time_idx on _hyper_5_20_chunk (actual rows=1152 loops=1)
-         Index Cond: (("time" > 'Sat Jan 15 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Fri Jan 21 00:00:00 2000 PST'::timestamp with time zone))
-         Heap Fetches: 1152
-(8 rows)
-
--- test constraint_exclusion with space partitioning and DATE/TIMESTAMP/TIMESTAMPTZ constraints
--- exclusion for constraints with non-matching datatypes not working for space partitioning atm
-:PREFIX SELECT time FROM metrics_space WHERE time > '2000-01-10'::date ORDER BY time;
-                                                             QUERY PLAN                                                              
--------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_space (actual rows=11520 loops=1)
-   Order: metrics_space."time"
-   ->  Merge Append (actual rows=0 loops=1)
-         Sort Key: _hyper_6_22_chunk."time"
-         ->  Index Only Scan Backward using _hyper_6_22_chunk_metrics_space_time_idx on _hyper_6_22_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > '01-10-2000'::date)
-               Heap Fetches: 0
-         ->  Index Only Scan Backward using _hyper_6_23_chunk_metrics_space_time_idx on _hyper_6_23_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > '01-10-2000'::date)
-               Heap Fetches: 0
-         ->  Index Only Scan Backward using _hyper_6_24_chunk_metrics_space_time_idx on _hyper_6_24_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > '01-10-2000'::date)
-               Heap Fetches: 0
-   ->  Merge Append (actual rows=7670 loops=1)
-         Sort Key: _hyper_6_25_chunk."time"
-         ->  Index Only Scan Backward using _hyper_6_25_chunk_metrics_space_time_idx on _hyper_6_25_chunk (actual rows=3068 loops=1)
-               Index Cond: ("time" > '01-10-2000'::date)
-               Heap Fetches: 3068
-         ->  Index Only Scan Backward using _hyper_6_26_chunk_metrics_space_time_idx on _hyper_6_26_chunk (actual rows=3068 loops=1)
-               Index Cond: ("time" > '01-10-2000'::date)
-               Heap Fetches: 3068
-         ->  Index Only Scan Backward using _hyper_6_27_chunk_metrics_space_time_idx on _hyper_6_27_chunk (actual rows=1534 loops=1)
-               Index Cond: ("time" > '01-10-2000'::date)
-               Heap Fetches: 1534
-   ->  Merge Append (actual rows=3850 loops=1)
-         Sort Key: _hyper_6_28_chunk."time"
-         ->  Index Only Scan Backward using _hyper_6_28_chunk_metrics_space_time_idx on _hyper_6_28_chunk (actual rows=1540 loops=1)
-               Index Cond: ("time" > '01-10-2000'::date)
-               Heap Fetches: 1540
-         ->  Index Only Scan Backward using _hyper_6_29_chunk_metrics_space_time_idx on _hyper_6_29_chunk (actual rows=1540 loops=1)
-               Index Cond: ("time" > '01-10-2000'::date)
-               Heap Fetches: 1540
-         ->  Index Only Scan Backward using _hyper_6_30_chunk_metrics_space_time_idx on _hyper_6_30_chunk (actual rows=770 loops=1)
-               Index Cond: ("time" > '01-10-2000'::date)
-               Heap Fetches: 770
-(35 rows)
-
-:PREFIX SELECT time FROM metrics_space WHERE time > '2000-01-10'::timestamp ORDER BY time;
-                                                             QUERY PLAN                                                              
--------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_space (actual rows=11520 loops=1)
-   Order: metrics_space."time"
-   ->  Merge Append (actual rows=0 loops=1)
-         Sort Key: _hyper_6_22_chunk."time"
-         ->  Index Only Scan Backward using _hyper_6_22_chunk_metrics_space_time_idx on _hyper_6_22_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000'::timestamp without time zone)
-               Heap Fetches: 0
-         ->  Index Only Scan Backward using _hyper_6_23_chunk_metrics_space_time_idx on _hyper_6_23_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000'::timestamp without time zone)
-               Heap Fetches: 0
-         ->  Index Only Scan Backward using _hyper_6_24_chunk_metrics_space_time_idx on _hyper_6_24_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000'::timestamp without time zone)
-               Heap Fetches: 0
-   ->  Merge Append (actual rows=7670 loops=1)
-         Sort Key: _hyper_6_25_chunk."time"
-         ->  Index Only Scan Backward using _hyper_6_25_chunk_metrics_space_time_idx on _hyper_6_25_chunk (actual rows=3068 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000'::timestamp without time zone)
-               Heap Fetches: 3068
-         ->  Index Only Scan Backward using _hyper_6_26_chunk_metrics_space_time_idx on _hyper_6_26_chunk (actual rows=3068 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000'::timestamp without time zone)
-               Heap Fetches: 3068
-         ->  Index Only Scan Backward using _hyper_6_27_chunk_metrics_space_time_idx on _hyper_6_27_chunk (actual rows=1534 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000'::timestamp without time zone)
-               Heap Fetches: 1534
-   ->  Merge Append (actual rows=3850 loops=1)
-         Sort Key: _hyper_6_28_chunk."time"
-         ->  Index Only Scan Backward using _hyper_6_28_chunk_metrics_space_time_idx on _hyper_6_28_chunk (actual rows=1540 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000'::timestamp without time zone)
-               Heap Fetches: 1540
-         ->  Index Only Scan Backward using _hyper_6_29_chunk_metrics_space_time_idx on _hyper_6_29_chunk (actual rows=1540 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000'::timestamp without time zone)
-               Heap Fetches: 1540
-         ->  Index Only Scan Backward using _hyper_6_30_chunk_metrics_space_time_idx on _hyper_6_30_chunk (actual rows=770 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000'::timestamp without time zone)
-               Heap Fetches: 770
-(35 rows)
-
-:PREFIX SELECT time FROM metrics_space WHERE time > '2000-01-10'::timestamptz ORDER BY time;
-                                                             QUERY PLAN                                                              
--------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_space (actual rows=11520 loops=1)
-   Order: metrics_space."time"
-   ->  Merge Append (actual rows=7670 loops=1)
-         Sort Key: _hyper_6_25_chunk."time"
-         ->  Index Only Scan Backward using _hyper_6_25_chunk_metrics_space_time_idx on _hyper_6_25_chunk (actual rows=3068 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               Heap Fetches: 3068
-         ->  Index Only Scan Backward using _hyper_6_26_chunk_metrics_space_time_idx on _hyper_6_26_chunk (actual rows=3068 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               Heap Fetches: 3068
-         ->  Index Only Scan Backward using _hyper_6_27_chunk_metrics_space_time_idx on _hyper_6_27_chunk (actual rows=1534 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               Heap Fetches: 1534
-   ->  Merge Append (actual rows=3850 loops=1)
-         Sort Key: _hyper_6_28_chunk."time"
-         ->  Index Only Scan Backward using _hyper_6_28_chunk_metrics_space_time_idx on _hyper_6_28_chunk (actual rows=1540 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               Heap Fetches: 1540
-         ->  Index Only Scan Backward using _hyper_6_29_chunk_metrics_space_time_idx on _hyper_6_29_chunk (actual rows=1540 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               Heap Fetches: 1540
-         ->  Index Only Scan Backward using _hyper_6_30_chunk_metrics_space_time_idx on _hyper_6_30_chunk (actual rows=770 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               Heap Fetches: 770
-(24 rows)
-
--- test Const OP Var
--- exclusion for constraints with non-matching datatypes not working for space partitioning atm
-:PREFIX SELECT time FROM metrics_space WHERE '2000-01-10'::date < time ORDER BY time;
-                                                             QUERY PLAN                                                              
--------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_space (actual rows=11520 loops=1)
-   Order: metrics_space."time"
-   ->  Merge Append (actual rows=0 loops=1)
-         Sort Key: _hyper_6_22_chunk."time"
-         ->  Index Only Scan Backward using _hyper_6_22_chunk_metrics_space_time_idx on _hyper_6_22_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > '01-10-2000'::date)
-               Heap Fetches: 0
-         ->  Index Only Scan Backward using _hyper_6_23_chunk_metrics_space_time_idx on _hyper_6_23_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > '01-10-2000'::date)
-               Heap Fetches: 0
-         ->  Index Only Scan Backward using _hyper_6_24_chunk_metrics_space_time_idx on _hyper_6_24_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > '01-10-2000'::date)
-               Heap Fetches: 0
-   ->  Merge Append (actual rows=7670 loops=1)
-         Sort Key: _hyper_6_25_chunk."time"
-         ->  Index Only Scan Backward using _hyper_6_25_chunk_metrics_space_time_idx on _hyper_6_25_chunk (actual rows=3068 loops=1)
-               Index Cond: ("time" > '01-10-2000'::date)
-               Heap Fetches: 3068
-         ->  Index Only Scan Backward using _hyper_6_26_chunk_metrics_space_time_idx on _hyper_6_26_chunk (actual rows=3068 loops=1)
-               Index Cond: ("time" > '01-10-2000'::date)
-               Heap Fetches: 3068
-         ->  Index Only Scan Backward using _hyper_6_27_chunk_metrics_space_time_idx on _hyper_6_27_chunk (actual rows=1534 loops=1)
-               Index Cond: ("time" > '01-10-2000'::date)
-               Heap Fetches: 1534
-   ->  Merge Append (actual rows=3850 loops=1)
-         Sort Key: _hyper_6_28_chunk."time"
-         ->  Index Only Scan Backward using _hyper_6_28_chunk_metrics_space_time_idx on _hyper_6_28_chunk (actual rows=1540 loops=1)
-               Index Cond: ("time" > '01-10-2000'::date)
-               Heap Fetches: 1540
-         ->  Index Only Scan Backward using _hyper_6_29_chunk_metrics_space_time_idx on _hyper_6_29_chunk (actual rows=1540 loops=1)
-               Index Cond: ("time" > '01-10-2000'::date)
-               Heap Fetches: 1540
-         ->  Index Only Scan Backward using _hyper_6_30_chunk_metrics_space_time_idx on _hyper_6_30_chunk (actual rows=770 loops=1)
-               Index Cond: ("time" > '01-10-2000'::date)
-               Heap Fetches: 770
-(35 rows)
-
-:PREFIX SELECT time FROM metrics_space WHERE '2000-01-10'::timestamp < time ORDER BY time;
-                                                             QUERY PLAN                                                              
--------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_space (actual rows=11520 loops=1)
-   Order: metrics_space."time"
-   ->  Merge Append (actual rows=0 loops=1)
-         Sort Key: _hyper_6_22_chunk."time"
-         ->  Index Only Scan Backward using _hyper_6_22_chunk_metrics_space_time_idx on _hyper_6_22_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000'::timestamp without time zone)
-               Heap Fetches: 0
-         ->  Index Only Scan Backward using _hyper_6_23_chunk_metrics_space_time_idx on _hyper_6_23_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000'::timestamp without time zone)
-               Heap Fetches: 0
-         ->  Index Only Scan Backward using _hyper_6_24_chunk_metrics_space_time_idx on _hyper_6_24_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000'::timestamp without time zone)
-               Heap Fetches: 0
-   ->  Merge Append (actual rows=7670 loops=1)
-         Sort Key: _hyper_6_25_chunk."time"
-         ->  Index Only Scan Backward using _hyper_6_25_chunk_metrics_space_time_idx on _hyper_6_25_chunk (actual rows=3068 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000'::timestamp without time zone)
-               Heap Fetches: 3068
-         ->  Index Only Scan Backward using _hyper_6_26_chunk_metrics_space_time_idx on _hyper_6_26_chunk (actual rows=3068 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000'::timestamp without time zone)
-               Heap Fetches: 3068
-         ->  Index Only Scan Backward using _hyper_6_27_chunk_metrics_space_time_idx on _hyper_6_27_chunk (actual rows=1534 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000'::timestamp without time zone)
-               Heap Fetches: 1534
-   ->  Merge Append (actual rows=3850 loops=1)
-         Sort Key: _hyper_6_28_chunk."time"
-         ->  Index Only Scan Backward using _hyper_6_28_chunk_metrics_space_time_idx on _hyper_6_28_chunk (actual rows=1540 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000'::timestamp without time zone)
-               Heap Fetches: 1540
-         ->  Index Only Scan Backward using _hyper_6_29_chunk_metrics_space_time_idx on _hyper_6_29_chunk (actual rows=1540 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000'::timestamp without time zone)
-               Heap Fetches: 1540
-         ->  Index Only Scan Backward using _hyper_6_30_chunk_metrics_space_time_idx on _hyper_6_30_chunk (actual rows=770 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000'::timestamp without time zone)
-               Heap Fetches: 770
-(35 rows)
-
-:PREFIX SELECT time FROM metrics_space WHERE '2000-01-10'::timestamptz < time ORDER BY time;
-                                                             QUERY PLAN                                                              
--------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_space (actual rows=11520 loops=1)
-   Order: metrics_space."time"
-   ->  Merge Append (actual rows=7670 loops=1)
-         Sort Key: _hyper_6_25_chunk."time"
-         ->  Index Only Scan Backward using _hyper_6_25_chunk_metrics_space_time_idx on _hyper_6_25_chunk (actual rows=3068 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               Heap Fetches: 3068
-         ->  Index Only Scan Backward using _hyper_6_26_chunk_metrics_space_time_idx on _hyper_6_26_chunk (actual rows=3068 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               Heap Fetches: 3068
-         ->  Index Only Scan Backward using _hyper_6_27_chunk_metrics_space_time_idx on _hyper_6_27_chunk (actual rows=1534 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               Heap Fetches: 1534
-   ->  Merge Append (actual rows=3850 loops=1)
-         Sort Key: _hyper_6_28_chunk."time"
-         ->  Index Only Scan Backward using _hyper_6_28_chunk_metrics_space_time_idx on _hyper_6_28_chunk (actual rows=1540 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               Heap Fetches: 1540
-         ->  Index Only Scan Backward using _hyper_6_29_chunk_metrics_space_time_idx on _hyper_6_29_chunk (actual rows=1540 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               Heap Fetches: 1540
-         ->  Index Only Scan Backward using _hyper_6_30_chunk_metrics_space_time_idx on _hyper_6_30_chunk (actual rows=770 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               Heap Fetches: 770
-(24 rows)
-
--- test 2 constraints
--- exclusion for constraints with non-matching datatypes not working for space partitioning atm
-:PREFIX SELECT time FROM metrics_space WHERE time > '2000-01-10'::date AND time < '2000-01-15'::date ORDER BY time;
-                                                             QUERY PLAN                                                              
--------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_space (actual rows=11520 loops=1)
-   Order: metrics_space."time"
-   ->  Merge Append (actual rows=0 loops=1)
-         Sort Key: _hyper_6_22_chunk."time"
-         ->  Index Only Scan Backward using _hyper_6_22_chunk_metrics_space_time_idx on _hyper_6_22_chunk (actual rows=0 loops=1)
-               Index Cond: (("time" > '01-10-2000'::date) AND ("time" < '01-15-2000'::date))
-               Heap Fetches: 0
-         ->  Index Only Scan Backward using _hyper_6_23_chunk_metrics_space_time_idx on _hyper_6_23_chunk (actual rows=0 loops=1)
-               Index Cond: (("time" > '01-10-2000'::date) AND ("time" < '01-15-2000'::date))
-               Heap Fetches: 0
-         ->  Index Only Scan Backward using _hyper_6_24_chunk_metrics_space_time_idx on _hyper_6_24_chunk (actual rows=0 loops=1)
-               Index Cond: (("time" > '01-10-2000'::date) AND ("time" < '01-15-2000'::date))
-               Heap Fetches: 0
-   ->  Merge Append (actual rows=7670 loops=1)
-         Sort Key: _hyper_6_25_chunk."time"
-         ->  Index Only Scan Backward using _hyper_6_25_chunk_metrics_space_time_idx on _hyper_6_25_chunk (actual rows=3068 loops=1)
-               Index Cond: (("time" > '01-10-2000'::date) AND ("time" < '01-15-2000'::date))
-               Heap Fetches: 3068
-         ->  Index Only Scan Backward using _hyper_6_26_chunk_metrics_space_time_idx on _hyper_6_26_chunk (actual rows=3068 loops=1)
-               Index Cond: (("time" > '01-10-2000'::date) AND ("time" < '01-15-2000'::date))
-               Heap Fetches: 3068
-         ->  Index Only Scan Backward using _hyper_6_27_chunk_metrics_space_time_idx on _hyper_6_27_chunk (actual rows=1534 loops=1)
-               Index Cond: (("time" > '01-10-2000'::date) AND ("time" < '01-15-2000'::date))
-               Heap Fetches: 1534
-   ->  Merge Append (actual rows=3850 loops=1)
-         Sort Key: _hyper_6_28_chunk."time"
-         ->  Index Only Scan Backward using _hyper_6_28_chunk_metrics_space_time_idx on _hyper_6_28_chunk (actual rows=1540 loops=1)
-               Index Cond: (("time" > '01-10-2000'::date) AND ("time" < '01-15-2000'::date))
-               Heap Fetches: 1540
-         ->  Index Only Scan Backward using _hyper_6_29_chunk_metrics_space_time_idx on _hyper_6_29_chunk (actual rows=1540 loops=1)
-               Index Cond: (("time" > '01-10-2000'::date) AND ("time" < '01-15-2000'::date))
-               Heap Fetches: 1540
-         ->  Index Only Scan Backward using _hyper_6_30_chunk_metrics_space_time_idx on _hyper_6_30_chunk (actual rows=770 loops=1)
-               Index Cond: (("time" > '01-10-2000'::date) AND ("time" < '01-15-2000'::date))
-               Heap Fetches: 770
-(35 rows)
-
-:PREFIX SELECT time FROM metrics_space WHERE time > '2000-01-10'::timestamp AND time < '2000-01-15'::timestamp ORDER BY time;
-                                                                              QUERY PLAN                                                                               
------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_space (actual rows=11520 loops=1)
-   Order: metrics_space."time"
-   ->  Merge Append (actual rows=0 loops=1)
-         Sort Key: _hyper_6_22_chunk."time"
-         ->  Index Only Scan Backward using _hyper_6_22_chunk_metrics_space_time_idx on _hyper_6_22_chunk (actual rows=0 loops=1)
-               Index Cond: (("time" > 'Mon Jan 10 00:00:00 2000'::timestamp without time zone) AND ("time" < 'Sat Jan 15 00:00:00 2000'::timestamp without time zone))
-               Heap Fetches: 0
-         ->  Index Only Scan Backward using _hyper_6_23_chunk_metrics_space_time_idx on _hyper_6_23_chunk (actual rows=0 loops=1)
-               Index Cond: (("time" > 'Mon Jan 10 00:00:00 2000'::timestamp without time zone) AND ("time" < 'Sat Jan 15 00:00:00 2000'::timestamp without time zone))
-               Heap Fetches: 0
-         ->  Index Only Scan Backward using _hyper_6_24_chunk_metrics_space_time_idx on _hyper_6_24_chunk (actual rows=0 loops=1)
-               Index Cond: (("time" > 'Mon Jan 10 00:00:00 2000'::timestamp without time zone) AND ("time" < 'Sat Jan 15 00:00:00 2000'::timestamp without time zone))
-               Heap Fetches: 0
-   ->  Merge Append (actual rows=7670 loops=1)
-         Sort Key: _hyper_6_25_chunk."time"
-         ->  Index Only Scan Backward using _hyper_6_25_chunk_metrics_space_time_idx on _hyper_6_25_chunk (actual rows=3068 loops=1)
-               Index Cond: (("time" > 'Mon Jan 10 00:00:00 2000'::timestamp without time zone) AND ("time" < 'Sat Jan 15 00:00:00 2000'::timestamp without time zone))
-               Heap Fetches: 3068
-         ->  Index Only Scan Backward using _hyper_6_26_chunk_metrics_space_time_idx on _hyper_6_26_chunk (actual rows=3068 loops=1)
-               Index Cond: (("time" > 'Mon Jan 10 00:00:00 2000'::timestamp without time zone) AND ("time" < 'Sat Jan 15 00:00:00 2000'::timestamp without time zone))
-               Heap Fetches: 3068
-         ->  Index Only Scan Backward using _hyper_6_27_chunk_metrics_space_time_idx on _hyper_6_27_chunk (actual rows=1534 loops=1)
-               Index Cond: (("time" > 'Mon Jan 10 00:00:00 2000'::timestamp without time zone) AND ("time" < 'Sat Jan 15 00:00:00 2000'::timestamp without time zone))
-               Heap Fetches: 1534
-   ->  Merge Append (actual rows=3850 loops=1)
-         Sort Key: _hyper_6_28_chunk."time"
-         ->  Index Only Scan Backward using _hyper_6_28_chunk_metrics_space_time_idx on _hyper_6_28_chunk (actual rows=1540 loops=1)
-               Index Cond: (("time" > 'Mon Jan 10 00:00:00 2000'::timestamp without time zone) AND ("time" < 'Sat Jan 15 00:00:00 2000'::timestamp without time zone))
-               Heap Fetches: 1540
-         ->  Index Only Scan Backward using _hyper_6_29_chunk_metrics_space_time_idx on _hyper_6_29_chunk (actual rows=1540 loops=1)
-               Index Cond: (("time" > 'Mon Jan 10 00:00:00 2000'::timestamp without time zone) AND ("time" < 'Sat Jan 15 00:00:00 2000'::timestamp without time zone))
-               Heap Fetches: 1540
-         ->  Index Only Scan Backward using _hyper_6_30_chunk_metrics_space_time_idx on _hyper_6_30_chunk (actual rows=770 loops=1)
-               Index Cond: (("time" > 'Mon Jan 10 00:00:00 2000'::timestamp without time zone) AND ("time" < 'Sat Jan 15 00:00:00 2000'::timestamp without time zone))
-               Heap Fetches: 770
-(35 rows)
-
-:PREFIX SELECT time FROM metrics_space WHERE time > '2000-01-10'::timestamptz AND time < '2000-01-15'::timestamptz ORDER BY time;
-                                                                               QUERY PLAN                                                                                
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_space (actual rows=11520 loops=1)
-   Order: metrics_space."time"
-   ->  Merge Append (actual rows=7670 loops=1)
-         Sort Key: _hyper_6_25_chunk."time"
-         ->  Index Only Scan Backward using _hyper_6_25_chunk_metrics_space_time_idx on _hyper_6_25_chunk (actual rows=3068 loops=1)
-               Index Cond: (("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Sat Jan 15 00:00:00 2000 PST'::timestamp with time zone))
-               Heap Fetches: 3068
-         ->  Index Only Scan Backward using _hyper_6_26_chunk_metrics_space_time_idx on _hyper_6_26_chunk (actual rows=3068 loops=1)
-               Index Cond: (("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Sat Jan 15 00:00:00 2000 PST'::timestamp with time zone))
-               Heap Fetches: 3068
-         ->  Index Only Scan Backward using _hyper_6_27_chunk_metrics_space_time_idx on _hyper_6_27_chunk (actual rows=1534 loops=1)
-               Index Cond: (("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Sat Jan 15 00:00:00 2000 PST'::timestamp with time zone))
-               Heap Fetches: 1534
-   ->  Merge Append (actual rows=3850 loops=1)
-         Sort Key: _hyper_6_28_chunk."time"
-         ->  Index Only Scan Backward using _hyper_6_28_chunk_metrics_space_time_idx on _hyper_6_28_chunk (actual rows=1540 loops=1)
-               Index Cond: (("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Sat Jan 15 00:00:00 2000 PST'::timestamp with time zone))
-               Heap Fetches: 1540
-         ->  Index Only Scan Backward using _hyper_6_29_chunk_metrics_space_time_idx on _hyper_6_29_chunk (actual rows=1540 loops=1)
-               Index Cond: (("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Sat Jan 15 00:00:00 2000 PST'::timestamp with time zone))
-               Heap Fetches: 1540
-         ->  Index Only Scan Backward using _hyper_6_30_chunk_metrics_space_time_idx on _hyper_6_30_chunk (actual rows=770 loops=1)
-               Index Cond: (("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Sat Jan 15 00:00:00 2000 PST'::timestamp with time zone))
-               Heap Fetches: 770
-(24 rows)
-
--- test filtering on space partition
-:PREFIX SELECT time FROM metrics_space WHERE time > '2000-01-10'::timestamptz AND device_id = 1 ORDER BY time;
-                                                               QUERY PLAN                                                               
-----------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_space (actual rows=1152 loops=1)
-   Order: metrics_space."time"
-   ->  Index Only Scan Backward using _hyper_6_25_chunk_metrics_space_device_id_time_idx on _hyper_6_25_chunk (actual rows=767 loops=1)
-         Index Cond: ((device_id = 1) AND ("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-         Heap Fetches: 767
-   ->  Index Only Scan Backward using _hyper_6_28_chunk_metrics_space_device_id_time_idx on _hyper_6_28_chunk (actual rows=385 loops=1)
-         Index Cond: ((device_id = 1) AND ("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-         Heap Fetches: 385
-(8 rows)
-
-:PREFIX SELECT time FROM metrics_space WHERE time > '2000-01-10'::timestamptz AND device_id IN (1,2) ORDER BY time;
-                                                          QUERY PLAN                                                           
--------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_space (actual rows=2304 loops=1)
-   Order: metrics_space."time"
-   ->  Merge Append (actual rows=1534 loops=1)
-         Sort Key: _hyper_6_25_chunk."time"
-         ->  Index Scan Backward using _hyper_6_25_chunk_metrics_space_time_idx on _hyper_6_25_chunk (actual rows=767 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               Filter: (device_id = ANY ('{1,2}'::integer[]))
-               Rows Removed by Filter: 2301
-         ->  Index Scan Backward using _hyper_6_26_chunk_metrics_space_time_idx on _hyper_6_26_chunk (actual rows=767 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               Filter: (device_id = ANY ('{1,2}'::integer[]))
-               Rows Removed by Filter: 2301
-   ->  Merge Append (actual rows=770 loops=1)
-         Sort Key: _hyper_6_28_chunk."time"
-         ->  Index Scan Backward using _hyper_6_28_chunk_metrics_space_time_idx on _hyper_6_28_chunk (actual rows=385 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               Filter: (device_id = ANY ('{1,2}'::integer[]))
-               Rows Removed by Filter: 1155
-         ->  Index Scan Backward using _hyper_6_29_chunk_metrics_space_time_idx on _hyper_6_29_chunk (actual rows=385 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               Filter: (device_id = ANY ('{1,2}'::integer[]))
-               Rows Removed by Filter: 1155
-(22 rows)
-
-:PREFIX SELECT time FROM metrics_space WHERE time > '2000-01-10'::timestamptz AND device_id IN (VALUES(1)) ORDER BY time;
-                                                               QUERY PLAN                                                               
-----------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_space (actual rows=1152 loops=1)
-   Order: metrics_space."time"
-   ->  Index Only Scan Backward using _hyper_6_25_chunk_metrics_space_device_id_time_idx on _hyper_6_25_chunk (actual rows=767 loops=1)
-         Index Cond: ((device_id = 1) AND ("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-         Heap Fetches: 767
-   ->  Index Only Scan Backward using _hyper_6_28_chunk_metrics_space_device_id_time_idx on _hyper_6_28_chunk (actual rows=385 loops=1)
-         Index Cond: ((device_id = 1) AND ("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-         Heap Fetches: 385
-(8 rows)
-
-:PREFIX SELECT time FROM metrics_space WHERE time > '2000-01-10'::timestamptz AND v3 IN (VALUES('1')) ORDER BY time;
-                                                          QUERY PLAN                                                           
--------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_space (actual rows=1152 loops=1)
-   Order: metrics_space."time"
-   ->  Merge Append (actual rows=767 loops=1)
-         Sort Key: _hyper_6_25_chunk."time"
-         ->  Index Scan Backward using _hyper_6_25_chunk_metrics_space_time_idx on _hyper_6_25_chunk (actual rows=767 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               Filter: (v3 = '1'::text)
-               Rows Removed by Filter: 2301
-         ->  Index Scan Backward using _hyper_6_26_chunk_metrics_space_time_idx on _hyper_6_26_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               Filter: (v3 = '1'::text)
-               Rows Removed by Filter: 3068
-         ->  Index Scan Backward using _hyper_6_27_chunk_metrics_space_time_idx on _hyper_6_27_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               Filter: (v3 = '1'::text)
-               Rows Removed by Filter: 1534
-   ->  Merge Append (actual rows=385 loops=1)
-         Sort Key: _hyper_6_28_chunk."time"
-         ->  Index Scan Backward using _hyper_6_28_chunk_metrics_space_time_idx on _hyper_6_28_chunk (actual rows=385 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               Filter: (v3 = '1'::text)
-               Rows Removed by Filter: 1155
-         ->  Index Scan Backward using _hyper_6_29_chunk_metrics_space_time_idx on _hyper_6_29_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               Filter: (v3 = '1'::text)
-               Rows Removed by Filter: 1540
-         ->  Index Scan Backward using _hyper_6_30_chunk_metrics_space_time_idx on _hyper_6_30_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               Filter: (v3 = '1'::text)
-               Rows Removed by Filter: 770
-(30 rows)
-
-:PREFIX SELECT * FROM metrics_space
-WHERE time = (VALUES ('2019-12-24' at time zone 'UTC'))
-  AND v3 NOT IN (VALUES ('1'));
-                                              QUERY PLAN                                               
--------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_space (actual rows=0 loops=1)
-   Chunks excluded during startup: 0
-   Chunks excluded during runtime: 9
-   InitPlan 1 (returns $0)
-     ->  Result (actual rows=1 loops=1)
-   ->  Index Scan using _hyper_6_22_chunk_metrics_space_time_idx on _hyper_6_22_chunk (never executed)
-         Index Cond: ("time" = $0)
-         Filter: (NOT (hashed SubPlan 2))
-         SubPlan 2
-           ->  Result (never executed)
-   ->  Index Scan using _hyper_6_23_chunk_metrics_space_time_idx on _hyper_6_23_chunk (never executed)
-         Index Cond: ("time" = $0)
-         Filter: (NOT (hashed SubPlan 2))
-   ->  Index Scan using _hyper_6_24_chunk_metrics_space_time_idx on _hyper_6_24_chunk (never executed)
-         Index Cond: ("time" = $0)
-         Filter: (NOT (hashed SubPlan 2))
-   ->  Index Scan using _hyper_6_25_chunk_metrics_space_time_idx on _hyper_6_25_chunk (never executed)
-         Index Cond: ("time" = $0)
-         Filter: (NOT (hashed SubPlan 2))
-   ->  Index Scan using _hyper_6_26_chunk_metrics_space_time_idx on _hyper_6_26_chunk (never executed)
-         Index Cond: ("time" = $0)
-         Filter: (NOT (hashed SubPlan 2))
-   ->  Index Scan using _hyper_6_27_chunk_metrics_space_time_idx on _hyper_6_27_chunk (never executed)
-         Index Cond: ("time" = $0)
-         Filter: (NOT (hashed SubPlan 2))
-   ->  Index Scan using _hyper_6_28_chunk_metrics_space_time_idx on _hyper_6_28_chunk (never executed)
-         Index Cond: ("time" = $0)
-         Filter: (NOT (hashed SubPlan 2))
-   ->  Index Scan using _hyper_6_29_chunk_metrics_space_time_idx on _hyper_6_29_chunk (never executed)
-         Index Cond: ("time" = $0)
-         Filter: (NOT (hashed SubPlan 2))
-   ->  Index Scan using _hyper_6_30_chunk_metrics_space_time_idx on _hyper_6_30_chunk (never executed)
-         Index Cond: ("time" = $0)
-         Filter: (NOT (hashed SubPlan 2))
-(34 rows)
-
--- test CURRENT_DATE
--- should be 0 chunks
-:PREFIX SELECT time FROM metrics_date WHERE time > CURRENT_DATE ORDER BY time;
-                            QUERY PLAN                             
--------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_date (actual rows=0 loops=1)
-   Order: metrics_date."time"
-   Chunks excluded during startup: 5
-(3 rows)
-
-:PREFIX SELECT time FROM metrics_timestamp WHERE time > CURRENT_DATE ORDER BY time;
-                               QUERY PLAN                               
-------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamp (actual rows=0 loops=1)
-   Order: metrics_timestamp."time"
-   Chunks excluded during startup: 5
-(3 rows)
-
-:PREFIX SELECT time FROM metrics_timestamptz WHERE time > CURRENT_DATE ORDER BY time;
-                                QUERY PLAN                                
---------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamptz (actual rows=0 loops=1)
-   Order: metrics_timestamptz."time"
-   Chunks excluded during startup: 5
-(3 rows)
-
-:PREFIX SELECT time FROM metrics_space WHERE time > CURRENT_DATE ORDER BY time;
-                                                            QUERY PLAN                                                            
-----------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_space (actual rows=0 loops=1)
-   Order: metrics_space."time"
-   ->  Merge Append (actual rows=0 loops=1)
-         Sort Key: _hyper_6_22_chunk."time"
-         ->  Index Only Scan Backward using _hyper_6_22_chunk_metrics_space_time_idx on _hyper_6_22_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > CURRENT_DATE)
-               Heap Fetches: 0
-         ->  Index Only Scan Backward using _hyper_6_23_chunk_metrics_space_time_idx on _hyper_6_23_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > CURRENT_DATE)
-               Heap Fetches: 0
-         ->  Index Only Scan Backward using _hyper_6_24_chunk_metrics_space_time_idx on _hyper_6_24_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > CURRENT_DATE)
-               Heap Fetches: 0
-   ->  Merge Append (actual rows=0 loops=1)
-         Sort Key: _hyper_6_25_chunk."time"
-         ->  Index Only Scan Backward using _hyper_6_25_chunk_metrics_space_time_idx on _hyper_6_25_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > CURRENT_DATE)
-               Heap Fetches: 0
-         ->  Index Only Scan Backward using _hyper_6_26_chunk_metrics_space_time_idx on _hyper_6_26_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > CURRENT_DATE)
-               Heap Fetches: 0
-         ->  Index Only Scan Backward using _hyper_6_27_chunk_metrics_space_time_idx on _hyper_6_27_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > CURRENT_DATE)
-               Heap Fetches: 0
-   ->  Merge Append (actual rows=0 loops=1)
-         Sort Key: _hyper_6_28_chunk."time"
-         ->  Index Only Scan Backward using _hyper_6_28_chunk_metrics_space_time_idx on _hyper_6_28_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > CURRENT_DATE)
-               Heap Fetches: 0
-         ->  Index Only Scan Backward using _hyper_6_29_chunk_metrics_space_time_idx on _hyper_6_29_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > CURRENT_DATE)
-               Heap Fetches: 0
-         ->  Index Only Scan Backward using _hyper_6_30_chunk_metrics_space_time_idx on _hyper_6_30_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > CURRENT_DATE)
-               Heap Fetches: 0
-(35 rows)
-
--- test CURRENT_TIMESTAMP
--- should be 0 chunks
-:PREFIX SELECT time FROM metrics_date WHERE time > CURRENT_TIMESTAMP ORDER BY time;
-                            QUERY PLAN                             
--------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_date (actual rows=0 loops=1)
-   Order: metrics_date."time"
-   Chunks excluded during startup: 5
-(3 rows)
-
-:PREFIX SELECT time FROM metrics_timestamp WHERE time > CURRENT_TIMESTAMP ORDER BY time;
-                               QUERY PLAN                               
-------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamp (actual rows=0 loops=1)
-   Order: metrics_timestamp."time"
-   Chunks excluded during startup: 5
-(3 rows)
-
-:PREFIX SELECT time FROM metrics_timestamptz WHERE time > CURRENT_TIMESTAMP ORDER BY time;
-                                QUERY PLAN                                
---------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamptz (actual rows=0 loops=1)
-   Order: metrics_timestamptz."time"
-   Chunks excluded during startup: 5
-(3 rows)
-
-:PREFIX SELECT time FROM metrics_space WHERE time > CURRENT_TIMESTAMP ORDER BY time;
-                                                            QUERY PLAN                                                            
-----------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_space (actual rows=0 loops=1)
-   Order: metrics_space."time"
-   ->  Merge Append (actual rows=0 loops=1)
-         Sort Key: _hyper_6_22_chunk."time"
-         ->  Index Only Scan Backward using _hyper_6_22_chunk_metrics_space_time_idx on _hyper_6_22_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > CURRENT_TIMESTAMP)
-               Heap Fetches: 0
-         ->  Index Only Scan Backward using _hyper_6_23_chunk_metrics_space_time_idx on _hyper_6_23_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > CURRENT_TIMESTAMP)
-               Heap Fetches: 0
-         ->  Index Only Scan Backward using _hyper_6_24_chunk_metrics_space_time_idx on _hyper_6_24_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > CURRENT_TIMESTAMP)
-               Heap Fetches: 0
-   ->  Merge Append (actual rows=0 loops=1)
-         Sort Key: _hyper_6_25_chunk."time"
-         ->  Index Only Scan Backward using _hyper_6_25_chunk_metrics_space_time_idx on _hyper_6_25_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > CURRENT_TIMESTAMP)
-               Heap Fetches: 0
-         ->  Index Only Scan Backward using _hyper_6_26_chunk_metrics_space_time_idx on _hyper_6_26_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > CURRENT_TIMESTAMP)
-               Heap Fetches: 0
-         ->  Index Only Scan Backward using _hyper_6_27_chunk_metrics_space_time_idx on _hyper_6_27_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > CURRENT_TIMESTAMP)
-               Heap Fetches: 0
-   ->  Merge Append (actual rows=0 loops=1)
-         Sort Key: _hyper_6_28_chunk."time"
-         ->  Index Only Scan Backward using _hyper_6_28_chunk_metrics_space_time_idx on _hyper_6_28_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > CURRENT_TIMESTAMP)
-               Heap Fetches: 0
-         ->  Index Only Scan Backward using _hyper_6_29_chunk_metrics_space_time_idx on _hyper_6_29_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > CURRENT_TIMESTAMP)
-               Heap Fetches: 0
-         ->  Index Only Scan Backward using _hyper_6_30_chunk_metrics_space_time_idx on _hyper_6_30_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > CURRENT_TIMESTAMP)
-               Heap Fetches: 0
-(35 rows)
-
--- test now()
--- should be 0 chunks
-:PREFIX SELECT time FROM metrics_date WHERE time > now() ORDER BY time;
-                            QUERY PLAN                             
--------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_date (actual rows=0 loops=1)
-   Order: metrics_date."time"
-   Chunks excluded during startup: 5
-(3 rows)
-
-:PREFIX SELECT time FROM metrics_timestamp WHERE time > now() ORDER BY time;
-                               QUERY PLAN                               
-------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamp (actual rows=0 loops=1)
-   Order: metrics_timestamp."time"
-   Chunks excluded during startup: 5
-(3 rows)
-
-:PREFIX SELECT time FROM metrics_timestamptz WHERE time > now() ORDER BY time;
-                                QUERY PLAN                                
---------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamptz (actual rows=0 loops=1)
-   Order: metrics_timestamptz."time"
-   Chunks excluded during startup: 5
-(3 rows)
-
-:PREFIX SELECT time FROM metrics_space WHERE time > now() ORDER BY time;
-                                                            QUERY PLAN                                                            
-----------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_space (actual rows=0 loops=1)
-   Order: metrics_space."time"
-   ->  Merge Append (actual rows=0 loops=1)
-         Sort Key: _hyper_6_22_chunk."time"
-         ->  Index Only Scan Backward using _hyper_6_22_chunk_metrics_space_time_idx on _hyper_6_22_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > now())
-               Heap Fetches: 0
-         ->  Index Only Scan Backward using _hyper_6_23_chunk_metrics_space_time_idx on _hyper_6_23_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > now())
-               Heap Fetches: 0
-         ->  Index Only Scan Backward using _hyper_6_24_chunk_metrics_space_time_idx on _hyper_6_24_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > now())
-               Heap Fetches: 0
-   ->  Merge Append (actual rows=0 loops=1)
-         Sort Key: _hyper_6_25_chunk."time"
-         ->  Index Only Scan Backward using _hyper_6_25_chunk_metrics_space_time_idx on _hyper_6_25_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > now())
-               Heap Fetches: 0
-         ->  Index Only Scan Backward using _hyper_6_26_chunk_metrics_space_time_idx on _hyper_6_26_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > now())
-               Heap Fetches: 0
-         ->  Index Only Scan Backward using _hyper_6_27_chunk_metrics_space_time_idx on _hyper_6_27_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > now())
-               Heap Fetches: 0
-   ->  Merge Append (actual rows=0 loops=1)
-         Sort Key: _hyper_6_28_chunk."time"
-         ->  Index Only Scan Backward using _hyper_6_28_chunk_metrics_space_time_idx on _hyper_6_28_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > now())
-               Heap Fetches: 0
-         ->  Index Only Scan Backward using _hyper_6_29_chunk_metrics_space_time_idx on _hyper_6_29_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > now())
-               Heap Fetches: 0
-         ->  Index Only Scan Backward using _hyper_6_30_chunk_metrics_space_time_idx on _hyper_6_30_chunk (actual rows=0 loops=1)
-               Index Cond: ("time" > now())
-               Heap Fetches: 0
-(35 rows)
-
--- query with tablesample and planner exclusion
-:PREFIX
-SELECT * FROM metrics_date TABLESAMPLE BERNOULLI(5) REPEATABLE(0)
-WHERE time > '2000-01-15'
-ORDER BY time DESC;
-                                    QUERY PLAN                                    
-----------------------------------------------------------------------------------
- Sort (actual rows=217 loops=1)
-   Sort Key: _hyper_3_11_chunk."time" DESC
-   Sort Method: quicksort 
-   ->  Append (actual rows=217 loops=1)
-         ->  Sample Scan on _hyper_3_11_chunk (actual rows=72 loops=1)
-               Sampling: bernoulli ('5'::real) REPEATABLE ('0'::double precision)
-               Filter: ("time" > '01-15-2000'::date)
-         ->  Sample Scan on _hyper_3_10_chunk (actual rows=94 loops=1)
-               Sampling: bernoulli ('5'::real) REPEATABLE ('0'::double precision)
-               Filter: ("time" > '01-15-2000'::date)
-         ->  Sample Scan on _hyper_3_9_chunk (actual rows=51 loops=1)
-               Sampling: bernoulli ('5'::real) REPEATABLE ('0'::double precision)
-               Filter: ("time" > '01-15-2000'::date)
-               Rows Removed by Filter: 43
-(14 rows)
-
--- query with tablesample and startup exclusion
-:PREFIX
-SELECT * FROM metrics_date TABLESAMPLE BERNOULLI(5) REPEATABLE(0)
-WHERE time > '2000-01-15'::text::date
-ORDER BY time DESC;
-                                    QUERY PLAN                                    
-----------------------------------------------------------------------------------
- Sort (actual rows=217 loops=1)
-   Sort Key: metrics_date."time" DESC
-   Sort Method: quicksort 
-   ->  Custom Scan (ChunkAppend) on metrics_date (actual rows=217 loops=1)
-         Chunks excluded during startup: 2
-         ->  Sample Scan on _hyper_3_11_chunk (actual rows=72 loops=1)
-               Sampling: bernoulli ('5'::real) REPEATABLE ('0'::double precision)
-               Filter: ("time" > ('2000-01-15'::cstring)::date)
-         ->  Sample Scan on _hyper_3_10_chunk (actual rows=94 loops=1)
-               Sampling: bernoulli ('5'::real) REPEATABLE ('0'::double precision)
-               Filter: ("time" > ('2000-01-15'::cstring)::date)
-         ->  Sample Scan on _hyper_3_9_chunk (actual rows=51 loops=1)
-               Sampling: bernoulli ('5'::real) REPEATABLE ('0'::double precision)
-               Filter: ("time" > ('2000-01-15'::cstring)::date)
-               Rows Removed by Filter: 43
-(15 rows)
-
--- query with tablesample, space partitioning and planner exclusion
-:PREFIX
-SELECT * FROM metrics_space TABLESAMPLE BERNOULLI(5) REPEATABLE(0)
-WHERE time > '2000-01-10'::timestamptz
-ORDER BY time DESC, device_id;
-                                        QUERY PLAN                                         
--------------------------------------------------------------------------------------------
- Sort (actual rows=522 loops=1)
-   Sort Key: _hyper_6_30_chunk."time" DESC, _hyper_6_30_chunk.device_id
-   Sort Method: quicksort 
-   ->  Append (actual rows=522 loops=1)
-         ->  Sample Scan on _hyper_6_30_chunk (actual rows=35 loops=1)
-               Sampling: bernoulli ('5'::real) REPEATABLE ('0'::double precision)
-               Filter: ("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-         ->  Sample Scan on _hyper_6_29_chunk (actual rows=61 loops=1)
-               Sampling: bernoulli ('5'::real) REPEATABLE ('0'::double precision)
-               Filter: ("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-         ->  Sample Scan on _hyper_6_28_chunk (actual rows=61 loops=1)
-               Sampling: bernoulli ('5'::real) REPEATABLE ('0'::double precision)
-               Filter: ("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-         ->  Sample Scan on _hyper_6_27_chunk (actual rows=65 loops=1)
-               Sampling: bernoulli ('5'::real) REPEATABLE ('0'::double precision)
-               Filter: ("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               Rows Removed by Filter: 113
-         ->  Sample Scan on _hyper_6_26_chunk (actual rows=150 loops=1)
-               Sampling: bernoulli ('5'::real) REPEATABLE ('0'::double precision)
-               Filter: ("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               Rows Removed by Filter: 218
-         ->  Sample Scan on _hyper_6_25_chunk (actual rows=150 loops=1)
-               Sampling: bernoulli ('5'::real) REPEATABLE ('0'::double precision)
-               Filter: ("time" > 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               Rows Removed by Filter: 218
-(25 rows)
-
--- test runtime exclusion
--- test runtime exclusion with LATERAL and 2 hypertables
-:PREFIX SELECT m1.time, m2.time FROM metrics_timestamptz m1 LEFT JOIN LATERAL(SELECT time FROM metrics_timestamptz m2 WHERE m1.time = m2.time LIMIT 1) m2 ON true ORDER BY m1.time;
-                                                                   QUERY PLAN                                                                   
-------------------------------------------------------------------------------------------------------------------------------------------------
- Nested Loop Left Join (actual rows=26787 loops=1)
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1 (actual rows=26787 loops=1)
-         Order: m1."time"
-         ->  Index Only Scan Backward using _hyper_5_17_chunk_metrics_timestamptz_time_idx on _hyper_5_17_chunk m1_1 (actual rows=4032 loops=1)
-               Heap Fetches: 4032
-         ->  Index Only Scan Backward using _hyper_5_18_chunk_metrics_timestamptz_time_idx on _hyper_5_18_chunk m1_2 (actual rows=6048 loops=1)
-               Heap Fetches: 6048
-         ->  Index Only Scan Backward using _hyper_5_19_chunk_metrics_timestamptz_time_idx on _hyper_5_19_chunk m1_3 (actual rows=6048 loops=1)
-               Heap Fetches: 6048
-         ->  Index Only Scan Backward using _hyper_5_20_chunk_metrics_timestamptz_time_idx on _hyper_5_20_chunk m1_4 (actual rows=6048 loops=1)
-               Heap Fetches: 6048
-         ->  Index Only Scan Backward using _hyper_5_21_chunk_metrics_timestamptz_time_idx on _hyper_5_21_chunk m1_5 (actual rows=4611 loops=1)
-               Heap Fetches: 4611
-   ->  Limit (actual rows=1 loops=26787)
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz m2 (actual rows=1 loops=26787)
-               Chunks excluded during runtime: 4
-               ->  Index Only Scan using _hyper_5_17_chunk_metrics_timestamptz_time_idx on _hyper_5_17_chunk m2_1 (actual rows=1 loops=4032)
-                     Index Cond: ("time" = m1."time")
-                     Heap Fetches: 4032
-               ->  Index Only Scan using _hyper_5_18_chunk_metrics_timestamptz_time_idx on _hyper_5_18_chunk m2_2 (actual rows=1 loops=6048)
-                     Index Cond: ("time" = m1."time")
-                     Heap Fetches: 6048
-               ->  Index Only Scan using _hyper_5_19_chunk_metrics_timestamptz_time_idx on _hyper_5_19_chunk m2_3 (actual rows=1 loops=6048)
-                     Index Cond: ("time" = m1."time")
-                     Heap Fetches: 6048
-               ->  Index Only Scan using _hyper_5_20_chunk_metrics_timestamptz_time_idx on _hyper_5_20_chunk m2_4 (actual rows=1 loops=6048)
-                     Index Cond: ("time" = m1."time")
-                     Heap Fetches: 6048
-               ->  Index Only Scan using _hyper_5_21_chunk_metrics_timestamptz_time_idx on _hyper_5_21_chunk m2_5 (actual rows=1 loops=4611)
-                     Index Cond: ("time" = m1."time")
-                     Heap Fetches: 4611
-(31 rows)
-
--- test runtime exclusion and startup exclusions
-:PREFIX SELECT m1.time, m2.time FROM metrics_timestamptz m1 LEFT JOIN LATERAL(SELECT time FROM metrics_timestamptz m2 WHERE m1.time = m2.time AND m2.time < '2000-01-10'::text::timestamptz LIMIT 1) m2 ON true ORDER BY m1.time;
-                                                                   QUERY PLAN                                                                   
-------------------------------------------------------------------------------------------------------------------------------------------------
- Nested Loop Left Join (actual rows=26787 loops=1)
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1 (actual rows=26787 loops=1)
-         Order: m1."time"
-         ->  Index Only Scan Backward using _hyper_5_17_chunk_metrics_timestamptz_time_idx on _hyper_5_17_chunk m1_1 (actual rows=4032 loops=1)
-               Heap Fetches: 4032
-         ->  Index Only Scan Backward using _hyper_5_18_chunk_metrics_timestamptz_time_idx on _hyper_5_18_chunk m1_2 (actual rows=6048 loops=1)
-               Heap Fetches: 6048
-         ->  Index Only Scan Backward using _hyper_5_19_chunk_metrics_timestamptz_time_idx on _hyper_5_19_chunk m1_3 (actual rows=6048 loops=1)
-               Heap Fetches: 6048
-         ->  Index Only Scan Backward using _hyper_5_20_chunk_metrics_timestamptz_time_idx on _hyper_5_20_chunk m1_4 (actual rows=6048 loops=1)
-               Heap Fetches: 6048
-         ->  Index Only Scan Backward using _hyper_5_21_chunk_metrics_timestamptz_time_idx on _hyper_5_21_chunk m1_5 (actual rows=4611 loops=1)
-               Heap Fetches: 4611
-   ->  Limit (actual rows=0 loops=26787)
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz m2 (actual rows=0 loops=26787)
-               Chunks excluded during startup: 3
-               Chunks excluded during runtime: 1
-               ->  Index Only Scan using _hyper_5_17_chunk_metrics_timestamptz_time_idx on _hyper_5_17_chunk m2_1 (actual rows=1 loops=4032)
-                     Index Cond: (("time" < ('2000-01-10'::cstring)::timestamp with time zone) AND ("time" = m1."time"))
-                     Heap Fetches: 4032
-               ->  Index Only Scan using _hyper_5_18_chunk_metrics_timestamptz_time_idx on _hyper_5_18_chunk m2_2 (actual rows=1 loops=6048)
-                     Index Cond: (("time" < ('2000-01-10'::cstring)::timestamp with time zone) AND ("time" = m1."time"))
-                     Heap Fetches: 3744
-(23 rows)
-
--- test runtime exclusion does not activate for constraints on non-partitioning columns
--- should not use runtime exclusion
-:PREFIX SELECT * FROM append_test a LEFT JOIN LATERAL(SELECT * FROM join_test j WHERE a.colorid = j.colorid ORDER BY time DESC LIMIT 1) j ON true ORDER BY a.time LIMIT 1;
-                                                            QUERY PLAN                                                             
------------------------------------------------------------------------------------------------------------------------------------
- Limit (actual rows=1 loops=1)
-   ->  Nested Loop Left Join (actual rows=1 loops=1)
-         ->  Custom Scan (ChunkAppend) on append_test a (actual rows=1 loops=1)
-               Order: a."time"
-               ->  Index Scan Backward using _hyper_1_1_chunk_append_test_time_idx on _hyper_1_1_chunk a_1 (actual rows=1 loops=1)
-               ->  Index Scan Backward using _hyper_1_2_chunk_append_test_time_idx on _hyper_1_2_chunk a_2 (never executed)
-               ->  Index Scan Backward using _hyper_1_3_chunk_append_test_time_idx on _hyper_1_3_chunk a_3 (never executed)
-         ->  Limit (actual rows=1 loops=1)
-               ->  Custom Scan (ChunkAppend) on join_test j (actual rows=1 loops=1)
-                     Order: j."time" DESC
-                     Hypertables excluded during runtime: 0
-                     ->  Index Scan using _hyper_2_6_chunk_join_test_time_idx on _hyper_2_6_chunk j_1 (actual rows=0 loops=1)
-                           Filter: (a.colorid = colorid)
-                           Rows Removed by Filter: 1
-                     ->  Index Scan using _hyper_2_5_chunk_join_test_time_idx on _hyper_2_5_chunk j_2 (actual rows=0 loops=1)
-                           Filter: (a.colorid = colorid)
-                           Rows Removed by Filter: 1
-                     ->  Index Scan using _hyper_2_4_chunk_join_test_time_idx on _hyper_2_4_chunk j_3 (actual rows=1 loops=1)
-                           Filter: (a.colorid = colorid)
-(19 rows)
-
--- test runtime exclusion with LATERAL and generate_series
-:PREFIX SELECT g.time FROM generate_series('2000-01-01'::timestamptz, '2000-02-01'::timestamptz, '1d'::interval) g(time) LEFT JOIN LATERAL(SELECT time FROM metrics_timestamptz m WHERE m.time=g.time LIMIT 1) m ON true;
-                                                                  QUERY PLAN                                                                   
------------------------------------------------------------------------------------------------------------------------------------------------
- Nested Loop Left Join (actual rows=32 loops=1)
-   ->  Function Scan on generate_series g (actual rows=32 loops=1)
-   ->  Limit (actual rows=1 loops=32)
-         ->  Result (actual rows=1 loops=32)
-               ->  Custom Scan (ChunkAppend) on metrics_timestamptz m (actual rows=1 loops=32)
-                     Chunks excluded during runtime: 4
-                     ->  Index Only Scan using _hyper_5_17_chunk_metrics_timestamptz_time_idx on _hyper_5_17_chunk m_1 (actual rows=1 loops=5)
-                           Index Cond: ("time" = g."time")
-                           Heap Fetches: 5
-                     ->  Index Only Scan using _hyper_5_18_chunk_metrics_timestamptz_time_idx on _hyper_5_18_chunk m_2 (actual rows=1 loops=7)
-                           Index Cond: ("time" = g."time")
-                           Heap Fetches: 7
-                     ->  Index Only Scan using _hyper_5_19_chunk_metrics_timestamptz_time_idx on _hyper_5_19_chunk m_3 (actual rows=1 loops=7)
-                           Index Cond: ("time" = g."time")
-                           Heap Fetches: 7
-                     ->  Index Only Scan using _hyper_5_20_chunk_metrics_timestamptz_time_idx on _hyper_5_20_chunk m_4 (actual rows=1 loops=7)
-                           Index Cond: ("time" = g."time")
-                           Heap Fetches: 7
-                     ->  Index Only Scan using _hyper_5_21_chunk_metrics_timestamptz_time_idx on _hyper_5_21_chunk m_5 (actual rows=1 loops=6)
-                           Index Cond: ("time" = g."time")
-                           Heap Fetches: 6
-(21 rows)
-
-:PREFIX SELECT * FROM generate_series('2000-01-01'::timestamptz,'2000-02-01'::timestamptz,'1d'::interval) AS g(time) INNER JOIN LATERAL (SELECT time FROM metrics_timestamptz m WHERE time=g.time) m ON true;
-                                   QUERY PLAN                                   
---------------------------------------------------------------------------------
- Hash Join (actual rows=96 loops=1)
-   Hash Cond: (g."time" = m_1."time")
-   ->  Function Scan on generate_series g (actual rows=32 loops=1)
-   ->  Hash (actual rows=26787 loops=1)
-         Buckets: 32768  Batches: 1 
-         ->  Append (actual rows=26787 loops=1)
-               ->  Seq Scan on _hyper_5_17_chunk m_1 (actual rows=4032 loops=1)
-               ->  Seq Scan on _hyper_5_18_chunk m_2 (actual rows=6048 loops=1)
-               ->  Seq Scan on _hyper_5_19_chunk m_3 (actual rows=6048 loops=1)
-               ->  Seq Scan on _hyper_5_20_chunk m_4 (actual rows=6048 loops=1)
-               ->  Seq Scan on _hyper_5_21_chunk m_5 (actual rows=4611 loops=1)
-(11 rows)
-
-:PREFIX SELECT * FROM generate_series('2000-01-01'::timestamptz,'2000-02-01'::timestamptz,'1d'::interval) AS g(time) INNER JOIN LATERAL (SELECT time FROM metrics_timestamptz m WHERE time=g.time ORDER BY time) m ON true;
-                                                            QUERY PLAN                                                             
------------------------------------------------------------------------------------------------------------------------------------
- Nested Loop (actual rows=96 loops=1)
-   ->  Function Scan on generate_series g (actual rows=32 loops=1)
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m (actual rows=3 loops=32)
-         Chunks excluded during runtime: 4
-         ->  Index Only Scan using _hyper_5_17_chunk_metrics_timestamptz_time_idx on _hyper_5_17_chunk m_1 (actual rows=3 loops=5)
-               Index Cond: ("time" = g."time")
-               Heap Fetches: 15
-         ->  Index Only Scan using _hyper_5_18_chunk_metrics_timestamptz_time_idx on _hyper_5_18_chunk m_2 (actual rows=3 loops=7)
-               Index Cond: ("time" = g."time")
-               Heap Fetches: 21
-         ->  Index Only Scan using _hyper_5_19_chunk_metrics_timestamptz_time_idx on _hyper_5_19_chunk m_3 (actual rows=3 loops=7)
-               Index Cond: ("time" = g."time")
-               Heap Fetches: 21
-         ->  Index Only Scan using _hyper_5_20_chunk_metrics_timestamptz_time_idx on _hyper_5_20_chunk m_4 (actual rows=3 loops=7)
-               Index Cond: ("time" = g."time")
-               Heap Fetches: 21
-         ->  Index Only Scan using _hyper_5_21_chunk_metrics_timestamptz_time_idx on _hyper_5_21_chunk m_5 (actual rows=3 loops=6)
-               Index Cond: ("time" = g."time")
-               Heap Fetches: 18
-(19 rows)
-
-:PREFIX SELECT * FROM generate_series('2000-01-01'::timestamptz,'2000-02-01'::timestamptz,'1d'::interval) AS g(time) INNER JOIN LATERAL (SELECT time FROM metrics_timestamptz m WHERE time>g.time + '1 day' ORDER BY time LIMIT 1) m ON true;
-                                                                    QUERY PLAN                                                                    
---------------------------------------------------------------------------------------------------------------------------------------------------
- Nested Loop (actual rows=30 loops=1)
-   ->  Function Scan on generate_series g (actual rows=32 loops=1)
-   ->  Limit (actual rows=1 loops=32)
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz m (actual rows=1 loops=32)
-               Order: m."time"
-               Chunks excluded during startup: 0
-               Chunks excluded during runtime: 2
-               ->  Index Only Scan Backward using _hyper_5_17_chunk_metrics_timestamptz_time_idx on _hyper_5_17_chunk m_1 (actual rows=1 loops=4)
-                     Index Cond: ("time" > (g."time" + '@ 1 day'::interval))
-                     Heap Fetches: 4
-               ->  Index Only Scan Backward using _hyper_5_18_chunk_metrics_timestamptz_time_idx on _hyper_5_18_chunk m_2 (actual rows=1 loops=7)
-                     Index Cond: ("time" > (g."time" + '@ 1 day'::interval))
-                     Heap Fetches: 7
-               ->  Index Only Scan Backward using _hyper_5_19_chunk_metrics_timestamptz_time_idx on _hyper_5_19_chunk m_3 (actual rows=1 loops=7)
-                     Index Cond: ("time" > (g."time" + '@ 1 day'::interval))
-                     Heap Fetches: 7
-               ->  Index Only Scan Backward using _hyper_5_20_chunk_metrics_timestamptz_time_idx on _hyper_5_20_chunk m_4 (actual rows=1 loops=7)
-                     Index Cond: ("time" > (g."time" + '@ 1 day'::interval))
-                     Heap Fetches: 7
-               ->  Index Only Scan Backward using _hyper_5_21_chunk_metrics_timestamptz_time_idx on _hyper_5_21_chunk m_5 (actual rows=1 loops=7)
-                     Index Cond: ("time" > (g."time" + '@ 1 day'::interval))
-                     Heap Fetches: 5
-(22 rows)
-
--- test runtime exclusion with subquery
-:PREFIX SELECT m1.time FROM metrics_timestamptz m1 WHERE m1.time=(SELECT max(time) FROM metrics_timestamptz);
-                                                                  QUERY PLAN                                                                   
------------------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamptz m1 (actual rows=3 loops=1)
-   Chunks excluded during runtime: 4
-   InitPlan 2 (returns $1)
-     ->  Result (actual rows=1 loops=1)
-           InitPlan 1 (returns $0)
-             ->  Limit (actual rows=1 loops=1)
-                   ->  Custom Scan (ChunkAppend) on metrics_timestamptz (actual rows=1 loops=1)
-                         Order: metrics_timestamptz."time" DESC
-                         ->  Index Only Scan using _hyper_5_21_chunk_metrics_timestamptz_time_idx on _hyper_5_21_chunk (actual rows=1 loops=1)
-                               Index Cond: ("time" IS NOT NULL)
-                               Heap Fetches: 1
-                         ->  Index Only Scan using _hyper_5_20_chunk_metrics_timestamptz_time_idx on _hyper_5_20_chunk (never executed)
-                               Index Cond: ("time" IS NOT NULL)
-                               Heap Fetches: 0
-                         ->  Index Only Scan using _hyper_5_19_chunk_metrics_timestamptz_time_idx on _hyper_5_19_chunk (never executed)
-                               Index Cond: ("time" IS NOT NULL)
-                               Heap Fetches: 0
-                         ->  Index Only Scan using _hyper_5_18_chunk_metrics_timestamptz_time_idx on _hyper_5_18_chunk (never executed)
-                               Index Cond: ("time" IS NOT NULL)
-                               Heap Fetches: 0
-                         ->  Index Only Scan using _hyper_5_17_chunk_metrics_timestamptz_time_idx on _hyper_5_17_chunk (never executed)
-                               Index Cond: ("time" IS NOT NULL)
-                               Heap Fetches: 0
-   ->  Index Only Scan using _hyper_5_17_chunk_metrics_timestamptz_time_idx on _hyper_5_17_chunk m1_1 (never executed)
-         Index Cond: ("time" = $1)
-         Heap Fetches: 0
-   ->  Index Only Scan using _hyper_5_18_chunk_metrics_timestamptz_time_idx on _hyper_5_18_chunk m1_2 (never executed)
-         Index Cond: ("time" = $1)
-         Heap Fetches: 0
-   ->  Index Only Scan using _hyper_5_19_chunk_metrics_timestamptz_time_idx on _hyper_5_19_chunk m1_3 (never executed)
-         Index Cond: ("time" = $1)
-         Heap Fetches: 0
-   ->  Index Only Scan using _hyper_5_20_chunk_metrics_timestamptz_time_idx on _hyper_5_20_chunk m1_4 (never executed)
-         Index Cond: ("time" = $1)
-         Heap Fetches: 0
-   ->  Index Only Scan using _hyper_5_21_chunk_metrics_timestamptz_time_idx on _hyper_5_21_chunk m1_5 (actual rows=3 loops=1)
-         Index Cond: ("time" = $1)
-         Heap Fetches: 3
-(38 rows)
-
--- test runtime exclusion with correlated subquery
-:PREFIX SELECT m1.time, (SELECT m2.time FROM metrics_timestamptz m2 WHERE m2.time < m1.time ORDER BY m2.time DESC LIMIT 1) FROM metrics_timestamptz m1 WHERE m1.time < '2000-01-10' ORDER BY m1.time;
-                                                                   QUERY PLAN                                                                   
-------------------------------------------------------------------------------------------------------------------------------------------------
- Result (actual rows=7776 loops=1)
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1 (actual rows=7776 loops=1)
-         Order: m1."time"
-         ->  Index Only Scan Backward using _hyper_5_17_chunk_metrics_timestamptz_time_idx on _hyper_5_17_chunk m1_1 (actual rows=4032 loops=1)
-               Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               Heap Fetches: 4032
-         ->  Index Only Scan Backward using _hyper_5_18_chunk_metrics_timestamptz_time_idx on _hyper_5_18_chunk m1_2 (actual rows=3744 loops=1)
-               Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               Heap Fetches: 3744
-   SubPlan 1
-     ->  Limit (actual rows=1 loops=7776)
-           ->  Custom Scan (ChunkAppend) on metrics_timestamptz m2 (actual rows=1 loops=7776)
-                 Order: m2."time" DESC
-                 Chunks excluded during runtime: 3
-                 ->  Index Only Scan using _hyper_5_21_chunk_metrics_timestamptz_time_idx on _hyper_5_21_chunk m2_1 (never executed)
-                       Index Cond: ("time" < m1."time")
-                       Heap Fetches: 0
-                 ->  Index Only Scan using _hyper_5_20_chunk_metrics_timestamptz_time_idx on _hyper_5_20_chunk m2_2 (never executed)
-                       Index Cond: ("time" < m1."time")
-                       Heap Fetches: 0
-                 ->  Index Only Scan using _hyper_5_19_chunk_metrics_timestamptz_time_idx on _hyper_5_19_chunk m2_3 (never executed)
-                       Index Cond: ("time" < m1."time")
-                       Heap Fetches: 0
-                 ->  Index Only Scan using _hyper_5_18_chunk_metrics_timestamptz_time_idx on _hyper_5_18_chunk m2_4 (actual rows=1 loops=3741)
-                       Index Cond: ("time" < m1."time")
-                       Heap Fetches: 3741
-                 ->  Index Only Scan using _hyper_5_17_chunk_metrics_timestamptz_time_idx on _hyper_5_17_chunk m2_5 (actual rows=1 loops=4035)
-                       Index Cond: ("time" < m1."time")
-                       Heap Fetches: 4032
-(29 rows)
-
--- test EXISTS
-:PREFIX SELECT m1.time FROM metrics_timestamptz m1 WHERE EXISTS(SELECT 1 FROM metrics_timestamptz m2 WHERE m1.time < m2.time) ORDER BY m1.time DESC limit 1000;
-                                                                 QUERY PLAN                                                                  
----------------------------------------------------------------------------------------------------------------------------------------------
- Limit (actual rows=1000 loops=1)
-   ->  Nested Loop Semi Join (actual rows=1000 loops=1)
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1 (actual rows=1003 loops=1)
-               Order: m1."time" DESC
-               ->  Index Only Scan using _hyper_5_21_chunk_metrics_timestamptz_time_idx on _hyper_5_21_chunk m1_1 (actual rows=1003 loops=1)
-                     Heap Fetches: 1003
-               ->  Index Only Scan using _hyper_5_20_chunk_metrics_timestamptz_time_idx on _hyper_5_20_chunk m1_2 (never executed)
-                     Heap Fetches: 0
-               ->  Index Only Scan using _hyper_5_19_chunk_metrics_timestamptz_time_idx on _hyper_5_19_chunk m1_3 (never executed)
-                     Heap Fetches: 0
-               ->  Index Only Scan using _hyper_5_18_chunk_metrics_timestamptz_time_idx on _hyper_5_18_chunk m1_4 (never executed)
-                     Heap Fetches: 0
-               ->  Index Only Scan using _hyper_5_17_chunk_metrics_timestamptz_time_idx on _hyper_5_17_chunk m1_5 (never executed)
-                     Heap Fetches: 0
-         ->  Append (actual rows=1 loops=1003)
-               ->  Index Only Scan using _hyper_5_17_chunk_metrics_timestamptz_time_idx on _hyper_5_17_chunk m2_1 (actual rows=0 loops=1003)
-                     Index Cond: ("time" > m1."time")
-                     Heap Fetches: 0
-               ->  Index Only Scan using _hyper_5_18_chunk_metrics_timestamptz_time_idx on _hyper_5_18_chunk m2_2 (actual rows=0 loops=1003)
-                     Index Cond: ("time" > m1."time")
-                     Heap Fetches: 0
-               ->  Index Only Scan using _hyper_5_19_chunk_metrics_timestamptz_time_idx on _hyper_5_19_chunk m2_3 (actual rows=0 loops=1003)
-                     Index Cond: ("time" > m1."time")
-                     Heap Fetches: 0
-               ->  Index Only Scan using _hyper_5_20_chunk_metrics_timestamptz_time_idx on _hyper_5_20_chunk m2_4 (actual rows=0 loops=1003)
-                     Index Cond: ("time" > m1."time")
-                     Heap Fetches: 0
-               ->  Index Only Scan using _hyper_5_21_chunk_metrics_timestamptz_time_idx on _hyper_5_21_chunk m2_5 (actual rows=1 loops=1003)
-                     Index Cond: ("time" > m1."time")
-                     Heap Fetches: 1000
-(30 rows)
-
--- test constraint exclusion for subqueries with append
--- should include 2 chunks
-:PREFIX SELECT time FROM (SELECT time FROM metrics_timestamptz WHERE time < '2000-01-10'::text::timestamptz ORDER BY time) m;
-                                                             QUERY PLAN                                                              
--------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamptz (actual rows=7776 loops=1)
-   Order: metrics_timestamptz."time"
-   Chunks excluded during startup: 3
-   ->  Index Only Scan Backward using _hyper_5_17_chunk_metrics_timestamptz_time_idx on _hyper_5_17_chunk (actual rows=4032 loops=1)
-         Index Cond: ("time" < ('2000-01-10'::cstring)::timestamp with time zone)
-         Heap Fetches: 4032
-   ->  Index Only Scan Backward using _hyper_5_18_chunk_metrics_timestamptz_time_idx on _hyper_5_18_chunk (actual rows=3744 loops=1)
-         Index Cond: ("time" < ('2000-01-10'::cstring)::timestamp with time zone)
-         Heap Fetches: 3744
-(9 rows)
-
--- test constraint exclusion for subqueries with mergeappend
--- should include 2 chunks
-:PREFIX SELECT device_id, time FROM (SELECT device_id, time FROM metrics_timestamptz WHERE time < '2000-01-10'::text::timestamptz ORDER BY device_id, time) m;
-                                                                 QUERY PLAN                                                                 
---------------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ConstraintAwareAppend) (actual rows=7776 loops=1)
-   Hypertable: metrics_timestamptz
-   Chunks excluded during startup: 3
-   ->  Merge Append (actual rows=7776 loops=1)
-         Sort Key: _hyper_5_17_chunk.device_id, _hyper_5_17_chunk."time"
-         ->  Index Only Scan using _hyper_5_17_chunk_metrics_timestamptz_device_id_time_idx on _hyper_5_17_chunk (actual rows=4032 loops=1)
-               Index Cond: ("time" < ('2000-01-10'::cstring)::timestamp with time zone)
-               Heap Fetches: 4032
-         ->  Index Only Scan using _hyper_5_18_chunk_metrics_timestamptz_device_id_time_idx on _hyper_5_18_chunk (actual rows=3744 loops=1)
-               Index Cond: ("time" < ('2000-01-10'::cstring)::timestamp with time zone)
-               Heap Fetches: 3744
-(11 rows)
-
--- test LIMIT pushdown
--- no aggregates/window functions/SRF should pushdown limit
-:PREFIX SELECT FROM metrics_timestamptz ORDER BY time LIMIT 1;
-                                                               QUERY PLAN                                                               
-----------------------------------------------------------------------------------------------------------------------------------------
- Limit (actual rows=1 loops=1)
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz (actual rows=1 loops=1)
-         Order: metrics_timestamptz."time"
-         ->  Index Only Scan Backward using _hyper_5_17_chunk_metrics_timestamptz_time_idx on _hyper_5_17_chunk (actual rows=1 loops=1)
-               Heap Fetches: 1
-         ->  Index Only Scan Backward using _hyper_5_18_chunk_metrics_timestamptz_time_idx on _hyper_5_18_chunk (never executed)
-               Heap Fetches: 0
-         ->  Index Only Scan Backward using _hyper_5_19_chunk_metrics_timestamptz_time_idx on _hyper_5_19_chunk (never executed)
-               Heap Fetches: 0
-         ->  Index Only Scan Backward using _hyper_5_20_chunk_metrics_timestamptz_time_idx on _hyper_5_20_chunk (never executed)
-               Heap Fetches: 0
-         ->  Index Only Scan Backward using _hyper_5_21_chunk_metrics_timestamptz_time_idx on _hyper_5_21_chunk (never executed)
-               Heap Fetches: 0
-(13 rows)
-
--- aggregates should prevent pushdown
-:PREFIX SELECT count(*) FROM metrics_timestamptz LIMIT 1;
-                                 QUERY PLAN                                 
-----------------------------------------------------------------------------
- Limit (actual rows=1 loops=1)
-   ->  Aggregate (actual rows=1 loops=1)
-         ->  Append (actual rows=26787 loops=1)
-               ->  Seq Scan on _hyper_5_17_chunk (actual rows=4032 loops=1)
-               ->  Seq Scan on _hyper_5_18_chunk (actual rows=6048 loops=1)
-               ->  Seq Scan on _hyper_5_19_chunk (actual rows=6048 loops=1)
-               ->  Seq Scan on _hyper_5_20_chunk (actual rows=6048 loops=1)
-               ->  Seq Scan on _hyper_5_21_chunk (actual rows=4611 loops=1)
-(8 rows)
-
-:PREFIX SELECT count(*) FROM metrics_space LIMIT 1;
-                                 QUERY PLAN                                 
-----------------------------------------------------------------------------
- Limit (actual rows=1 loops=1)
-   ->  Aggregate (actual rows=1 loops=1)
-         ->  Append (actual rows=37450 loops=1)
-               ->  Seq Scan on _hyper_6_22_chunk (actual rows=5376 loops=1)
-               ->  Seq Scan on _hyper_6_23_chunk (actual rows=5376 loops=1)
-               ->  Seq Scan on _hyper_6_24_chunk (actual rows=2688 loops=1)
-               ->  Seq Scan on _hyper_6_25_chunk (actual rows=8064 loops=1)
-               ->  Seq Scan on _hyper_6_26_chunk (actual rows=8064 loops=1)
-               ->  Seq Scan on _hyper_6_27_chunk (actual rows=4032 loops=1)
-               ->  Seq Scan on _hyper_6_28_chunk (actual rows=1540 loops=1)
-               ->  Seq Scan on _hyper_6_29_chunk (actual rows=1540 loops=1)
-               ->  Seq Scan on _hyper_6_30_chunk (actual rows=770 loops=1)
-(12 rows)
-
--- HAVING should prevent pushdown
-:PREFIX SELECT 1 FROM metrics_timestamptz HAVING count(*) > 1 LIMIT 1;
-                                 QUERY PLAN                                 
-----------------------------------------------------------------------------
- Limit (actual rows=1 loops=1)
-   ->  Aggregate (actual rows=1 loops=1)
-         Filter: (count(*) > 1)
-         ->  Append (actual rows=26787 loops=1)
-               ->  Seq Scan on _hyper_5_17_chunk (actual rows=4032 loops=1)
-               ->  Seq Scan on _hyper_5_18_chunk (actual rows=6048 loops=1)
-               ->  Seq Scan on _hyper_5_19_chunk (actual rows=6048 loops=1)
-               ->  Seq Scan on _hyper_5_20_chunk (actual rows=6048 loops=1)
-               ->  Seq Scan on _hyper_5_21_chunk (actual rows=4611 loops=1)
-(9 rows)
-
-:PREFIX SELECT 1 FROM metrics_space HAVING count(*) > 1 LIMIT 1;
-                                 QUERY PLAN                                 
-----------------------------------------------------------------------------
- Limit (actual rows=1 loops=1)
-   ->  Aggregate (actual rows=1 loops=1)
-         Filter: (count(*) > 1)
-         ->  Append (actual rows=37450 loops=1)
-               ->  Seq Scan on _hyper_6_22_chunk (actual rows=5376 loops=1)
-               ->  Seq Scan on _hyper_6_23_chunk (actual rows=5376 loops=1)
-               ->  Seq Scan on _hyper_6_24_chunk (actual rows=2688 loops=1)
-               ->  Seq Scan on _hyper_6_25_chunk (actual rows=8064 loops=1)
-               ->  Seq Scan on _hyper_6_26_chunk (actual rows=8064 loops=1)
-               ->  Seq Scan on _hyper_6_27_chunk (actual rows=4032 loops=1)
-               ->  Seq Scan on _hyper_6_28_chunk (actual rows=1540 loops=1)
-               ->  Seq Scan on _hyper_6_29_chunk (actual rows=1540 loops=1)
-               ->  Seq Scan on _hyper_6_30_chunk (actual rows=770 loops=1)
-(13 rows)
-
--- DISTINCT should prevent pushdown
-SET enable_hashagg TO false;
-:PREFIX SELECT DISTINCT device_id FROM metrics_timestamptz ORDER BY device_id LIMIT 3;
-                                                                    QUERY PLAN                                                                    
---------------------------------------------------------------------------------------------------------------------------------------------------
- Limit (actual rows=3 loops=1)
-   ->  Unique (actual rows=3 loops=1)
-         ->  Merge Append (actual rows=17859 loops=1)
-               Sort Key: _hyper_5_17_chunk.device_id
-               ->  Index Only Scan using _hyper_5_17_chunk_metrics_timestamptz_device_id_time_idx on _hyper_5_17_chunk (actual rows=2689 loops=1)
-                     Heap Fetches: 2689
-               ->  Index Only Scan using _hyper_5_18_chunk_metrics_timestamptz_device_id_time_idx on _hyper_5_18_chunk (actual rows=4033 loops=1)
-                     Heap Fetches: 4033
-               ->  Index Only Scan using _hyper_5_19_chunk_metrics_timestamptz_device_id_time_idx on _hyper_5_19_chunk (actual rows=4033 loops=1)
-                     Heap Fetches: 4033
-               ->  Index Only Scan using _hyper_5_20_chunk_metrics_timestamptz_device_id_time_idx on _hyper_5_20_chunk (actual rows=4033 loops=1)
-                     Heap Fetches: 4033
-               ->  Index Only Scan using _hyper_5_21_chunk_metrics_timestamptz_device_id_time_idx on _hyper_5_21_chunk (actual rows=3075 loops=1)
-                     Heap Fetches: 3075
-(14 rows)
-
-:PREFIX SELECT DISTINCT device_id FROM metrics_space ORDER BY device_id LIMIT 3;
-                                                                 QUERY PLAN                                                                 
---------------------------------------------------------------------------------------------------------------------------------------------
- Limit (actual rows=3 loops=1)
-   ->  Unique (actual rows=3 loops=1)
-         ->  Merge Append (actual rows=7491 loops=1)
-               Sort Key: _hyper_6_22_chunk.device_id
-               ->  Index Only Scan using _hyper_6_22_chunk_metrics_space_device_id_time_idx on _hyper_6_22_chunk (actual rows=1345 loops=1)
-                     Heap Fetches: 1345
-               ->  Index Only Scan using _hyper_6_23_chunk_metrics_space_device_id_time_idx on _hyper_6_23_chunk (actual rows=1345 loops=1)
-                     Heap Fetches: 1345
-               ->  Index Only Scan using _hyper_6_24_chunk_metrics_space_device_id_time_idx on _hyper_6_24_chunk (actual rows=1 loops=1)
-                     Heap Fetches: 1
-               ->  Index Only Scan using _hyper_6_25_chunk_metrics_space_device_id_time_idx on _hyper_6_25_chunk (actual rows=2017 loops=1)
-                     Heap Fetches: 2017
-               ->  Index Only Scan using _hyper_6_26_chunk_metrics_space_device_id_time_idx on _hyper_6_26_chunk (actual rows=2017 loops=1)
-                     Heap Fetches: 2017
-               ->  Index Only Scan using _hyper_6_27_chunk_metrics_space_device_id_time_idx on _hyper_6_27_chunk (actual rows=1 loops=1)
-                     Heap Fetches: 1
-               ->  Index Only Scan using _hyper_6_28_chunk_metrics_space_device_id_time_idx on _hyper_6_28_chunk (actual rows=386 loops=1)
-                     Heap Fetches: 386
-               ->  Index Only Scan using _hyper_6_29_chunk_metrics_space_device_id_time_idx on _hyper_6_29_chunk (actual rows=386 loops=1)
-                     Heap Fetches: 386
-               ->  Index Only Scan using _hyper_6_30_chunk_metrics_space_device_id_time_idx on _hyper_6_30_chunk (actual rows=1 loops=1)
-                     Heap Fetches: 1
-(22 rows)
-
-RESET enable_hashagg;
--- JOINs should prevent pushdown
--- when LIMIT gets pushed to a Sort node it will switch to top-N heapsort
--- if more tuples then LIMIT are requested this will trigger an error
--- to trigger this we need a Sort node that is below ChunkAppend
-CREATE TABLE join_limit (time timestamptz, device_id int);
-SELECT table_name FROM create_hypertable('join_limit','time',create_default_indexes:=false);
-psql:include/append_query.sql:315: NOTICE:  adding not-null constraint to column "time"
- table_name 
-------------
- join_limit
-(1 row)
-
-CREATE INDEX ON join_limit(time,device_id);
-INSERT INTO join_limit
-SELECT time, device_id
-FROM generate_series('2000-01-01'::timestamptz,'2000-01-21','30m') g1(time),
-  generate_series(1,10,1) g2(device_id)
-ORDER BY time, device_id;
--- get 2nd chunk oid
-SELECT tableoid AS "CHUNK_OID" FROM join_limit WHERE time > '2000-01-07' ORDER BY time LIMIT 1
-\gset
---get index name for 2nd chunk
-SELECT indexrelid::regclass AS "INDEX_NAME" FROM pg_index WHERE indrelid = :CHUNK_OID
-\gset
-DROP INDEX :INDEX_NAME;
-:PREFIX SELECT * FROM metrics_timestamptz m1 INNER JOIN join_limit m2 ON m1.time = m2.time AND m1.device_id=m2.device_id WHERE m1.time > '2000-01-07' ORDER BY m1.time, m1.device_id LIMIT 3;
-                                                                     QUERY PLAN                                                                      
------------------------------------------------------------------------------------------------------------------------------------------------------
- Limit (actual rows=3 loops=1)
-   ->  Merge Join (actual rows=3 loops=1)
-         Merge Cond: (m2."time" = m1."time")
-         Join Filter: (m1.device_id = m2.device_id)
-         Rows Removed by Join Filter: 4
-         ->  Custom Scan (ChunkAppend) on join_limit m2 (actual rows=3 loops=1)
-               Order: m2."time", m2.device_id
-               ->  Sort (actual rows=3 loops=1)
-                     Sort Key: m2_1."time", m2_1.device_id
-                     Sort Method: quicksort 
-                     ->  Seq Scan on _hyper_8_35_chunk m2_1 (actual rows=2710 loops=1)
-                           Filter: ("time" > 'Fri Jan 07 00:00:00 2000 PST'::timestamp with time zone)
-                           Rows Removed by Filter: 650
-               ->  Index Only Scan using _hyper_8_36_chunk_join_limit_time_device_id_idx on _hyper_8_36_chunk m2_2 (never executed)
-                     Index Cond: ("time" > 'Fri Jan 07 00:00:00 2000 PST'::timestamp with time zone)
-                     Heap Fetches: 0
-               ->  Index Only Scan using _hyper_8_37_chunk_join_limit_time_device_id_idx on _hyper_8_37_chunk m2_3 (never executed)
-                     Index Cond: ("time" > 'Fri Jan 07 00:00:00 2000 PST'::timestamp with time zone)
-                     Heap Fetches: 0
-         ->  Materialize (actual rows=22 loops=1)
-               ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1 (actual rows=19 loops=1)
-                     Order: m1."time"
-                     ->  Index Scan Backward using _hyper_5_18_chunk_metrics_timestamptz_time_idx on _hyper_5_18_chunk m1_1 (actual rows=19 loops=1)
-                           Index Cond: ("time" > 'Fri Jan 07 00:00:00 2000 PST'::timestamp with time zone)
-                     ->  Index Scan Backward using _hyper_5_19_chunk_metrics_timestamptz_time_idx on _hyper_5_19_chunk m1_2 (never executed)
-                           Index Cond: ("time" > 'Fri Jan 07 00:00:00 2000 PST'::timestamp with time zone)
-                     ->  Index Scan Backward using _hyper_5_20_chunk_metrics_timestamptz_time_idx on _hyper_5_20_chunk m1_3 (never executed)
-                           Index Cond: ("time" > 'Fri Jan 07 00:00:00 2000 PST'::timestamp with time zone)
-                     ->  Index Scan Backward using _hyper_5_21_chunk_metrics_timestamptz_time_idx on _hyper_5_21_chunk m1_4 (never executed)
-                           Index Cond: ("time" > 'Fri Jan 07 00:00:00 2000 PST'::timestamp with time zone)
-(30 rows)
-
-DROP TABLE join_limit;
--- test ChunkAppend projection #2661
-:PREFIX SELECT ts.timestamp, ht.timestamp
-FROM (
-  SELECT generate_series(
-    to_timestamp(FLOOR(EXTRACT (EPOCH FROM '2020-01-01T00:01:00Z'::timestamp) / 300) * 300) AT TIME ZONE 'UTC',
-    '2020-01-01T01:00:00Z',
-    '5 minutes'::interval
-  ) AS timestamp
-) ts
-LEFT JOIN i2661 ht ON
-  (FLOOR(EXTRACT (EPOCH FROM ht."timestamp") / 300) * 300 = EXTRACT (EPOCH FROM ts.timestamp))
-  AND ht.timestamp > '2019-12-30T00:00:00Z'::timestamp;
-                                                                QUERY PLAN                                                                
-------------------------------------------------------------------------------------------------------------------------------------------
- Merge Left Join (actual rows=33 loops=1)
-   Merge Cond: ((EXTRACT(epoch FROM ts."timestamp")) = ((floor((EXTRACT(epoch FROM ht."timestamp") / '300'::numeric)) * '300'::numeric)))
-   ->  Sort (actual rows=13 loops=1)
-         Sort Key: (EXTRACT(epoch FROM ts."timestamp"))
-         Sort Method: quicksort 
-         ->  Subquery Scan on ts (actual rows=13 loops=1)
-               ->  ProjectSet (actual rows=13 loops=1)
-                     ->  Result (actual rows=1 loops=1)
-   ->  Sort (actual rows=514 loops=1)
-         Sort Key: ((floor((EXTRACT(epoch FROM ht."timestamp") / '300'::numeric)) * '300'::numeric))
-         Sort Method: quicksort 
-         ->  Result (actual rows=7201 loops=1)
-               ->  Custom Scan (ChunkAppend) on i2661 ht (actual rows=7201 loops=1)
-                     Chunks excluded during startup: 0
-                     ->  Seq Scan on _hyper_7_31_chunk ht_1 (actual rows=1200 loops=1)
-                           Filter: ("timestamp" > 'Mon Dec 30 00:00:00 2019'::timestamp without time zone)
-                     ->  Seq Scan on _hyper_7_32_chunk ht_2 (actual rows=5040 loops=1)
-                           Filter: ("timestamp" > 'Mon Dec 30 00:00:00 2019'::timestamp without time zone)
-                     ->  Seq Scan on _hyper_7_33_chunk ht_3 (actual rows=961 loops=1)
-                           Filter: ("timestamp" > 'Mon Dec 30 00:00:00 2019'::timestamp without time zone)
-(20 rows)
-
--- #3030 test chunkappend keeps pathkeys when subpath is append
--- on PG11 this will not use ChunkAppend but MergeAppend
-SET enable_seqscan TO FALSE;
-CREATE TABLE i3030(time timestamptz NOT NULL, a int, b int);
-SELECT table_name FROM create_hypertable('i3030', 'time', create_default_indexes=>false);
- table_name 
-------------
- i3030
-(1 row)
-
-CREATE INDEX ON i3030(a,time);
-INSERT INTO i3030 (time,a) SELECT time, a FROM generate_series('2000-01-01'::timestamptz,'2000-01-01 3:00:00'::timestamptz,'1min'::interval) time, generate_series(1,30) a;
-ANALYZE i3030;
-:PREFIX SELECT * FROM i3030 where time BETWEEN '2000-01-01'::text::timestamptz AND '2000-01-03'::text::timestamptz ORDER BY a,time LIMIT 1;
-                                                                         QUERY PLAN                                                                          
--------------------------------------------------------------------------------------------------------------------------------------------------------------
- Limit (actual rows=1 loops=1)
-   ->  Custom Scan (ChunkAppend) on i3030 (actual rows=1 loops=1)
-         Order: i3030.a, i3030."time"
-         Chunks excluded during startup: 0
-         ->  Index Scan using _hyper_9_38_chunk_i3030_a_time_idx on _hyper_9_38_chunk (actual rows=1 loops=1)
-               Index Cond: (("time" >= ('2000-01-01'::cstring)::timestamp with time zone) AND ("time" <= ('2000-01-03'::cstring)::timestamp with time zone))
-(6 rows)
-
-DROP TABLE i3030;
-RESET enable_seqscan;
---parent runtime exclusion tests:
---optimization works with ANY (array)
-:PREFIX
-SELECT *
-FROM append_test a
-WHERE a.attr @> ANY((SELECT coalesce(array_agg(attr), array[]::jsonb[]) FROM join_test_plain WHERE temp > 100)::jsonb[]);
-                             QUERY PLAN                             
---------------------------------------------------------------------
- Custom Scan (ChunkAppend) on append_test a (actual rows=0 loops=1)
-   Hypertables excluded during runtime: 1
-   InitPlan 1 (returns $0)
-     ->  Aggregate (actual rows=1 loops=1)
-           ->  Seq Scan on join_test_plain (actual rows=0 loops=1)
-                 Filter: (temp > '100'::double precision)
-                 Rows Removed by Filter: 3
-   ->  Seq Scan on _hyper_1_1_chunk a_1 (never executed)
-         Filter: (attr @> ANY ($0))
-   ->  Seq Scan on _hyper_1_2_chunk a_2 (never executed)
-         Filter: (attr @> ANY ($0))
-   ->  Seq Scan on _hyper_1_3_chunk a_3 (never executed)
-         Filter: (attr @> ANY ($0))
-(13 rows)
-
---optimization does not work for ANY subquery (does not force an initplan)
-:PREFIX
-SELECT *
-FROM append_test a
-WHERE a.attr @> ANY((SELECT attr FROM join_test_plain WHERE temp > 100));
-                              QUERY PLAN                              
-----------------------------------------------------------------------
- Nested Loop Semi Join (actual rows=0 loops=1)
-   Join Filter: (a_1.attr @> join_test_plain.attr)
-   ->  Append (actual rows=5 loops=1)
-         ->  Seq Scan on _hyper_1_1_chunk a_1 (actual rows=2 loops=1)
-         ->  Seq Scan on _hyper_1_2_chunk a_2 (actual rows=2 loops=1)
-         ->  Seq Scan on _hyper_1_3_chunk a_3 (actual rows=1 loops=1)
-   ->  Materialize (actual rows=0 loops=5)
-         ->  Seq Scan on join_test_plain (actual rows=0 loops=1)
-               Filter: (temp > '100'::double precision)
-               Rows Removed by Filter: 3
-(10 rows)
-
---works on any strict operator without ANY
-:PREFIX
-SELECT *
-FROM append_test a
-WHERE a.attr @> (SELECT attr FROM join_test_plain WHERE temp > 100 limit 1);
-                             QUERY PLAN                             
---------------------------------------------------------------------
- Custom Scan (ChunkAppend) on append_test a (actual rows=0 loops=1)
-   Hypertables excluded during runtime: 1
-   InitPlan 1 (returns $0)
-     ->  Limit (actual rows=0 loops=1)
-           ->  Seq Scan on join_test_plain (actual rows=0 loops=1)
-                 Filter: (temp > '100'::double precision)
-                 Rows Removed by Filter: 3
-   ->  Seq Scan on _hyper_1_1_chunk a_1 (never executed)
-         Filter: (attr @> $0)
-   ->  Seq Scan on _hyper_1_2_chunk a_2 (never executed)
-         Filter: (attr @> $0)
-   ->  Seq Scan on _hyper_1_3_chunk a_3 (never executed)
-         Filter: (attr @> $0)
-(13 rows)
-
---optimization works with function calls
-CREATE OR REPLACE FUNCTION select_tag(_min_temp int)
- RETURNS jsonb[]
- LANGUAGE sql
- STABLE PARALLEL SAFE
-AS $function$
-   SELECT coalesce(array_agg(attr), array[]::jsonb[])
-  FROM join_test_plain
-  WHERE temp > _min_temp
-$function$;
-:PREFIX
-SELECT *
-FROM append_test a
-WHERE a.attr @> ANY((SELECT select_tag(100))::jsonb[]);
-                             QUERY PLAN                             
---------------------------------------------------------------------
- Custom Scan (ChunkAppend) on append_test a (actual rows=0 loops=1)
-   Hypertables excluded during runtime: 1
-   InitPlan 1 (returns $0)
-     ->  Result (actual rows=1 loops=1)
-   ->  Seq Scan on _hyper_1_1_chunk a_1 (never executed)
-         Filter: (attr @> ANY ($0))
-   ->  Seq Scan on _hyper_1_2_chunk a_2 (never executed)
-         Filter: (attr @> ANY ($0))
-   ->  Seq Scan on _hyper_1_3_chunk a_3 (never executed)
-         Filter: (attr @> ANY ($0))
-(10 rows)
-
---optimization does not work when result is null
-:PREFIX
-SELECT *
-FROM append_test a
-WHERE a.attr @> ANY((SELECT array_agg(attr) FROM join_test_plain WHERE temp > 100)::jsonb[]);
-                             QUERY PLAN                             
---------------------------------------------------------------------
- Custom Scan (ChunkAppend) on append_test a (actual rows=0 loops=1)
-   Hypertables excluded during runtime: 0
-   InitPlan 1 (returns $0)
-     ->  Aggregate (actual rows=1 loops=1)
-           ->  Seq Scan on join_test_plain (actual rows=0 loops=1)
-                 Filter: (temp > '100'::double precision)
-                 Rows Removed by Filter: 3
-   ->  Seq Scan on _hyper_1_1_chunk a_1 (actual rows=0 loops=1)
-         Filter: (attr @> ANY ($0))
-         Rows Removed by Filter: 2
-   ->  Seq Scan on _hyper_1_2_chunk a_2 (actual rows=0 loops=1)
-         Filter: (attr @> ANY ($0))
-         Rows Removed by Filter: 2
-   ->  Seq Scan on _hyper_1_3_chunk a_3 (actual rows=0 loops=1)
-         Filter: (attr @> ANY ($0))
-         Rows Removed by Filter: 1
-(16 rows)
-
---generate the results into two different files
-\set ECHO errors
---- Unoptimized results
-+++ Optimized results
-@@ -1,6 +1,6 @@
-              setting              | value 
- ----------------------------------+-------
-- timescaledb.enable_optimizations | off
-+ timescaledb.enable_optimizations | on
-  timescaledb.enable_chunk_append  | on
- (2 rows)
- 
---- Unoptimized results
-+++ Optimized results
-@@ -1,7 +1,7 @@
-              setting              | value 
- ----------------------------------+-------
-- timescaledb.enable_optimizations | off
-- timescaledb.enable_chunk_append  | on
-+ timescaledb.enable_optimizations | on
-+ timescaledb.enable_chunk_append  | off
- (2 rows)
- 
-  time | temp | colorid | attr 
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/baserel_cache.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/baserel_cache.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/baserel_cache.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/baserel_cache.out	2023-11-25 05:27:33.769052841 +0000
@@ -1,28 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
--- Test that the baserel cache is not clobbered if there's an error
--- in a SQL function.
-CREATE TABLE valid_ids
-(
-  id UUID PRIMARY KEY
-);
-CREATE FUNCTION DEFAULT_UUID(TEXT DEFAULT '') RETURNS UUID AS $$
-  BEGIN
-    RETURN COALESCE($1, '')::UUID;
-  EXCEPTION WHEN invalid_text_representation THEN
-    RETURN '00000000-0000-0000-0000-000000000000';
-  END;
-$$ LANGUAGE PLPGSQL IMMUTABLE;
-CREATE FUNCTION KNOWN_ID(UUID, TEXT) RETURNS UUID AS $$
-  SELECT COALESCE(
-    (SELECT id FROM valid_ids WHERE id = $1),
-    DEFAULT_UUID()
-  );
-$$ LANGUAGE SQL;
-SELECT KNOWN_ID(NULL, ''), KNOWN_ID(NULL, '');
-               known_id               |               known_id               
---------------------------------------+--------------------------------------
- 00000000-0000-0000-0000-000000000000 | 00000000-0000-0000-0000-000000000000
-(1 row)
-
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/broken_tables.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/broken_tables.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/broken_tables.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/broken_tables.out	2023-11-25 05:27:33.765052852 +0000
@@ -1,128 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
--- Hypertables can break as a result of race conditions, but we should
--- still not crash when trying to truncate or delete the broken table.
-\c :TEST_DBNAME :ROLE_SUPERUSER
-CREATE VIEW missing_slices AS
-SELECT DISTINCT
-    dimension_slice_id,
-    constraint_name,
-    attname AS column_name,
-    pg_get_expr(conbin, conrelid) AS constraint_expr
-FROM
-    _timescaledb_catalog.chunk_constraint cc
-    JOIN _timescaledb_catalog.chunk ch ON cc.chunk_id = ch.id
-    JOIN pg_constraint ON conname = constraint_name
-    JOIN pg_namespace ns ON connamespace = ns.oid
-        AND ns.nspname = ch.schema_name
-    JOIN pg_attribute ON attnum = conkey[1]
-        AND attrelid = conrelid
-WHERE
-    dimension_slice_id NOT IN (SELECT id FROM _timescaledb_catalog.dimension_slice);
--- To drop rows from dimension_slice table, we need to remove some
--- constraints.
-ALTER TABLE _timescaledb_catalog.chunk_constraint
-      DROP CONSTRAINT chunk_constraint_dimension_slice_id_fkey;
-CREATE TABLE chunk_test_int(time integer, temp float8, tag integer, color integer);
-SELECT create_hypertable('chunk_test_int', 'time', 'tag', 2, chunk_time_interval => 3);
-NOTICE:  adding not-null constraint to column "time"
-      create_hypertable      
------------------------------
- (1,public,chunk_test_int,t)
-(1 row)
-
-INSERT INTO chunk_test_int VALUES
-       (4, 24.3, 1, 1),
-       (4, 24.3, 2, 1),
-       (10, 24.3, 2, 1);
-SELECT * FROM _timescaledb_catalog.dimension_slice ORDER BY id;
- id | dimension_id |     range_start      |      range_end      
-----+--------------+----------------------+---------------------
-  1 |            1 |                    3 |                   6
-  2 |            2 | -9223372036854775808 |          1073741823
-  3 |            2 |           1073741823 | 9223372036854775807
-  4 |            1 |                    9 |                  12
-(4 rows)
-
-SELECT DISTINCT
-       chunk_id,
-       dimension_slice_id,
-       constraint_name,
-       pg_get_expr(conbin, conrelid) AS constraint_expr
-FROM _timescaledb_catalog.chunk_constraint,
-     LATERAL (
-     	     SELECT *
-	     FROM pg_constraint JOIN pg_namespace ns ON connamespace = ns.oid
-	     WHERE conname = constraint_name
-     ) AS con
-ORDER BY chunk_id, dimension_slice_id;
- chunk_id | dimension_slice_id | constraint_name |                        constraint_expr                        
-----------+--------------------+-----------------+---------------------------------------------------------------
-        1 |                  1 | constraint_1    | (("time" >= 3) AND ("time" < 6))
-        1 |                  2 | constraint_2    | (_timescaledb_internal.get_partition_hash(tag) < 1073741823)
-        2 |                  1 | constraint_1    | (("time" >= 3) AND ("time" < 6))
-        2 |                  3 | constraint_3    | (_timescaledb_internal.get_partition_hash(tag) >= 1073741823)
-        3 |                  3 | constraint_3    | (_timescaledb_internal.get_partition_hash(tag) >= 1073741823)
-        3 |                  4 | constraint_4    | (("time" >= 9) AND ("time" < 12))
-(6 rows)
-
-DELETE FROM _timescaledb_catalog.dimension_slice WHERE id = 1;
-SELECT * FROM missing_slices;
- dimension_slice_id | constraint_name | column_name |         constraint_expr          
---------------------+-----------------+-------------+----------------------------------
-                  1 | constraint_1    | time        | (("time" >= 3) AND ("time" < 6))
-(1 row)
-
--- Setting level to ERROR since warnings are printed in different
--- order on PG11 and PG12.
-SET client_min_messages TO error;
-TRUNCATE TABLE chunk_test_int;
-DROP TABLE chunk_test_int;
-RESET client_min_messages;
-CREATE TABLE chunk_test_int(time integer, temp float8, tag integer, color integer);
-SELECT create_hypertable('chunk_test_int', 'time', 'tag', 2, chunk_time_interval => 3);
-NOTICE:  adding not-null constraint to column "time"
-      create_hypertable      
------------------------------
- (2,public,chunk_test_int,t)
-(1 row)
-
-INSERT INTO chunk_test_int VALUES
-       (4, 24.3, 1, 1),
-       (4, 24.3, 2, 1),
-       (10, 24.3, 2, 1);
-SELECT DISTINCT
-       chunk_id,
-       dimension_slice_id,
-       constraint_name,
-       pg_get_expr(conbin, conrelid) AS constraint_expr
-FROM _timescaledb_catalog.chunk_constraint,
-     LATERAL (
-     	     SELECT *
-	     FROM pg_constraint JOIN pg_namespace ns ON connamespace = ns.oid
-	     WHERE conname = constraint_name
-     ) AS con
-ORDER BY chunk_id, dimension_slice_id;
- chunk_id | dimension_slice_id | constraint_name |                        constraint_expr                        
-----------+--------------------+-----------------+---------------------------------------------------------------
-        4 |                  5 | constraint_5    | (("time" >= 3) AND ("time" < 6))
-        4 |                  6 | constraint_6    | (_timescaledb_internal.get_partition_hash(tag) < 1073741823)
-        5 |                  5 | constraint_5    | (("time" >= 3) AND ("time" < 6))
-        5 |                  7 | constraint_7    | (_timescaledb_internal.get_partition_hash(tag) >= 1073741823)
-        6 |                  7 | constraint_7    | (_timescaledb_internal.get_partition_hash(tag) >= 1073741823)
-        6 |                  8 | constraint_8    | (("time" >= 9) AND ("time" < 12))
-(6 rows)
-
-DELETE FROM _timescaledb_catalog.dimension_slice WHERE id = 5;
-SELECT * FROM missing_slices;
- dimension_slice_id | constraint_name | column_name |         constraint_expr          
---------------------+-----------------+-------------+----------------------------------
-                  5 | constraint_5    | time        | (("time" >= 3) AND ("time" < 6))
-(1 row)
-
--- Setting level to ERROR since warnings are printed in different
--- order on PG11 and PG12.
-SET client_min_messages TO error;
-DROP TABLE chunk_test_int;
-RESET client_min_messages;
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/chunk_adaptive.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/chunk_adaptive.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/chunk_adaptive.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/chunk_adaptive.out	2023-11-25 05:27:33.769052841 +0000
@@ -1,773 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
--- test error handling _timescaledb_internal.calculate_chunk_interval
-\set ON_ERROR_STOP 0
-SELECT _timescaledb_internal.calculate_chunk_interval(0,0,-0);
-ERROR:  could not find a matching hypertable for dimension 0
-SELECT _timescaledb_internal.calculate_chunk_interval(1,0,-1);
-ERROR:  chunk_target_size must be positive
-\set ON_ERROR_STOP 1
--- Valid chunk sizing function for testing
-CREATE OR REPLACE FUNCTION calculate_chunk_interval(
-        dimension_id INTEGER,
-        dimension_coord BIGINT,
-        chunk_target_size BIGINT
-)
-    RETURNS BIGINT LANGUAGE PLPGSQL AS
-$BODY$
-DECLARE
-BEGIN
-    RETURN -1;
-END
-$BODY$;
--- Chunk sizing function with bad signature
-CREATE OR REPLACE FUNCTION bad_calculate_chunk_interval(
-        dimension_id INTEGER
-)
-    RETURNS BIGINT LANGUAGE PLPGSQL AS
-$BODY$
-DECLARE
-BEGIN
-    RETURN -1;
-END
-$BODY$;
--- Set a fixed memory cache size to make tests determinstic
--- (independent of available machine memory)
-SELECT * FROM test.set_memory_cache_size('2GB');
- set_memory_cache_size 
------------------------
-            2147483648
-(1 row)
-
--- test NULL handling
-\set ON_ERROR_STOP 0
-SELECT * FROM set_adaptive_chunking(NULL,NULL);
-ERROR:  invalid hypertable: cannot be NULL
-\set ON_ERROR_STOP 1
-CREATE TABLE test_adaptive(time timestamptz, temp float, location int);
-\set ON_ERROR_STOP 0
--- Bad signature of sizing func should fail
-SELECT create_hypertable('test_adaptive', 'time',
-                         chunk_target_size => '1MB',
-                         chunk_sizing_func => 'bad_calculate_chunk_interval');
-ERROR:  invalid function signature
-\set ON_ERROR_STOP 1
--- Setting sizing func with correct signature should work
-SELECT create_hypertable('test_adaptive', 'time',
-                         chunk_target_size => '1MB',
-                         chunk_sizing_func => 'calculate_chunk_interval');
-WARNING:  target chunk size for adaptive chunking is less than 10 MB
-NOTICE:  adaptive chunking is a BETA feature and is not recommended for production deployments
-NOTICE:  adding not-null constraint to column "time"
-     create_hypertable      
-----------------------------
- (1,public,test_adaptive,t)
-(1 row)
-
-DROP TABLE test_adaptive;
-CREATE TABLE test_adaptive(time timestamptz, temp float, location int);
--- Size but no explicit func should use default func
-SELECT create_hypertable('test_adaptive', 'time',
-                         chunk_target_size => '1MB',
-                         create_default_indexes => true);
-WARNING:  target chunk size for adaptive chunking is less than 10 MB
-NOTICE:  adaptive chunking is a BETA feature and is not recommended for production deployments
-NOTICE:  adding not-null constraint to column "time"
-     create_hypertable      
-----------------------------
- (2,public,test_adaptive,t)
-(1 row)
-
-SELECT table_name, chunk_sizing_func_schema, chunk_sizing_func_name, chunk_target_size
-FROM _timescaledb_catalog.hypertable;
-  table_name   | chunk_sizing_func_schema |  chunk_sizing_func_name  | chunk_target_size 
----------------+--------------------------+--------------------------+-------------------
- test_adaptive | _timescaledb_internal    | calculate_chunk_interval |           1048576
-(1 row)
-
--- Check that adaptive chunking sets a 1 day default chunk time
--- interval => 86400000000 microseconds
-SELECT * FROM _timescaledb_catalog.dimension;
- id | hypertable_id | column_name |       column_type        | aligned | num_slices | partitioning_func_schema | partitioning_func | interval_length | compress_interval_length | integer_now_func_schema | integer_now_func 
-----+---------------+-------------+--------------------------+---------+------------+--------------------------+-------------------+-----------------+--------------------------+-------------------------+------------------
-  2 |             2 | time        | timestamp with time zone | t       |            |                          |                   |     86400000000 |                          |                         | 
-(1 row)
-
--- Change the target size
-SELECT * FROM set_adaptive_chunking('test_adaptive', '2MB');
-WARNING:  target chunk size for adaptive chunking is less than 10 MB
-               chunk_sizing_func                | chunk_target_size 
-------------------------------------------------+-------------------
- _timescaledb_internal.calculate_chunk_interval |           2097152
-(1 row)
-
-SELECT table_name, chunk_sizing_func_schema, chunk_sizing_func_name, chunk_target_size
-FROM _timescaledb_catalog.hypertable;
-  table_name   | chunk_sizing_func_schema |  chunk_sizing_func_name  | chunk_target_size 
----------------+--------------------------+--------------------------+-------------------
- test_adaptive | _timescaledb_internal    | calculate_chunk_interval |           2097152
-(1 row)
-
-\set ON_ERROR_STOP 0
--- Setting NULL func should fail
-SELECT * FROM set_adaptive_chunking('test_adaptive', '1MB', NULL);
-ERROR:  invalid chunk sizing function
-\set ON_ERROR_STOP 1
--- Setting NULL size disables adaptive chunking
-SELECT * FROM set_adaptive_chunking('test_adaptive', NULL);
-               chunk_sizing_func                | chunk_target_size 
-------------------------------------------------+-------------------
- _timescaledb_internal.calculate_chunk_interval |                 0
-(1 row)
-
-SELECT table_name, chunk_sizing_func_schema, chunk_sizing_func_name, chunk_target_size
-FROM _timescaledb_catalog.hypertable;
-  table_name   | chunk_sizing_func_schema |  chunk_sizing_func_name  | chunk_target_size 
----------------+--------------------------+--------------------------+-------------------
- test_adaptive | _timescaledb_internal    | calculate_chunk_interval |                 0
-(1 row)
-
-SELECT * FROM set_adaptive_chunking('test_adaptive', '1MB');
-WARNING:  target chunk size for adaptive chunking is less than 10 MB
-               chunk_sizing_func                | chunk_target_size 
-------------------------------------------------+-------------------
- _timescaledb_internal.calculate_chunk_interval |           1048576
-(1 row)
-
--- Setting size to 'off' should also disable
-SELECT * FROM set_adaptive_chunking('test_adaptive', 'off');
-               chunk_sizing_func                | chunk_target_size 
-------------------------------------------------+-------------------
- _timescaledb_internal.calculate_chunk_interval |                 0
-(1 row)
-
-SELECT table_name, chunk_sizing_func_schema, chunk_sizing_func_name, chunk_target_size
-FROM _timescaledb_catalog.hypertable;
-  table_name   | chunk_sizing_func_schema |  chunk_sizing_func_name  | chunk_target_size 
----------------+--------------------------+--------------------------+-------------------
- test_adaptive | _timescaledb_internal    | calculate_chunk_interval |                 0
-(1 row)
-
--- Setting 0 size should also disable
-SELECT * FROM set_adaptive_chunking('test_adaptive', '0MB');
-               chunk_sizing_func                | chunk_target_size 
-------------------------------------------------+-------------------
- _timescaledb_internal.calculate_chunk_interval |                 0
-(1 row)
-
-SELECT table_name, chunk_sizing_func_schema, chunk_sizing_func_name, chunk_target_size
-FROM _timescaledb_catalog.hypertable;
-  table_name   | chunk_sizing_func_schema |  chunk_sizing_func_name  | chunk_target_size 
----------------+--------------------------+--------------------------+-------------------
- test_adaptive | _timescaledb_internal    | calculate_chunk_interval |                 0
-(1 row)
-
-SELECT * FROM set_adaptive_chunking('test_adaptive', '1MB');
-WARNING:  target chunk size for adaptive chunking is less than 10 MB
-               chunk_sizing_func                | chunk_target_size 
-------------------------------------------------+-------------------
- _timescaledb_internal.calculate_chunk_interval |           1048576
-(1 row)
-
--- No warning about small target size if > 10MB
-SELECT * FROM set_adaptive_chunking('test_adaptive', '11MB');
-               chunk_sizing_func                | chunk_target_size 
-------------------------------------------------+-------------------
- _timescaledb_internal.calculate_chunk_interval |          11534336
-(1 row)
-
--- Setting size to 'estimate' should also estimate size
-SELECT * FROM set_adaptive_chunking('test_adaptive', 'estimate');
-               chunk_sizing_func                | chunk_target_size 
-------------------------------------------------+-------------------
- _timescaledb_internal.calculate_chunk_interval |        1932735283
-(1 row)
-
-SELECT table_name, chunk_sizing_func_schema, chunk_sizing_func_name, chunk_target_size
-FROM _timescaledb_catalog.hypertable;
-  table_name   | chunk_sizing_func_schema |  chunk_sizing_func_name  | chunk_target_size 
----------------+--------------------------+--------------------------+-------------------
- test_adaptive | _timescaledb_internal    | calculate_chunk_interval |        1932735283
-(1 row)
-
--- Use a lower memory setting to test that the calculated chunk_target_size is reduced
-SELECT * FROM test.set_memory_cache_size('512MB');
- set_memory_cache_size 
------------------------
-             536870912
-(1 row)
-
-SELECT * FROM set_adaptive_chunking('test_adaptive', 'estimate');
-               chunk_sizing_func                | chunk_target_size 
-------------------------------------------------+-------------------
- _timescaledb_internal.calculate_chunk_interval |         483183820
-(1 row)
-
-SELECT table_name, chunk_sizing_func_schema, chunk_sizing_func_name, chunk_target_size
-FROM _timescaledb_catalog.hypertable;
-  table_name   | chunk_sizing_func_schema |  chunk_sizing_func_name  | chunk_target_size 
----------------+--------------------------+--------------------------+-------------------
- test_adaptive | _timescaledb_internal    | calculate_chunk_interval |         483183820
-(1 row)
-
--- Reset memory settings
-SELECT * FROM test.set_memory_cache_size('2GB');
- set_memory_cache_size 
------------------------
-            2147483648
-(1 row)
-
--- Set a reasonable test value
-SELECT * FROM set_adaptive_chunking('test_adaptive', '1MB');
-WARNING:  target chunk size for adaptive chunking is less than 10 MB
-               chunk_sizing_func                | chunk_target_size 
-------------------------------------------------+-------------------
- _timescaledb_internal.calculate_chunk_interval |           1048576
-(1 row)
-
--- Show the interval length before and after adaptation
-SELECT id, hypertable_id, interval_length FROM _timescaledb_catalog.dimension;
- id | hypertable_id | interval_length 
-----+---------------+-----------------
-  2 |             2 |     86400000000
-(1 row)
-
--- Generate data to create chunks. We use the hash of the time value
--- to get determinstic location IDs so that we always spread these
--- values the same way across space partitions
-INSERT INTO test_adaptive
-SELECT time, random() * 35, _timescaledb_internal.get_partition_hash(time) FROM
-generate_series('2017-03-07T18:18:03+00'::timestamptz - interval '175 days',
-                '2017-03-07T18:18:03+00'::timestamptz,
-                '2 minutes') as time;
-SELECT chunk_name, primary_dimension, range_start, range_end
-FROM  timescaledb_information.chunks
-WHERE hypertable_name = 'test_adaptive' ORDER BY chunk_name;
-    chunk_name     | primary_dimension |             range_start             |              range_end              
--------------------+-------------------+-------------------------------------+-------------------------------------
- _hyper_2_10_chunk | time              | Fri Sep 23 22:08:15.728855 2016 PDT | Sat Oct 01 13:16:09.024252 2016 PDT
- _hyper_2_11_chunk | time              | Sat Oct 01 13:16:09.024252 2016 PDT | Fri Oct 14 03:19:44.231212 2016 PDT
- _hyper_2_12_chunk | time              | Fri Oct 14 03:19:44.231212 2016 PDT | Wed Oct 26 19:20:54.4938 2016 PDT
- _hyper_2_13_chunk | time              | Wed Oct 26 19:20:54.4938 2016 PDT   | Fri Nov 04 04:03:56.248528 2016 PDT
- _hyper_2_14_chunk | time              | Fri Nov 04 04:03:56.248528 2016 PDT | Fri Nov 18 21:58:20.411232 2016 PST
- _hyper_2_15_chunk | time              | Fri Nov 18 21:58:20.411232 2016 PST | Sat Dec 03 16:52:44.573936 2016 PST
- _hyper_2_16_chunk | time              | Sat Dec 03 16:52:44.573936 2016 PST | Sun Dec 18 11:47:08.73664 2016 PST
- _hyper_2_17_chunk | time              | Sun Dec 18 11:47:08.73664 2016 PST  | Mon Jan 02 06:41:32.899344 2017 PST
- _hyper_2_18_chunk | time              | Mon Jan 02 06:41:32.899344 2017 PST | Tue Jan 17 01:35:57.062048 2017 PST
- _hyper_2_19_chunk | time              | Tue Jan 17 01:35:57.062048 2017 PST | Tue Jan 31 20:30:21.224752 2017 PST
- _hyper_2_1_chunk  | time              | Mon Sep 12 17:00:00 2016 PDT        | Tue Sep 13 17:00:00 2016 PDT
- _hyper_2_20_chunk | time              | Tue Jan 31 20:30:21.224752 2017 PST | Wed Feb 15 15:24:45.387456 2017 PST
- _hyper_2_21_chunk | time              | Wed Feb 15 15:24:45.387456 2017 PST | Thu Mar 02 10:19:09.55016 2017 PST
- _hyper_2_22_chunk | time              | Thu Mar 02 10:19:09.55016 2017 PST  | Fri Mar 17 06:13:33.712864 2017 PDT
- _hyper_2_2_chunk  | time              | Tue Sep 13 17:00:00 2016 PDT        | Wed Sep 14 17:00:00 2016 PDT
- _hyper_2_3_chunk  | time              | Wed Sep 14 17:00:00 2016 PDT        | Thu Sep 15 17:00:00 2016 PDT
- _hyper_2_4_chunk  | time              | Thu Sep 15 17:00:00 2016 PDT        | Fri Sep 16 15:02:54.2208 2016 PDT
- _hyper_2_5_chunk  | time              | Fri Sep 16 15:02:54.2208 2016 PDT   | Sun Sep 18 03:12:14.342144 2016 PDT
- _hyper_2_6_chunk  | time              | Sun Sep 18 03:12:14.342144 2016 PDT | Mon Sep 19 15:21:34.463488 2016 PDT
- _hyper_2_7_chunk  | time              | Mon Sep 19 15:21:34.463488 2016 PDT | Wed Sep 21 03:30:54.584832 2016 PDT
- _hyper_2_8_chunk  | time              | Wed Sep 21 03:30:54.584832 2016 PDT | Thu Sep 22 03:45:14.901568 2016 PDT
- _hyper_2_9_chunk  | time              | Thu Sep 22 03:45:14.901568 2016 PDT | Fri Sep 23 22:08:15.728855 2016 PDT
-(22 rows)
-
--- Do same thing without an index on the time column. This affects
--- both the calculation of fill-factor of the chunk and its size
-CREATE TABLE test_adaptive_no_index(time timestamptz, temp float, location int);
--- Size but no explicit func should use default func
--- No default indexes should warn and use heap scan for min and max
-SELECT create_hypertable('test_adaptive_no_index', 'time',
-                         chunk_target_size => '1MB',
-                         create_default_indexes => false);
-WARNING:  target chunk size for adaptive chunking is less than 10 MB
-WARNING:  no index on "time" found for adaptive chunking on hypertable "test_adaptive_no_index"
-NOTICE:  adaptive chunking is a BETA feature and is not recommended for production deployments
-NOTICE:  adding not-null constraint to column "time"
-          create_hypertable          
--------------------------------------
- (3,public,test_adaptive_no_index,t)
-(1 row)
-
-SELECT id, hypertable_id, interval_length FROM _timescaledb_catalog.dimension;
- id | hypertable_id | interval_length 
-----+---------------+-----------------
-  2 |             2 |   1277664162704
-  3 |             3 |     86400000000
-(2 rows)
-
-INSERT INTO test_adaptive_no_index
-SELECT time, random() * 35, _timescaledb_internal.get_partition_hash(time) FROM
-generate_series('2017-03-07T18:18:03+00'::timestamptz - interval '175 days',
-                '2017-03-07T18:18:03+00'::timestamptz,
-                '2 minutes') as time;
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_23_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_23_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_24_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_23_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_24_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_25_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_24_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_25_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_26_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_25_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_26_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_27_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_26_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_27_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_28_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_27_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_28_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_29_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_28_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_29_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_30_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_29_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_30_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_31_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_30_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_31_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_32_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_31_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_32_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_33_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_32_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_33_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_34_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_33_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_34_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_35_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_34_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_35_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_36_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_35_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_36_chunk"
-WARNING:  no index on "time" found for adaptive chunking on chunk "_hyper_3_37_chunk"
-SELECT chunk_name, primary_dimension, range_start, range_end
-FROM  timescaledb_information.chunks
-WHERE hypertable_name = 'test_adaptive_no_index' ORDER BY chunk_name;
-    chunk_name     | primary_dimension |             range_start             |              range_end              
--------------------+-------------------+-------------------------------------+-------------------------------------
- _hyper_3_23_chunk | time              | Mon Sep 12 17:00:00 2016 PDT        | Tue Sep 13 17:00:00 2016 PDT
- _hyper_3_24_chunk | time              | Tue Sep 13 17:00:00 2016 PDT        | Wed Sep 14 17:00:00 2016 PDT
- _hyper_3_25_chunk | time              | Wed Sep 14 17:00:00 2016 PDT        | Thu Sep 15 17:00:00 2016 PDT
- _hyper_3_26_chunk | time              | Thu Sep 15 17:00:00 2016 PDT        | Sun Sep 18 02:18:45.310968 2016 PDT
- _hyper_3_27_chunk | time              | Sun Sep 18 02:18:45.310968 2016 PDT | Sun Sep 18 06:20:21.359312 2016 PDT
- _hyper_3_28_chunk | time              | Sun Sep 18 06:20:21.359312 2016 PDT | Wed Sep 21 08:25:00.957966 2016 PDT
- _hyper_3_29_chunk | time              | Wed Sep 21 08:25:00.957966 2016 PDT | Thu Sep 22 03:26:42.599807 2016 PDT
- _hyper_3_30_chunk | time              | Thu Sep 22 03:26:42.599807 2016 PDT | Sun Sep 25 18:03:30.59359 2016 PDT
- _hyper_3_31_chunk | time              | Sun Sep 25 18:03:30.59359 2016 PDT  | Sat Oct 08 05:32:02.75732 2016 PDT
- _hyper_3_32_chunk | time              | Sat Oct 08 05:32:02.75732 2016 PDT  | Mon Oct 31 07:33:42.652938 2016 PDT
- _hyper_3_33_chunk | time              | Mon Oct 31 07:33:42.652938 2016 PDT | Wed Nov 23 08:35:22.548556 2016 PST
- _hyper_3_34_chunk | time              | Wed Nov 23 08:35:22.548556 2016 PST | Thu Dec 15 09:48:28.1888 2016 PST
- _hyper_3_35_chunk | time              | Thu Dec 15 09:48:28.1888 2016 PST   | Wed Jan 11 04:57:38.357845 2017 PST
- _hyper_3_36_chunk | time              | Wed Jan 11 04:57:38.357845 2017 PST | Tue Feb 07 00:06:48.52689 2017 PST
- _hyper_3_37_chunk | time              | Tue Feb 07 00:06:48.52689 2017 PST  | Sun Mar 05 19:15:58.695935 2017 PST
- _hyper_3_38_chunk | time              | Sun Mar 05 19:15:58.695935 2017 PST | Sat Apr 01 15:25:08.86498 2017 PDT
-(16 rows)
-
--- Test added to check that the correct index (i.e. time index) is being used
--- to find the min and max. Previously a bug selected the first index listed,
--- which in this case is location rather than time and therefore could return
--- the wrong min and max if items at the start and end of the index did not have
--- the correct min and max timestamps.
---
--- In this test, we create chunks with a lot of locations with only one reading
--- that is at the beginning of the time frame, and then one location in the middle
--- of the range that has two readings, one that is the same as the others and one
--- that is larger. The algorithm should use these two readings for min & max; however,
--- if it's broken (as it was before), it would choose just the reading that is common
--- to all the locations.
-CREATE TABLE test_adaptive_correct_index(time timestamptz, temp float, location int);
-SELECT create_hypertable('test_adaptive_correct_index', 'time',
-                         chunk_target_size => '100MB',
-                         chunk_time_interval => 86400000000,
-                         create_default_indexes => false);
-WARNING:  no index on "time" found for adaptive chunking on hypertable "test_adaptive_correct_index"
-NOTICE:  adaptive chunking is a BETA feature and is not recommended for production deployments
-NOTICE:  adding not-null constraint to column "time"
-            create_hypertable             
-------------------------------------------
- (4,public,test_adaptive_correct_index,t)
-(1 row)
-
-CREATE INDEX ON test_adaptive_correct_index(location);
-CREATE INDEX ON test_adaptive_correct_index(time DESC);
--- First chunk
-INSERT INTO test_adaptive_correct_index
-SELECT '2018-01-01T00:00:00+00'::timestamptz, val, val + 1 FROM
-generate_series(1, 1000) as val;
-INSERT INTO test_adaptive_correct_index
-SELECT time, 0.0, '1500' FROM
-generate_series('2018-01-01T00:00:00+00'::timestamptz,
-                '2018-01-01T20:00:00+00'::timestamptz,
-                '10 hours') as time;
-INSERT INTO test_adaptive_correct_index
-SELECT '2018-01-01T00:00:00+00'::timestamptz, val, val + 1 FROM
-generate_series(2001, 3000) as val;
--- Second chunk
-INSERT INTO test_adaptive_correct_index
-SELECT '2018-01-02T00:00:00+00'::timestamptz, val, val + 1 FROM
-generate_series(1, 1000) as val;
-INSERT INTO test_adaptive_correct_index
-SELECT time, 0.0, '1500' FROM
-generate_series('2018-01-02T00:00:00+00'::timestamptz,
-                '2018-01-02T20:00:00+00'::timestamptz,
-                '10 hours') as time;
-INSERT INTO test_adaptive_correct_index
-SELECT '2018-01-02T00:00:00+00'::timestamptz, val, val + 1 FROM
-generate_series(2001, 3000) as val;
--- Third chunk
-INSERT INTO test_adaptive_correct_index
-SELECT '2018-01-03T00:00:00+00'::timestamptz, val, val + 1 FROM
-generate_series(1, 1000) as val;
-INSERT INTO test_adaptive_correct_index
-SELECT time, 0.0, '1500' FROM
-generate_series('2018-01-03T00:00:00+00'::timestamptz,
-                '2018-01-03T20:00:00+00'::timestamptz,
-                '10 hours') as time;
-INSERT INTO test_adaptive_correct_index
-SELECT '2018-01-03T00:00:00+00'::timestamptz, val, val + 1 FROM
-generate_series(2001, 3000) as val;
--- This should be the start of the fourth chunk
-INSERT INTO test_adaptive_correct_index
-SELECT '2018-01-04T00:00:00+00'::timestamptz, val, val + 1 FROM
-generate_series(1, 1000) as val;
-INSERT INTO test_adaptive_correct_index
-SELECT time, 0.0, '1500' FROM
-generate_series('2018-01-04T00:00:00+00'::timestamptz,
-                '2018-01-04T20:00:00+00'::timestamptz,
-                '10 hours') as time;
-INSERT INTO test_adaptive_correct_index
-SELECT '2018-01-04T00:00:00+00'::timestamptz, val, val + 1 FROM
-generate_series(2001, 3000) as val;
--- If working correctly, this goes in the 4th chunk, otherwise its a separate 5th chunk
-INSERT INTO test_adaptive_correct_index
-SELECT '2018-01-05T00:00:00+00'::timestamptz, val, val + 1 FROM
-generate_series(1, 1000) as val;
-INSERT INTO test_adaptive_correct_index
-SELECT time, 0.0, '1500' FROM
-generate_series('2018-01-05T00:00:00+00'::timestamptz,
-                '2018-01-05T20:00:00+00'::timestamptz,
-                '10 hours') as time;
-INSERT INTO test_adaptive_correct_index
-SELECT '2018-01-05T00:00:00+00'::timestamptz, val, val + 1 FROM
-generate_series(2001, 3000) as val;
--- This should show 4 chunks, rather than 5
-SELECT count(*)
-FROM  timescaledb_information.chunks
-WHERE hypertable_name = 'test_adaptive_correct_index';
- count 
--------
-     4
-(1 row)
-
--- The interval_length should no longer be 86400000000 for our hypertable, so 3rd column so be true.
--- Note: the exact interval_length is non-deterministic, so we can't use its actual value for tests
-SELECT id, hypertable_id, interval_length > 86400000000 FROM _timescaledb_catalog.dimension;
- id | hypertable_id | ?column? 
-----+---------------+----------
-  2 |             2 | t
-  3 |             3 | t
-  4 |             4 | t
-(3 rows)
-
--- Drop because it's size and estimated chunk_interval is non-deterministic so
--- we don't want to make other tests flaky.
-DROP TABLE test_adaptive_correct_index;
--- Test with space partitioning. This might affect the estimation
--- since there are more chunks in the same time interval and space
--- chunks might be unevenly filled.
-CREATE TABLE test_adaptive_space(time timestamptz, temp float, location int);
-SELECT create_hypertable('test_adaptive_space', 'time', 'location', 2,
-                         chunk_target_size => '1MB',
-                         create_default_indexes => true);
-WARNING:  target chunk size for adaptive chunking is less than 10 MB
-NOTICE:  adaptive chunking is a BETA feature and is not recommended for production deployments
-NOTICE:  adding not-null constraint to column "time"
-        create_hypertable         
-----------------------------------
- (5,public,test_adaptive_space,t)
-(1 row)
-
-SELECT id, hypertable_id, interval_length FROM _timescaledb_catalog.dimension;
- id | hypertable_id | interval_length 
-----+---------------+-----------------
-  2 |             2 |   1277664162704
-  3 |             3 |   2315350169045
-  5 |             5 |     86400000000
-  6 |             5 |                
-(4 rows)
-
-INSERT INTO test_adaptive_space
-SELECT time, random() * 35, _timescaledb_internal.get_partition_hash(time) FROM
-generate_series('2017-03-07T18:18:03+00'::timestamptz - interval '175 days',
-                '2017-03-07T18:18:03+00'::timestamptz,
-                '2 minutes') as time;
-\x
-SELECT chunk_name, range_start, range_end
-FROM  timescaledb_information.chunks
-WHERE hypertable_name = 'test_adaptive_space' ORDER BY chunk_name;
--[ RECORD 1 ]------------------------------------
-chunk_name  | _hyper_5_43_chunk
-range_start | Mon Sep 12 17:00:00 2016 PDT
-range_end   | Tue Sep 13 17:00:00 2016 PDT
--[ RECORD 2 ]------------------------------------
-chunk_name  | _hyper_5_44_chunk
-range_start | Mon Sep 12 17:00:00 2016 PDT
-range_end   | Tue Sep 13 17:00:00 2016 PDT
--[ RECORD 3 ]------------------------------------
-chunk_name  | _hyper_5_45_chunk
-range_start | Tue Sep 13 17:00:00 2016 PDT
-range_end   | Wed Sep 14 17:00:00 2016 PDT
--[ RECORD 4 ]------------------------------------
-chunk_name  | _hyper_5_46_chunk
-range_start | Tue Sep 13 17:00:00 2016 PDT
-range_end   | Wed Sep 14 17:00:00 2016 PDT
--[ RECORD 5 ]------------------------------------
-chunk_name  | _hyper_5_47_chunk
-range_start | Wed Sep 14 17:00:00 2016 PDT
-range_end   | Thu Sep 15 11:47:51.47376 2016 PDT
--[ RECORD 6 ]------------------------------------
-chunk_name  | _hyper_5_48_chunk
-range_start | Wed Sep 14 17:00:00 2016 PDT
-range_end   | Thu Sep 15 11:47:51.47376 2016 PDT
--[ RECORD 7 ]------------------------------------
-chunk_name  | _hyper_5_49_chunk
-range_start | Thu Sep 15 11:47:51.47376 2016 PDT
-range_end   | Sat Sep 17 02:40:49.182352 2016 PDT
--[ RECORD 8 ]------------------------------------
-chunk_name  | _hyper_5_50_chunk
-range_start | Thu Sep 15 11:47:51.47376 2016 PDT
-range_end   | Sat Sep 17 02:40:49.182352 2016 PDT
--[ RECORD 9 ]------------------------------------
-chunk_name  | _hyper_5_51_chunk
-range_start | Sat Sep 17 02:40:49.182352 2016 PDT
-range_end   | Sun Sep 18 17:33:46.890944 2016 PDT
--[ RECORD 10 ]-----------------------------------
-chunk_name  | _hyper_5_52_chunk
-range_start | Sat Sep 17 02:40:49.182352 2016 PDT
-range_end   | Sun Sep 18 17:33:46.890944 2016 PDT
--[ RECORD 11 ]-----------------------------------
-chunk_name  | _hyper_5_53_chunk
-range_start | Sun Sep 18 17:33:46.890944 2016 PDT
-range_end   | Sun Sep 18 20:35:55.67676 2016 PDT
--[ RECORD 12 ]-----------------------------------
-chunk_name  | _hyper_5_54_chunk
-range_start | Sun Sep 18 17:33:46.890944 2016 PDT
-range_end   | Sun Sep 18 20:35:55.67676 2016 PDT
--[ RECORD 13 ]-----------------------------------
-chunk_name  | _hyper_5_55_chunk
-range_start | Sun Sep 18 20:35:55.67676 2016 PDT
-range_end   | Tue Sep 20 18:46:40.16883 2016 PDT
--[ RECORD 14 ]-----------------------------------
-chunk_name  | _hyper_5_56_chunk
-range_start | Sun Sep 18 20:35:55.67676 2016 PDT
-range_end   | Tue Sep 20 18:46:40.16883 2016 PDT
--[ RECORD 15 ]-----------------------------------
-chunk_name  | _hyper_5_57_chunk
-range_start | Tue Sep 20 18:46:40.16883 2016 PDT
-range_end   | Sun Oct 02 16:44:29.071032 2016 PDT
--[ RECORD 16 ]-----------------------------------
-chunk_name  | _hyper_5_58_chunk
-range_start | Tue Sep 20 18:46:40.16883 2016 PDT
-range_end   | Sun Oct 02 16:44:29.071032 2016 PDT
--[ RECORD 17 ]-----------------------------------
-chunk_name  | _hyper_5_59_chunk
-range_start | Sun Oct 02 16:44:29.071032 2016 PDT
-range_end   | Tue Oct 11 00:37:03.738979 2016 PDT
--[ RECORD 18 ]-----------------------------------
-chunk_name  | _hyper_5_60_chunk
-range_start | Sun Oct 02 16:44:29.071032 2016 PDT
-range_end   | Tue Oct 11 00:37:03.738979 2016 PDT
--[ RECORD 19 ]-----------------------------------
-chunk_name  | _hyper_5_61_chunk
-range_start | Tue Oct 11 00:37:03.738979 2016 PDT
-range_end   | Thu Oct 27 03:05:25.740618 2016 PDT
--[ RECORD 20 ]-----------------------------------
-chunk_name  | _hyper_5_62_chunk
-range_start | Tue Oct 11 00:37:03.738979 2016 PDT
-range_end   | Thu Oct 27 03:05:25.740618 2016 PDT
--[ RECORD 21 ]-----------------------------------
-chunk_name  | _hyper_5_63_chunk
-range_start | Thu Oct 27 03:05:25.740618 2016 PDT
-range_end   | Sun Nov 13 12:38:49.541703 2016 PST
--[ RECORD 22 ]-----------------------------------
-chunk_name  | _hyper_5_64_chunk
-range_start | Thu Oct 27 03:05:25.740618 2016 PDT
-range_end   | Sun Nov 13 12:38:49.541703 2016 PST
--[ RECORD 23 ]-----------------------------------
-chunk_name  | _hyper_5_65_chunk
-range_start | Sun Nov 13 12:38:49.541703 2016 PST
-range_end   | Fri Dec 02 17:45:40.237036 2016 PST
--[ RECORD 24 ]-----------------------------------
-chunk_name  | _hyper_5_66_chunk
-range_start | Sun Nov 13 12:38:49.541703 2016 PST
-range_end   | Fri Dec 02 17:45:40.237036 2016 PST
--[ RECORD 25 ]-----------------------------------
-chunk_name  | _hyper_5_67_chunk
-range_start | Fri Dec 02 17:45:40.237036 2016 PST
-range_end   | Wed Dec 21 22:52:30.932369 2016 PST
--[ RECORD 26 ]-----------------------------------
-chunk_name  | _hyper_5_68_chunk
-range_start | Fri Dec 02 17:45:40.237036 2016 PST
-range_end   | Wed Dec 21 22:52:30.932369 2016 PST
--[ RECORD 27 ]-----------------------------------
-chunk_name  | _hyper_5_69_chunk
-range_start | Wed Dec 21 22:52:30.932369 2016 PST
-range_end   | Tue Jan 10 03:59:21.627702 2017 PST
--[ RECORD 28 ]-----------------------------------
-chunk_name  | _hyper_5_70_chunk
-range_start | Wed Dec 21 22:52:30.932369 2016 PST
-range_end   | Tue Jan 10 03:59:21.627702 2017 PST
--[ RECORD 29 ]-----------------------------------
-chunk_name  | _hyper_5_71_chunk
-range_start | Tue Jan 10 03:59:21.627702 2017 PST
-range_end   | Sun Jan 29 09:06:12.323035 2017 PST
--[ RECORD 30 ]-----------------------------------
-chunk_name  | _hyper_5_72_chunk
-range_start | Tue Jan 10 03:59:21.627702 2017 PST
-range_end   | Sun Jan 29 09:06:12.323035 2017 PST
--[ RECORD 31 ]-----------------------------------
-chunk_name  | _hyper_5_73_chunk
-range_start | Sun Jan 29 09:06:12.323035 2017 PST
-range_end   | Fri Feb 17 14:13:03.018368 2017 PST
--[ RECORD 32 ]-----------------------------------
-chunk_name  | _hyper_5_74_chunk
-range_start | Sun Jan 29 09:06:12.323035 2017 PST
-range_end   | Fri Feb 17 14:13:03.018368 2017 PST
--[ RECORD 33 ]-----------------------------------
-chunk_name  | _hyper_5_75_chunk
-range_start | Fri Feb 17 14:13:03.018368 2017 PST
-range_end   | Wed Mar 08 19:19:53.713701 2017 PST
--[ RECORD 34 ]-----------------------------------
-chunk_name  | _hyper_5_76_chunk
-range_start | Fri Feb 17 14:13:03.018368 2017 PST
-range_end   | Wed Mar 08 19:19:53.713701 2017 PST
-
-SELECT *
-FROM  timescaledb_information.dimensions
-WHERE hypertable_name = 'test_adaptive_space' ORDER BY dimension_number;
--[ RECORD 1 ]-----+----------------------------------------
-hypertable_schema | public
-hypertable_name   | test_adaptive_space
-dimension_number  | 1
-column_name       | time
-column_type       | timestamp with time zone
-dimension_type    | Time
-time_interval     | @ 19 days 5 hours 6 mins 50.695333 secs
-integer_interval  | 
-integer_now_func  | 
-num_partitions    | 
--[ RECORD 2 ]-----+----------------------------------------
-hypertable_schema | public
-hypertable_name   | test_adaptive_space
-dimension_number  | 2
-column_name       | location
-column_type       | integer
-dimension_type    | Space
-time_interval     | 
-integer_interval  | 
-integer_now_func  | 
-num_partitions    | 2
-
-\x
-SELECT *
-FROM chunks_detailed_size('test_adaptive_space') ORDER BY chunk_name;
-     chunk_schema      |    chunk_name     | table_bytes | index_bytes | toast_bytes | total_bytes | node_name 
------------------------+-------------------+-------------+-------------+-------------+-------------+-----------
- _timescaledb_internal | _hyper_5_43_chunk |        8192 |       32768 |           0 |       40960 | 
- _timescaledb_internal | _hyper_5_44_chunk |        8192 |       32768 |           0 |       40960 | 
- _timescaledb_internal | _hyper_5_45_chunk |       49152 |       57344 |           0 |      106496 | 
- _timescaledb_internal | _hyper_5_46_chunk |       49152 |       57344 |           0 |      106496 | 
- _timescaledb_internal | _hyper_5_47_chunk |       40960 |       49152 |           0 |       90112 | 
- _timescaledb_internal | _hyper_5_48_chunk |       40960 |       32768 |           0 |       73728 | 
- _timescaledb_internal | _hyper_5_49_chunk |       57344 |       81920 |           0 |      139264 | 
- _timescaledb_internal | _hyper_5_50_chunk |       57344 |       81920 |           0 |      139264 | 
- _timescaledb_internal | _hyper_5_51_chunk |       57344 |       81920 |           0 |      139264 | 
- _timescaledb_internal | _hyper_5_52_chunk |       57344 |       81920 |           0 |      139264 | 
- _timescaledb_internal | _hyper_5_53_chunk |        8192 |       32768 |           0 |       40960 | 
- _timescaledb_internal | _hyper_5_54_chunk |        8192 |       32768 |           0 |       40960 | 
- _timescaledb_internal | _hyper_5_55_chunk |       65536 |      106496 |           0 |      172032 | 
- _timescaledb_internal | _hyper_5_56_chunk |       65536 |       98304 |           0 |      163840 | 
- _timescaledb_internal | _hyper_5_57_chunk |      253952 |      360448 |           0 |      614400 | 
- _timescaledb_internal | _hyper_5_58_chunk |      253952 |      368640 |           0 |      622592 | 
- _timescaledb_internal | _hyper_5_59_chunk |      180224 |      303104 |           0 |      483328 | 
- _timescaledb_internal | _hyper_5_60_chunk |      188416 |      303104 |           0 |      491520 | 
- _timescaledb_internal | _hyper_5_61_chunk |      327680 |      540672 |           0 |      868352 | 
- _timescaledb_internal | _hyper_5_62_chunk |      327680 |      532480 |           0 |      860160 | 
- _timescaledb_internal | _hyper_5_63_chunk |      360448 |      581632 |           0 |      942080 | 
- _timescaledb_internal | _hyper_5_64_chunk |      352256 |      589824 |           0 |      942080 | 
- _timescaledb_internal | _hyper_5_65_chunk |      385024 |      598016 |           0 |      983040 | 
- _timescaledb_internal | _hyper_5_66_chunk |      393216 |      614400 |           0 |     1007616 | 
- _timescaledb_internal | _hyper_5_67_chunk |      385024 |      598016 |           0 |      983040 | 
- _timescaledb_internal | _hyper_5_68_chunk |      393216 |      598016 |           0 |      991232 | 
- _timescaledb_internal | _hyper_5_69_chunk |      393216 |      622592 |           0 |     1015808 | 
- _timescaledb_internal | _hyper_5_70_chunk |      385024 |      606208 |           0 |      991232 | 
- _timescaledb_internal | _hyper_5_71_chunk |      385024 |      614400 |           0 |      999424 | 
- _timescaledb_internal | _hyper_5_72_chunk |      393216 |      622592 |           0 |     1015808 | 
- _timescaledb_internal | _hyper_5_73_chunk |      393216 |      614400 |           0 |     1007616 | 
- _timescaledb_internal | _hyper_5_74_chunk |      385024 |      614400 |           0 |      999424 | 
- _timescaledb_internal | _hyper_5_75_chunk |      360448 |      581632 |           0 |      942080 | 
- _timescaledb_internal | _hyper_5_76_chunk |      368640 |      598016 |           0 |      966656 | 
-(34 rows)
-
-SELECT id, hypertable_id, interval_length FROM _timescaledb_catalog.dimension;
- id | hypertable_id | interval_length 
-----+---------------+-----------------
-  2 |             2 |   1277664162704
-  3 |             3 |   2315350169045
-  6 |             5 |                
-  5 |             5 |   1660010695333
-(4 rows)
-
--- A previous version stopped working as soon as hypertable_id stopped being
--- equal to dimension_id (i.e., there was a hypertable with more than 1 dimension).
--- This test comes after test_adaptive_space, which has 2 dimensions, and makes
--- sure that it still works.
-CREATE TABLE test_adaptive_after_multiple_dims(time timestamptz, temp float, location int);
-SELECT create_hypertable('test_adaptive_after_multiple_dims', 'time',
-                         chunk_target_size => '100MB',
-                         create_default_indexes => true);
-NOTICE:  adaptive chunking is a BETA feature and is not recommended for production deployments
-NOTICE:  adding not-null constraint to column "time"
-               create_hypertable                
-------------------------------------------------
- (6,public,test_adaptive_after_multiple_dims,t)
-(1 row)
-
-INSERT INTO test_adaptive_after_multiple_dims VALUES('2018-01-01T00:00:00+00'::timestamptz, 0.0, 5);
-\c  :TEST_DBNAME :ROLE_DEFAULT_PERM_USER_2
-\set ON_ERROR_STOP 0
-SELECT * FROM set_adaptive_chunking('test_adaptive', '2MB');
-ERROR:  must be owner of hypertable "test_adaptive"
-\set ON_ERROR_STOP 1
-\c  :TEST_DBNAME :ROLE_DEFAULT_PERM_USER
--- Now make sure renaming schema gets propagated to the func_schema
-DROP TABLE test_adaptive;
-\c :TEST_DBNAME :ROLE_SUPERUSER
-CREATE SCHEMA IF NOT EXISTS my_chunk_func_schema;
-CREATE OR REPLACE FUNCTION my_chunk_func_schema.calculate_chunk_interval(
-        dimension_id INTEGER,
-        dimension_coord BIGINT,
-        chunk_target_size BIGINT
-)
-    RETURNS BIGINT LANGUAGE PLPGSQL AS
-$BODY$
-DECLARE
-BEGIN
-    RETURN 2;
-END
-$BODY$;
-CREATE TABLE test_adaptive(time timestamptz, temp float, location int);
-SELECT create_hypertable('test_adaptive', 'time',
-                         chunk_target_size => '1MB',
-                         chunk_sizing_func => 'my_chunk_func_schema.calculate_chunk_interval');
-WARNING:  target chunk size for adaptive chunking is less than 10 MB
-NOTICE:  adaptive chunking is a BETA feature and is not recommended for production deployments
-NOTICE:  adding not-null constraint to column "time"
-     create_hypertable      
-----------------------------
- (7,public,test_adaptive,t)
-(1 row)
-
-ALTER SCHEMA my_chunk_func_schema RENAME TO new_chunk_func_schema;
-INSERT INTO test_adaptive VALUES (now(), 1.0, 1);
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/chunks.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/chunks.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/chunks.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/chunks.out	2023-11-25 05:27:33.765052852 +0000
@@ -1,4 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-\unset ECHO
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/cluster-15.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/cluster-15.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/cluster-15.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/cluster-15.out	2023-11-25 05:27:33.773052829 +0000
@@ -1,172 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-CREATE TABLE cluster_test(time timestamptz, temp float, location int);
-SELECT create_hypertable('cluster_test', 'time', chunk_time_interval => interval '1 day');
-NOTICE:  adding not-null constraint to column "time"
-     create_hypertable     
----------------------------
- (1,public,cluster_test,t)
-(1 row)
-
--- Show default indexes
-SELECT * FROM test.show_indexes('cluster_test');
-         Index         | Columns | Expr | Unique | Primary | Exclusion | Tablespace 
------------------------+---------+------+--------+---------+-----------+------------
- cluster_test_time_idx | {time}  |      | f      | f       | f         | 
-(1 row)
-
--- Create two chunks
-INSERT INTO cluster_test VALUES ('2017-01-20T09:00:01', 23.4, 1),
-       ('2017-01-21T09:00:01', 21.3, 2);
--- Run cluster
-CLUSTER VERBOSE cluster_test USING cluster_test_time_idx;
-INFO:  clustering "_timescaledb_internal._hyper_1_1_chunk" using index scan on "_hyper_1_1_chunk_cluster_test_time_idx"
-INFO:  "_timescaledb_internal._hyper_1_1_chunk": found 0 removable, 1 nonremovable row versions in 1 pages
-INFO:  clustering "_timescaledb_internal._hyper_1_2_chunk" using index scan on "_hyper_1_2_chunk_cluster_test_time_idx"
-INFO:  "_timescaledb_internal._hyper_1_2_chunk": found 0 removable, 1 nonremovable row versions in 1 pages
--- Create a third chunk
-INSERT INTO cluster_test VALUES ('2017-01-22T09:00:01', 19.5, 3);
--- Show clustered indexes
-SELECT indexrelid::regclass, indisclustered
-FROM pg_index
-WHERE indisclustered = true ORDER BY 1;
-                          indexrelid                          | indisclustered 
---------------------------------------------------------------+----------------
- cluster_test_time_idx                                        | t
- _timescaledb_internal._hyper_1_1_chunk_cluster_test_time_idx | t
- _timescaledb_internal._hyper_1_2_chunk_cluster_test_time_idx | t
-(3 rows)
-
--- Reorder just our table
-CLUSTER VERBOSE cluster_test;
-INFO:  clustering "_timescaledb_internal._hyper_1_1_chunk" using sequential scan and sort
-INFO:  "_timescaledb_internal._hyper_1_1_chunk": found 0 removable, 1 nonremovable row versions in 1 pages
-INFO:  clustering "_timescaledb_internal._hyper_1_2_chunk" using sequential scan and sort
-INFO:  "_timescaledb_internal._hyper_1_2_chunk": found 0 removable, 1 nonremovable row versions in 1 pages
-INFO:  clustering "_timescaledb_internal._hyper_1_3_chunk" using index scan on "_hyper_1_3_chunk_cluster_test_time_idx"
-INFO:  "_timescaledb_internal._hyper_1_3_chunk": found 0 removable, 1 nonremovable row versions in 1 pages
--- Show clustered indexes, including new chunk
-SELECT indexrelid::regclass, indisclustered
-FROM pg_index
-WHERE indisclustered = true ORDER BY 1;
-                          indexrelid                          | indisclustered 
---------------------------------------------------------------+----------------
- cluster_test_time_idx                                        | t
- _timescaledb_internal._hyper_1_1_chunk_cluster_test_time_idx | t
- _timescaledb_internal._hyper_1_2_chunk_cluster_test_time_idx | t
- _timescaledb_internal._hyper_1_3_chunk_cluster_test_time_idx | t
-(4 rows)
-
--- Reorder all tables (although will only be our test table)
-CLUSTER VERBOSE;
-INFO:  clustering "public.cluster_test" using sequential scan and sort
-INFO:  "public.cluster_test": found 0 removable, 0 nonremovable row versions in 0 pages
-INFO:  clustering "_timescaledb_internal._hyper_1_1_chunk" using sequential scan and sort
-INFO:  "_timescaledb_internal._hyper_1_1_chunk": found 0 removable, 1 nonremovable row versions in 1 pages
-INFO:  clustering "_timescaledb_internal._hyper_1_2_chunk" using sequential scan and sort
-INFO:  "_timescaledb_internal._hyper_1_2_chunk": found 0 removable, 1 nonremovable row versions in 1 pages
-INFO:  clustering "_timescaledb_internal._hyper_1_3_chunk" using sequential scan and sort
-INFO:  "_timescaledb_internal._hyper_1_3_chunk": found 0 removable, 1 nonremovable row versions in 1 pages
--- Change the clustered index
-CREATE INDEX ON cluster_test (time, location);
-CLUSTER VERBOSE cluster_test using cluster_test_time_location_idx;
-INFO:  clustering "_timescaledb_internal._hyper_1_1_chunk" using sequential scan and sort
-INFO:  "_timescaledb_internal._hyper_1_1_chunk": found 0 removable, 1 nonremovable row versions in 1 pages
-INFO:  clustering "_timescaledb_internal._hyper_1_2_chunk" using sequential scan and sort
-INFO:  "_timescaledb_internal._hyper_1_2_chunk": found 0 removable, 1 nonremovable row versions in 1 pages
-INFO:  clustering "_timescaledb_internal._hyper_1_3_chunk" using sequential scan and sort
-INFO:  "_timescaledb_internal._hyper_1_3_chunk": found 0 removable, 1 nonremovable row versions in 1 pages
--- Show updated clustered indexes
-SELECT indexrelid::regclass, indisclustered
-FROM pg_index
-WHERE indisclustered = true ORDER BY 1;
-                              indexrelid                               | indisclustered 
------------------------------------------------------------------------+----------------
- cluster_test_time_location_idx                                        | t
- _timescaledb_internal._hyper_1_1_chunk_cluster_test_time_location_idx | t
- _timescaledb_internal._hyper_1_2_chunk_cluster_test_time_location_idx | t
- _timescaledb_internal._hyper_1_3_chunk_cluster_test_time_location_idx | t
-(4 rows)
-
---check the setting of cluster indexes on hypertables and chunks
-ALTER TABLE cluster_test CLUSTER ON cluster_test_time_idx;
-SELECT indexrelid::regclass, indisclustered
-FROM pg_index
-WHERE indisclustered = true
-ORDER BY 1,2;
-                          indexrelid                          | indisclustered 
---------------------------------------------------------------+----------------
- cluster_test_time_idx                                        | t
- _timescaledb_internal._hyper_1_1_chunk_cluster_test_time_idx | t
- _timescaledb_internal._hyper_1_2_chunk_cluster_test_time_idx | t
- _timescaledb_internal._hyper_1_3_chunk_cluster_test_time_idx | t
-(4 rows)
-
-CLUSTER VERBOSE cluster_test;
-INFO:  clustering "_timescaledb_internal._hyper_1_1_chunk" using sequential scan and sort
-INFO:  "_timescaledb_internal._hyper_1_1_chunk": found 0 removable, 1 nonremovable row versions in 1 pages
-INFO:  clustering "_timescaledb_internal._hyper_1_2_chunk" using sequential scan and sort
-INFO:  "_timescaledb_internal._hyper_1_2_chunk": found 0 removable, 1 nonremovable row versions in 1 pages
-INFO:  clustering "_timescaledb_internal._hyper_1_3_chunk" using sequential scan and sort
-INFO:  "_timescaledb_internal._hyper_1_3_chunk": found 0 removable, 1 nonremovable row versions in 1 pages
-ALTER TABLE cluster_test SET WITHOUT CLUSTER;
-SELECT indexrelid::regclass, indisclustered
-FROM pg_index
-WHERE indisclustered = true
-ORDER BY 1,2;
- indexrelid | indisclustered 
-------------+----------------
-(0 rows)
-
-\set ON_ERROR_STOP 0
-CLUSTER VERBOSE cluster_test;
-ERROR:  there is no previously clustered index for table "cluster_test"
-\set ON_ERROR_STOP 1
-ALTER TABLE _timescaledb_internal._hyper_1_1_chunk CLUSTER ON _hyper_1_1_chunk_cluster_test_time_idx;
-SELECT indexrelid::regclass, indisclustered
-FROM pg_index
-WHERE indisclustered = true
-ORDER BY 1,2;
-                          indexrelid                          | indisclustered 
---------------------------------------------------------------+----------------
- _timescaledb_internal._hyper_1_1_chunk_cluster_test_time_idx | t
-(1 row)
-
-CLUSTER VERBOSE _timescaledb_internal._hyper_1_1_chunk;
-INFO:  clustering "_timescaledb_internal._hyper_1_1_chunk" using sequential scan and sort
-INFO:  "_timescaledb_internal._hyper_1_1_chunk": found 0 removable, 1 nonremovable row versions in 1 pages
-ALTER TABLE _timescaledb_internal._hyper_1_1_chunk SET WITHOUT CLUSTER;
-SELECT indexrelid::regclass, indisclustered
-FROM pg_index
-WHERE indisclustered = true
-ORDER BY 1,2;
- indexrelid | indisclustered 
-------------+----------------
-(0 rows)
-
-\set ON_ERROR_STOP 0
-CLUSTER VERBOSE _timescaledb_internal._hyper_1_1_chunk;
-ERROR:  there is no previously clustered index for table "_hyper_1_1_chunk"
-\set ON_ERROR_STOP 1
--- test alter column type on hypertable with clustering
-CREATE TABLE cluster_alter(time timestamp, id text, val int);
-CREATE INDEX idstuff ON cluster_alter USING btree (id ASC NULLS LAST, time);
-SELECT table_name FROM create_hypertable('cluster_alter', 'time');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-  table_name   
----------------
- cluster_alter
-(1 row)
-
-INSERT INTO cluster_alter VALUES('2020-01-01', '123', 1);
-CLUSTER cluster_alter using idstuff;
---attempt the alter table
-ALTER TABLE cluster_alter ALTER COLUMN id TYPE int USING id::int;
--- try recluster
--- this fails on PG11 < 11.8 and PG12 < 12.3
-\set ON_ERROR_STOP 0
-CLUSTER cluster_alter;
-\set ON_ERROR_STOP 1
-CLUSTER cluster_alter using idstuff;
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/constraint-15.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/constraint-15.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/constraint-15.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/constraint-15.out	2023-11-25 05:27:33.781052806 +0000
@@ -1,834 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-CREATE TABLE hyper (
-  time BIGINT NOT NULL,
-  device_id TEXT NOT NULL,
-  sensor_1 NUMERIC NULL DEFAULT 1 CHECK (sensor_1 > 10)
-);
-SELECT * FROM create_hypertable('hyper', 'time', chunk_time_interval => 10);
- hypertable_id | schema_name | table_name | created 
----------------+-------------+------------+---------
-             1 | public      | hyper      | t
-(1 row)
-
---check and not-null constraints are inherited through regular inheritance.
-\set ON_ERROR_STOP 0
-INSERT INTO hyper(time, device_id,sensor_1) VALUES
-(1257987700000000000, 'dev2', 9);
-ERROR:  new row for relation "_hyper_1_1_chunk" violates check constraint "hyper_sensor_1_check"
-INSERT INTO hyper(time, device_id,sensor_1) VALUES
-(1257987700000000000, NULL, 11);
-ERROR:  null value in column "device_id" of relation "_hyper_1_2_chunk" violates not-null constraint
-ALTER TABLE hyper ALTER COLUMN time DROP NOT NULL;
-ERROR:  cannot drop not-null constraint from a time-partitioned column
-ALTER TABLE ONLY hyper ALTER COLUMN sensor_1 SET NOT NULL;
-ERROR:  ONLY option not supported on hypertable operations
-ALTER TABLE ONLY hyper ALTER COLUMN device_id DROP NOT NULL;
-ERROR:  ONLY option not supported on hypertable operations
-\set ON_ERROR_STOP 1
-INSERT INTO hyper(time, device_id,sensor_1) VALUES
-(1257987700000000000, 'dev2', 11);
-INSERT INTO hyper(time, device_id,sensor_1) VALUES
-(1257987700000000000, 'dev2', 11);
-ALTER TABLE hyper ALTER COLUMN device_id DROP NOT NULL;
-INSERT INTO hyper(time, device_id,sensor_1) VALUES
-(1257987700000000000, NULL, 11);
---make sure validate works
-\set ON_ERROR_STOP 0
-ALTER TABLE hyper ADD CONSTRAINT bad_check_const CHECK (sensor_1 > 100);
-ERROR:  check constraint "bad_check_const" of relation "_hyper_1_3_chunk" is violated by some row
-\set ON_ERROR_STOP 1
-ALTER TABLE hyper ADD CONSTRAINT bad_check_const CHECK (sensor_1 > 100) NOT VALID;
-\set ON_ERROR_STOP 0
-ALTER TABLE hyper VALIDATE CONSTRAINT bad_check_const;
-ERROR:  check constraint "bad_check_const" of relation "_hyper_1_3_chunk" is violated by some row
-\set ON_ERROR_STOP 1
------------------------ UNIQUE CONSTRAINTS ------------------
-CREATE TABLE hyper_unique_with_looooooooooooooooooooooooooooooooooooong_name (
-  time BIGINT NOT NULL UNIQUE,
-  device_id TEXT NOT NULL,
-  sensor_1 NUMERIC NULL DEFAULT 1 CHECK (sensor_1 > 10)
-);
-SELECT * FROM create_hypertable('hyper_unique_with_looooooooooooooooooooooooooooooooooooong_name', 'time', chunk_time_interval => 10);
- hypertable_id | schema_name |                           table_name                            | created 
----------------+-------------+-----------------------------------------------------------------+---------
-             2 | public      | hyper_unique_with_looooooooooooooooooooooooooooooooooooong_name | t
-(1 row)
-
-INSERT INTO hyper_unique_with_looooooooooooooooooooooooooooooooooooong_name(time, device_id,sensor_1) VALUES
-(1257987700000000000, 'dev2', 11);
-INSERT INTO hyper_unique_with_looooooooooooooooooooooooooooooooooooong_name(time, device_id,sensor_1) VALUES
-(1257987800000000000, 'dev2', 11);
-\set ON_ERROR_STOP 0
-INSERT INTO hyper_unique_with_looooooooooooooooooooooooooooooooooooong_name(time, device_id,sensor_1) VALUES
-(1257987700000000000, 'dev2', 11);
-ERROR:  duplicate key value violates unique constraint "4_1_hyper_unique_with_looooooooooooooooooooooooooooooooooo_time"
-\set ON_ERROR_STOP 1
--- Show constraints on main tables
-SELECT * FROM _timescaledb_catalog.chunk_constraint;
- chunk_id | dimension_slice_id |                         constraint_name                         |                   hypertable_constraint_name                    
-----------+--------------------+-----------------------------------------------------------------+-----------------------------------------------------------------
-        3 |                  3 | constraint_3                                                    | 
-        4 |                  4 | constraint_4                                                    | 
-        4 |                    | 4_1_hyper_unique_with_looooooooooooooooooooooooooooooooooo_time | hyper_unique_with_looooooooooooooooooooooooooooooooooo_time_key
-        5 |                  5 | constraint_5                                                    | 
-        5 |                    | 5_2_hyper_unique_with_looooooooooooooooooooooooooooooooooo_time | hyper_unique_with_looooooooooooooooooooooooooooooooooo_time_key
-(5 rows)
-
-SELECT * FROM _timescaledb_catalog.chunk_index;
- chunk_id |                           index_name                            | hypertable_id |                      hypertable_index_name                      
-----------+-----------------------------------------------------------------+---------------+-----------------------------------------------------------------
-        3 | _hyper_1_3_chunk_hyper_time_idx                                 |             1 | hyper_time_idx
-        4 | 4_1_hyper_unique_with_looooooooooooooooooooooooooooooooooo_time |             2 | hyper_unique_with_looooooooooooooooooooooooooooooooooo_time_key
-        5 | 5_2_hyper_unique_with_looooooooooooooooooooooooooooooooooo_time |             2 | hyper_unique_with_looooooooooooooooooooooooooooooooooo_time_key
-(3 rows)
-
-SELECT * FROM test.show_constraints('hyper');
-      Constraint      | Type |  Columns   | Index |            Expr             | Deferrable | Deferred | Validated 
-----------------------+------+------------+-------+-----------------------------+------------+----------+-----------
- bad_check_const      | c    | {sensor_1} | -     | (sensor_1 > (100)::numeric) | f          | f        | f
- hyper_sensor_1_check | c    | {sensor_1} | -     | (sensor_1 > (10)::numeric)  | f          | f        | t
-(2 rows)
-
-SELECT * FROM test.show_constraints('hyper_unique_with_looooooooooooooooooooooooooooooooooooong_name');
-                           Constraint                            | Type |  Columns   |                              Index                              |            Expr            | Deferrable | Deferred | Validated 
------------------------------------------------------------------+------+------------+-----------------------------------------------------------------+----------------------------+------------+----------+-----------
- hyper_unique_with_looooooooooooooooooooooooooooo_sensor_1_check | c    | {sensor_1} | -                                                               | (sensor_1 > (10)::numeric) | f          | f        | t
- hyper_unique_with_looooooooooooooooooooooooooooooooooo_time_key | u    | {time}     | hyper_unique_with_looooooooooooooooooooooooooooooooooo_time_key |                            | f          | f        | t
-(2 rows)
-
---should have unique constraint not just unique index
-SELECT * FROM test.show_constraints('_timescaledb_internal._hyper_2_4_chunk');
-                           Constraint                            | Type |  Columns   |                                          Index                                          |                                           Expr                                           | Deferrable | Deferred | Validated 
------------------------------------------------------------------+------+------------+-----------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+------------+----------+-----------
- 4_1_hyper_unique_with_looooooooooooooooooooooooooooooooooo_time | u    | {time}     | _timescaledb_internal."4_1_hyper_unique_with_looooooooooooooooooooooooooooooooooo_time" |                                                                                          | f          | f        | t
- constraint_4                                                    | c    | {time}     | -                                                                                       | (("time" >= '1257987700000000000'::bigint) AND ("time" < '1257987700000000010'::bigint)) | f          | f        | t
- hyper_unique_with_looooooooooooooooooooooooooooo_sensor_1_check | c    | {sensor_1} | -                                                                                       | (sensor_1 > (10)::numeric)                                                               | f          | f        | t
-(3 rows)
-
-ALTER TABLE hyper_unique_with_looooooooooooooooooooooooooooooooooooong_name DROP CONSTRAINT hyper_unique_with_looooooooooooooooooooooooooooooooooo_time_key;
--- The constraint should have been removed from the chunk as well
-SELECT * FROM _timescaledb_catalog.chunk_constraint;
- chunk_id | dimension_slice_id | constraint_name | hypertable_constraint_name 
-----------+--------------------+-----------------+----------------------------
-        3 |                  3 | constraint_3    | 
-        4 |                  4 | constraint_4    | 
-        5 |                  5 | constraint_5    | 
-(3 rows)
-
--- The index should also have been removed
-SELECT * FROM _timescaledb_catalog.chunk_index;
- chunk_id |           index_name            | hypertable_id | hypertable_index_name 
-----------+---------------------------------+---------------+-----------------------
-        3 | _hyper_1_3_chunk_hyper_time_idx |             1 | hyper_time_idx
-(1 row)
-
-SELECT * FROM test.show_constraints('_timescaledb_internal._hyper_2_4_chunk');
-                           Constraint                            | Type |  Columns   | Index |                                           Expr                                           | Deferrable | Deferred | Validated 
------------------------------------------------------------------+------+------------+-------+------------------------------------------------------------------------------------------+------------+----------+-----------
- constraint_4                                                    | c    | {time}     | -     | (("time" >= '1257987700000000000'::bigint) AND ("time" < '1257987700000000010'::bigint)) | f          | f        | t
- hyper_unique_with_looooooooooooooooooooooooooooo_sensor_1_check | c    | {sensor_1} | -     | (sensor_1 > (10)::numeric)                                                               | f          | f        | t
-(2 rows)
-
---uniqueness not enforced
-INSERT INTO hyper_unique_with_looooooooooooooooooooooooooooooooooooong_name(time, device_id,sensor_1) VALUES
-(1257987700000000000, 'dev3', 11);
---shouldn't be able to create constraint
-\set ON_ERROR_STOP 0
-ALTER TABLE hyper_unique_with_looooooooooooooooooooooooooooooooooooong_name ADD CONSTRAINT hyper_unique_time_key UNIQUE (time);
-ERROR:  could not create unique index "4_3_hyper_unique_time_key"
-\set ON_ERROR_STOP 1
-DELETE FROM hyper_unique_with_looooooooooooooooooooooooooooooooooooong_name WHERE device_id = 'dev3';
--- Try multi-alter table statement with a constraint without a name
-ALTER TABLE hyper_unique_with_looooooooooooooooooooooooooooooooooooong_name
-      ADD CHECK (time > 0),
-      ADD UNIQUE (time) DEFERRABLE INITIALLY DEFERRED;
-\set ON_ERROR_STOP 0
-BEGIN;
---testing deferred checking. The following row has an error, which will not appear until the commit
-INSERT INTO hyper_unique_with_looooooooooooooooooooooooooooooooooooong_name(time, device_id,sensor_1) VALUES
-(1257987700000000000, 'dev3', 11);
-SELECT 1;
- ?column? 
-----------
-        1
-(1 row)
-
-COMMIT;
-ERROR:  duplicate key value violates unique constraint "4_4_hyper_unique_with_looooooooooooooooooooooooooooooooooo_time"
-\set ON_ERROR_STOP 1
-SELECT * FROM _timescaledb_catalog.chunk_constraint;
- chunk_id | dimension_slice_id |                         constraint_name                         |                   hypertable_constraint_name                    
-----------+--------------------+-----------------------------------------------------------------+-----------------------------------------------------------------
-        3 |                  3 | constraint_3                                                    | 
-        4 |                  4 | constraint_4                                                    | 
-        5 |                  5 | constraint_5                                                    | 
-        4 |                    | 4_4_hyper_unique_with_looooooooooooooooooooooooooooooooooo_time | hyper_unique_with_looooooooooooooooooooooooooooooooooo_time_key
-        5 |                    | 5_5_hyper_unique_with_looooooooooooooooooooooooooooooooooo_time | hyper_unique_with_looooooooooooooooooooooooooooooooooo_time_key
-(5 rows)
-
-SELECT * FROM test.show_constraints('hyper_unique_with_looooooooooooooooooooooooooooooooooooong_name');
-                           Constraint                            | Type |  Columns   |                              Index                              |            Expr            | Deferrable | Deferred | Validated 
------------------------------------------------------------------+------+------------+-----------------------------------------------------------------+----------------------------+------------+----------+-----------
- hyper_unique_with_looooooooooooooooooooooooooooo_sensor_1_check | c    | {sensor_1} | -                                                               | (sensor_1 > (10)::numeric) | f          | f        | t
- hyper_unique_with_looooooooooooooooooooooooooooooooo_time_check | c    | {time}     | -                                                               | ("time" > 0)               | f          | f        | t
- hyper_unique_with_looooooooooooooooooooooooooooooooooo_time_key | u    | {time}     | hyper_unique_with_looooooooooooooooooooooooooooooooooo_time_key |                            | t          | t        | t
-(3 rows)
-
-SELECT * FROM test.show_constraints('_timescaledb_internal._hyper_2_4_chunk');
-                           Constraint                            | Type |  Columns   |                                          Index                                          |                                           Expr                                           | Deferrable | Deferred | Validated 
------------------------------------------------------------------+------+------------+-----------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+------------+----------+-----------
- 4_4_hyper_unique_with_looooooooooooooooooooooooooooooooooo_time | u    | {time}     | _timescaledb_internal."4_4_hyper_unique_with_looooooooooooooooooooooooooooooooooo_time" |                                                                                          | t          | t        | t
- constraint_4                                                    | c    | {time}     | -                                                                                       | (("time" >= '1257987700000000000'::bigint) AND ("time" < '1257987700000000010'::bigint)) | f          | f        | t
- hyper_unique_with_looooooooooooooooooooooooooooo_sensor_1_check | c    | {sensor_1} | -                                                                                       | (sensor_1 > (10)::numeric)                                                               | f          | f        | t
- hyper_unique_with_looooooooooooooooooooooooooooooooo_time_check | c    | {time}     | -                                                                                       | ("time" > 0)                                                                             | f          | f        | t
-(4 rows)
-
-ALTER TABLE  hyper_unique_with_looooooooooooooooooooooooooooooooooooong_name
-DROP CONSTRAINT hyper_unique_with_looooooooooooooooooooooooooooooooooo_time_key,
-DROP CONSTRAINT hyper_unique_with_looooooooooooooooooooooooooooooooo_time_check;
-SELECT * FROM _timescaledb_catalog.chunk_constraint;
- chunk_id | dimension_slice_id | constraint_name | hypertable_constraint_name 
-----------+--------------------+-----------------+----------------------------
-        3 |                  3 | constraint_3    | 
-        4 |                  4 | constraint_4    | 
-        5 |                  5 | constraint_5    | 
-(3 rows)
-
-SELECT * FROM test.show_constraints('hyper_unique_with_looooooooooooooooooooooooooooooooooooong_name');
-                           Constraint                            | Type |  Columns   | Index |            Expr            | Deferrable | Deferred | Validated 
------------------------------------------------------------------+------+------------+-------+----------------------------+------------+----------+-----------
- hyper_unique_with_looooooooooooooooooooooooooooo_sensor_1_check | c    | {sensor_1} | -     | (sensor_1 > (10)::numeric) | f          | f        | t
-(1 row)
-
-SELECT * FROM test.show_constraints('_timescaledb_internal._hyper_2_4_chunk');
-                           Constraint                            | Type |  Columns   | Index |                                           Expr                                           | Deferrable | Deferred | Validated 
------------------------------------------------------------------+------+------------+-------+------------------------------------------------------------------------------------------+------------+----------+-----------
- constraint_4                                                    | c    | {time}     | -     | (("time" >= '1257987700000000000'::bigint) AND ("time" < '1257987700000000010'::bigint)) | f          | f        | t
- hyper_unique_with_looooooooooooooooooooooooooooo_sensor_1_check | c    | {sensor_1} | -     | (sensor_1 > (10)::numeric)                                                               | f          | f        | t
-(2 rows)
-
-CREATE UNIQUE INDEX hyper_unique_with_looooooooooooooooooooooooooooooooo_time_idx
-ON hyper_unique_with_looooooooooooooooooooooooooooooooooooong_name (time);
-\set ON_ERROR_STOP 0
--- Try adding constraint using existing index
-ALTER TABLE hyper_unique_with_looooooooooooooooooooooooooooooooooooong_name
-ADD CONSTRAINT hyper_unique_with_looooooooooooooooooooooooooooooooooo_time_key UNIQUE
-USING INDEX hyper_unique_with_looooooooooooooooooooooooooooooooo_time_idx;
-NOTICE:  ALTER TABLE / ADD CONSTRAINT USING INDEX will rename index "hyper_unique_with_looooooooooooooooooooooooooooooooo_time_idx" to "hyper_unique_with_looooooooooooooooooooooooooooooooooo_time_key"
-ERROR:  hypertables do not support adding a constraint using an existing index
-\set ON_ERROR_STOP 1
-DROP INDEX hyper_unique_with_looooooooooooooooooooooooooooooooo_time_idx;
---now can create
-ALTER TABLE  hyper_unique_with_looooooooooooooooooooooooooooooooooooong_name
-ADD CONSTRAINT hyper_unique_with_looooooooooooooooooooooooooooooooooo_time_key UNIQUE (time);
-SELECT * FROM test.show_constraints('_timescaledb_internal._hyper_2_4_chunk');
-                           Constraint                            | Type |  Columns   |                                          Index                                          |                                           Expr                                           | Deferrable | Deferred | Validated 
------------------------------------------------------------------+------+------------+-----------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+------------+----------+-----------
- 4_6_hyper_unique_with_looooooooooooooooooooooooooooooooooo_time | u    | {time}     | _timescaledb_internal."4_6_hyper_unique_with_looooooooooooooooooooooooooooooooooo_time" |                                                                                          | f          | f        | t
- constraint_4                                                    | c    | {time}     | -                                                                                       | (("time" >= '1257987700000000000'::bigint) AND ("time" < '1257987700000000010'::bigint)) | f          | f        | t
- hyper_unique_with_looooooooooooooooooooooooooooo_sensor_1_check | c    | {sensor_1} | -                                                                                       | (sensor_1 > (10)::numeric)                                                               | f          | f        | t
-(3 rows)
-
---test adding constraint with same name to different table -- should fail
-\set ON_ERROR_STOP 0
-ALTER TABLE hyper
-ADD CONSTRAINT hyper_unique_with_looooooooooooooooooooooooooooooooooo_time_key UNIQUE (time);
-ERROR:  relation "hyper_unique_with_looooooooooooooooooooooooooooooooooo_time_key" already exists
-\set ON_ERROR_STOP 1
---uniquness violation fails
-\set ON_ERROR_STOP 0
-INSERT INTO hyper_unique_with_looooooooooooooooooooooooooooooooooooong_name(time, device_id,sensor_1)
-VALUES (1257987700000000000, 'dev2', 11);
-ERROR:  duplicate key value violates unique constraint "4_6_hyper_unique_with_looooooooooooooooooooooooooooooooooo_time"
-\set ON_ERROR_STOP 1
---cannot create unique constraint on non-partition column
-\set ON_ERROR_STOP 0
-ALTER TABLE hyper_unique_with_looooooooooooooooooooooooooooooooooooong_name
-ADD CONSTRAINT hyper_unique_invalid UNIQUE (device_id);
-ERROR:  cannot create a unique index without the column "time" (used in partitioning)
-ALTER TABLE hyper_unique_with_looooooooooooooooooooooooooooooooooooong_name
-ADD COLUMN new_device_id int UNIQUE;
-ERROR:  cannot create a unique index without the column "time" (used in partitioning)
-ALTER TABLE hyper_unique_with_looooooooooooooooooooooooooooooooooooong_name
-DROP COLUMN device_id,
-ADD COLUMN new_device_id int UNIQUE;
-ERROR:  cannot create a unique index without the column "time" (used in partitioning)
-\set ON_ERROR_STOP 1
------------------------ RENAME CONSTRAINT  ------------------
-ALTER TABLE hyper_unique_with_looooooooooooooooooooooooooooooooooooong_name
-RENAME CONSTRAINT hyper_unique_with_looooooooooooooooooooooooooooooooooo_time_key TO new_name;
-ALTER TABLE hyper_unique_with_looooooooooooooooooooooooooooooooooooong_name *
-RENAME CONSTRAINT new_name TO new_name2;
-ALTER TABLE IF EXISTS hyper_unique_with_looooooooooooooooooooooooooooooooooooong_name
-RENAME CONSTRAINT  hyper_unique_with_looooooooooooooooooooooooooooo_sensor_1_check TO check_2;
-SELECT * FROM test.show_constraints('hyper_unique_with_looooooooooooooooooooooooooooooooooooong_name');
- Constraint | Type |  Columns   |   Index   |            Expr            | Deferrable | Deferred | Validated 
-------------+------+------------+-----------+----------------------------+------------+----------+-----------
- check_2    | c    | {sensor_1} | -         | (sensor_1 > (10)::numeric) | f          | f        | t
- new_name2  | u    | {time}     | new_name2 |                            | f          | f        | t
-(2 rows)
-
-SELECT * FROM test.show_constraints('_timescaledb_internal._hyper_2_4_chunk');
-   Constraint   | Type |  Columns   |                 Index                  |                                           Expr                                           | Deferrable | Deferred | Validated 
-----------------+------+------------+----------------------------------------+------------------------------------------------------------------------------------------+------------+----------+-----------
- 4_10_new_name2 | u    | {time}     | _timescaledb_internal."4_10_new_name2" |                                                                                          | f          | f        | t
- check_2        | c    | {sensor_1} | -                                      | (sensor_1 > (10)::numeric)                                                               | f          | f        | t
- constraint_4   | c    | {time}     | -                                      | (("time" >= '1257987700000000000'::bigint) AND ("time" < '1257987700000000010'::bigint)) | f          | f        | t
-(3 rows)
-
-SELECT * FROM _timescaledb_catalog.chunk_constraint;
- chunk_id | dimension_slice_id | constraint_name | hypertable_constraint_name 
-----------+--------------------+-----------------+----------------------------
-        3 |                  3 | constraint_3    | 
-        4 |                  4 | constraint_4    | 
-        5 |                  5 | constraint_5    | 
-        4 |                    | 4_10_new_name2  | new_name2
-        5 |                    | 5_11_new_name2  | new_name2
-(5 rows)
-
-\set ON_ERROR_STOP 0
-ALTER TABLE hyper_unique_with_looooooooooooooooooooooooooooooooooooong_name
-RENAME CONSTRAINT new_name TO new_name2;
-ERROR:  constraint "new_name" for table "hyper_unique_with_looooooooooooooooooooooooooooooooooooong_name" does not exist
-ALTER TABLE hyper_unique_with_looooooooooooooooooooooooooooooooooooong_name
-RENAME CONSTRAINT new_name2 TO check_2;
-ERROR:  constraint "check_2" for relation "hyper_unique_with_looooooooooooooooooooooooooooooooooooong_name" already exists
-ALTER TABLE ONLY hyper_unique_with_looooooooooooooooooooooooooooooooooooong_name
-RENAME CONSTRAINT new_name2 TO new_name;
-ERROR:  ONLY option not supported on hypertable operations
-ALTER TABLE _timescaledb_internal._hyper_2_4_chunk
-RENAME CONSTRAINT "4_10_new_name2" TO new_name;
-ERROR:  renaming constraints on chunks is not supported
-\set ON_ERROR_STOP 1
------------------------ PRIMARY KEY  ------------------
-CREATE TABLE hyper_pk (
-  time BIGINT NOT NULL PRIMARY KEY,
-  device_id TEXT NOT NULL,
-  sensor_1 NUMERIC NULL DEFAULT 1 CHECK (sensor_1 > 10)
-);
-SELECT * FROM create_hypertable('hyper_pk', 'time', chunk_time_interval => 10);
- hypertable_id | schema_name | table_name | created 
----------------+-------------+------------+---------
-             3 | public      | hyper_pk   | t
-(1 row)
-
-INSERT INTO hyper_pk(time, device_id,sensor_1) VALUES
-(1257987700000000000, 'dev2', 11);
-\set ON_ERROR_STOP 0
-INSERT INTO hyper_pk(time, device_id,sensor_1) VALUES
-(1257987700000000000, 'dev2', 11);
-ERROR:  duplicate key value violates unique constraint "6_14_hyper_pk_pkey"
-\set ON_ERROR_STOP 1
---should have unique constraint not just unique index
-SELECT * FROM test.show_constraints('_timescaledb_internal._hyper_3_6_chunk');
-       Constraint        | Type |  Columns   |                   Index                    |                                           Expr                                           | Deferrable | Deferred | Validated 
--------------------------+------+------------+--------------------------------------------+------------------------------------------------------------------------------------------+------------+----------+-----------
- 6_14_hyper_pk_pkey      | p    | {time}     | _timescaledb_internal."6_14_hyper_pk_pkey" |                                                                                          | f          | f        | t
- constraint_6            | c    | {time}     | -                                          | (("time" >= '1257987700000000000'::bigint) AND ("time" < '1257987700000000010'::bigint)) | f          | f        | t
- hyper_pk_sensor_1_check | c    | {sensor_1} | -                                          | (sensor_1 > (10)::numeric)                                                               | f          | f        | t
-(3 rows)
-
-ALTER TABLE hyper_pk DROP CONSTRAINT hyper_pk_pkey;
-SELECT * FROM test.show_constraints('_timescaledb_internal._hyper_3_6_chunk');
-       Constraint        | Type |  Columns   | Index |                                           Expr                                           | Deferrable | Deferred | Validated 
--------------------------+------+------------+-------+------------------------------------------------------------------------------------------+------------+----------+-----------
- constraint_6            | c    | {time}     | -     | (("time" >= '1257987700000000000'::bigint) AND ("time" < '1257987700000000010'::bigint)) | f          | f        | t
- hyper_pk_sensor_1_check | c    | {sensor_1} | -     | (sensor_1 > (10)::numeric)                                                               | f          | f        | t
-(2 rows)
-
---uniqueness not enforced
-INSERT INTO hyper_pk(time, device_id,sensor_1) VALUES
-(1257987700000000000, 'dev3', 11);
---shouldn't be able to create pk
-\set ON_ERROR_STOP 0
-ALTER TABLE hyper_pk ADD CONSTRAINT hyper_pk_pkey PRIMARY KEY (time);
-ERROR:  could not create unique index "6_15_hyper_pk_pkey"
-ALTER TABLE hyper_unique_with_looooooooooooooooooooooooooooooooooooong_name
-ADD COLUMN new_device_id int PRIMARY KEY;
-ERROR:  cannot create a unique index without the column "time" (used in partitioning)
-\set ON_ERROR_STOP 1
-DELETE FROM hyper_pk WHERE device_id = 'dev3';
---cannot create pk constraint on non-partition column
-\set ON_ERROR_STOP 0
-ALTER TABLE hyper_pk ADD CONSTRAINT hyper_pk_invalid PRIMARY KEY (device_id);
-ERROR:  cannot create a unique index without the column "time" (used in partitioning)
-\set ON_ERROR_STOP 1
---now can create
-ALTER TABLE hyper_pk ADD CONSTRAINT hyper_pk_pkey PRIMARY KEY (time) DEFERRABLE INITIALLY DEFERRED;
-SELECT * FROM test.show_constraints('_timescaledb_internal._hyper_3_6_chunk');
-       Constraint        | Type |  Columns   |                   Index                    |                                           Expr                                           | Deferrable | Deferred | Validated 
--------------------------+------+------------+--------------------------------------------+------------------------------------------------------------------------------------------+------------+----------+-----------
- 6_16_hyper_pk_pkey      | p    | {time}     | _timescaledb_internal."6_16_hyper_pk_pkey" |                                                                                          | t          | t        | t
- constraint_6            | c    | {time}     | -                                          | (("time" >= '1257987700000000000'::bigint) AND ("time" < '1257987700000000010'::bigint)) | f          | f        | t
- hyper_pk_sensor_1_check | c    | {sensor_1} | -                                          | (sensor_1 > (10)::numeric)                                                               | f          | f        | t
-(3 rows)
-
---test adding constraint with same name to different table -- should fail
-\set ON_ERROR_STOP 0
-ALTER TABLE hyper ADD CONSTRAINT hyper_pk_pkey UNIQUE (time);
-ERROR:  relation "hyper_pk_pkey" already exists
-\set ON_ERROR_STOP 1
---uniquness violation fails
-\set ON_ERROR_STOP 0
-BEGIN;
-  --error here deferred until commit
-  INSERT INTO hyper_pk(time, device_id,sensor_1) VALUES
-  (1257987700000000000, 'dev2', 11);
-  SELECT 1;
- ?column? 
-----------
-        1
-(1 row)
-
-COMMIT;
-ERROR:  duplicate key value violates unique constraint "6_16_hyper_pk_pkey"
-\set ON_ERROR_STOP 1
------------------------ FOREIGN KEY  ------------------
-CREATE TABLE devices(
-    device_id TEXT NOT NULL,
-    PRIMARY KEY (device_id)
-);
-CREATE TABLE hyper_fk (
-  time BIGINT NOT NULL PRIMARY KEY,
-  device_id TEXT NOT NULL REFERENCES devices(device_id),
-  sensor_1 NUMERIC NULL DEFAULT 1 CHECK (sensor_1 > 10)
-);
-SELECT * FROM create_hypertable('hyper_fk', 'time', chunk_time_interval => 10);
- hypertable_id | schema_name | table_name | created 
----------------+-------------+------------+---------
-             4 | public      | hyper_fk   | t
-(1 row)
-
---fail fk constraint
-\set ON_ERROR_STOP 0
-INSERT INTO hyper_fk(time, device_id,sensor_1) VALUES
-(1257987700000000000, 'dev2', 11);
-ERROR:  insert or update on table "_hyper_4_7_chunk" violates foreign key constraint "7_17_hyper_fk_device_id_fkey"
-\set ON_ERROR_STOP 1
-INSERT INTO devices VALUES ('dev2');
-INSERT INTO hyper_fk(time, device_id,sensor_1) VALUES
-(1257987700000000000, 'dev2', 11);
---delete should fail
-\set ON_ERROR_STOP 0
-DELETE FROM devices;
-ERROR:  update or delete on table "devices" violates foreign key constraint "8_19_hyper_fk_device_id_fkey" on table "_hyper_4_8_chunk"
-\set ON_ERROR_STOP 1
-ALTER TABLE hyper_fk DROP CONSTRAINT hyper_fk_device_id_fkey;
---should now be able to add non-fk rows
-INSERT INTO hyper_fk(time, device_id,sensor_1) VALUES
-(1257987700000000001, 'dev3', 11);
---can't add fk because of dev3 row
-\set ON_ERROR_STOP 0
-ALTER TABLE hyper_fk ADD CONSTRAINT hyper_fk_device_id_fkey
-FOREIGN KEY (device_id) REFERENCES devices(device_id);
-ERROR:  insert or update on table "_hyper_4_8_chunk" violates foreign key constraint "8_21_hyper_fk_device_id_fkey"
-\set ON_ERROR_STOP 1
---but can add a NOT VALID one
-ALTER TABLE hyper_fk ADD CONSTRAINT hyper_fk_device_id_fkey
-FOREIGN KEY (device_id) REFERENCES devices(device_id) NOT VALID;
---which will fail when validated
-\set ON_ERROR_STOP 0
-ALTER TABLE hyper_fk VALIDATE CONSTRAINT hyper_fk_device_id_fkey;
-ERROR:  insert or update on table "_hyper_4_8_chunk" violates foreign key constraint "8_22_hyper_fk_device_id_fkey"
-\set ON_ERROR_STOP 1
-ALTER TABLE hyper_fk DROP CONSTRAINT hyper_fk_device_id_fkey;
-DELETE FROM hyper_fk WHERE device_id = 'dev3';
-ALTER TABLE hyper_fk ADD CONSTRAINT hyper_fk_device_id_fkey
-FOREIGN KEY (device_id) REFERENCES devices(device_id);
-\set ON_ERROR_STOP 0
-INSERT INTO hyper_fk(time, device_id,sensor_1) VALUES
-(1257987700000000002, 'dev3', 11);
-ERROR:  insert or update on table "_hyper_4_8_chunk" violates foreign key constraint "8_23_hyper_fk_device_id_fkey"
-\set ON_ERROR_STOP 1
-SELECT * FROM test.show_constraints('_timescaledb_internal._hyper_4_8_chunk');
-          Constraint          | Type |   Columns   |                   Index                    |                                           Expr                                           | Deferrable | Deferred | Validated 
-------------------------------+------+-------------+--------------------------------------------+------------------------------------------------------------------------------------------+------------+----------+-----------
- 8_20_hyper_fk_pkey           | p    | {time}      | _timescaledb_internal."8_20_hyper_fk_pkey" |                                                                                          | f          | f        | t
- 8_23_hyper_fk_device_id_fkey | f    | {device_id} | devices_pkey                               |                                                                                          | f          | f        | t
- constraint_8                 | c    | {time}      | -                                          | (("time" >= '1257987700000000000'::bigint) AND ("time" < '1257987700000000010'::bigint)) | f          | f        | t
- hyper_fk_sensor_1_check      | c    | {sensor_1}  | -                                          | (sensor_1 > (10)::numeric)                                                               | f          | f        | t
-(4 rows)
-
-SELECT * FROM _timescaledb_catalog.chunk_constraint;
- chunk_id | dimension_slice_id |       constraint_name        | hypertable_constraint_name 
-----------+--------------------+------------------------------+----------------------------
-        3 |                  3 | constraint_3                 | 
-        4 |                  4 | constraint_4                 | 
-        5 |                  5 | constraint_5                 | 
-        4 |                    | 4_10_new_name2               | new_name2
-        5 |                    | 5_11_new_name2               | new_name2
-        6 |                  6 | constraint_6                 | 
-        6 |                    | 6_16_hyper_pk_pkey           | hyper_pk_pkey
-        8 |                  8 | constraint_8                 | 
-        8 |                    | 8_20_hyper_fk_pkey           | hyper_fk_pkey
-        8 |                    | 8_23_hyper_fk_device_id_fkey | hyper_fk_device_id_fkey
-(10 rows)
-
---test CASCADE drop behavior
-DROP TABLE devices CASCADE;
-NOTICE:  drop cascades to 2 other objects
-SELECT * FROM test.show_constraints('_timescaledb_internal._hyper_4_8_chunk');
-       Constraint        | Type |  Columns   |                   Index                    |                                           Expr                                           | Deferrable | Deferred | Validated 
--------------------------+------+------------+--------------------------------------------+------------------------------------------------------------------------------------------+------------+----------+-----------
- 8_20_hyper_fk_pkey      | p    | {time}     | _timescaledb_internal."8_20_hyper_fk_pkey" |                                                                                          | f          | f        | t
- constraint_8            | c    | {time}     | -                                          | (("time" >= '1257987700000000000'::bigint) AND ("time" < '1257987700000000010'::bigint)) | f          | f        | t
- hyper_fk_sensor_1_check | c    | {sensor_1} | -                                          | (sensor_1 > (10)::numeric)                                                               | f          | f        | t
-(3 rows)
-
-SELECT * FROM _timescaledb_catalog.chunk_constraint;
- chunk_id | dimension_slice_id |  constraint_name   | hypertable_constraint_name 
-----------+--------------------+--------------------+----------------------------
-        3 |                  3 | constraint_3       | 
-        4 |                  4 | constraint_4       | 
-        5 |                  5 | constraint_5       | 
-        4 |                    | 4_10_new_name2     | new_name2
-        5 |                    | 5_11_new_name2     | new_name2
-        6 |                  6 | constraint_6       | 
-        6 |                    | 6_16_hyper_pk_pkey | hyper_pk_pkey
-        8 |                  8 | constraint_8       | 
-        8 |                    | 8_20_hyper_fk_pkey | hyper_fk_pkey
-(9 rows)
-
---the fk went away.
-INSERT INTO hyper_fk(time, device_id,sensor_1) VALUES
-(1257987700000000002, 'dev3', 11);
-CREATE TABLE devices(
-    device_id TEXT NOT NULL,
-    PRIMARY KEY (device_id)
-);
-INSERT INTO devices VALUES ('dev2'), ('dev3');
-ALTER TABLE hyper_fk ADD CONSTRAINT hyper_fk_device_id_fkey
-FOREIGN KEY (device_id) REFERENCES devices(device_id) DEFERRABLE INITIALLY DEFERRED;
-\set ON_ERROR_STOP 0
-BEGIN;
-  --error deferred until commmit
-  INSERT INTO hyper_fk(time, device_id,sensor_1) VALUES
-  (1257987700000000003, 'dev4', 11);
-  SELECT 1;
- ?column? 
-----------
-        1
-(1 row)
-
-COMMIT;
-ERROR:  insert or update on table "_hyper_4_8_chunk" violates foreign key constraint "8_24_hyper_fk_device_id_fkey"
-\set ON_ERROR_STOP 1
-ALTER TABLE hyper_fk ALTER CONSTRAINT hyper_fk_device_id_fkey NOT DEFERRABLE;
-\set ON_ERROR_STOP 0
-BEGIN;
-  --error detected right away
-  INSERT INTO hyper_fk(time, device_id,sensor_1) VALUES
-  (1257987700000000003, 'dev4', 11);
-ERROR:  insert or update on table "_hyper_4_8_chunk" violates foreign key constraint "8_24_hyper_fk_device_id_fkey"
-  SELECT 1;
-ERROR:  current transaction is aborted, commands ignored until end of transaction block
-COMMIT;
-\set ON_ERROR_STOP 1
---this tests that there are no extra chunk_constraints left on hyper_fk
-TRUNCATE hyper_fk;
------------------------ FOREIGN KEY INTO A HYPERTABLE  ------------------
---FOREIGN KEY references into a hypertable are currently broken.
---The referencing table will never find the corresponding row in the hypertable
---since it will only search the parent. Thus any insert will result in an ERROR
---Block such foreign keys or fix. (Hard to block on create table so punting for now)
-CREATE TABLE hyper_for_ref (
-  time BIGINT NOT NULL PRIMARY KEY,
-  device_id TEXT NOT NULL,
-  sensor_1 NUMERIC NULL DEFAULT 1 CHECK (sensor_1 > 10)
-);
-SELECT * FROM create_hypertable('hyper_for_ref', 'time', chunk_time_interval => 10);
- hypertable_id | schema_name |  table_name   | created 
----------------+-------------+---------------+---------
-             5 | public      | hyper_for_ref | t
-(1 row)
-
-\set ON_ERROR_STOP 0
-CREATE TABLE referrer (
-    time BIGINT NOT NULL REFERENCES hyper_for_ref(time)
-);
-ERROR:  foreign keys to hypertables are not supported
-\set ON_ERROR_STOP 1
-CREATE TABLE referrer2 (
-   time BIGINT NOT NULL
-);
-\set ON_ERROR_STOP 0
-ALTER TABLE referrer2 ADD CONSTRAINT hyper_fk_device_id_fkey
-FOREIGN KEY (time) REFERENCES  hyper_for_ref(time);
-ERROR:  foreign keys to hypertables are not supported
-\set ON_ERROR_STOP 1
------------------------ EXCLUSION CONSTRAINT  ------------------
-CREATE TABLE hyper_ex (
-    time BIGINT,
-    device_id TEXT NOT NULL REFERENCES devices(device_id),
-    sensor_1 NUMERIC NULL DEFAULT 1 CHECK (sensor_1 > 10),
-    canceled boolean DEFAULT false,
-    EXCLUDE USING btree (
-        time WITH =, device_id WITH =
-    ) WHERE (not canceled)
-);
-SELECT * FROM create_hypertable('hyper_ex', 'time', chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
-NOTICE:  adding not-null constraint to column "time"
- hypertable_id | schema_name | table_name | created 
----------------+-------------+------------+---------
-             6 | public      | hyper_ex   | t
-(1 row)
-
-INSERT INTO hyper_ex(time, device_id,sensor_1) VALUES
-(1257987700000000000, 'dev2', 11);
-\set ON_ERROR_STOP 0
-INSERT INTO hyper_ex(time, device_id,sensor_1) VALUES
-(1257987700000000000, 'dev2', 12);
-ERROR:  conflicting key value violates exclusion constraint "9_26_hyper_ex_time_device_id_excl"
-\set ON_ERROR_STOP 1
-ALTER TABLE hyper_ex DROP CONSTRAINT hyper_ex_time_device_id_excl;
---can now add
-INSERT INTO hyper_ex(time, device_id,sensor_1) VALUES
-(1257987700000000000, 'dev2', 12);
---cannot add because of conflicts
-\set ON_ERROR_STOP 0
-ALTER TABLE hyper_ex ADD CONSTRAINT hyper_ex_time_device_id_excl
-    EXCLUDE USING btree (
-        time WITH =, device_id WITH =
-    ) WHERE (not canceled)
-;
-ERROR:  could not create exclusion constraint "9_27_hyper_ex_time_device_id_excl"
-\set ON_ERROR_STOP 1
-DELETE FROM hyper_ex WHERE sensor_1 = 12;
-ALTER TABLE hyper_ex ADD CONSTRAINT hyper_ex_time_device_id_excl
-    EXCLUDE USING btree (
-        time WITH =, device_id WITH =
-    ) WHERE (not canceled) DEFERRABLE INITIALLY DEFERRED
-;
-\set ON_ERROR_STOP 0
-BEGIN;
-  --error deferred til commit
-  INSERT INTO hyper_ex(time, device_id,sensor_1) VALUES
-  (1257987700000000000, 'dev2', 12);
-  SELECT 1;
- ?column? 
-----------
-        1
-(1 row)
-
-COMMIT;
-ERROR:  conflicting key value violates exclusion constraint "9_28_hyper_ex_time_device_id_excl"
-\set ON_ERROR_STOP 1
---cannot add exclusion constraint without partition key.
-CREATE TABLE hyper_ex_invalid (
-    time BIGINT,
-    device_id TEXT NOT NULL REFERENCES devices(device_id),
-    sensor_1 NUMERIC NULL DEFAULT 1 CHECK (sensor_1 > 10),
-    canceled boolean DEFAULT false,
-    EXCLUDE USING btree (
-        device_id WITH =
-    ) WHERE (not canceled)
-);
-\set ON_ERROR_STOP 0
-SELECT * FROM create_hypertable('hyper_ex_invalid', 'time', chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
-NOTICE:  adding not-null constraint to column "time"
-ERROR:  cannot create a unique index without the column "time" (used in partitioning)
-\set ON_ERROR_STOP 1
---- NO INHERIT constraints (not allowed) ----
-CREATE TABLE hyper_noinherit (
-    time BIGINT,
-    sensor_1 NUMERIC NULL DEFAULT 1 CHECK (sensor_1 > 0) NO INHERIT
-);
-SELECT * FROM test.show_constraints('hyper_noinherit');
-           Constraint           | Type |  Columns   | Index |           Expr            | Deferrable | Deferred | Validated 
---------------------------------+------+------------+-------+---------------------------+------------+----------+-----------
- hyper_noinherit_sensor_1_check | c    | {sensor_1} | -     | (sensor_1 > (0)::numeric) | f          | f        | t
-(1 row)
-
-\set ON_ERROR_STOP 0
-SELECT * FROM create_hypertable('hyper_noinherit', 'time', chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
-ERROR:  cannot have NO INHERIT constraints on hypertable "hyper_noinherit"
-\set ON_ERROR_STOP 1
-CREATE TABLE hyper_noinherit_alter (
-    time BIGINT,
-    sensor_1 NUMERIC NULL DEFAULT 1
-);
-SELECT * FROM create_hypertable('hyper_noinherit_alter', 'time', chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
-NOTICE:  adding not-null constraint to column "time"
- hypertable_id | schema_name |      table_name       | created 
----------------+-------------+-----------------------+---------
-             8 | public      | hyper_noinherit_alter | t
-(1 row)
-
-\set ON_ERROR_STOP 0
-ALTER TABLE hyper_noinherit_alter ADD CONSTRAINT check_noinherit CHECK (sensor_1 > 0) NO INHERIT;
-ERROR:  cannot have NO INHERIT constraints on hypertable "hyper_noinherit_alter"
---  CREATE TABLE WITH DEFERRED CONSTRAINTS --
-CREATE TABLE hyper_unique_deferred (
-  time BIGINT UNIQUE DEFERRABLE INITIALLY DEFERRED,
-  device_id TEXT NOT NULL,
-  sensor_1 NUMERIC NULL DEFAULT 1 CHECK (sensor_1 > 10)
-);
-SELECT * FROM create_hypertable('hyper_unique_deferred', 'time', chunk_time_interval => 10);
-NOTICE:  adding not-null constraint to column "time"
- hypertable_id | schema_name |      table_name       | created 
----------------+-------------+-----------------------+---------
-             9 | public      | hyper_unique_deferred | t
-(1 row)
-
-INSERT INTO hyper_unique_deferred(time, device_id,sensor_1) VALUES (1257987700000000000, 'dev2', 11);
-\set ON_ERROR_STOP 0
-BEGIN;
-  --error here deferred until commit
-  INSERT INTO hyper_unique_deferred(time, device_id,sensor_1) VALUES (1257987700000000000, 'dev2', 11);
-  SELECT 1;
- ?column? 
-----------
-        1
-(1 row)
-
-COMMIT;
-ERROR:  duplicate key value violates unique constraint "10_29_hyper_unique_deferred_time_key"
-\set ON_ERROR_STOP 1
---test deferred on create table
-CREATE TABLE hyper_pk_deferred (
-  time BIGINT NOT NULL PRIMARY KEY DEFERRABLE INITIALLY DEFERRED,
-  device_id TEXT NOT NULL,
-  sensor_1 NUMERIC NULL DEFAULT 1 CHECK (sensor_1 > 10)
-);
-SELECT * FROM create_hypertable('hyper_pk_deferred', 'time', chunk_time_interval => 10);
- hypertable_id | schema_name |    table_name     | created 
----------------+-------------+-------------------+---------
-            10 | public      | hyper_pk_deferred | t
-(1 row)
-
-INSERT INTO hyper_pk_deferred(time, device_id,sensor_1) VALUES (1257987700000000000, 'dev2', 11);
-\set ON_ERROR_STOP 0
-BEGIN;
-  --error here deferred until commit
-  INSERT INTO hyper_pk_deferred(time, device_id,sensor_1) VALUES (1257987700000000000, 'dev2', 11);
-  SELECT 1;
- ?column? 
-----------
-        1
-(1 row)
-
-COMMIT;
-ERROR:  duplicate key value violates unique constraint "11_30_hyper_pk_deferred_pkey"
-\set ON_ERROR_STOP 1
---test that deferred works on create table too
-CREATE TABLE hyper_fk_deferred (
-  time BIGINT NOT NULL PRIMARY KEY,
-  device_id TEXT NOT NULL REFERENCES devices(device_id) DEFERRABLE INITIALLY DEFERRED,
-  sensor_1 NUMERIC NULL DEFAULT 1 CHECK (sensor_1 > 10)
-);
-SELECT * FROM create_hypertable('hyper_fk_deferred', 'time', chunk_time_interval => 10);
- hypertable_id | schema_name |    table_name     | created 
----------------+-------------+-------------------+---------
-            11 | public      | hyper_fk_deferred | t
-(1 row)
-
-\set ON_ERROR_STOP 0
-BEGIN;
-  --error deferred until commmit
-  INSERT INTO hyper_fk_deferred(time, device_id,sensor_1) VALUES (1257987700000000003, 'dev4', 11);
-  SELECT 1;
- ?column? 
-----------
-        1
-(1 row)
-
-COMMIT;
-ERROR:  insert or update on table "_hyper_11_12_chunk" violates foreign key constraint "12_31_hyper_fk_deferred_device_id_fkey"
-\set ON_ERROR_STOP 1
-CREATE TABLE hyper_ex_deferred (
-    time BIGINT,
-    device_id TEXT NOT NULL REFERENCES devices(device_id),
-    sensor_1 NUMERIC NULL DEFAULT 1 CHECK (sensor_1 > 10),
-    canceled boolean DEFAULT false,
-    EXCLUDE USING btree (
-        time WITH =, device_id WITH =
-    ) WHERE (not canceled) DEFERRABLE INITIALLY DEFERRED
-);
-SELECT * FROM create_hypertable('hyper_ex_deferred', 'time', chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
-NOTICE:  adding not-null constraint to column "time"
- hypertable_id | schema_name |    table_name     | created 
----------------+-------------+-------------------+---------
-            12 | public      | hyper_ex_deferred | t
-(1 row)
-
-INSERT INTO hyper_ex_deferred(time, device_id,sensor_1) VALUES (1257987700000000000, 'dev2', 12);
-\set ON_ERROR_STOP 0
-BEGIN;
-  --error deferred til commit
-  INSERT INTO hyper_ex_deferred(time, device_id,sensor_1) VALUES (1257987700000000000, 'dev2', 12);
-  SELECT 1;
- ?column? 
-----------
-        1
-(1 row)
-
-COMMIT;
-ERROR:  conflicting key value violates exclusion constraint "13_34_hyper_ex_deferred_time_device_id_excl"
-\set ON_ERROR_STOP 1
--- Make sure renaming schemas won't break dropping constraints
-\c :TEST_DBNAME :ROLE_SUPERUSER
-CREATE TABLE hyper_unique (
-  time BIGINT NOT NULL UNIQUE,
-  device_id TEXT NOT NULL,
-  sensor_1 NUMERIC NULL DEFAULT 1 CHECK (sensor_1 > 10)
-);
-SELECT * FROM create_hypertable('hyper_unique', 'time', chunk_time_interval => 10, associated_schema_name => 'my_associated_schema');
- hypertable_id | schema_name |  table_name  | created 
----------------+-------------+--------------+---------
-            13 | public      | hyper_unique | t
-(1 row)
-
-INSERT INTO hyper_unique(time, device_id,sensor_1) VALUES (1257987700000000000, 'dev2', 11);
-ALTER SCHEMA my_associated_schema RENAME TO new_associated_schema;
-ALTER TABLE hyper_unique DROP CONSTRAINT hyper_unique_time_key;
--- test for constraint validation crash, see #1183
-CREATE TABLE test_validate(time timestamp NOT NULL, a TEXT, b TEXT);
-SELECT * FROM create_hypertable('test_validate', 'time');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
- hypertable_id | schema_name |  table_name   | created 
----------------+-------------+---------------+---------
-            14 | public      | test_validate | t
-(1 row)
-
-INSERT INTO test_validate values(now(), 'a', 'b');
-ALTER TABLE test_validate
-ADD COLUMN c TEXT,
-ADD CONSTRAINT c_not_null CHECK (c IS NOT NULL) NOT VALID;
-UPDATE test_validate SET c = '';
-ALTER TABLE test_validate
-VALIDATE CONSTRAINT c_not_null;
-DROP TABLE test_validate;
--- test for hypertables constraints both using index tablespaces and not See #2604
-SET client_min_messages = ERROR;
-DROP TABLESPACE IF EXISTS tablespace1;
-SET client_min_messages = NOTICE;
-CREATE TABLESPACE tablespace1 OWNER :ROLE_DEFAULT_PERM_USER LOCATION :TEST_TABLESPACE1_PATH;
-CREATE TABLE fk_tbl (
-id int,
-CONSTRAINT pkfk PRIMARY KEY (id) USING INDEX TABLESPACE tablespace1);
-CREATE TABLE tbl (
-fk_id int,
-id int,
-time timestamp,
-CONSTRAINT pk PRIMARY KEY (time, id) USING INDEX TABLESPACE tablespace1);
-SELECT create_hypertable('tbl', 'time');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
- create_hypertable 
--------------------
- (15,public,tbl,t)
-(1 row)
-
-ALTER TABLE tbl
-ADD CONSTRAINT fk_con
-FOREIGN KEY (fk_id) REFERENCES fk_tbl(id)
-ON UPDATE SET NULL
-ON DELETE SET NULL;
-INSERT INTO fk_tbl VALUES(1);
-INSERT INTO tbl VALUES (
-1, 1, now()
-);
-DROP TABLE tbl;
-DROP TABLE fk_tbl;
-DROP TABLESPACE IF EXISTS tablespace1;
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/copy-15.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/copy-15.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/copy-15.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/copy-15.out	2023-11-25 05:27:33.765052852 +0000
@@ -1,719 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-\o /dev/null
-\ir include/insert_two_partitions.sql
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-CREATE TABLE PUBLIC."two_Partitions" (
-  "timeCustom" BIGINT NOT NULL,
-  device_id TEXT NOT NULL,
-  series_0 DOUBLE PRECISION NULL,
-  series_1 DOUBLE PRECISION NULL,
-  series_2 DOUBLE PRECISION NULL,
-  series_bool BOOLEAN NULL
-);
-CREATE INDEX ON PUBLIC."two_Partitions" (device_id, "timeCustom" DESC NULLS LAST) WHERE device_id IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, series_0) WHERE series_0 IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, series_1)  WHERE series_1 IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, series_2) WHERE series_2 IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, series_bool) WHERE series_bool IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, device_id);
-SELECT * FROM create_hypertable('"public"."two_Partitions"'::regclass, 'timeCustom'::name, 'device_id'::name, associated_schema_name=>'_timescaledb_internal'::text, number_partitions => 2, chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
-\set QUIET off
-BEGIN;
-\COPY public."two_Partitions" FROM 'data/ds1_dev1_1.tsv' NULL AS '';
-COMMIT;
-INSERT INTO public."two_Partitions"("timeCustom", device_id, series_0, series_1) VALUES
-(1257987600000000000, 'dev1', 1.5, 1),
-(1257987600000000000, 'dev1', 1.5, 2),
-(1257894000000000000, 'dev2', 1.5, 1),
-(1257894002000000000, 'dev1', 2.5, 3);
-INSERT INTO "two_Partitions"("timeCustom", device_id, series_0, series_1) VALUES
-(1257894000000000000, 'dev2', 1.5, 2);
-\set QUIET on
-\o
---old chunks
-COPY "two_Partitions"("timeCustom", device_id, series_0, series_1) FROM STDIN DELIMITER ',';
-\copy "two_Partitions"("timeCustom", device_id, series_0, series_1) FROM STDIN DELIMITER ',';
---new chunks
-COPY "two_Partitions"("timeCustom", device_id, series_0, series_1) FROM STDIN DELIMITER ',';
-\copy "two_Partitions"("timeCustom", device_id, series_0, series_1) FROM STDIN DELIMITER ',';
-COPY (SELECT * FROM "two_Partitions" ORDER BY "timeCustom", device_id, series_0, series_1) TO STDOUT;
-1257894000000000000	dev1	1.5	1	2	t
-1257894000000000000	dev1	1.5	2	\N	\N
-1257894000000000000	dev2	1.5	1	\N	\N
-1257894000000000000	dev2	1.5	2	\N	\N
-1257894000000000000	dev3	1.5	2	\N	\N
-1257894000000000000	dev3	1.5	2	\N	\N
-1257894000000001000	dev1	2.5	3	\N	\N
-1257894001000000000	dev1	3.5	4	\N	\N
-1257894002000000000	dev1	2.5	3	\N	\N
-1257894002000000000	dev1	5.5	6	\N	t
-1257894002000000000	dev1	5.5	7	\N	f
-1257897600000000000	dev1	4.5	5	\N	f
-1257987600000000000	dev1	1.5	1	\N	\N
-1257987600000000000	dev1	1.5	2	\N	\N
-2257894000000000000	dev3	1.5	2	\N	\N
-2257894000000000000	dev3	1.5	2	\N	\N
----test hypertable with FK
-CREATE TABLE "meta" ("id" serial PRIMARY KEY);
-CREATE TABLE "hyper" (
-    "meta_id" integer NOT NULL REFERENCES meta(id),
-    "time" bigint NOT NULL,
-    "value" double precision NOT NULL
-);
-SELECT create_hypertable('hyper', 'time', chunk_time_interval => 100);
- create_hypertable  
---------------------
- (2,public,hyper,t)
-(1 row)
-
-INSERT INTO "meta" ("id") values (1);
-\copy hyper (time, meta_id, value) FROM STDIN DELIMITER ',';
-COPY hyper (time, meta_id, value) FROM STDIN DELIMITER ',';
-\set ON_ERROR_STOP 0
-\copy hyper (time, meta_id, value) FROM STDIN DELIMITER ',';
-ERROR:  insert or update on table "_hyper_2_6_chunk" violates foreign key constraint "6_1_hyper_meta_id_fkey"
-COPY hyper (time, meta_id, value) FROM STDIN DELIMITER ',';
-ERROR:  insert or update on table "_hyper_2_6_chunk" violates foreign key constraint "6_1_hyper_meta_id_fkey"
-\set ON_ERROR_STOP 1
-COPY (SELECT * FROM hyper ORDER BY time, meta_id) TO STDOUT;
-1	1	1
-1	2	1
---test that copy works with a low setting for max_open_chunks_per_insert
-set timescaledb.max_open_chunks_per_insert = 1;
-CREATE TABLE "hyper2" (
-    "time" bigint NOT NULL,
-    "value" double precision NOT NULL
-);
-SELECT create_hypertable('hyper2', 'time', chunk_time_interval => 10);
-  create_hypertable  
----------------------
- (3,public,hyper2,t)
-(1 row)
-
-\copy hyper2 from data/copy_data.csv with csv header ;
--- test copy with blocking trigger
-CREATE FUNCTION gt_10() RETURNS trigger AS
-$func$
-BEGIN
-    IF NEW."time" < 11
-        THEN RETURN NULL;
-    END IF;
-    RETURN NEW;
-END
-$func$ LANGUAGE plpgsql;
-CREATE TABLE "trigger_test" (
-    "time" bigint NOT NULL,
-    "value" double precision NOT NULL
-);
-SELECT create_hypertable('trigger_test', 'time', chunk_time_interval => 10);
-     create_hypertable     
----------------------------
- (4,public,trigger_test,t)
-(1 row)
-
-CREATE TRIGGER check_time BEFORE INSERT ON trigger_test
-FOR EACH ROW EXECUTE FUNCTION gt_10();
-\copy trigger_test from data/copy_data.csv with csv header ;
-SELECT * FROM trigger_test ORDER BY time;
- time |       value        
-------+--------------------
-   11 |  0.795640022493899
-   12 |  0.631451691035181
-   13 | 0.0958626130595803
-   14 |  0.929304684977978
-   15 |  0.524866581428796
-   16 |  0.919249163009226
-   17 |  0.878917074296623
-   18 |   0.68551931809634
-   19 |  0.594833800103515
-   20 |  0.819584367796779
-   21 |  0.474171321373433
-   22 |  0.938535195309669
-   23 |  0.333933369256556
-   24 |  0.274582070298493
-   25 |  0.602348630782217
-(15 rows)
-
--- Test that if we copy from stdin to a hypertable and violate a null
--- constraint, it does not crash and generate an appropriate error
--- message.
-CREATE TABLE test(a INT NOT NULL, b TIMESTAMPTZ);
-SELECT create_hypertable('test', 'b');
-NOTICE:  adding not-null constraint to column "b"
- create_hypertable 
--------------------
- (5,public,test,t)
-(1 row)
-
-\set ON_ERROR_STOP 0
-COPY TEST (a,b) FROM STDIN (delimiter ',', null 'N');
-ERROR:  null value in column "a" of relation "_hyper_5_13_chunk" violates not-null constraint
-\set ON_ERROR_STOP 1
-----------------------------------------------------------------
--- Testing COPY TO.
-----------------------------------------------------------------
-\c :TEST_DBNAME :ROLE_SUPERUSER
-SET client_min_messages TO NOTICE;
--- COPY TO using a hypertable will not copy any tuples, but should
--- show a notice.
-COPY hyper TO STDOUT DELIMITER ',';
-NOTICE:  hypertable data are in the chunks, no data will be copied
--- COPY TO using a query should display all the tuples and not show a
--- notice.
-COPY (SELECT * FROM hyper) TO STDOUT DELIMITER ',';
-1,1,1
-1,2,1
-----------------------------------------------------------------
--- Testing multi-buffer optimization.
-----------------------------------------------------------------
-CREATE TABLE "hyper_copy" (
-    "time" bigint NOT NULL,
-    "value" double precision NOT NULL
-);
-SELECT create_hypertable('hyper_copy', 'time', chunk_time_interval => 2);
-    create_hypertable    
--------------------------
- (6,public,hyper_copy,t)
-(1 row)
-
--- First copy call with default client_min_messages, to get rid of the
--- building index "_hyper_XXX_chunk_hyper_copy_time_idx" on table "_hyper_XXX_chunk" serially
--- messages
-\copy hyper_copy FROM data/copy_data.csv WITH csv header;
-SET client_min_messages TO DEBUG1;
-\copy hyper_copy FROM data/copy_data.csv WITH csv header;
-DEBUG:  Using optimized multi-buffer copy operation (CIM_MULTI_CONDITIONAL).
-SELECT count(*) FROM hyper_copy;
- count 
--------
-    50
-(1 row)
-
--- Limit number of open chunks
-SET timescaledb.max_open_chunks_per_insert = 1;
-\copy hyper_copy FROM data/copy_data.csv WITH csv header;
-DEBUG:  Using optimized multi-buffer copy operation (CIM_MULTI_CONDITIONAL).
-SELECT count(*) FROM hyper_copy;
- count 
--------
-    75
-(1 row)
-
--- Before trigger disable the multi-buffer optimization
-CREATE OR REPLACE FUNCTION empty_test_trigger()
-    RETURNS TRIGGER LANGUAGE PLPGSQL AS
-$BODY$
-BEGIN
-    IF TG_OP = 'DELETE' THEN
-        RETURN OLD;
-    END IF;
-    RETURN NEW;
-END
-$BODY$;
--- Before trigger (CIM_SINGLE should be used)
-CREATE TRIGGER hyper_copy_trigger_insert_before
-    BEFORE INSERT ON hyper_copy
-    FOR EACH ROW EXECUTE FUNCTION empty_test_trigger();
-\copy hyper_copy FROM data/copy_data.csv WITH csv header;
-DEBUG:  Using normal unbuffered copy operation (CIM_SINGLE) because triggers are defined on the destination table.
-SELECT count(*) FROM hyper_copy;
- count 
--------
-   100
-(1 row)
-
--- Suppress 'DEBUG:  EventTriggerInvoke XXXX' messages
-RESET client_min_messages;
-DROP TRIGGER hyper_copy_trigger_insert_before ON hyper_copy;
-SET client_min_messages TO DEBUG1;
--- After trigger (CIM_MULTI_CONDITIONAL should be used)
-CREATE TRIGGER hyper_copy_trigger_insert_after
-    AFTER INSERT ON hyper_copy
-    FOR EACH ROW EXECUTE FUNCTION empty_test_trigger();
-\copy hyper_copy FROM data/copy_data.csv WITH csv header;
-DEBUG:  Using optimized multi-buffer copy operation (CIM_MULTI_CONDITIONAL).
-SELECT count(*) FROM hyper_copy;
- count 
--------
-   125
-(1 row)
-
--- Insert data into the chunks in random order
-COPY hyper_copy FROM STDIN DELIMITER ',' NULL AS 'null';
-DEBUG:  Using optimized multi-buffer copy operation (CIM_MULTI_CONDITIONAL).
-SELECT count(*) FROM hyper_copy;
- count 
--------
-   154
-(1 row)
-
-RESET client_min_messages;
-RESET timescaledb.max_open_chunks_per_insert;
-----------------------------------------------------------------
--- Testing multi-buffer optimization
--- (no index on destination hypertable).
-----------------------------------------------------------------
-CREATE TABLE "hyper_copy_noindex" (
-    "time" bigint NOT NULL,
-    "value" double precision NOT NULL
-);
-SELECT create_hypertable('hyper_copy_noindex', 'time', chunk_time_interval => 10, create_default_indexes => false);
-        create_hypertable        
----------------------------------
- (7,public,hyper_copy_noindex,t)
-(1 row)
-
--- No trigger
-\copy hyper_copy_noindex FROM data/copy_data.csv WITH csv header;
-SET client_min_messages TO DEBUG1;
-\copy hyper_copy_noindex FROM data/copy_data.csv WITH csv header;
-DEBUG:  Using optimized multi-buffer copy operation (CIM_MULTI_CONDITIONAL).
-RESET client_min_messages;
-SELECT count(*) FROM hyper_copy_noindex;
- count 
--------
-    50
-(1 row)
-
--- Before trigger (CIM_SINGLE should be used)
-CREATE TRIGGER hyper_copy_trigger_insert_before
-    BEFORE INSERT ON hyper_copy_noindex
-    FOR EACH ROW EXECUTE FUNCTION empty_test_trigger();
-\copy hyper_copy_noindex FROM data/copy_data.csv WITH csv header;
-SET client_min_messages TO DEBUG1;
-\copy hyper_copy_noindex FROM data/copy_data.csv WITH csv header;
-DEBUG:  Using normal unbuffered copy operation (CIM_SINGLE) because triggers are defined on the destination table.
-RESET client_min_messages;
-SELECT count(*) FROM hyper_copy_noindex;
- count 
--------
-   100
-(1 row)
-
--- After trigger (CIM_MULTI_CONDITIONAL should be used)
-DROP TRIGGER hyper_copy_trigger_insert_before ON hyper_copy_noindex;
-CREATE TRIGGER hyper_copy_trigger_insert_after
-    AFTER INSERT ON hyper_copy_noindex
-    FOR EACH ROW EXECUTE FUNCTION empty_test_trigger();
-\copy hyper_copy_noindex FROM data/copy_data.csv WITH csv header;
-SET client_min_messages TO DEBUG1;
-\copy hyper_copy_noindex FROM data/copy_data.csv WITH csv header;
-DEBUG:  Using optimized multi-buffer copy operation (CIM_MULTI_CONDITIONAL).
-RESET client_min_messages;
-SELECT count(*) FROM hyper_copy_noindex;
- count 
--------
-   150
-(1 row)
-
-----------------------------------------------------------------
--- Testing multi-buffer optimization
--- (more chunks than MAX_PARTITION_BUFFERS).
-----------------------------------------------------------------
-CREATE TABLE "hyper_copy_large" (
-    "time" timestamp NOT NULL,
-    "value" double precision NOT NULL
-);
--- Genate data that will create more than 32 (MAX_PARTITION_BUFFERS)
--- chunks on the 10 second chunk_time_interval partitioned hypertable.
-INSERT INTO hyper_copy_large
-SELECT time,
-random() AS value
-FROM
-generate_series('2022-01-01', '2022-01-31', INTERVAL '1 hour') AS g1(time)
-ORDER BY time;
-SELECT COUNT(*) FROM hyper_copy_large;
- count 
--------
-   721
-(1 row)
-
--- Migrate data to chunks by using copy
-SELECT create_hypertable('hyper_copy_large', 'time',
-   chunk_time_interval => INTERVAL '1 hour', migrate_data => 'true');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  migrating data to chunks
-       create_hypertable       
--------------------------------
- (8,public,hyper_copy_large,t)
-(1 row)
-
-SELECT COUNT(*) FROM hyper_copy_large;
- count 
--------
-   721
-(1 row)
-
-----------------------------------------------------------------
--- Testing multi-buffer optimization
--- (triggers on chunks).
-----------------------------------------------------------------
-CREATE TABLE "table_with_chunk_trigger" (
-    "time" bigint NOT NULL,
-    "value" double precision NOT NULL
-);
--- This trigger counts the already inserted tuples in
--- the table table_with_chunk_trigger.
-CREATE OR REPLACE FUNCTION count_test_chunk_trigger()
-    RETURNS TRIGGER LANGUAGE PLPGSQL AS
-$BODY$
-DECLARE
-    cnt INTEGER;
-BEGIN
-    SELECT count(*) FROM table_with_chunk_trigger INTO cnt;
-    RAISE WARNING 'Trigger counted % tuples in table table_with_chunk_trigger', cnt;
-    IF TG_OP = 'DELETE' THEN
-        RETURN OLD;
-    END IF;
-    RETURN NEW;
-END
-$BODY$;
--- Create hypertable and chunks
-SELECT create_hypertable('table_with_chunk_trigger', 'time', chunk_time_interval => 1);
-           create_hypertable           
----------------------------------------
- (9,public,table_with_chunk_trigger,t)
-(1 row)
-
--- Insert data to create all missing chunks
-\copy table_with_chunk_trigger from data/copy_data.csv with csv header;
-SELECT count(*) FROM table_with_chunk_trigger;
- count 
--------
-    25
-(1 row)
-
--- Chunk 1: 1-2, Chunk 2: 2-3, Chunk 3: 3-4, Chunk 4: 4-5
-SELECT chunk_schema, chunk_name FROM timescaledb_information.chunks
-    WHERE hypertable_name = 'table_with_chunk_trigger' AND range_end_integer=5 \gset
--- Create before trigger on the 4th chunk
-CREATE TRIGGER table_with_chunk_trigger_before_trigger
-    BEFORE INSERT ON :chunk_schema.:chunk_name
-    FOR EACH ROW EXECUTE FUNCTION count_test_chunk_trigger();
--- Insert data
--- 25 tuples are already imported. The trigger is executed before tuples
--- are copied into the 4th chunk. So, the trigger should report 25+3 = 28
--- This test requires that the multi-insert buffers of the other chunks
--- are flushed before the trigger is executed.
-SET client_min_messages TO DEBUG1;
-\copy table_with_chunk_trigger FROM data/copy_data.csv WITH csv header;
-DEBUG:  Using optimized multi-buffer copy operation (CIM_MULTI_CONDITIONAL).
-WARNING:  Trigger counted 28 tuples in table table_with_chunk_trigger
-RESET client_min_messages;
-SELECT count(*) FROM table_with_chunk_trigger;
- count 
--------
-    50
-(1 row)
-
-DROP TRIGGER table_with_chunk_trigger_before_trigger ON :chunk_schema.:chunk_name;
--- Create after trigger
-CREATE TRIGGER table_with_chunk_trigger_after_trigger
-    AFTER INSERT ON :chunk_schema.:chunk_name
-    FOR EACH ROW EXECUTE FUNCTION count_test_chunk_trigger();
--- Insert data
--- 50 tuples are already imported. The trigger is executed after all
--- tuples are imported. So, the trigger should report 50+25 = 75
-SET client_min_messages TO DEBUG1;
-\copy table_with_chunk_trigger FROM data/copy_data.csv WITH csv header;
-DEBUG:  Using optimized multi-buffer copy operation (CIM_MULTI_CONDITIONAL).
-WARNING:  Trigger counted 75 tuples in table table_with_chunk_trigger
-RESET client_min_messages;
-SELECT count(*) FROM table_with_chunk_trigger;
- count 
--------
-    75
-(1 row)
-
--- Hypertable with after row trigger and no index
-DROP TABLE table_with_chunk_trigger;
-CREATE TABLE "table_with_chunk_trigger" (
-    "time" bigint NOT NULL,
-    "value" double precision NOT NULL
-);
--- Create hypertable and chunks
-SELECT create_hypertable('table_with_chunk_trigger', 'time', chunk_time_interval => 1, create_default_indexes => false);
-           create_hypertable            
-----------------------------------------
- (10,public,table_with_chunk_trigger,t)
-(1 row)
-
--- Insert data to create all missing chunks
-\copy table_with_chunk_trigger from data/copy_data.csv with csv header;
-SELECT count(*) FROM table_with_chunk_trigger;
- count 
--------
-    25
-(1 row)
-
--- Chunk 1: 1-2, Chunk 2: 2-3, Chunk 3: 3-4, Chunk 4: 4-5
-SELECT chunk_schema, chunk_name FROM timescaledb_information.chunks
-    WHERE hypertable_name = 'table_with_chunk_trigger' AND range_end_integer=5 \gset
--- Create after trigger
-CREATE TRIGGER table_with_chunk_trigger_after_trigger
-    AFTER INSERT ON :chunk_schema.:chunk_name
-    FOR EACH ROW EXECUTE FUNCTION count_test_chunk_trigger();
-\copy table_with_chunk_trigger from data/copy_data.csv with csv header;
-WARNING:  Trigger counted 50 tuples in table table_with_chunk_trigger
-SELECT count(*) FROM table_with_chunk_trigger;
- count 
--------
-    50
-(1 row)
-
-----------------------------------------------------------------
--- Testing multi-buffer optimization
--- (Hypertable without before insert trigger)
-----------------------------------------------------------------
-CREATE TABLE "table_without_bf_trigger" (
-    "time" bigint NOT NULL,
-    "value" double precision NOT NULL
-);
-SELECT create_hypertable('table_without_bf_trigger', 'time', chunk_time_interval => 1);
-           create_hypertable            
-----------------------------------------
- (11,public,table_without_bf_trigger,t)
-(1 row)
-
--- Drop the default insert block trigger
-DROP TRIGGER ts_insert_blocker ON table_without_bf_trigger;
-\copy table_without_bf_trigger from data/copy_data.csv with csv header;
-SET client_min_messages TO DEBUG1;
-\copy table_without_bf_trigger from data/copy_data.csv with csv header;
-DEBUG:  Using optimized multi-buffer copy operation (CIM_MULTI_CONDITIONAL).
-RESET client_min_messages;
-SELECT count(*) FROM table_without_bf_trigger;
- count 
--------
-    50
-(1 row)
-
--- After trigger (CIM_MULTI_CONDITIONAL should be used)
-CREATE TRIGGER table_with_chunk_trigger_after_trigger
-    AFTER INSERT ON table_without_bf_trigger
-    FOR EACH ROW EXECUTE FUNCTION empty_test_trigger();
-SET client_min_messages TO DEBUG1;
-\copy table_without_bf_trigger from data/copy_data.csv with csv header;
-DEBUG:  Using optimized multi-buffer copy operation (CIM_MULTI_CONDITIONAL).
-RESET client_min_messages;
-SELECT count(*) FROM table_without_bf_trigger;
- count 
--------
-    75
-(1 row)
-
-----------------------------------------------------------------
--- Testing multi-buffer optimization
--- (Chunks with different layouts)
-----------------------------------------------------------------
--- Time is not the first attribute of the hypertable
-CREATE TABLE "table_with_layout_change" (
-    "value1" real NOT NULL DEFAULT 1,
-    "value2" smallint DEFAULT NULL,
-    "value3" bigint DEFAULT NULL,
-    "time" bigint NOT NULL,
-    "value4" double precision NOT NULL DEFAULT 4,
-    "value5" double precision NOT NULL DEFAULT 5
-);
-SELECT create_hypertable('table_with_layout_change', 'time', chunk_time_interval => 1);
-           create_hypertable            
-----------------------------------------
- (12,public,table_with_layout_change,t)
-(1 row)
-
--- Chunk 1 (time = 1)
-COPY table_with_layout_change FROM STDIN DELIMITER ',' NULL AS 'null';
-SELECT * FROM table_with_layout_change;
- value1 | value2 | value3 | time | value4 | value5 
---------+--------+--------+------+--------+--------
-    100 |    200 |    300 |    1 |    400 |    500
-(1 row)
-
--- Drop the first attribute
-ALTER TABLE table_with_layout_change DROP COLUMN value1;
-SELECT * FROM table_with_layout_change;
- value2 | value3 | time | value4 | value5 
---------+--------+------+--------+--------
-    200 |    300 |    1 |    400 |    500
-(1 row)
-
--- COPY into existing chunk (time = 1)
-COPY table_with_layout_change FROM STDIN DELIMITER ',' NULL AS 'null';
--- Create new chunk (time = 2)
-COPY table_with_layout_change FROM STDIN DELIMITER ',' NULL AS 'null';
-SELECT * FROM table_with_layout_change ORDER BY time, value2, value3, value4, value5;
- value2 | value3 | time | value4 | value5 
---------+--------+------+--------+--------
-    200 |    300 |    1 |    400 |    500
-    201 |    301 |    1 |    401 |    501
-    202 |    302 |    2 |    402 |    502
-(3 rows)
-
--- Create new chunk (time = 2), insert in different order
-COPY table_with_layout_change (time, value5, value4, value3, value2) FROM STDIN DELIMITER ',' NULL AS 'null';
-COPY table_with_layout_change (value5, value4, value3, value2, time) FROM STDIN DELIMITER ',' NULL AS 'null';
-COPY table_with_layout_change (value5, value4, value3, time, value2) FROM STDIN DELIMITER ',' NULL AS 'null';
-SELECT * FROM table_with_layout_change ORDER BY time, value2, value3, value4, value5;
- value2 | value3 | time | value4 | value5 
---------+--------+------+--------+--------
-    200 |    300 |    1 |    400 |    500
-    201 |    301 |    1 |    401 |    501
-    202 |    302 |    2 |    402 |    502
-    203 |    303 |    2 |    403 |    503
-    204 |    304 |    2 |    404 |    504
-    205 |    305 |    2 |    405 |    505
-(6 rows)
-
--- Drop the last attribute and add a new one
-ALTER TABLE table_with_layout_change DROP COLUMN value5;
-ALTER TABLE table_with_layout_change ADD COLUMN value6 double precision NOT NULL default 600;
-SELECT * FROM table_with_layout_change ORDER BY time, value2, value3, value4, value6;
- value2 | value3 | time | value4 | value6 
---------+--------+------+--------+--------
-    200 |    300 |    1 |    400 |    600
-    201 |    301 |    1 |    401 |    600
-    202 |    302 |    2 |    402 |    600
-    203 |    303 |    2 |    403 |    600
-    204 |    304 |    2 |    404 |    600
-    205 |    305 |    2 |    405 |    600
-(6 rows)
-
--- COPY in first chunk (time = 1)
-COPY table_with_layout_change (time, value2, value3, value4, value6) FROM STDIN DELIMITER ',' NULL AS 'null';
--- COPY in second chunk (time = 2)
-COPY table_with_layout_change (time, value2, value3, value4, value6) FROM STDIN DELIMITER ',' NULL AS 'null';
--- COPY in new chunk (time = 3)
-COPY table_with_layout_change (time, value2, value3, value4, value6) FROM STDIN DELIMITER ',' NULL AS 'null';
--- COPY in all chunks, different attribute order
-COPY table_with_layout_change (value3, value4, time, value6, value2) FROM STDIN DELIMITER ',' NULL AS 'null';
-SELECT * FROM table_with_layout_change ORDER BY time, value2, value3, value4, value6;
- value2 | value3 | time | value4 | value6 
---------+--------+------+--------+--------
-    200 |    300 |    1 |    400 |    600
-    201 |    301 |    1 |    401 |    600
-    206 |    306 |    1 |    406 |    606
-    211 |    311 |    1 |    411 |    611
-    202 |    302 |    2 |    402 |    600
-    203 |    303 |    2 |    403 |    600
-    204 |    304 |    2 |    404 |    600
-    205 |    305 |    2 |    405 |    600
-    207 |    307 |    2 |    407 |    607
-    210 |    310 |    2 |    410 |    610
-    208 |    308 |    3 |    408 |    608
-    209 |    309 |    3 |    409 |    609
-(12 rows)
-
--- Drop first column
-ALTER TABLE table_with_layout_change DROP COLUMN value2;
-SELECT * FROM table_with_layout_change ORDER BY time, value3, value4, value6;
- value3 | time | value4 | value6 
---------+------+--------+--------
-    300 |    1 |    400 |    600
-    301 |    1 |    401 |    600
-    306 |    1 |    406 |    606
-    311 |    1 |    411 |    611
-    302 |    2 |    402 |    600
-    303 |    2 |    403 |    600
-    304 |    2 |    404 |    600
-    305 |    2 |    405 |    600
-    307 |    2 |    407 |    607
-    310 |    2 |    410 |    610
-    308 |    3 |    408 |    608
-    309 |    3 |    409 |    609
-(12 rows)
-
--- COPY in all exiting chunks and create a new one (time 4)
-COPY table_with_layout_change (value3, value4, time, value6) FROM STDIN DELIMITER ',' NULL AS 'null';
-SELECT * FROM table_with_layout_change ORDER BY time, value3, value4, value6;
- value3 | time | value4 | value6 
---------+------+--------+--------
-    300 |    1 |    400 |    600
-    301 |    1 |    401 |    600
-    306 |    1 |    406 |    606
-    311 |    1 |    411 |    611
-    315 |    1 |    415 |    615
-    302 |    2 |    402 |    600
-    303 |    2 |    403 |    600
-    304 |    2 |    404 |    600
-    305 |    2 |    405 |    600
-    307 |    2 |    407 |    607
-    310 |    2 |    410 |    610
-    313 |    2 |    413 |    613
-    308 |    3 |    408 |    608
-    309 |    3 |    409 |    609
-    312 |    3 |    412 |    612
-    314 |    4 |    414 |    614
-(16 rows)
-
--- Drop the last two columns
-ALTER TABLE table_with_layout_change DROP COLUMN value4;
-ALTER TABLE table_with_layout_change DROP COLUMN value6;
--- COPY in all exiting chunks and create a new one (time 5)
-COPY table_with_layout_change (value3, time) FROM STDIN DELIMITER ',' NULL AS 'null';
-SELECT * FROM table_with_layout_change ORDER BY time, value3;
- value3 | time 
---------+------
-    300 |    1
-    301 |    1
-    306 |    1
-    311 |    1
-    315 |    1
-    317 |    1
-    302 |    2
-    303 |    2
-    304 |    2
-    305 |    2
-    307 |    2
-    310 |    2
-    313 |    2
-    316 |    2
-    308 |    3
-    309 |    3
-    312 |    3
-    318 |    3
-    314 |    4
-    320 |    4
-    319 |    5
-(21 rows)
-
--- Drop the last of the initial attributes and add a new one
-ALTER TABLE table_with_layout_change DROP COLUMN value3;
-ALTER TABLE table_with_layout_change ADD COLUMN value7 double precision NOT NULL default 700;
--- COPY in all exiting chunks and create a new one (time 6)
-COPY table_with_layout_change (value7, time) FROM STDIN DELIMITER ',' NULL AS 'null';
-SELECT * FROM table_with_layout_change ORDER BY time, value7;
- time | value7 
-------+--------
-    1 |    700
-    1 |    700
-    1 |    700
-    1 |    700
-    1 |    700
-    1 |    700
-    1 |    722
-    2 |    700
-    2 |    700
-    2 |    700
-    2 |    700
-    2 |    700
-    2 |    700
-    2 |    700
-    2 |    700
-    2 |    721
-    3 |    700
-    3 |    700
-    3 |    700
-    3 |    700
-    3 |    723
-    4 |    700
-    4 |    700
-    4 |    726
-    5 |    700
-    5 |    724
-    6 |    725
-(27 rows)
-
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/create_chunks.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/create_chunks.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/create_chunks.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/create_chunks.out	2023-11-25 05:27:33.781052806 +0000
@@ -1,223 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
---
---  This test will create chunks in two dimenisions, time (x) and
---  space (y), where the time dimension is aligned. The figure below
---  shows the expected result. The chunk number in the figure
---  indicates the creation order.
---
---  +
---  +
---  +     +-----+     +-----+
---  +     |  2  |     |  3  |
---  +     |     +---+-+     |
---  +     +-----+ 5 |6+-----+
---  +     |  1  +---+-+-----+     +---------+
---  +     |     |   |4|  7  |     |    8    |
---  +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-
---  0         5         10        15        20
---
--- Partitioning:
---
--- Chunk #  |  time  | space
---    1     |   3    |   2
---    4     |   1    |   3
---    5     |   5    |   3
---
-CREATE TABLE chunk_test(time integer, temp float8, tag integer, color integer);
-SELECT create_hypertable('chunk_test', 'time', 'tag', 2, chunk_time_interval => 3);
-NOTICE:  adding not-null constraint to column "time"
-    create_hypertable    
--------------------------
- (1,public,chunk_test,t)
-(1 row)
-
-INSERT INTO chunk_test VALUES (4, 24.3, 1, 1);
-SELECT * FROM _timescaledb_catalog.dimension_slice;
- id | dimension_id |     range_start      | range_end  
-----+--------------+----------------------+------------
-  1 |            1 |                    3 |          6
-  2 |            2 | -9223372036854775808 | 1073741823
-(2 rows)
-
-INSERT INTO chunk_test VALUES (4, 24.3, 2, 1);
-INSERT INTO chunk_test VALUES (10, 24.3, 2, 1);
-SELECT c.table_name AS chunk_name, d.id AS dimension_id, ds.id AS slice_id, range_start, range_end FROM _timescaledb_catalog.chunk c
-LEFT JOIN _timescaledb_catalog.chunk_constraint cc ON (c.id = cc.chunk_id)
-LEFT JOIN _timescaledb_catalog.dimension_slice ds ON (ds.id = cc.dimension_slice_id)
-LEFT JOIN _timescaledb_catalog.dimension d ON (d.id = ds.dimension_id)
-LEFT JOIN _timescaledb_catalog.hypertable h ON (d.hypertable_id = h.id)
-WHERE h.schema_name = 'public' AND h.table_name = 'chunk_test'
-ORDER BY c.id, d.id;
-    chunk_name    | dimension_id | slice_id |     range_start      |      range_end      
-------------------+--------------+----------+----------------------+---------------------
- _hyper_1_1_chunk |            1 |        1 |                    3 |                   6
- _hyper_1_1_chunk |            2 |        2 | -9223372036854775808 |          1073741823
- _hyper_1_2_chunk |            1 |        1 |                    3 |                   6
- _hyper_1_2_chunk |            2 |        3 |           1073741823 | 9223372036854775807
- _hyper_1_3_chunk |            1 |        4 |                    9 |                  12
- _hyper_1_3_chunk |            2 |        3 |           1073741823 | 9223372036854775807
-(6 rows)
-
-\c :TEST_DBNAME :ROLE_SUPERUSER
-SELECT set_number_partitions('chunk_test', 3);
- set_number_partitions 
------------------------
- 
-(1 row)
-
-\c :TEST_DBNAME :ROLE_DEFAULT_PERM_USER
-SELECT set_chunk_time_interval('chunk_test', 1::bigint);
- set_chunk_time_interval 
--------------------------
- 
-(1 row)
-
-INSERT INTO chunk_test VALUES (8, 24.3, 11233, 1);
-SELECT set_chunk_time_interval('chunk_test', 5::bigint);
- set_chunk_time_interval 
--------------------------
- 
-(1 row)
-
-SELECT * FROM _timescaledb_catalog.dimension;
- id | hypertable_id | column_name | column_type | aligned | num_slices | partitioning_func_schema | partitioning_func  | interval_length | compress_interval_length | integer_now_func_schema | integer_now_func 
-----+---------------+-------------+-------------+---------+------------+--------------------------+--------------------+-----------------+--------------------------+-------------------------+------------------
-  2 |             1 | tag         | integer     | f       |          3 | _timescaledb_internal    | get_partition_hash |                 |                          |                         | 
-  1 |             1 | time        | integer     | t       |            |                          |                    |               5 |                          |                         | 
-(2 rows)
-
-INSERT INTO chunk_test VALUES (7, 24.3, 79669, 1);
-INSERT INTO chunk_test VALUES (8, 24.3, 79669, 1);
-INSERT INTO chunk_test VALUES (10, 24.3, 11233, 1);
-INSERT INTO chunk_test VALUES (16, 24.3, 11233, 1);
-SELECT c.table_name AS chunk_name, d.id AS dimension_id, ds.id AS slice_id, range_start, range_end FROM _timescaledb_catalog.chunk c
-LEFT JOIN _timescaledb_catalog.chunk_constraint cc ON (c.id = cc.chunk_id)
-LEFT JOIN _timescaledb_catalog.dimension_slice ds ON (ds.id = cc.dimension_slice_id)
-LEFT JOIN _timescaledb_catalog.dimension d ON (d.id = ds.dimension_id)
-LEFT JOIN _timescaledb_catalog.hypertable h ON (d.hypertable_id = h.id)
-WHERE h.schema_name = 'public' AND h.table_name = 'chunk_test'
-ORDER BY c.id, d.id;
-    chunk_name    | dimension_id | slice_id |     range_start      |      range_end      
-------------------+--------------+----------+----------------------+---------------------
- _hyper_1_1_chunk |            1 |        1 |                    3 |                   6
- _hyper_1_1_chunk |            2 |        2 | -9223372036854775808 |          1073741823
- _hyper_1_2_chunk |            1 |        1 |                    3 |                   6
- _hyper_1_2_chunk |            2 |        3 |           1073741823 | 9223372036854775807
- _hyper_1_3_chunk |            1 |        4 |                    9 |                  12
- _hyper_1_3_chunk |            2 |        3 |           1073741823 | 9223372036854775807
- _hyper_1_4_chunk |            1 |        5 |                    8 |                   9
- _hyper_1_4_chunk |            2 |        6 | -9223372036854775808 |           715827882
- _hyper_1_5_chunk |            1 |        7 |                    6 |                   8
- _hyper_1_5_chunk |            2 |        8 |            715827882 |          1431655764
- _hyper_1_6_chunk |            1 |        5 |                    8 |                   9
- _hyper_1_6_chunk |            2 |        8 |            715827882 |          1431655764
- _hyper_1_7_chunk |            1 |        4 |                    9 |                  12
- _hyper_1_7_chunk |            2 |        6 | -9223372036854775808 |           715827882
- _hyper_1_8_chunk |            1 |        9 |                   15 |                  20
- _hyper_1_8_chunk |            2 |        6 | -9223372036854775808 |           715827882
-(16 rows)
-
---test the edges of an open partition -- INT_64_MAX and INT_64_MIN.
-CREATE TABLE chunk_test_ends(time bigint, temp float8, tag integer, color integer);
-SELECT create_hypertable('chunk_test_ends', 'time', chunk_time_interval => 5);
-NOTICE:  adding not-null constraint to column "time"
-      create_hypertable       
-------------------------------
- (2,public,chunk_test_ends,t)
-(1 row)
-
-INSERT INTO chunk_test_ends VALUES ((-9223372036854775808)::bigint, 23.1, 11233, 1);
-INSERT INTO chunk_test_ends VALUES (9223372036854775807::bigint, 24.1, 11233, 1);
---try to hit cache
-INSERT INTO chunk_test_ends VALUES (9223372036854775807::bigint, 24.2, 11233, 1);
-INSERT INTO chunk_test_ends VALUES (9223372036854775807::bigint, 24.3, 11233, 1), (9223372036854775807::bigint, 24.4, 11233, 1);
-INSERT INTO chunk_test_ends VALUES ((-9223372036854775808)::bigint, 23.2, 11233, 1);
-INSERT INTO chunk_test_ends VALUES ((-9223372036854775808)::bigint, 23.3, 11233, 1), ((-9223372036854775808)::bigint, 23.4, 11233, 1);
-SELECT * FROM chunk_test_ends ORDER BY time asc, tag, temp;
-         time         | temp |  tag  | color 
-----------------------+------+-------+-------
- -9223372036854775808 | 23.1 | 11233 |     1
- -9223372036854775808 | 23.2 | 11233 |     1
- -9223372036854775808 | 23.3 | 11233 |     1
- -9223372036854775808 | 23.4 | 11233 |     1
-  9223372036854775807 | 24.1 | 11233 |     1
-  9223372036854775807 | 24.2 | 11233 |     1
-  9223372036854775807 | 24.3 | 11233 |     1
-  9223372036854775807 | 24.4 | 11233 |     1
-(8 rows)
-
---further tests of set_chunk_time_interval
-CREATE TABLE chunk_test2(time TIMESTAMPTZ, temp float8, tag integer, color integer);
-SELECT create_hypertable('chunk_test2', 'time', 'tag', 2, chunk_time_interval => 3);
-WARNING:  unexpected interval: smaller than one second
-NOTICE:  adding not-null constraint to column "time"
-    create_hypertable     
---------------------------
- (3,public,chunk_test2,t)
-(1 row)
-
-SELECT interval_length
-FROM _timescaledb_catalog.dimension d
-LEFT JOIN _timescaledb_catalog.hypertable h ON (d.hypertable_id = h.id)
-WHERE h.schema_name = 'public' AND h.table_name = 'chunk_test2'
-ORDER BY d.id;
- interval_length 
------------------
-               3
-                
-(2 rows)
-
--- should work since time column is non-INT
-SELECT set_chunk_time_interval('chunk_test2', INTERVAL '1 minute');
- set_chunk_time_interval 
--------------------------
- 
-(1 row)
-
-SELECT interval_length
-FROM _timescaledb_catalog.dimension d
-LEFT JOIN _timescaledb_catalog.hypertable h ON (d.hypertable_id = h.id)
-WHERE h.schema_name = 'public' AND h.table_name = 'chunk_test2'
-ORDER BY d.id;
- interval_length 
------------------
-        60000000
-                
-(2 rows)
-
--- should still work for non-INT time columns
-SELECT set_chunk_time_interval('chunk_test2', 1000000);
- set_chunk_time_interval 
--------------------------
- 
-(1 row)
-
-SELECT interval_length
-FROM _timescaledb_catalog.dimension d
-LEFT JOIN _timescaledb_catalog.hypertable h ON (d.hypertable_id = h.id)
-WHERE h.schema_name = 'public' AND h.table_name = 'chunk_test2'
-ORDER BY d.id;
- interval_length 
------------------
-         1000000
-                
-(2 rows)
-
-\set ON_ERROR_STOP 0
-select set_chunk_time_interval(NULL,NULL::interval);
-ERROR:  hypertable cannot be NULL
--- should fail since time column is an int
-SELECT set_chunk_time_interval('chunk_test', INTERVAL '1 minute');
-ERROR:  invalid interval type for integer dimension
--- should fail since its not a valid way to represent time
-SELECT set_chunk_time_interval('chunk_test', 'foo'::TEXT);
-ERROR:  invalid interval type for integer dimension
-SELECT set_chunk_time_interval('chunk_test', NULL::BIGINT);
-ERROR:  invalid interval: an explicit interval must be specified
-SELECT set_chunk_time_interval('chunk_test2', NULL::BIGINT);
-ERROR:  invalid interval: an explicit interval must be specified
-SELECT set_chunk_time_interval('chunk_test2', NULL::INTERVAL);
-ERROR:  invalid interval: an explicit interval must be specified
-\set ON_ERROR_STOP 1
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/create_hypertable.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/create_hypertable.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/create_hypertable.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/create_hypertable.out	2023-11-25 05:27:33.781052806 +0000
@@ -1,1008 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-\c :TEST_DBNAME :ROLE_SUPERUSER
-create schema test_schema AUTHORIZATION :ROLE_DEFAULT_PERM_USER;
-create schema chunk_schema AUTHORIZATION :ROLE_DEFAULT_PERM_USER_2;
-SET ROLE :ROLE_DEFAULT_PERM_USER;
-create table test_schema.test_table(time BIGINT, temp float8, device_id text, device_type text, location text, id int, id2 int);
-\set ON_ERROR_STOP 0
--- get_create_command should fail since hypertable isn't made yet
-SELECT * FROM _timescaledb_internal.get_create_command('test_table');
-ERROR:  hypertable "test_table" not found
-\set ON_ERROR_STOP 1
-\dt "test_schema".*
-                  List of relations
-   Schema    |    Name    | Type  |       Owner       
--------------+------------+-------+-------------------
- test_schema | test_table | table | default_perm_user
-(1 row)
-
-create table test_schema.test_table_no_not_null(time BIGINT, device_id text);
-\set ON_ERROR_STOP 0
--- Permission denied with unprivileged role
-SET ROLE :ROLE_DEFAULT_PERM_USER_2;
-select * from create_hypertable('test_schema.test_table_no_not_null', 'time', 'device_id', 2, chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
-ERROR:  permission denied for schema test_schema at character 33
--- CREATE on schema is not enough
-SET ROLE :ROLE_DEFAULT_PERM_USER;
-GRANT ALL ON SCHEMA test_schema TO :ROLE_DEFAULT_PERM_USER_2;
-SET ROLE :ROLE_DEFAULT_PERM_USER_2;
-select * from create_hypertable('test_schema.test_table_no_not_null', 'time', 'device_id', 2, chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
-ERROR:  must be owner of hypertable "test_table_no_not_null"
-\set ON_ERROR_STOP 1
--- Should work with when granted table owner role
-RESET ROLE;
-GRANT :ROLE_DEFAULT_PERM_USER TO :ROLE_DEFAULT_PERM_USER_2;
-SET ROLE :ROLE_DEFAULT_PERM_USER_2;
-select * from create_hypertable('test_schema.test_table_no_not_null', 'time', 'device_id', 2, chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
-NOTICE:  adding not-null constraint to column "time"
- hypertable_id | schema_name |       table_name       | created 
----------------+-------------+------------------------+---------
-             1 | test_schema | test_table_no_not_null | t
-(1 row)
-
-\set ON_ERROR_STOP 0
-insert into test_schema.test_table_no_not_null (device_id) VALUES('foo');
-ERROR:  NULL value in column "time" violates not-null constraint
-\set ON_ERROR_STOP 1
-insert into test_schema.test_table_no_not_null (time, device_id) VALUES(1, 'foo');
-RESET ROLE;
-SET ROLE :ROLE_DEFAULT_PERM_USER;
-\set ON_ERROR_STOP 0
--- No permissions on associated schema should fail
-select * from create_hypertable('test_schema.test_table', 'time', 'device_id', 2, chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'), associated_schema_name => 'chunk_schema');
-ERROR:  permissions denied: cannot create chunks in schema "chunk_schema"
-\set ON_ERROR_STOP 1
--- Granting permissions on chunk_schema should make things work
-RESET ROLE;
-GRANT CREATE ON SCHEMA chunk_schema TO :ROLE_DEFAULT_PERM_USER;
-SET ROLE :ROLE_DEFAULT_PERM_USER;
-select * from create_hypertable('test_schema.test_table', 'time', 'device_id', 2, chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'), associated_schema_name => 'chunk_schema');
-NOTICE:  adding not-null constraint to column "time"
- hypertable_id | schema_name | table_name | created 
----------------+-------------+------------+---------
-             2 | test_schema | test_table | t
-(1 row)
-
--- Check that the insert block trigger exists
-SELECT * FROM test.show_triggers('test_schema.test_table');
-      Trigger      | Type |               Function               
--------------------+------+--------------------------------------
- ts_insert_blocker |    7 | _timescaledb_internal.insert_blocker
-(1 row)
-
-SELECT * FROM _timescaledb_internal.get_create_command('test_table');
-                                                                get_create_command                                                                
---------------------------------------------------------------------------------------------------------------------------------------------------
- SELECT create_hypertable('test_schema.test_table', 'time', 'device_id', 2, chunk_time_interval => 2592000000000, create_default_indexes=>FALSE);
-(1 row)
-
---test adding one more closed dimension
-select add_dimension('test_schema.test_table', 'location', 4);
-             add_dimension             
----------------------------------------
- (5,test_schema,test_table,location,t)
-(1 row)
-
-select * from _timescaledb_catalog.hypertable where table_name = 'test_table';
- id | schema_name | table_name | associated_schema_name | associated_table_prefix | num_dimensions | chunk_sizing_func_schema |  chunk_sizing_func_name  | chunk_target_size | compression_state | compressed_hypertable_id | replication_factor 
-----+-------------+------------+------------------------+-------------------------+----------------+--------------------------+--------------------------+-------------------+-------------------+--------------------------+--------------------
-  2 | test_schema | test_table | chunk_schema           | _hyper_2                |              3 | _timescaledb_internal    | calculate_chunk_interval |                 0 |                 0 |                          |                   
-(1 row)
-
-select * from _timescaledb_catalog.dimension;
- id | hypertable_id | column_name | column_type | aligned | num_slices | partitioning_func_schema | partitioning_func  | interval_length | compress_interval_length | integer_now_func_schema | integer_now_func 
-----+---------------+-------------+-------------+---------+------------+--------------------------+--------------------+-----------------+--------------------------+-------------------------+------------------
-  1 |             1 | time        | bigint      | t       |            |                          |                    |   2592000000000 |                          |                         | 
-  2 |             1 | device_id   | text        | f       |          2 | _timescaledb_internal    | get_partition_hash |                 |                          |                         | 
-  3 |             2 | time        | bigint      | t       |            |                          |                    |   2592000000000 |                          |                         | 
-  4 |             2 | device_id   | text        | f       |          2 | _timescaledb_internal    | get_partition_hash |                 |                          |                         | 
-  5 |             2 | location    | text        | f       |          4 | _timescaledb_internal    | get_partition_hash |                 |                          |                         | 
-(5 rows)
-
---test that we can change the number of partitions and that 1 is allowed
-SELECT set_number_partitions('test_schema.test_table', 1, 'location');
- set_number_partitions 
------------------------
- 
-(1 row)
-
-select * from _timescaledb_catalog.dimension WHERE column_name = 'location';
- id | hypertable_id | column_name | column_type | aligned | num_slices | partitioning_func_schema | partitioning_func  | interval_length | compress_interval_length | integer_now_func_schema | integer_now_func 
-----+---------------+-------------+-------------+---------+------------+--------------------------+--------------------+-----------------+--------------------------+-------------------------+------------------
-  5 |             2 | location    | text        | f       |          1 | _timescaledb_internal    | get_partition_hash |                 |                          |                         | 
-(1 row)
-
-SELECT set_number_partitions('test_schema.test_table', 2, 'location');
- set_number_partitions 
------------------------
- 
-(1 row)
-
-select * from _timescaledb_catalog.dimension WHERE column_name = 'location';
- id | hypertable_id | column_name | column_type | aligned | num_slices | partitioning_func_schema | partitioning_func  | interval_length | compress_interval_length | integer_now_func_schema | integer_now_func 
-----+---------------+-------------+-------------+---------+------------+--------------------------+--------------------+-----------------+--------------------------+-------------------------+------------------
-  5 |             2 | location    | text        | f       |          2 | _timescaledb_internal    | get_partition_hash |                 |                          |                         | 
-(1 row)
-
-\set ON_ERROR_STOP 0
---must give an explicit dimension when there are multiple space dimensions
-SELECT set_number_partitions('test_schema.test_table', 3);
-ERROR:  hypertable "test_table" has multiple space dimensions
---too few
-SELECT set_number_partitions('test_schema.test_table', 0, 'location');
-ERROR:  invalid number of partitions: must be between 1 and 32767
--- Too many
-SELECT set_number_partitions('test_schema.test_table', 32768, 'location');
-ERROR:  invalid number of partitions: must be between 1 and 32767
--- get_create_command only works on tables w/ 1 or 2 dimensions
-SELECT * FROM _timescaledb_internal.get_create_command('test_table');
-ERROR:  get_create_command only supports hypertables with up to 2 dimensions
-\set ON_ERROR_STOP 1
---test adding one more open dimension
-select add_dimension('test_schema.test_table', 'id', chunk_time_interval => 1000);
-NOTICE:  adding not-null constraint to column "id"
-          add_dimension          
----------------------------------
- (6,test_schema,test_table,id,t)
-(1 row)
-
-select * from _timescaledb_catalog.hypertable where table_name = 'test_table';
- id | schema_name | table_name | associated_schema_name | associated_table_prefix | num_dimensions | chunk_sizing_func_schema |  chunk_sizing_func_name  | chunk_target_size | compression_state | compressed_hypertable_id | replication_factor 
-----+-------------+------------+------------------------+-------------------------+----------------+--------------------------+--------------------------+-------------------+-------------------+--------------------------+--------------------
-  2 | test_schema | test_table | chunk_schema           | _hyper_2                |              4 | _timescaledb_internal    | calculate_chunk_interval |                 0 |                 0 |                          |                   
-(1 row)
-
-select * from _timescaledb_catalog.dimension;
- id | hypertable_id | column_name | column_type | aligned | num_slices | partitioning_func_schema | partitioning_func  | interval_length | compress_interval_length | integer_now_func_schema | integer_now_func 
-----+---------------+-------------+-------------+---------+------------+--------------------------+--------------------+-----------------+--------------------------+-------------------------+------------------
-  1 |             1 | time        | bigint      | t       |            |                          |                    |   2592000000000 |                          |                         | 
-  2 |             1 | device_id   | text        | f       |          2 | _timescaledb_internal    | get_partition_hash |                 |                          |                         | 
-  3 |             2 | time        | bigint      | t       |            |                          |                    |   2592000000000 |                          |                         | 
-  4 |             2 | device_id   | text        | f       |          2 | _timescaledb_internal    | get_partition_hash |                 |                          |                         | 
-  5 |             2 | location    | text        | f       |          2 | _timescaledb_internal    | get_partition_hash |                 |                          |                         | 
-  6 |             2 | id          | integer     | t       |            |                          |                    |            1000 |                          |                         | 
-(6 rows)
-
--- Test add_dimension: can use interval types for TIMESTAMPTZ columns
-CREATE TABLE dim_test_time(time TIMESTAMPTZ, time2 TIMESTAMPTZ, time3 BIGINT, temp float8, device int, location int);
-SELECT create_hypertable('dim_test_time', 'time');
-NOTICE:  adding not-null constraint to column "time"
-     create_hypertable      
-----------------------------
- (3,public,dim_test_time,t)
-(1 row)
-
-SELECT add_dimension('dim_test_time', 'time2', chunk_time_interval => INTERVAL '1 day');
-NOTICE:  adding not-null constraint to column "time2"
-          add_dimension           
-----------------------------------
- (8,public,dim_test_time,time2,t)
-(1 row)
-
--- Test add_dimension: only integral should work on BIGINT columns
-\set ON_ERROR_STOP 0
-SELECT add_dimension('dim_test_time', 'time3', chunk_time_interval => INTERVAL '1 day');
-ERROR:  invalid interval type for bigint dimension
--- string is not a valid type
-SELECT add_dimension('dim_test_time', 'time3', chunk_time_interval => 'foo'::TEXT);
-ERROR:  invalid interval type for bigint dimension
-\set ON_ERROR_STOP 1
-SELECT add_dimension('dim_test_time', 'time3', chunk_time_interval => 500);
-NOTICE:  adding not-null constraint to column "time3"
-          add_dimension           
-----------------------------------
- (9,public,dim_test_time,time3,t)
-(1 row)
-
--- Test add_dimension: integrals should work on TIMESTAMPTZ columns
-CREATE TABLE dim_test_time2(time TIMESTAMPTZ, time2 TIMESTAMPTZ, temp float8, device int, location int);
-SELECT create_hypertable('dim_test_time2', 'time');
-NOTICE:  adding not-null constraint to column "time"
-      create_hypertable      
------------------------------
- (4,public,dim_test_time2,t)
-(1 row)
-
-SELECT add_dimension('dim_test_time2', 'time2', chunk_time_interval => 500);
-WARNING:  unexpected interval: smaller than one second
-NOTICE:  adding not-null constraint to column "time2"
-           add_dimension            
-------------------------------------
- (11,public,dim_test_time2,time2,t)
-(1 row)
-
---adding a dimension twice should not fail with 'if_not_exists'
-SELECT add_dimension('dim_test_time2', 'time2', chunk_time_interval => 500, if_not_exists => true);
-NOTICE:  column "time2" is already a dimension, skipping
-           add_dimension            
-------------------------------------
- (11,public,dim_test_time2,time2,f)
-(1 row)
-
-\set ON_ERROR_STOP 0
---adding on a non-hypertable
-CREATE TABLE not_hypertable(time TIMESTAMPTZ, temp float8, device int, location int);
-SELECT add_dimension('not_hypertable', 'time', chunk_time_interval => 500);
-ERROR:  table "not_hypertable" is not a hypertable
---adding a non-exist column
-SELECT add_dimension('test_schema.test_table', 'nope', 2);
-ERROR:  column "nope" does not exist
---adding the same dimension twice should fail
-select add_dimension('test_schema.test_table', 'location', 2);
-ERROR:  column "location" is already a dimension
---adding dimension with both number_partitions and chunk_time_interval should fail
-select add_dimension('test_schema.test_table', 'id2', number_partitions => 2, chunk_time_interval => 1000);
-ERROR:  cannot specify both the number of partitions and an interval
-\set ON_ERROR_STOP 1
--- test adding a new dimension on a non-empty table
-CREATE TABLE dim_test(time TIMESTAMPTZ, device int);
-SELECT create_hypertable('dim_test', 'time', chunk_time_interval => INTERVAL '1 day');
-NOTICE:  adding not-null constraint to column "time"
-   create_hypertable   
------------------------
- (5,public,dim_test,t)
-(1 row)
-
-CREATE VIEW dim_test_slices AS
-SELECT c.id AS chunk_id, c.hypertable_id, ds.dimension_id, cc.dimension_slice_id, c.schema_name AS chunk_schema, c.table_name AS chunk_table, ds.range_start, ds.range_end
-FROM _timescaledb_catalog.chunk c
-INNER JOIN _timescaledb_catalog.hypertable h ON (c.hypertable_id = h.id)
-INNER JOIN _timescaledb_catalog.dimension td ON (h.id = td.hypertable_id)
-INNER JOIN _timescaledb_catalog.dimension_slice ds ON (ds.dimension_id = td.id)
-INNER JOIN _timescaledb_catalog.chunk_constraint cc ON (cc.dimension_slice_id = ds.id AND cc.chunk_id = c.id)
-WHERE h.table_name = 'dim_test'
-ORDER BY c.id, ds.dimension_id;
-INSERT INTO dim_test VALUES ('2004-10-10 00:00:00+00', 1);
-INSERT INTO dim_test VALUES ('2004-10-20 00:00:00+00', 2);
-SELECT * FROM dim_test_slices;
- chunk_id | hypertable_id | dimension_id | dimension_slice_id |     chunk_schema      |   chunk_table    |   range_start    |    range_end     
-----------+---------------+--------------+--------------------+-----------------------+------------------+------------------+------------------
-        2 |             5 |           12 |                  3 | _timescaledb_internal | _hyper_5_2_chunk | 1097366400000000 | 1097452800000000
-        3 |             5 |           12 |                  4 | _timescaledb_internal | _hyper_5_3_chunk | 1098230400000000 | 1098316800000000
-(2 rows)
-
-SELECT * FROM test.show_constraints('_timescaledb_internal._hyper_5_2_chunk');
-  Constraint  | Type | Columns | Index |                                                                      Expr                                                                      | Deferrable | Deferred | Validated 
---------------+------+---------+-------+------------------------------------------------------------------------------------------------------------------------------------------------+------------+----------+-----------
- constraint_3 | c    | {time}  | -     | (("time" >= 'Sat Oct 09 17:00:00 2004 PDT'::timestamp with time zone) AND ("time" < 'Sun Oct 10 17:00:00 2004 PDT'::timestamp with time zone)) | f          | f        | t
-(1 row)
-
-SELECT * FROM test.show_constraints('_timescaledb_internal._hyper_5_3_chunk');
-  Constraint  | Type | Columns | Index |                                                                      Expr                                                                      | Deferrable | Deferred | Validated 
---------------+------+---------+-------+------------------------------------------------------------------------------------------------------------------------------------------------+------------+----------+-----------
- constraint_4 | c    | {time}  | -     | (("time" >= 'Tue Oct 19 17:00:00 2004 PDT'::timestamp with time zone) AND ("time" < 'Wed Oct 20 17:00:00 2004 PDT'::timestamp with time zone)) | f          | f        | t
-(1 row)
-
--- add dimension to the existing chunks by adding -inf/inf dimension slices
-SELECT add_dimension('dim_test', 'device', 2);
-         add_dimension         
--------------------------------
- (13,public,dim_test,device,t)
-(1 row)
-
-SELECT * FROM test.show_constraints('_timescaledb_internal._hyper_5_2_chunk');
-  Constraint  | Type | Columns | Index |                                                                      Expr                                                                      | Deferrable | Deferred | Validated 
---------------+------+---------+-------+------------------------------------------------------------------------------------------------------------------------------------------------+------------+----------+-----------
- constraint_3 | c    | {time}  | -     | (("time" >= 'Sat Oct 09 17:00:00 2004 PDT'::timestamp with time zone) AND ("time" < 'Sun Oct 10 17:00:00 2004 PDT'::timestamp with time zone)) | f          | f        | t
-(1 row)
-
-SELECT * FROM test.show_constraints('_timescaledb_internal._hyper_5_3_chunk');
-  Constraint  | Type | Columns | Index |                                                                      Expr                                                                      | Deferrable | Deferred | Validated 
---------------+------+---------+-------+------------------------------------------------------------------------------------------------------------------------------------------------+------------+----------+-----------
- constraint_4 | c    | {time}  | -     | (("time" >= 'Tue Oct 19 17:00:00 2004 PDT'::timestamp with time zone) AND ("time" < 'Wed Oct 20 17:00:00 2004 PDT'::timestamp with time zone)) | f          | f        | t
-(1 row)
-
-SELECT * FROM dim_test_slices;
- chunk_id | hypertable_id | dimension_id | dimension_slice_id |     chunk_schema      |   chunk_table    |     range_start      |      range_end      
-----------+---------------+--------------+--------------------+-----------------------+------------------+----------------------+---------------------
-        2 |             5 |           12 |                  3 | _timescaledb_internal | _hyper_5_2_chunk |     1097366400000000 |    1097452800000000
-        2 |             5 |           13 |                  5 | _timescaledb_internal | _hyper_5_2_chunk | -9223372036854775808 | 9223372036854775807
-        3 |             5 |           12 |                  4 | _timescaledb_internal | _hyper_5_3_chunk |     1098230400000000 |    1098316800000000
-        3 |             5 |           13 |                  5 | _timescaledb_internal | _hyper_5_3_chunk | -9223372036854775808 | 9223372036854775807
-(4 rows)
-
--- newer chunks have proper dimension slices range
-INSERT INTO dim_test VALUES ('2004-10-30 00:00:00+00', 3);
-SELECT * FROM dim_test_slices;
- chunk_id | hypertable_id | dimension_id | dimension_slice_id |     chunk_schema      |   chunk_table    |     range_start      |      range_end      
-----------+---------------+--------------+--------------------+-----------------------+------------------+----------------------+---------------------
-        2 |             5 |           12 |                  3 | _timescaledb_internal | _hyper_5_2_chunk |     1097366400000000 |    1097452800000000
-        2 |             5 |           13 |                  5 | _timescaledb_internal | _hyper_5_2_chunk | -9223372036854775808 | 9223372036854775807
-        3 |             5 |           12 |                  4 | _timescaledb_internal | _hyper_5_3_chunk |     1098230400000000 |    1098316800000000
-        3 |             5 |           13 |                  5 | _timescaledb_internal | _hyper_5_3_chunk | -9223372036854775808 | 9223372036854775807
-        4 |             5 |           12 |                  6 | _timescaledb_internal | _hyper_5_4_chunk |     1099094400000000 |    1099180800000000
-        4 |             5 |           13 |                  7 | _timescaledb_internal | _hyper_5_4_chunk |           1073741823 | 9223372036854775807
-(6 rows)
-
-SELECT * FROM dim_test ORDER BY time;
-             time             | device 
-------------------------------+--------
- Sat Oct 09 17:00:00 2004 PDT |      1
- Tue Oct 19 17:00:00 2004 PDT |      2
- Fri Oct 29 17:00:00 2004 PDT |      3
-(3 rows)
-
-DROP VIEW dim_test_slices;
-DROP TABLE dim_test;
--- test add_dimension() with existing data on table with space partitioning
-CREATE TABLE dim_test(time TIMESTAMPTZ, device int, data int);
-SELECT create_hypertable('dim_test', 'time', 'device', 2, chunk_time_interval => INTERVAL '1 day');
-NOTICE:  adding not-null constraint to column "time"
-   create_hypertable   
------------------------
- (6,public,dim_test,t)
-(1 row)
-
-CREATE VIEW dim_test_slices AS
-SELECT c.id AS chunk_id, c.hypertable_id, ds.dimension_id, cc.dimension_slice_id, c.schema_name AS chunk_schema, c.table_name AS chunk_table, ds.range_start, ds.range_end
-FROM _timescaledb_catalog.chunk c
-INNER JOIN _timescaledb_catalog.hypertable h ON (c.hypertable_id = h.id)
-INNER JOIN _timescaledb_catalog.dimension td ON (h.id = td.hypertable_id)
-INNER JOIN _timescaledb_catalog.dimension_slice ds ON (ds.dimension_id = td.id)
-INNER JOIN _timescaledb_catalog.chunk_constraint cc ON (cc.dimension_slice_id = ds.id AND cc.chunk_id = c.id)
-WHERE h.table_name = 'dim_test'
-ORDER BY c.id, ds.dimension_id;
-INSERT INTO dim_test VALUES ('2004-10-10 00:00:00+00', 1, 3);
-INSERT INTO dim_test VALUES ('2004-10-20 00:00:00+00', 2, 2);
-SELECT * FROM dim_test_slices;
- chunk_id | hypertable_id | dimension_id | dimension_slice_id |     chunk_schema      |   chunk_table    |     range_start      |      range_end      
-----------+---------------+--------------+--------------------+-----------------------+------------------+----------------------+---------------------
-        5 |             6 |           14 |                  8 | _timescaledb_internal | _hyper_6_5_chunk |     1097366400000000 |    1097452800000000
-        5 |             6 |           15 |                  9 | _timescaledb_internal | _hyper_6_5_chunk | -9223372036854775808 |          1073741823
-        6 |             6 |           14 |                 10 | _timescaledb_internal | _hyper_6_6_chunk |     1098230400000000 |    1098316800000000
-        6 |             6 |           15 |                 11 | _timescaledb_internal | _hyper_6_6_chunk |           1073741823 | 9223372036854775807
-(4 rows)
-
--- new dimension slice will cover full range on existing chunks
-SELECT add_dimension('dim_test', 'data', 1);
-        add_dimension        
------------------------------
- (16,public,dim_test,data,t)
-(1 row)
-
-SELECT * FROM dim_test_slices;
- chunk_id | hypertable_id | dimension_id | dimension_slice_id |     chunk_schema      |   chunk_table    |     range_start      |      range_end      
-----------+---------------+--------------+--------------------+-----------------------+------------------+----------------------+---------------------
-        5 |             6 |           14 |                  8 | _timescaledb_internal | _hyper_6_5_chunk |     1097366400000000 |    1097452800000000
-        5 |             6 |           15 |                  9 | _timescaledb_internal | _hyper_6_5_chunk | -9223372036854775808 |          1073741823
-        5 |             6 |           16 |                 12 | _timescaledb_internal | _hyper_6_5_chunk | -9223372036854775808 | 9223372036854775807
-        6 |             6 |           14 |                 10 | _timescaledb_internal | _hyper_6_6_chunk |     1098230400000000 |    1098316800000000
-        6 |             6 |           15 |                 11 | _timescaledb_internal | _hyper_6_6_chunk |           1073741823 | 9223372036854775807
-        6 |             6 |           16 |                 12 | _timescaledb_internal | _hyper_6_6_chunk | -9223372036854775808 | 9223372036854775807
-(6 rows)
-
-INSERT INTO dim_test VALUES ('2004-10-30 00:00:00+00', 3, 1);
-SELECT * FROM dim_test_slices;
- chunk_id | hypertable_id | dimension_id | dimension_slice_id |     chunk_schema      |   chunk_table    |     range_start      |      range_end      
-----------+---------------+--------------+--------------------+-----------------------+------------------+----------------------+---------------------
-        5 |             6 |           14 |                  8 | _timescaledb_internal | _hyper_6_5_chunk |     1097366400000000 |    1097452800000000
-        5 |             6 |           15 |                  9 | _timescaledb_internal | _hyper_6_5_chunk | -9223372036854775808 |          1073741823
-        5 |             6 |           16 |                 12 | _timescaledb_internal | _hyper_6_5_chunk | -9223372036854775808 | 9223372036854775807
-        6 |             6 |           14 |                 10 | _timescaledb_internal | _hyper_6_6_chunk |     1098230400000000 |    1098316800000000
-        6 |             6 |           15 |                 11 | _timescaledb_internal | _hyper_6_6_chunk |           1073741823 | 9223372036854775807
-        6 |             6 |           16 |                 12 | _timescaledb_internal | _hyper_6_6_chunk | -9223372036854775808 | 9223372036854775807
-        7 |             6 |           14 |                 13 | _timescaledb_internal | _hyper_6_7_chunk |     1099094400000000 |    1099180800000000
-        7 |             6 |           15 |                 11 | _timescaledb_internal | _hyper_6_7_chunk |           1073741823 | 9223372036854775807
-        7 |             6 |           16 |                 12 | _timescaledb_internal | _hyper_6_7_chunk | -9223372036854775808 | 9223372036854775807
-(9 rows)
-
-SELECT * FROM dim_test ORDER BY time;
-             time             | device | data 
-------------------------------+--------+------
- Sat Oct 09 17:00:00 2004 PDT |      1 |    3
- Tue Oct 19 17:00:00 2004 PDT |      2 |    2
- Fri Oct 29 17:00:00 2004 PDT |      3 |    1
-(3 rows)
-
-DROP VIEW dim_test_slices;
-DROP TABLE dim_test;
--- should not fail on non-empty table with 'if_not_exists' in case the dimension exists
-select add_dimension('test_schema.test_table', 'location', 2, if_not_exists => true);
-NOTICE:  column "location" is already a dimension, skipping
-             add_dimension             
----------------------------------------
- (5,test_schema,test_table,location,f)
-(1 row)
-
---test partitioning in only time dimension
-create table test_schema.test_1dim(time timestamp, temp float);
-select create_hypertable('test_schema.test_1dim', 'time');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-      create_hypertable      
------------------------------
- (7,test_schema,test_1dim,t)
-(1 row)
-
-SELECT * FROM _timescaledb_internal.get_create_command('test_1dim');
-                                                       get_create_command                                                       
---------------------------------------------------------------------------------------------------------------------------------
- SELECT create_hypertable('test_schema.test_1dim', 'time', chunk_time_interval => 604800000000, create_default_indexes=>FALSE);
-(1 row)
-
-\dt "test_schema".*
-                        List of relations
-   Schema    |          Name          | Type  |       Owner       
--------------+------------------------+-------+-------------------
- test_schema | test_1dim              | table | default_perm_user
- test_schema | test_table             | table | default_perm_user
- test_schema | test_table_no_not_null | table | default_perm_user
-(3 rows)
-
-select create_hypertable('test_schema.test_1dim', 'time', if_not_exists => true);
-NOTICE:  table "test_1dim" is already a hypertable, skipping
-      create_hypertable      
------------------------------
- (7,test_schema,test_1dim,f)
-(1 row)
-
--- Should error when creating again without if_not_exists set to true
-\set ON_ERROR_STOP 0
-select create_hypertable('test_schema.test_1dim', 'time');
-ERROR:  table "test_1dim" is already a hypertable
-\set ON_ERROR_STOP 1
--- if_not_exist should also work with data in the hypertable
-insert into test_schema.test_1dim VALUES ('2004-10-19 10:23:54+02', 1.0);
-select create_hypertable('test_schema.test_1dim', 'time', if_not_exists => true);
-NOTICE:  table "test_1dim" is already a hypertable, skipping
-      create_hypertable      
------------------------------
- (7,test_schema,test_1dim,f)
-(1 row)
-
--- Should error when creating again without if_not_exists set to true
-\set ON_ERROR_STOP 0
-select create_hypertable('test_schema.test_1dim', 'time');
-ERROR:  table "test_1dim" is already a hypertable
-\set ON_ERROR_STOP 1
--- Test partitioning functions
-CREATE OR REPLACE FUNCTION invalid_partfunc(source integer)
-    RETURNS INTEGER LANGUAGE PLPGSQL IMMUTABLE AS
-$BODY$
-BEGIN
-    RETURN NULL;
-END
-$BODY$;
-CREATE OR REPLACE FUNCTION time_partfunc(source text)
-    RETURNS TIMESTAMPTZ LANGUAGE PLPGSQL IMMUTABLE AS
-$BODY$
-BEGIN
-    RETURN timezone('UTC', to_timestamp(source));
-END
-$BODY$;
-CREATE TABLE test_schema.test_invalid_func(time timestamptz, temp float8, device text);
-\set ON_ERROR_STOP 0
--- should fail due to invalid signature
-SELECT create_hypertable('test_schema.test_invalid_func', 'time', 'device', 2, partitioning_func => 'invalid_partfunc');
-ERROR:  invalid partitioning function
-SELECT create_hypertable('test_schema.test_invalid_func', 'time');
-NOTICE:  adding not-null constraint to column "time"
-          create_hypertable          
--------------------------------------
- (8,test_schema,test_invalid_func,t)
-(1 row)
-
--- should also fail due to invalid signature
-SELECT add_dimension('test_schema.test_invalid_func', 'device', 2, partitioning_func => 'invalid_partfunc');
-ERROR:  invalid partitioning function
-\set ON_ERROR_STOP 1
--- Test open-dimension function
-CREATE TABLE test_schema.open_dim_part_func(time text, temp float8, device text, event_time text);
-\set ON_ERROR_STOP 0
--- should fail due to invalid signature
-SELECT create_hypertable('test_schema.open_dim_part_func', 'time', time_partitioning_func => 'invalid_partfunc');
-ERROR:  invalid partitioning function
-\set ON_ERROR_STOP 1
-SELECT create_hypertable('test_schema.open_dim_part_func', 'time', time_partitioning_func => 'time_partfunc');
-NOTICE:  adding not-null constraint to column "time"
-          create_hypertable           
---------------------------------------
- (9,test_schema,open_dim_part_func,t)
-(1 row)
-
-\set ON_ERROR_STOP 0
--- should fail due to invalid signature
-SELECT add_dimension('test_schema.open_dim_part_func', 'event_time', chunk_time_interval => interval '1 day', partitioning_func => 'invalid_partfunc');
-ERROR:  invalid partitioning function
-\set ON_ERROR_STOP 1
-SELECT add_dimension('test_schema.open_dim_part_func', 'event_time', chunk_time_interval => interval '1 day', partitioning_func => 'time_partfunc');
-NOTICE:  adding not-null constraint to column "event_time"
-                  add_dimension                   
---------------------------------------------------
- (20,test_schema,open_dim_part_func,event_time,t)
-(1 row)
-
---test data migration
-create table test_schema.test_migrate(time timestamp, temp float);
-insert into test_schema.test_migrate VALUES ('2004-10-19 10:23:54+02', 1.0), ('2004-12-19 10:23:54+02', 2.0);
-select * from only test_schema.test_migrate;
-           time           | temp 
---------------------------+------
- Tue Oct 19 10:23:54 2004 |    1
- Sun Dec 19 10:23:54 2004 |    2
-(2 rows)
-
-\set ON_ERROR_STOP 0
---should fail without migrate_data => true
-select create_hypertable('test_schema.test_migrate', 'time');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-ERROR:  table "test_migrate" is not empty
-\set ON_ERROR_STOP 1
-select create_hypertable('test_schema.test_migrate', 'time', migrate_data => true);
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-NOTICE:  migrating data to chunks
-        create_hypertable        
----------------------------------
- (10,test_schema,test_migrate,t)
-(1 row)
-
---there should be two new chunks
-select * from _timescaledb_catalog.hypertable where table_name = 'test_migrate';
- id | schema_name |  table_name  | associated_schema_name | associated_table_prefix | num_dimensions | chunk_sizing_func_schema |  chunk_sizing_func_name  | chunk_target_size | compression_state | compressed_hypertable_id | replication_factor 
-----+-------------+--------------+------------------------+-------------------------+----------------+--------------------------+--------------------------+-------------------+-------------------+--------------------------+--------------------
- 10 | test_schema | test_migrate | _timescaledb_internal  | _hyper_10               |              1 | _timescaledb_internal    | calculate_chunk_interval |                 0 |                 0 |                          |                   
-(1 row)
-
-select * from _timescaledb_catalog.chunk;
- id | hypertable_id |      schema_name      |     table_name     | compressed_chunk_id | dropped | status | osm_chunk 
-----+---------------+-----------------------+--------------------+---------------------+---------+--------+-----------
-  1 |             1 | _timescaledb_internal | _hyper_1_1_chunk   |                     | f       |      0 | f
-  8 |             7 | _timescaledb_internal | _hyper_7_8_chunk   |                     | f       |      0 | f
-  9 |            10 | _timescaledb_internal | _hyper_10_9_chunk  |                     | f       |      0 | f
- 10 |            10 | _timescaledb_internal | _hyper_10_10_chunk |                     | f       |      0 | f
-(4 rows)
-
-select * from test_schema.test_migrate;
-           time           | temp 
---------------------------+------
- Tue Oct 19 10:23:54 2004 |    1
- Sun Dec 19 10:23:54 2004 |    2
-(2 rows)
-
---main table should now be empty
-select * from only test_schema.test_migrate;
- time | temp 
-------+------
-(0 rows)
-
-select * from only _timescaledb_internal._hyper_10_9_chunk;
-           time           | temp 
---------------------------+------
- Tue Oct 19 10:23:54 2004 |    1
-(1 row)
-
-select * from only _timescaledb_internal._hyper_10_10_chunk;
-           time           | temp 
---------------------------+------
- Sun Dec 19 10:23:54 2004 |    2
-(1 row)
-
-create table test_schema.test_migrate_empty(time timestamp, temp float);
-select create_hypertable('test_schema.test_migrate_empty', 'time', migrate_data => true);
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-           create_hypertable           
----------------------------------------
- (11,test_schema,test_migrate_empty,t)
-(1 row)
-
-CREATE TYPE test_type AS (time timestamp, temp float);
-CREATE TABLE test_table_of_type OF test_type;
-SELECT create_hypertable('test_table_of_type', 'time');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-        create_hypertable         
-----------------------------------
- (12,public,test_table_of_type,t)
-(1 row)
-
-INSERT INTO test_table_of_type VALUES ('2004-10-19 10:23:54+02', 1.0), ('2004-12-19 10:23:54+02', 2.0);
-\set ON_ERROR_STOP 0
-DROP TYPE test_type;
-ERROR:  cannot drop type test_type because other objects depend on it
-\set ON_ERROR_STOP 1
-DROP TYPE test_type CASCADE;
-NOTICE:  drop cascades to 3 other objects
-CREATE TABLE test_table_of_type (time timestamp, temp float);
-SELECT create_hypertable('test_table_of_type', 'time');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-        create_hypertable         
-----------------------------------
- (13,public,test_table_of_type,t)
-(1 row)
-
-INSERT INTO test_table_of_type VALUES ('2004-10-19 10:23:54+02', 1.0), ('2004-12-19 10:23:54+02', 2.0);
-CREATE TYPE test_type AS (time timestamp, temp float);
-ALTER TABLE test_table_of_type OF test_type;
-\set ON_ERROR_STOP 0
-DROP TYPE test_type;
-ERROR:  cannot drop type test_type because other objects depend on it
-\set ON_ERROR_STOP 1
-BEGIN;
-DROP TYPE test_type CASCADE;
-NOTICE:  drop cascades to 3 other objects
-ROLLBACK;
-ALTER TABLE test_table_of_type NOT OF;
-DROP TYPE test_type;
--- Reset GRANTS
-\c :TEST_DBNAME :ROLE_SUPERUSER
-REVOKE :ROLE_DEFAULT_PERM_USER FROM :ROLE_DEFAULT_PERM_USER_2;
--- Test custom partitioning functions
-CREATE OR REPLACE FUNCTION partfunc_not_immutable(source anyelement)
-    RETURNS INTEGER LANGUAGE PLPGSQL AS
-$BODY$
-BEGIN
-    RETURN _timescaledb_internal.get_partition_hash(source);
-END
-$BODY$;
-CREATE OR REPLACE FUNCTION partfunc_bad_return_type(source anyelement)
-    RETURNS BIGINT LANGUAGE PLPGSQL IMMUTABLE AS
-$BODY$
-BEGIN
-    RETURN _timescaledb_internal.get_partition_hash(source);
-END
-$BODY$;
-CREATE OR REPLACE FUNCTION partfunc_bad_arg_type(source text)
-    RETURNS INTEGER LANGUAGE PLPGSQL IMMUTABLE AS
-$BODY$
-BEGIN
-    RETURN _timescaledb_internal.get_partition_hash(source);
-END
-$BODY$;
-CREATE OR REPLACE FUNCTION partfunc_bad_multi_arg(source anyelement, extra_arg integer)
-    RETURNS INTEGER LANGUAGE PLPGSQL IMMUTABLE AS
-$BODY$
-BEGIN
-    RETURN _timescaledb_internal.get_partition_hash(source);
-END
-$BODY$;
-CREATE OR REPLACE FUNCTION partfunc_valid(source anyelement)
-    RETURNS INTEGER LANGUAGE PLPGSQL IMMUTABLE AS
-$BODY$
-BEGIN
-    RETURN _timescaledb_internal.get_partition_hash(source);
-END
-$BODY$;
-create table test_schema.test_partfunc(time timestamptz, temp float, device int);
--- Test that create_hypertable fails due to invalid partitioning function
-\set ON_ERROR_STOP 0
-select create_hypertable('test_schema.test_partfunc', 'time', 'device', 2, partitioning_func => 'partfunc_not_immutable');
-ERROR:  invalid partitioning function
-select create_hypertable('test_schema.test_partfunc', 'time', 'device', 2, partitioning_func => 'partfunc_bad_return_type');
-ERROR:  invalid partitioning function
-select create_hypertable('test_schema.test_partfunc', 'time', 'device', 2, partitioning_func => 'partfunc_bad_arg_type');
-ERROR:  invalid partitioning function
-select create_hypertable('test_schema.test_partfunc', 'time', 'device', 2, partitioning_func => 'partfunc_bad_multi_arg');
-ERROR:  invalid partitioning function
-\set ON_ERROR_STOP 1
--- Test that add_dimension fails due to invalid partitioning function
-select create_hypertable('test_schema.test_partfunc', 'time');
-NOTICE:  adding not-null constraint to column "time"
-        create_hypertable         
-----------------------------------
- (14,test_schema,test_partfunc,t)
-(1 row)
-
-\set ON_ERROR_STOP 0
-select add_dimension('test_schema.test_partfunc', 'device', 2, partitioning_func => 'partfunc_not_immutable');
-ERROR:  invalid partitioning function
-select add_dimension('test_schema.test_partfunc', 'device', 2, partitioning_func => 'partfunc_bad_return_type');
-ERROR:  invalid partitioning function
-select add_dimension('test_schema.test_partfunc', 'device', 2, partitioning_func => 'partfunc_bad_arg_type');
-ERROR:  invalid partitioning function
-select add_dimension('test_schema.test_partfunc', 'device', 2, partitioning_func => 'partfunc_bad_multi_arg');
-ERROR:  invalid partitioning function
-\set ON_ERROR_STOP 1
--- A valid function should work:
-select add_dimension('test_schema.test_partfunc', 'device', 2, partitioning_func => 'partfunc_valid');
-              add_dimension              
------------------------------------------
- (26,test_schema,test_partfunc,device,t)
-(1 row)
-
--- check get_create_command produces valid command
-CREATE TABLE test_schema.test_sql_cmd(time TIMESTAMPTZ, temp FLOAT8, device_id TEXT, device_type TEXT, location TEXT, id INT, id2 INT);
-SELECT create_hypertable('test_schema.test_sql_cmd','time');
-NOTICE:  adding not-null constraint to column "time"
-        create_hypertable        
----------------------------------
- (15,test_schema,test_sql_cmd,t)
-(1 row)
-
-SELECT * FROM _timescaledb_internal.get_create_command('test_sql_cmd');
-                                                        get_create_command                                                         
------------------------------------------------------------------------------------------------------------------------------------
- SELECT create_hypertable('test_schema.test_sql_cmd', 'time', chunk_time_interval => 604800000000, create_default_indexes=>FALSE);
-(1 row)
-
-SELECT _timescaledb_internal.get_create_command('test_sql_cmd') AS create_cmd; \gset
-                                                            create_cmd                                                             
------------------------------------------------------------------------------------------------------------------------------------
- SELECT create_hypertable('test_schema.test_sql_cmd', 'time', chunk_time_interval => 604800000000, create_default_indexes=>FALSE);
-(1 row)
-
-DROP TABLE test_schema.test_sql_cmd CASCADE;
-CREATE TABLE test_schema.test_sql_cmd(time TIMESTAMPTZ, temp FLOAT8, device_id TEXT, device_type TEXT, location TEXT, id INT, id2 INT);
-SELECT test.execute_sql(:'create_cmd');
-NOTICE:  adding not-null constraint to column "time"
-                                                            execute_sql                                                            
------------------------------------------------------------------------------------------------------------------------------------
- SELECT create_hypertable('test_schema.test_sql_cmd', 'time', chunk_time_interval => 604800000000, create_default_indexes=>FALSE);
-(1 row)
-
-\c :TEST_DBNAME :ROLE_DEFAULT_PERM_USER
-CREATE TABLE test_table_int(time bigint, junk int);
-SELECT hypertable_id AS "TEST_TABLE_INT_HYPERTABLE_ID" FROM create_hypertable('test_table_int', 'time', chunk_time_interval => 1) \gset
-NOTICE:  adding not-null constraint to column "time"
-\c :TEST_DBNAME :ROLE_SUPERUSER
-CREATE SCHEMA IF NOT EXISTS my_schema;
-create or replace function my_schema.dummy_now2() returns BIGINT LANGUAGE SQL IMMUTABLE as  'SELECT 1::BIGINT';
-grant execute on ALL FUNCTIONS IN SCHEMA my_schema to public;
-create or replace function dummy_now3() returns BIGINT LANGUAGE SQL IMMUTABLE as  'SELECT 1::BIGINT';
-grant execute on ALL FUNCTIONS IN SCHEMA my_schema to public;
-REVOKE execute ON function dummy_now3() FROM PUBLIC;
-CREATE SCHEMA IF NOT EXISTS my_user_schema;
-GRANT ALL ON SCHEMA my_user_schema to PUBLIC;
-\c :TEST_DBNAME :ROLE_DEFAULT_PERM_USER
-create or replace function dummy_now() returns BIGINT LANGUAGE SQL IMMUTABLE as  'SELECT 1::BIGINT';
-create or replace function my_user_schema.dummy_now4() returns BIGINT LANGUAGE SQL IMMUTABLE as  'SELECT 1::BIGINT';
-select set_integer_now_func('test_table_int', 'dummy_now');
- set_integer_now_func 
-----------------------
- 
-(1 row)
-
-select * from _timescaledb_catalog.dimension WHERE hypertable_id = :TEST_TABLE_INT_HYPERTABLE_ID;
- id | hypertable_id | column_name | column_type | aligned | num_slices | partitioning_func_schema | partitioning_func | interval_length | compress_interval_length | integer_now_func_schema | integer_now_func 
-----+---------------+-------------+-------------+---------+------------+--------------------------+-------------------+-----------------+--------------------------+-------------------------+------------------
- 29 |            17 | time        | bigint      | t       |            |                          |                   |               1 |                          | public                  | dummy_now
-(1 row)
-
-\set ON_ERROR_STOP 0
-select set_integer_now_func('test_table_int', 'dummy_now');
-ERROR:  custom time function already set for hypertable "test_table_int"
-select set_integer_now_func('test_table_int', 'my_schema.dummy_now2', replace_if_exists => TRUE);
-ERROR:  permission denied for schema my_schema at character 47
-select set_integer_now_func('test_table_int', 'dummy_now3', replace_if_exists => TRUE);
-ERROR:  permission denied for function dummy_now3
-\set ON_ERROR_STOP
-select set_integer_now_func('test_table_int', 'my_user_schema.dummy_now4', replace_if_exists => TRUE);
- set_integer_now_func 
-----------------------
- 
-(1 row)
-
-\c :TEST_DBNAME :ROLE_SUPERUSER
-ALTER SCHEMA my_user_schema RENAME TO my_new_schema;
-select * from _timescaledb_catalog.dimension WHERE hypertable_id = :TEST_TABLE_INT_HYPERTABLE_ID;
- id | hypertable_id | column_name | column_type | aligned | num_slices | partitioning_func_schema | partitioning_func | interval_length | compress_interval_length | integer_now_func_schema | integer_now_func 
-----+---------------+-------------+-------------+---------+------------+--------------------------+-------------------+-----------------+--------------------------+-------------------------+------------------
- 29 |            17 | time        | bigint      | t       |            |                          |                   |               1 |                          | my_new_schema           | dummy_now4
-(1 row)
-
--- github issue #4650
-CREATE TABLE sample_table (
-       cpu double precision null,
-       time TIMESTAMP WITH TIME ZONE NOT NULL,
-       sensor_id INTEGER NOT NULL,
-       name varchar(100) default 'this is a default string value',
-       UNIQUE(sensor_id, time)
-);
-ALTER TABLE sample_table DROP COLUMN name;
--- below creation should not report any warnings.
-SELECT * FROM create_hypertable('sample_table', 'time');
- hypertable_id | schema_name |  table_name  | created 
----------------+-------------+--------------+---------
-            18 | public      | sample_table | t
-(1 row)
-
--- cleanup
-DROP TABLE sample_table CASCADE;
--- github issue 4684
--- test PARTITION BY HASH
-CREATE TABLE regular(
-   id INT NOT NULL,
-   dev INT NOT NULL,
-   value INT,
-   CONSTRAINT cstr_regular_pky PRIMARY KEY (id)
-) PARTITION BY HASH (id);
-DO $$
-BEGIN
-   FOR i IN 1..2
-   LOOP
-      EXECUTE format('
-         CREATE TABLE %I
-         PARTITION OF regular
-         FOR VALUES WITH (MODULUS 2, REMAINDER %s)',
-         'regular_' || i, i - 1
-      );
-   END LOOP;
-END;
-$$;
-INSERT INTO regular SELECT generate_series(1,1000), 44,55;
-CREATE TABLE timescale (
-   ts TIMESTAMP WITH TIME ZONE NOT NULL,
-   id INT NOT NULL,
-   dev INT NOT NULL,
-   FOREIGN KEY (id)  REFERENCES regular(id) ON DELETE CASCADE
-);
-SELECT create_hypertable(
-   relation => 'timescale',
-   time_column_name => 'ts'
-);
-    create_hypertable    
--------------------------
- (19,public,timescale,t)
-(1 row)
-
--- creates chunk1
-INSERT INTO timescale SELECT now(), generate_series(1,200), 43;
--- creates chunk2
-INSERT INTO timescale SELECT now() + interval '20' day, generate_series(1,200), 43;
--- creates chunk3
-INSERT INTO timescale SELECT now() + interval '40' day, generate_series(1,200), 43;
--- show chunks
-SELECT SHOW_CHUNKS('timescale');
-               show_chunks                
-------------------------------------------
- _timescaledb_internal._hyper_19_15_chunk
- _timescaledb_internal._hyper_19_16_chunk
- _timescaledb_internal._hyper_19_17_chunk
-(3 rows)
-
-\set ON_ERROR_STOP 0
--- record goes into chunk1 violating FK constraint as value 1001 is not present in regular table
-INSERT INTO timescale SELECT now(), 1001, 43;
-ERROR:  insert or update on table "_hyper_19_15_chunk" violates foreign key constraint "15_1_timescale_id_fkey"
--- record goes into chunk2 violating FK constraint as value 1002 is not present in regular table
-INSERT INTO timescale SELECT now() + interval '20' day, 1002, 43;
-ERROR:  insert or update on table "_hyper_19_16_chunk" violates foreign key constraint "16_2_timescale_id_fkey"
--- record goes into chunk3 violating FK constraint as value 1003 is not present in regular table
-INSERT INTO timescale SELECT now() + interval '40' day, 1003, 43;
-ERROR:  insert or update on table "_hyper_19_17_chunk" violates foreign key constraint "17_3_timescale_id_fkey"
-\set ON_ERROR_STOP 1
--- cleanup
-DROP TABLE regular cascade;
-NOTICE:  drop cascades to 4 other objects
-DROP TABLE timescale cascade;
--- test PARTITION BY RANGE
-CREATE TABLE regular(
-   id INT NOT NULL,
-   dev INT NOT NULL,
-   value INT,
-   CONSTRAINT cstr_regular_pky PRIMARY KEY (id)
-) PARTITION BY RANGE (id);
-CREATE TABLE regular_1_500 PARTITION OF regular
-    FOR VALUES FROM (1) TO (500);
-CREATE TABLE regular_500_1000 PARTITION OF regular
-    FOR VALUES FROM (500) TO (801);
-INSERT INTO regular SELECT generate_series(1,800), 44,55;
-CREATE TABLE timescale (
-   ts TIMESTAMP WITH TIME ZONE NOT NULL,
-   id INT NOT NULL,
-   dev INT NOT NULL,
-   FOREIGN KEY (id)  REFERENCES regular(id) ON DELETE CASCADE
-);
-SELECT create_hypertable(
-   relation => 'timescale',
-   time_column_name => 'ts'
-);
-    create_hypertable    
--------------------------
- (20,public,timescale,t)
-(1 row)
-
--- creates chunk1
-INSERT INTO timescale SELECT now(), generate_series(1,200), 43;
--- creates chunk2
-INSERT INTO timescale SELECT now() + interval '20' day, generate_series(200,400), 43;
--- creates chunk3
-INSERT INTO timescale SELECT now() + interval '40' day, generate_series(400,600), 43;
--- show chunks
-SELECT SHOW_CHUNKS('timescale');
-               show_chunks                
-------------------------------------------
- _timescaledb_internal._hyper_20_18_chunk
- _timescaledb_internal._hyper_20_19_chunk
- _timescaledb_internal._hyper_20_20_chunk
-(3 rows)
-
-\set ON_ERROR_STOP 0
--- FK constraint violation as value 801 is not present in regular table
-INSERT INTO timescale SELECT now(), 801, 43;
-ERROR:  insert or update on table "_hyper_20_18_chunk" violates foreign key constraint "18_4_timescale_id_fkey"
--- FK constraint violation as value 902 is not present in regular table
-INSERT INTO timescale SELECT now() + interval '20' day, 902, 43;
-ERROR:  insert or update on table "_hyper_20_19_chunk" violates foreign key constraint "19_5_timescale_id_fkey"
--- FK constraint violation as value 1003 is not present in regular table
-INSERT INTO timescale SELECT now() + interval '40' day, 1003, 43;
-ERROR:  insert or update on table "_hyper_20_20_chunk" violates foreign key constraint "20_6_timescale_id_fkey"
-\set ON_ERROR_STOP 1
--- cleanup
-DROP TABLE regular cascade;
-NOTICE:  drop cascades to 4 other objects
-DROP TABLE timescale cascade;
--- test PARTITION BY LIST
-CREATE TABLE regular(
-   id INT NOT NULL,
-   dev INT NOT NULL,
-   value INT,
-   CONSTRAINT cstr_regular_pky PRIMARY KEY (id)
-) PARTITION BY LIST (id);
-CREATE TABLE regular_1_2_3_4 PARTITION OF regular FOR VALUES IN (1,2,3,4);
-CREATE TABLE regular_5_6_7_8 PARTITION OF regular FOR VALUES IN (5,6,7,8);
-INSERT INTO regular SELECT generate_series(1,8), 44,55;
-CREATE TABLE timescale (
-   ts TIMESTAMP WITH TIME ZONE NOT NULL,
-   id INT NOT NULL,
-   dev INT NOT NULL,
-   FOREIGN KEY (id)  REFERENCES regular(id) ON DELETE CASCADE
-);
-SELECT create_hypertable(
-   relation => 'timescale',
-   time_column_name => 'ts'
-);
-    create_hypertable    
--------------------------
- (21,public,timescale,t)
-(1 row)
-
-insert into timescale values (now(), 1,2);
-insert into timescale values (now(), 2,2);
-insert into timescale values (now(), 3,2);
-insert into timescale values (now(), 4,2);
-insert into timescale values (now(), 5,2);
-insert into timescale values (now(), 6,2);
-insert into timescale values (now(), 7,2);
-insert into timescale values (now(), 8,2);
-\set ON_ERROR_STOP 0
--- FK constraint violation as value 9 is not present in regular table
-insert into timescale values (now(), 9,2);
-ERROR:  insert or update on table "_hyper_21_21_chunk" violates foreign key constraint "21_7_timescale_id_fkey"
--- FK constraint violation as value 10 is not present in regular table
-insert into timescale values (now(), 10,2);
-ERROR:  insert or update on table "_hyper_21_21_chunk" violates foreign key constraint "21_7_timescale_id_fkey"
--- FK constraint violation as value 111 is not present in regular table
-insert into timescale values (now(), 111,2);
-ERROR:  insert or update on table "_hyper_21_21_chunk" violates foreign key constraint "21_7_timescale_id_fkey"
-\set ON_ERROR_STOP 1
--- cleanup
-DROP TABLE regular cascade;
-NOTICE:  drop cascades to 2 other objects
-DROP TABLE timescale cascade;
--- github issue 4872
--- If subplan of ChunkAppend is TidRangeScan, then SELECT on
--- hypertable fails with error "invalid child of chunk append: Node (26)"
-create table tidrangescan_test (
-  time timestamp with time zone,
-  some_column bigint
-);
-select create_hypertable('tidrangescan_test', 'time');
-NOTICE:  adding not-null constraint to column "time"
-        create_hypertable        
----------------------------------
- (22,public,tidrangescan_test,t)
-(1 row)
-
-insert into tidrangescan_test (time, some_column) values ('2023-02-12 00:00:00+02:40', 1);
-insert into tidrangescan_test (time, some_column) values ('2023-02-12 00:00:10+02:40', 2);
-insert into tidrangescan_test (time, some_column) values ('2023-02-12 00:00:20+02:40', 3);
--- Below query will generate plan as
--- Custom Scan (ChunkAppend)
---   ->  Tid Range Scan
--- However when traversing ChunkAppend node, Tid Range Scan node is not
--- recognised as a valid child node of ChunkAppend which causes error
--- "invalid child of chunk append: Node (26)" when below query is executed
-select * from tidrangescan_test where time > '2023-02-12 00:00:00+02:40'::timestamp with time zone - interval '5 years' and ctid < '(1,1)'::tid ORDER BY time;
-             time             | some_column 
-------------------------------+-------------
- Sat Feb 11 13:20:00 2023 PST |           1
- Sat Feb 11 13:20:10 2023 PST |           2
- Sat Feb 11 13:20:20 2023 PST |           3
-(3 rows)
-
-drop table tidrangescan_test;
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/create_table.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/create_table.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/create_table.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/create_table.out	2023-11-25 05:27:33.761052864 +0000
@@ -1,59 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
--- Test that we can verify constraints on regular tables
-CREATE TABLE test_hyper_pk(time TIMESTAMPTZ PRIMARY KEY, temp FLOAT, device INT);
-CREATE TABLE test_pk(device INT PRIMARY KEY);
-CREATE TABLE test_like(LIKE test_pk);
-SELECT create_hypertable('test_hyper_pk', 'time');
-     create_hypertable      
-----------------------------
- (1,public,test_hyper_pk,t)
-(1 row)
-
-\set ON_ERROR_STOP 0
--- Foreign key constraints that reference hypertables are currently unsupported
-CREATE TABLE test_fk(time TIMESTAMPTZ REFERENCES test_hyper_pk(time));
-ERROR:  foreign keys to hypertables are not supported
-\set ON_ERROR_STOP 1
-CREATE TABLE test_delete(time timestamp with time zone PRIMARY KEY, temp float);
-SELECT create_hypertable('test_delete', 'time');
-    create_hypertable     
---------------------------
- (2,public,test_delete,t)
-(1 row)
-
-INSERT INTO test_delete VALUES('2017-01-20T09:00:01', 22.5);
-INSERT INTO test_delete VALUES('2017-01-20T09:00:21', 21.2);
-INSERT INTO test_delete VALUES('2017-01-20T09:00:47', 25.1);
-INSERT INTO test_delete VALUES('2020-01-20T09:00:47', 25.1);
-INSERT INTO test_delete VALUES('2021-01-20T09:00:47', 25.1);
-SELECT * FROM test_delete WHERE temp = 25.1 ORDER BY time;
-             time             | temp 
-------------------------------+------
- Fri Jan 20 09:00:47 2017 PST | 25.1
- Mon Jan 20 09:00:47 2020 PST | 25.1
- Wed Jan 20 09:00:47 2021 PST | 25.1
-(3 rows)
-
-CREATE OR replace FUNCTION test_delete_row_count()
-RETURNS void AS $$
-DECLARE
-    v_cnt numeric;
-BEGIN
-    v_cnt := 0;
-
-        DELETE FROM test_delete WHERE temp = 25.1;
-        GET DIAGNOSTICS v_cnt = ROW_COUNT;
-
-        IF v_cnt != 3 THEN
-           RAISE EXCEPTION 'unexpected result';
-        END IF;
-END;
-$$ LANGUAGE plpgsql;
-SELECT test_delete_row_count();
- test_delete_row_count 
------------------------
- 
-(1 row)
-
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/cursor-15.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/cursor-15.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/cursor-15.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/cursor-15.out	2023-11-25 05:27:33.765052852 +0000
@@ -1,52 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-CREATE TABLE cursor_test(time timestamptz, device_id int, temp float);
-SELECT create_hypertable('cursor_test','time');
-NOTICE:  adding not-null constraint to column "time"
-    create_hypertable     
---------------------------
- (1,public,cursor_test,t)
-(1 row)
-
-INSERT INTO cursor_test SELECT '2000-01-01',1,0.5;
-INSERT INTO cursor_test SELECT '2001-01-01',1,0.5;
-INSERT INTO cursor_test SELECT '2002-01-01',1,0.5;
-\set ON_ERROR_STOP 0
-BEGIN;
-DECLARE c1 SCROLL CURSOR FOR SELECT * FROM cursor_test;
-FETCH NEXT FROM c1;
-             time             | device_id | temp 
-------------------------------+-----------+------
- Sat Jan 01 00:00:00 2000 PST |         1 |  0.5
-(1 row)
-
--- this will produce an error on PG < 14 because PostgreSQL checks
--- for the existence of a scan node with the relation id for every relation
--- used in the update plan in the plan of the cursor.
-UPDATE cursor_test SET temp = 0.7 WHERE CURRENT OF c1;
-COMMIT;
--- test cursor with no chunks left after runtime exclusion
-BEGIN;
-DECLARE c1 SCROLL CURSOR FOR SELECT * FROM cursor_test WHERE time > now();
-UPDATE cursor_test SET temp = 0.7 WHERE CURRENT OF c1;
-ERROR:  cursor "c1" is not a simply updatable scan of table "_hyper_1_1_chunk"
-COMMIT;
--- test cursor with no chunks left after planning exclusion
-BEGIN;
-DECLARE c1 SCROLL CURSOR FOR SELECT * FROM cursor_test WHERE time > '2010-01-01';
-UPDATE cursor_test SET temp = 0.7 WHERE CURRENT OF c1;
-ERROR:  cursor "c1" is not a simply updatable scan of table "_hyper_1_1_chunk"
-COMMIT;
-\set ON_ERROR_STOP 1
-SET timescaledb.enable_constraint_exclusion TO off;
-BEGIN;
-DECLARE c1 SCROLL CURSOR FOR SELECT * FROM cursor_test;
-FETCH NEXT FROM c1;
-             time             | device_id | temp 
-------------------------------+-----------+------
- Sat Jan 01 00:00:00 2000 PST |         1 |  0.7
-(1 row)
-
-UPDATE cursor_test SET temp = 0.7 WHERE CURRENT OF c1;
-COMMIT;
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/custom_type.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/custom_type.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/custom_type.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/custom_type.out	2023-11-25 05:27:33.765052852 +0000
@@ -1,100 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-\c :TEST_DBNAME :ROLE_SUPERUSER
-CREATE OR REPLACE FUNCTION customtype_in(cstring) RETURNS customtype AS
-'timestamptz_in'
-LANGUAGE internal IMMUTABLE STRICT;
-NOTICE:  type "customtype" is not yet defined
-CREATE OR REPLACE FUNCTION customtype_out(customtype) RETURNS cstring AS
-'timestamptz_out'
-LANGUAGE internal IMMUTABLE STRICT;
-NOTICE:  argument type customtype is only a shell
-CREATE OR REPLACE FUNCTION customtype_recv(internal) RETURNS customtype AS
-'timestamptz_recv'
-LANGUAGE internal IMMUTABLE STRICT;
-NOTICE:  return type customtype is only a shell
-CREATE OR REPLACE FUNCTION customtype_send(customtype) RETURNS bytea AS
-'timestamptz_send'
-LANGUAGE internal IMMUTABLE STRICT;
-NOTICE:  argument type customtype is only a shell
-CREATE TYPE customtype (
- INPUT = customtype_in,
- OUTPUT = customtype_out,
- RECEIVE = customtype_recv,
- SEND = customtype_send,
- LIKE = TIMESTAMPTZ
-);
-CREATE CAST (customtype AS bigint)
-WITHOUT FUNCTION AS ASSIGNMENT;
-CREATE CAST (bigint AS customtype)
-WITHOUT FUNCTION AS IMPLICIT;
-CREATE CAST (customtype AS timestamptz)
-WITHOUT FUNCTION AS ASSIGNMENT;
-CREATE CAST (timestamptz AS customtype)
-WITHOUT FUNCTION AS ASSIGNMENT;
-CREATE OR REPLACE FUNCTION customtype_lt(customtype, customtype) RETURNS bool AS
-'timestamp_lt'
-LANGUAGE internal IMMUTABLE STRICT;
-CREATE OPERATOR < (
-	LEFTARG = customtype,
-	RIGHTARG = customtype,
-	PROCEDURE = customtype_lt,
-	COMMUTATOR = >,
-	NEGATOR = >=,
-	RESTRICT = scalarltsel,
-	JOIN = scalarltjoinsel
-);
-CREATE OR REPLACE FUNCTION customtype_ge(customtype, customtype) RETURNS bool AS
-'timestamp_ge'
-LANGUAGE internal IMMUTABLE STRICT;
-CREATE OPERATOR >= (
-	LEFTARG = customtype,
-	RIGHTARG = customtype,
-	PROCEDURE = customtype_ge,
-	COMMUTATOR = <=,
-	NEGATOR = <,
-	RESTRICT = scalargtsel,
-	JOIN = scalargtjoinsel
-);
-\c :TEST_DBNAME :ROLE_DEFAULT_PERM_USER
-CREATE TABLE customtype_test(time_custom customtype, val int);
-\set ON_ERROR_STOP 0
--- Using interval type for chunk time interval should fail with custom time type
-SELECT create_hypertable('customtype_test', 'time_custom', chunk_time_interval => INTERVAL '1 day', create_default_indexes=>false);
-ERROR:  invalid interval type for customtype dimension
-\set ON_ERROR_STOP 1
-SELECT create_hypertable('customtype_test', 'time_custom', chunk_time_interval => 10e6::bigint, create_default_indexes=>false);
-NOTICE:  adding not-null constraint to column "time_custom"
-      create_hypertable       
-------------------------------
- (1,public,customtype_test,t)
-(1 row)
-
-INSERT INTO customtype_test VALUES ('2001-01-01 01:02:03'::customtype, 10);
-INSERT INTO customtype_test VALUES ('2001-01-01 01:02:03'::customtype, 10);
-INSERT INTO customtype_test VALUES ('2001-01-01 01:02:03'::customtype, 10);
-EXPLAIN (costs off) SELECT * FROM customtype_test;
-          QUERY PLAN          
-------------------------------
- Seq Scan on _hyper_1_1_chunk
-(1 row)
-
-INSERT INTO customtype_test VALUES ('2001-01-01 01:02:23'::customtype, 11);
-EXPLAIN (costs off) SELECT * FROM customtype_test;
-             QUERY PLAN             
-------------------------------------
- Append
-   ->  Seq Scan on _hyper_1_1_chunk
-   ->  Seq Scan on _hyper_1_2_chunk
-(3 rows)
-
-SELECT * FROM customtype_test;
-         time_custom          | val 
-------------------------------+-----
- Mon Jan 01 01:02:03 2001 PST |  10
- Mon Jan 01 01:02:03 2001 PST |  10
- Mon Jan 01 01:02:03 2001 PST |  10
- Mon Jan 01 01:02:23 2001 PST |  11
-(4 rows)
-
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/ddl-15.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/ddl-15.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/ddl-15.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/ddl-15.out	2023-11-25 05:27:33.761052864 +0000
@@ -1,594 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-\c :TEST_DBNAME :ROLE_SUPERUSER
-CREATE SCHEMA IF NOT EXISTS "customSchema" AUTHORIZATION :ROLE_DEFAULT_PERM_USER;
-\c :TEST_DBNAME :ROLE_DEFAULT_PERM_USER
-\ir include/ddl_ops_1.sql
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-CREATE TABLE PUBLIC."Hypertable_1" (
-  time BIGINT NOT NULL,
-  "Device_id" TEXT NOT NULL,
-  temp_c int NOT NULL DEFAULT -1,
-  humidity numeric NULL DEFAULT 0,
-  sensor_1 NUMERIC NULL DEFAULT 1,
-  sensor_2 NUMERIC NOT NULL DEFAULT 1,
-  sensor_3 NUMERIC NOT NULL DEFAULT 1,
-  sensor_4 NUMERIC NOT NULL DEFAULT 1
-);
-CREATE INDEX ON PUBLIC."Hypertable_1" (time, "Device_id");
-CREATE TABLE "customSchema"."Hypertable_1" (
-  time BIGINT NOT NULL,
-  "Device_id" TEXT NOT NULL,
-  temp_c int NOT NULL DEFAULT -1,
-  humidity numeric NULL DEFAULT 0,
-  sensor_1 NUMERIC NULL DEFAULT 1,
-  sensor_2 NUMERIC NOT NULL DEFAULT 1,
-  sensor_3 NUMERIC NOT NULL DEFAULT 1,
-  sensor_4 NUMERIC NOT NULL DEFAULT 1
-);
-CREATE INDEX ON "customSchema"."Hypertable_1" (time, "Device_id");
-SELECT * FROM create_hypertable('"public"."Hypertable_1"', 'time', 'Device_id', 1, chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
- hypertable_id | schema_name |  table_name  | created 
----------------+-------------+--------------+---------
-             1 | public      | Hypertable_1 | t
-(1 row)
-
-SELECT * FROM create_hypertable('"customSchema"."Hypertable_1"', 'time', NULL, 1, chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
- hypertable_id | schema_name  |  table_name  | created 
----------------+--------------+--------------+---------
-             2 | customSchema | Hypertable_1 | t
-(1 row)
-
-SELECT * FROM _timescaledb_catalog.hypertable;
- id | schema_name  |  table_name  | associated_schema_name | associated_table_prefix | num_dimensions | chunk_sizing_func_schema |  chunk_sizing_func_name  | chunk_target_size | compression_state | compressed_hypertable_id | replication_factor 
-----+--------------+--------------+------------------------+-------------------------+----------------+--------------------------+--------------------------+-------------------+-------------------+--------------------------+--------------------
-  1 | public       | Hypertable_1 | _timescaledb_internal  | _hyper_1                |              2 | _timescaledb_internal    | calculate_chunk_interval |                 0 |                 0 |                          |                   
-  2 | customSchema | Hypertable_1 | _timescaledb_internal  | _hyper_2                |              1 | _timescaledb_internal    | calculate_chunk_interval |                 0 |                 0 |                          |                   
-(2 rows)
-
-CREATE INDEX ON PUBLIC."Hypertable_1" (time, "temp_c");
-CREATE INDEX "ind_humidity" ON PUBLIC."Hypertable_1" (time, "humidity");
-CREATE INDEX "ind_sensor_1" ON PUBLIC."Hypertable_1" (time, "sensor_1");
-INSERT INTO PUBLIC."Hypertable_1"(time, "Device_id", temp_c, humidity, sensor_1, sensor_2, sensor_3, sensor_4)
-VALUES(1257894000000000000, 'dev1', 30, 70, 1, 2, 3, 100);
-CREATE UNIQUE INDEX "Unique1" ON PUBLIC."Hypertable_1" (time, "Device_id");
-CREATE UNIQUE INDEX "Unique1" ON "customSchema"."Hypertable_1" (time);
-INSERT INTO "customSchema"."Hypertable_1"(time, "Device_id", temp_c, humidity, sensor_1, sensor_2, sensor_3, sensor_4)
-VALUES(1257894000000000000, 'dev1', 30, 70, 1, 2, 3, 100);
-INSERT INTO "customSchema"."Hypertable_1"(time, "Device_id", temp_c, humidity, sensor_1, sensor_2, sensor_3, sensor_4)
-VALUES(1257894000000000001, 'dev1', 30, 70, 1, 2, 3, 100);
-SELECT * FROM _timescaledb_catalog.chunk_index ORDER BY hypertable_id, hypertable_index_name, chunk_id;
- chunk_id |                    index_name                    | hypertable_id |      hypertable_index_name      
-----------+--------------------------------------------------+---------------+---------------------------------
-        1 | _hyper_1_1_chunk_Hypertable_1_Device_id_time_idx |             1 | Hypertable_1_Device_id_time_idx
-        1 | _hyper_1_1_chunk_Hypertable_1_time_Device_id_idx |             1 | Hypertable_1_time_Device_id_idx
-        1 | _hyper_1_1_chunk_Hypertable_1_time_idx           |             1 | Hypertable_1_time_idx
-        1 | _hyper_1_1_chunk_Hypertable_1_time_temp_c_idx    |             1 | Hypertable_1_time_temp_c_idx
-        1 | _hyper_1_1_chunk_Unique1                         |             1 | Unique1
-        1 | _hyper_1_1_chunk_ind_humidity                    |             1 | ind_humidity
-        1 | _hyper_1_1_chunk_ind_sensor_1                    |             1 | ind_sensor_1
-        2 | _hyper_2_2_chunk_Hypertable_1_time_Device_id_idx |             2 | Hypertable_1_time_Device_id_idx
-        2 | _hyper_2_2_chunk_Hypertable_1_time_idx           |             2 | Hypertable_1_time_idx
-        2 | _hyper_2_2_chunk_Unique1                         |             2 | Unique1
-(10 rows)
-
---expect error cases
-\set ON_ERROR_STOP 0
-INSERT INTO "customSchema"."Hypertable_1"(time, "Device_id", temp_c, humidity, sensor_1, sensor_2, sensor_3, sensor_4)
-VALUES(1257894000000000000, 'dev1', 31, 71, 72, 4, 1, 102);
-psql:include/ddl_ops_1.sql:56: ERROR:  duplicate key value violates unique constraint "_hyper_2_2_chunk_Unique1"
-CREATE UNIQUE INDEX "Unique2" ON PUBLIC."Hypertable_1" ("Device_id");
-psql:include/ddl_ops_1.sql:57: ERROR:  cannot create a unique index without the column "time" (used in partitioning)
-CREATE UNIQUE INDEX "Unique2" ON PUBLIC."Hypertable_1" (time);
-psql:include/ddl_ops_1.sql:58: ERROR:  cannot create a unique index without the column "Device_id" (used in partitioning)
-CREATE UNIQUE INDEX "Unique2" ON PUBLIC."Hypertable_1" (sensor_1);
-psql:include/ddl_ops_1.sql:59: ERROR:  cannot create a unique index without the column "time" (used in partitioning)
-UPDATE ONLY PUBLIC."Hypertable_1" SET time = 0 WHERE TRUE;
-DELETE FROM ONLY PUBLIC."Hypertable_1" WHERE "Device_id" = 'dev1';
-\set ON_ERROR_STOP 1
-CREATE TABLE my_ht (time BIGINT, val integer);
-SELECT * FROM create_hypertable('my_ht', 'time', chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
-psql:include/ddl_ops_1.sql:66: NOTICE:  adding not-null constraint to column "time"
- hypertable_id | schema_name | table_name | created 
----------------+-------------+------------+---------
-             3 | public      | my_ht      | t
-(1 row)
-
-ALTER TABLE my_ht ADD COLUMN val2 integer;
-SELECT * FROM test.show_columns('my_ht');
- Column |  Type   | NotNull 
---------+---------+---------
- time   | bigint  | t
- val    | integer | f
- val2   | integer | f
-(3 rows)
-
--- Should error when adding again
-\set ON_ERROR_STOP 0
-ALTER TABLE my_ht ADD COLUMN val2 integer;
-psql:include/ddl_ops_1.sql:72: ERROR:  column "val2" of relation "my_ht" already exists
-\set ON_ERROR_STOP 1
--- Should create
-ALTER TABLE my_ht ADD COLUMN IF NOT EXISTS val3 integer;
-SELECT * FROM test.show_columns('my_ht');
- Column |  Type   | NotNull 
---------+---------+---------
- time   | bigint  | t
- val    | integer | f
- val2   | integer | f
- val3   | integer | f
-(4 rows)
-
--- Should skip and not error
-ALTER TABLE my_ht ADD COLUMN IF NOT EXISTS val3 integer;
-psql:include/ddl_ops_1.sql:80: NOTICE:  column "val3" of relation "my_ht" already exists, skipping
-SELECT * FROM test.show_columns('my_ht');
- Column |  Type   | NotNull 
---------+---------+---------
- time   | bigint  | t
- val    | integer | f
- val2   | integer | f
- val3   | integer | f
-(4 rows)
-
--- Should drop
-ALTER TABLE my_ht DROP COLUMN IF EXISTS val3;
-SELECT * FROM test.show_columns('my_ht');
- Column |  Type   | NotNull 
---------+---------+---------
- time   | bigint  | t
- val    | integer | f
- val2   | integer | f
-(3 rows)
-
--- Should skip and not error
-ALTER TABLE my_ht DROP COLUMN IF EXISTS val3;
-psql:include/ddl_ops_1.sql:88: NOTICE:  column "val3" of relation "my_ht" does not exist, skipping
-SELECT * FROM test.show_columns('my_ht');
- Column |  Type   | NotNull 
---------+---------+---------
- time   | bigint  | t
- val    | integer | f
- val2   | integer | f
-(3 rows)
-
---Test default index creation on create_hypertable().
---Make sure that we do not duplicate indexes that already exists
---
---No existing indexes: both time and space-time indexes created
-BEGIN;
-CREATE TABLE PUBLIC."Hypertable_1_with_default_index_enabled" (
-  "Time" BIGINT NOT NULL,
-  "Device_id" TEXT NOT NULL,
-  sensor_1 NUMERIC NULL DEFAULT 1
-);
-SELECT * FROM create_hypertable('"public"."Hypertable_1_with_default_index_enabled"', 'Time', 'Device_id', 1, chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
- hypertable_id | schema_name |               table_name                | created 
----------------+-------------+-----------------------------------------+---------
-             4 | public      | Hypertable_1_with_default_index_enabled | t
-(1 row)
-
-SELECT * FROM test.show_indexes('"Hypertable_1_with_default_index_enabled"');
-                            Index                             |     Columns      | Expr | Unique | Primary | Exclusion | Tablespace 
---------------------------------------------------------------+------------------+------+--------+---------+-----------+------------
- "Hypertable_1_with_default_index_enabled_Device_id_Time_idx" | {Device_id,Time} |      | f      | f       | f         | 
- "Hypertable_1_with_default_index_enabled_Time_idx"           | {Time}           |      | f      | f       | f         | 
-(2 rows)
-
-ROLLBACK;
---Space index exists: only time index created
-BEGIN;
-CREATE TABLE PUBLIC."Hypertable_1_with_default_index_enabled" (
-  "Time" BIGINT NOT NULL,
-  "Device_id" TEXT NOT NULL,
-  sensor_1 NUMERIC NULL DEFAULT 1
-);
-CREATE INDEX ON PUBLIC."Hypertable_1_with_default_index_enabled" ("Device_id", "Time" DESC);
-SELECT * FROM create_hypertable('"public"."Hypertable_1_with_default_index_enabled"', 'Time', 'Device_id', 1, chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
- hypertable_id | schema_name |               table_name                | created 
----------------+-------------+-----------------------------------------+---------
-             5 | public      | Hypertable_1_with_default_index_enabled | t
-(1 row)
-
-SELECT * FROM test.show_indexes('"Hypertable_1_with_default_index_enabled"');
-                            Index                             |     Columns      | Expr | Unique | Primary | Exclusion | Tablespace 
---------------------------------------------------------------+------------------+------+--------+---------+-----------+------------
- "Hypertable_1_with_default_index_enabled_Device_id_Time_idx" | {Device_id,Time} |      | f      | f       | f         | 
- "Hypertable_1_with_default_index_enabled_Time_idx"           | {Time}           |      | f      | f       | f         | 
-(2 rows)
-
-ROLLBACK;
---Time index exists, only partition index created
-BEGIN;
-CREATE TABLE PUBLIC."Hypertable_1_with_default_index_enabled" (
-  "Time" BIGINT NOT NULL,
-  "Device_id" TEXT NOT NULL,
-  sensor_1 NUMERIC NULL DEFAULT 1
-);
-CREATE INDEX ON PUBLIC."Hypertable_1_with_default_index_enabled" ("Time" DESC);
-SELECT * FROM create_hypertable('"public"."Hypertable_1_with_default_index_enabled"', 'Time', 'Device_id', 1, chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
- hypertable_id | schema_name |               table_name                | created 
----------------+-------------+-----------------------------------------+---------
-             6 | public      | Hypertable_1_with_default_index_enabled | t
-(1 row)
-
-SELECT * FROM test.show_indexes('"Hypertable_1_with_default_index_enabled"');
-                            Index                             |     Columns      | Expr | Unique | Primary | Exclusion | Tablespace 
---------------------------------------------------------------+------------------+------+--------+---------+-----------+------------
- "Hypertable_1_with_default_index_enabled_Device_id_Time_idx" | {Device_id,Time} |      | f      | f       | f         | 
- "Hypertable_1_with_default_index_enabled_Time_idx"           | {Time}           |      | f      | f       | f         | 
-(2 rows)
-
-ROLLBACK;
---No space partitioning, only time index created
-BEGIN;
-CREATE TABLE PUBLIC."Hypertable_1_with_default_index_enabled" (
-  "Time" BIGINT NOT NULL,
-  "Device_id" TEXT NOT NULL,
-  sensor_1 NUMERIC NULL DEFAULT 1
-);
-SELECT * FROM create_hypertable('"public"."Hypertable_1_with_default_index_enabled"', 'Time', chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
- hypertable_id | schema_name |               table_name                | created 
----------------+-------------+-----------------------------------------+---------
-             7 | public      | Hypertable_1_with_default_index_enabled | t
-(1 row)
-
-SELECT * FROM test.show_indexes('"Hypertable_1_with_default_index_enabled"');
-                       Index                        | Columns | Expr | Unique | Primary | Exclusion | Tablespace 
-----------------------------------------------------+---------+------+--------+---------+-----------+------------
- "Hypertable_1_with_default_index_enabled_Time_idx" | {Time}  |      | f      | f       | f         | 
-(1 row)
-
-ROLLBACK;
---Disable index creation: no default indexes created
-BEGIN;
-CREATE TABLE PUBLIC."Hypertable_1_with_default_index_enabled" (
-  "Time" BIGINT NOT NULL,
-  "Device_id" TEXT NOT NULL,
-  sensor_1 NUMERIC NULL DEFAULT 1
-);
-SELECT * FROM create_hypertable('"public"."Hypertable_1_with_default_index_enabled"', 'Time', 'Device_id', 1, create_default_indexes=>FALSE, chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
- hypertable_id | schema_name |               table_name                | created 
----------------+-------------+-----------------------------------------+---------
-             8 | public      | Hypertable_1_with_default_index_enabled | t
-(1 row)
-
-SELECT * FROM test.show_indexes('"Hypertable_1_with_default_index_enabled"');
- Index | Columns | Expr | Unique | Primary | Exclusion | Tablespace 
--------+---------+------+--------+---------+-----------+------------
-(0 rows)
-
-ROLLBACK;
-SELECT * FROM PUBLIC."Hypertable_1";
-        time         | Device_id | temp_c | humidity | sensor_1 | sensor_2 | sensor_3 | sensor_4 
----------------------+-----------+--------+----------+----------+----------+----------+----------
- 1257894000000000000 | dev1      |     30 |       70 |        1 |        2 |        3 |      100
-(1 row)
-
-SELECT * FROM ONLY PUBLIC."Hypertable_1";
- time | Device_id | temp_c | humidity | sensor_1 | sensor_2 | sensor_3 | sensor_4 
-------+-----------+--------+----------+----------+----------+----------+----------
-(0 rows)
-
-EXPLAIN (costs off) SELECT * FROM ONLY PUBLIC."Hypertable_1";
-         QUERY PLAN         
-----------------------------
- Seq Scan on "Hypertable_1"
-(1 row)
-
-SELECT * FROM test.show_columns('PUBLIC."Hypertable_1"');
-  Column   |  Type   | NotNull 
------------+---------+---------
- time      | bigint  | t
- Device_id | text    | t
- temp_c    | integer | t
- humidity  | numeric | f
- sensor_1  | numeric | f
- sensor_2  | numeric | t
- sensor_3  | numeric | t
- sensor_4  | numeric | t
-(8 rows)
-
-SELECT * FROM test.show_columns('_timescaledb_internal._hyper_1_1_chunk');
-  Column   |  Type   | NotNull 
------------+---------+---------
- time      | bigint  | t
- Device_id | text    | t
- temp_c    | integer | t
- humidity  | numeric | f
- sensor_1  | numeric | f
- sensor_2  | numeric | t
- sensor_3  | numeric | t
- sensor_4  | numeric | t
-(8 rows)
-
-\ir include/ddl_ops_2.sql
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-ALTER TABLE PUBLIC."Hypertable_1" ADD COLUMN temp_f INTEGER NOT NULL DEFAULT 31;
-ALTER TABLE PUBLIC."Hypertable_1" DROP COLUMN temp_c;
-ALTER TABLE PUBLIC."Hypertable_1" DROP COLUMN sensor_4;
-ALTER TABLE PUBLIC."Hypertable_1" ALTER COLUMN humidity SET DEFAULT 100;
-ALTER TABLE PUBLIC."Hypertable_1" ALTER COLUMN sensor_1 DROP DEFAULT;
-ALTER TABLE PUBLIC."Hypertable_1" ALTER COLUMN sensor_2 SET DEFAULT NULL;
-ALTER TABLE PUBLIC."Hypertable_1" ALTER COLUMN sensor_1 SET NOT NULL;
-ALTER TABLE PUBLIC."Hypertable_1" ALTER COLUMN sensor_2 DROP NOT NULL;
-ALTER TABLE PUBLIC."Hypertable_1" RENAME COLUMN sensor_2 TO sensor_2_renamed;
-ALTER TABLE PUBLIC."Hypertable_1" RENAME COLUMN sensor_3 TO sensor_3_renamed;
-DROP INDEX "ind_sensor_1";
-CREATE OR REPLACE FUNCTION empty_trigger_func()
-    RETURNS TRIGGER LANGUAGE PLPGSQL AS
-$BODY$
-BEGIN
-END
-$BODY$;
-CREATE TRIGGER test_trigger BEFORE UPDATE OR DELETE ON PUBLIC."Hypertable_1"
-FOR EACH STATEMENT EXECUTE FUNCTION empty_trigger_func();
-ALTER TABLE PUBLIC."Hypertable_1" ALTER COLUMN sensor_2_renamed SET DATA TYPE int;
-ALTER INDEX "ind_humidity" RENAME TO "ind_humdity2";
--- Change should be reflected here
-SELECT * FROM _timescaledb_catalog.chunk_index;
- chunk_id |                    index_name                    | hypertable_id |      hypertable_index_name      
-----------+--------------------------------------------------+---------------+---------------------------------
-        1 | _hyper_1_1_chunk_Hypertable_1_time_Device_id_idx |             1 | Hypertable_1_time_Device_id_idx
-        1 | _hyper_1_1_chunk_Hypertable_1_time_idx           |             1 | Hypertable_1_time_idx
-        1 | _hyper_1_1_chunk_Hypertable_1_Device_id_time_idx |             1 | Hypertable_1_Device_id_time_idx
-        1 | _hyper_1_1_chunk_Unique1                         |             1 | Unique1
-        2 | _hyper_2_2_chunk_Hypertable_1_time_Device_id_idx |             2 | Hypertable_1_time_Device_id_idx
-        2 | _hyper_2_2_chunk_Hypertable_1_time_idx           |             2 | Hypertable_1_time_idx
-        2 | _hyper_2_2_chunk_Unique1                         |             2 | Unique1
-        1 | _hyper_1_1_chunk_ind_humdity2                    |             1 | ind_humdity2
-(8 rows)
-
---create column with same name as previously renamed one
-ALTER TABLE PUBLIC."Hypertable_1" ADD COLUMN sensor_3 BIGINT NOT NULL DEFAULT 131;
---create column with same name as previously dropped one
-ALTER TABLE PUBLIC."Hypertable_1" ADD COLUMN sensor_4 BIGINT NOT NULL DEFAULT 131;
-SELECT * FROM test.show_columns('PUBLIC."Hypertable_1"');
-      Column      |  Type   | NotNull 
-------------------+---------+---------
- time             | bigint  | t
- Device_id        | text    | t
- humidity         | numeric | f
- sensor_1         | numeric | t
- sensor_2_renamed | integer | f
- sensor_3_renamed | numeric | t
- temp_f           | integer | t
- sensor_3         | bigint  | t
- sensor_4         | bigint  | t
-(9 rows)
-
-SELECT * FROM test.show_columns('_timescaledb_internal._hyper_1_1_chunk');
-      Column      |  Type   | NotNull 
-------------------+---------+---------
- time             | bigint  | t
- Device_id        | text    | t
- humidity         | numeric | f
- sensor_1         | numeric | t
- sensor_2_renamed | integer | f
- sensor_3_renamed | numeric | t
- temp_f           | integer | t
- sensor_3         | bigint  | t
- sensor_4         | bigint  | t
-(9 rows)
-
-SELECT * FROM PUBLIC."Hypertable_1";
-        time         | Device_id | humidity | sensor_1 | sensor_2_renamed | sensor_3_renamed | temp_f | sensor_3 | sensor_4 
----------------------+-----------+----------+----------+------------------+------------------+--------+----------+----------
- 1257894000000000000 | dev1      |       70 |        1 |                2 |                3 |     31 |      131 |      131
-(1 row)
-
--- alter column tests
-CREATE TABLE alter_test(time timestamptz, temp float, color varchar(10));
--- create hypertable with two chunks
-SELECT create_hypertable('alter_test', 'time', 'color', 2, chunk_time_interval => 2628000000000);
-WARNING:  column type "character varying" used for "color" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-    create_hypertable    
--------------------------
- (9,public,alter_test,t)
-(1 row)
-
-INSERT INTO alter_test VALUES ('2017-01-20T09:00:01', 17.5, 'blue'),
-                              ('2017-01-21T09:00:01', 19.1, 'yellow'),
-                              ('2017-04-20T09:00:01', 89.5, 'green'),
-                              ('2017-04-21T09:00:01', 17.1, 'black');
-SELECT * FROM test.show_columns('alter_test');
- Column |           Type           | NotNull 
---------+--------------------------+---------
- time   | timestamp with time zone | t
- temp   | double precision         | f
- color  | character varying        | f
-(3 rows)
-
-SELECT * FROM test.show_columnsp('_timescaledb_internal._hyper_9_%chunk');
-                Relation                | Kind | Column |       Column type        | NotNull 
-----------------------------------------+------+--------+--------------------------+---------
- _timescaledb_internal._hyper_9_3_chunk | r    | time   | timestamp with time zone | t
- _timescaledb_internal._hyper_9_3_chunk | r    | temp   | double precision         | f
- _timescaledb_internal._hyper_9_3_chunk | r    | color  | character varying        | f
- _timescaledb_internal._hyper_9_4_chunk | r    | time   | timestamp with time zone | t
- _timescaledb_internal._hyper_9_4_chunk | r    | temp   | double precision         | f
- _timescaledb_internal._hyper_9_4_chunk | r    | color  | character varying        | f
- _timescaledb_internal._hyper_9_5_chunk | r    | time   | timestamp with time zone | t
- _timescaledb_internal._hyper_9_5_chunk | r    | temp   | double precision         | f
- _timescaledb_internal._hyper_9_5_chunk | r    | color  | character varying        | f
-(9 rows)
-
--- show the column name and type of the partitioning dimension in the
--- metadata table
-SELECT * FROM _timescaledb_catalog.dimension WHERE hypertable_id = 9;
- id | hypertable_id | column_name |       column_type        | aligned | num_slices | partitioning_func_schema | partitioning_func  | interval_length | compress_interval_length | integer_now_func_schema | integer_now_func 
-----+---------------+-------------+--------------------------+---------+------------+--------------------------+--------------------+-----------------+--------------------------+-------------------------+------------------
- 15 |             9 | color       | character varying        | f       |          2 | _timescaledb_internal    | get_partition_hash |                 |                          |                         | 
- 14 |             9 | time        | timestamp with time zone | t       |            |                          |                    |   2628000000000 |                          |                         | 
-(2 rows)
-
-EXPLAIN (costs off)
-SELECT * FROM alter_test WHERE time > '2017-05-20T10:00:01';
-                                       QUERY PLAN                                        
------------------------------------------------------------------------------------------
- Append
-   ->  Index Scan using _hyper_9_4_chunk_alter_test_time_idx on _hyper_9_4_chunk
-         Index Cond: ("time" > 'Sat May 20 10:00:01 2017 PDT'::timestamp with time zone)
-   ->  Index Scan using _hyper_9_5_chunk_alter_test_time_idx on _hyper_9_5_chunk
-         Index Cond: ("time" > 'Sat May 20 10:00:01 2017 PDT'::timestamp with time zone)
-(5 rows)
-
--- rename column and change its type
-ALTER TABLE alter_test RENAME COLUMN time TO time_us;
---converting timestamptz->timestamp should happen under UTC
-SET timezone = 'UTC';
-ALTER TABLE alter_test ALTER COLUMN time_us TYPE timestamp;
-RESET timezone;
-ALTER TABLE alter_test RENAME COLUMN color TO colorname;
-\set ON_ERROR_STOP 0
--- Changing types on hash-partitioned columns is not safe for some
--- types and is therefore blocked.
-ALTER TABLE alter_test ALTER COLUMN colorname TYPE text;
-ERROR:  cannot change the type of a hash-partitioned column
-\set ON_ERROR_STOP 1
-SELECT * FROM test.show_columns('alter_test');
-  Column   |            Type             | NotNull 
------------+-----------------------------+---------
- time_us   | timestamp without time zone | t
- temp      | double precision            | f
- colorname | character varying           | f
-(3 rows)
-
-SELECT * FROM test.show_columnsp('_timescaledb_internal._hyper_9_%chunk');
-                Relation                | Kind |  Column   |         Column type         | NotNull 
-----------------------------------------+------+-----------+-----------------------------+---------
- _timescaledb_internal._hyper_9_3_chunk | r    | time_us   | timestamp without time zone | t
- _timescaledb_internal._hyper_9_3_chunk | r    | temp      | double precision            | f
- _timescaledb_internal._hyper_9_3_chunk | r    | colorname | character varying           | f
- _timescaledb_internal._hyper_9_4_chunk | r    | time_us   | timestamp without time zone | t
- _timescaledb_internal._hyper_9_4_chunk | r    | temp      | double precision            | f
- _timescaledb_internal._hyper_9_4_chunk | r    | colorname | character varying           | f
- _timescaledb_internal._hyper_9_5_chunk | r    | time_us   | timestamp without time zone | t
- _timescaledb_internal._hyper_9_5_chunk | r    | temp      | double precision            | f
- _timescaledb_internal._hyper_9_5_chunk | r    | colorname | character varying           | f
-(9 rows)
-
--- show that the metadata has been updated
-SELECT * FROM _timescaledb_catalog.dimension WHERE hypertable_id = 9;
- id | hypertable_id | column_name |         column_type         | aligned | num_slices | partitioning_func_schema | partitioning_func  | interval_length | compress_interval_length | integer_now_func_schema | integer_now_func 
-----+---------------+-------------+-----------------------------+---------+------------+--------------------------+--------------------+-----------------+--------------------------+-------------------------+------------------
- 15 |             9 | colorname   | character varying           | f       |          2 | _timescaledb_internal    | get_partition_hash |                 |                          |                         | 
- 14 |             9 | time_us     | timestamp without time zone | t       |            |                          |                    |   2628000000000 |                          |                         | 
-(2 rows)
-
--- constraint exclusion should still work with updated column
-EXPLAIN (costs off)
-SELECT * FROM alter_test WHERE time_us > '2017-05-20T10:00:01';
-                                     QUERY PLAN                                      
--------------------------------------------------------------------------------------
- Append
-   ->  Seq Scan on _hyper_9_4_chunk
-         Filter: (time_us > 'Sat May 20 10:00:01 2017'::timestamp without time zone)
-   ->  Seq Scan on _hyper_9_5_chunk
-         Filter: (time_us > 'Sat May 20 10:00:01 2017'::timestamp without time zone)
-(5 rows)
-
-\set ON_ERROR_STOP 0
--- verify that we cannot change the column type to something incompatible
-ALTER TABLE alter_test ALTER COLUMN colorname TYPE varchar(3);
-ERROR:  cannot change the type of a hash-partitioned column
--- conversion that messes up partitioning fails
-ALTER TABLE alter_test ALTER COLUMN time_us TYPE timestamptz USING time_us::timestamptz+INTERVAL '1 year';
-ERROR:  check constraint "constraint_4" of relation "_hyper_9_3_chunk" is violated by some row
--- dropping column that messes up partiitoning fails
-ALTER TABLE alter_test DROP COLUMN colorname;
-ERROR:  cannot drop column named in partition key
---ONLY blocked
-ALTER TABLE ONLY alter_test RENAME COLUMN colorname TO colorname2;
-ERROR:  inherited column "colorname" must be renamed in child tables too
-ALTER TABLE ONLY alter_test ALTER COLUMN colorname TYPE varchar(10);
-ERROR:  ONLY option not supported on hypertable operations
-\set ON_ERROR_STOP 1
-CREATE TABLE alter_test_bigint(time bigint, temp float);
-SELECT create_hypertable('alter_test_bigint', 'time', chunk_time_interval => 2628000000000);
-NOTICE:  adding not-null constraint to column "time"
-        create_hypertable        
----------------------------------
- (10,public,alter_test_bigint,t)
-(1 row)
-
-\set ON_ERROR_STOP 0
--- Changing type of time dimension to a non-supported type
--- shall not be allowed
-ALTER TABLE alter_test_bigint
-ALTER COLUMN time TYPE TEXT;
-ERROR:  cannot change data type of hypertable column "time" from bigint to text
--- dropping open time dimension shall not be allowed.
-ALTER TABLE alter_test_bigint
-DROP COLUMN time;
-ERROR:  cannot drop column named in partition key
-\set ON_ERROR_STOP 1
--- test expression index creation where physical layout of chunks differs from hypertable
-CREATE TABLE i2504(time timestamp NOT NULL, a int, b int, c int, d int);
-select create_hypertable('i2504', 'time');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-  create_hypertable  
----------------------
- (11,public,i2504,t)
-(1 row)
-
-INSERT INTO i2504 VALUES (now(), 1, 2, 3, 4);
-ALTER TABLE i2504 DROP COLUMN b;
-INSERT INTO i2504(time, a, c, d) VALUES
-(now() - interval '1 year', 1, 2, 3),
-(now() - interval '2 years', 1, 2, 3);
-CREATE INDEX idx2 ON i2504(a,d) WHERE c IS NOT NULL;
-DROP INDEX idx2;
-CREATE INDEX idx2 ON i2504(a,d) WITH (timescaledb.transaction_per_chunk) WHERE c IS NOT NULL;
--- Make sure custom composite types are supported as dimensions
-CREATE TYPE TUPLE as (val1 int4, val2 int4);
-CREATE TABLE part_custom_dim (time TIMESTAMPTZ, combo TUPLE, device TEXT);
-\set ON_ERROR_STOP 0
--- should fail on PG < 14 because no partitioning function supplied and the given custom type
--- has no default hash function
--- on PG14 custom types are hashable
-SELECT create_hypertable('part_custom_dim', 'time', 'combo', 4);
-NOTICE:  adding not-null constraint to column "time"
-       create_hypertable       
--------------------------------
- (12,public,part_custom_dim,t)
-(1 row)
-
-\set ON_ERROR_STOP 1
--- immutable functions with sub-transaction (issue #4489)
-CREATE FUNCTION i4489(value TEXT DEFAULT '') RETURNS INTEGER
-AS
-$$
-BEGIN
-  RETURN value::INTEGER;
-EXCEPTION WHEN invalid_text_representation THEN
-  RETURN 0;
-END;
-$$
-LANGUAGE PLPGSQL IMMUTABLE;
--- should return 1 (one) in both cases
-SELECT i4489('1'), i4489('1');
- i4489 | i4489 
--------+-------
-     1 |     1
-(1 row)
-
--- should return 0 (zero) in all cases handled by the exception
-SELECT i4489(), i4489();
- i4489 | i4489 
--------+-------
-     0 |     0
-(1 row)
-
-SELECT i4489('a'), i4489('a');
- i4489 | i4489 
--------+-------
-     0 |     0
-(1 row)
-
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/ddl_errors.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/ddl_errors.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/ddl_errors.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/ddl_errors.out	2023-11-25 05:27:33.765052852 +0000
@@ -1,146 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-CREATE TABLE PUBLIC."Hypertable_1" (
-  time BIGINT NOT NULL,
-  "Device_id" TEXT NOT NULL,
-  temp_c int NOT NULL DEFAULT -1
-);
-CREATE INDEX ON PUBLIC."Hypertable_1" (time, "Device_id");
-\set ON_ERROR_STOP 0
-SELECT * FROM create_hypertable(NULL, NULL);
-ERROR:  relation cannot be NULL
-SELECT * FROM create_hypertable('"public"."Hypertable_1"', NULL);
-ERROR:  time column cannot be NULL
--- integer time dimensions require an explicit interval
-SELECT * FROM create_hypertable('"public"."Hypertable_1"', 'time');
-ERROR:  integer dimensions require an explicit interval
--- space dimensions require explicit number of partitions
-SELECT * FROM create_hypertable('"public"."Hypertable_1"', 'time', 'Device_id', chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
-ERROR:  invalid number of partitions for dimension "Device_id"
-SELECT * FROM create_hypertable('"public"."Hypertable_1_mispelled"', 'time', 'Device_id', 2, chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
-ERROR:  relation "public.Hypertable_1_mispelled" does not exist at character 33
-SELECT * FROM create_hypertable('"public"."Hypertable_1"', 'time_mispelled', 'Device_id', 2, chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
-ERROR:  column "time_mispelled" does not exist
-SELECT * FROM create_hypertable('"public"."Hypertable_1"', 'Device_id', 'Device_id', 2, chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
-ERROR:  invalid type for dimension "Device_id"
-SELECT * FROM create_hypertable('"public"."Hypertable_1"', 'time', 'Device_id_mispelled', 2, chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
-ERROR:  column "Device_id_mispelled" does not exist
-INSERT INTO PUBLIC."Hypertable_1" VALUES(1,'dev_1', 3);
-SELECT * FROM create_hypertable('"public"."Hypertable_1"', 'time', 'Device_id', 2, chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
-ERROR:  table "Hypertable_1" is not empty
-DELETE FROM  PUBLIC."Hypertable_1" ;
-\set ON_ERROR_STOP 1
-SELECT * FROM create_hypertable('"public"."Hypertable_1"', 'time', 'Device_id', 2, chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
- hypertable_id | schema_name |  table_name  | created 
----------------+-------------+--------------+---------
-             1 | public      | Hypertable_1 | t
-(1 row)
-
-\set ON_ERROR_STOP 0
-SELECT * FROM create_hypertable('"public"."Hypertable_1"', 'time', 'Device_id', 2, chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
-ERROR:  table "Hypertable_1" is already a hypertable
-\set ON_ERROR_STOP 1
-INSERT INTO "Hypertable_1" VALUES (0, 1, 0);
-\set ON_ERROR_STOP 0
-ALTER TABLE _timescaledb_internal._hyper_1_1_chunk ALTER COLUMN temp_c DROP NOT NULL;
-ERROR:  operation not supported on chunk tables
-\set ON_ERROR_STOP 1
-CREATE TABLE PUBLIC."Parent" (
-  time BIGINT NOT NULL,
-  "Device_id" TEXT NOT NULL,
-  temp_c int NOT NULL DEFAULT -1
-);
-\set ON_ERROR_STOP 0
-ALTER TABLE "Hypertable_1" INHERIT "Parent";
-ERROR:  hypertables do not support inheritance
-ALTER TABLE _timescaledb_internal._hyper_1_1_chunk INHERIT "Parent";
-ERROR:  operation not supported on chunk tables
-ALTER TABLE _timescaledb_internal._hyper_1_1_chunk NO INHERIT "Parent";
-ERROR:  operation not supported on chunk tables
-\set ON_ERROR_STOP 1
-CREATE TABLE PUBLIC."Child" () INHERITS ("Parent");
-\set ON_ERROR_STOP 0
-SELECT * FROM create_hypertable('"public"."Parent"', 'time', chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
-ERROR:  table "Parent" is already partitioned
-SELECT * FROM create_hypertable('"public"."Child"', 'time', chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
-ERROR:  table "Child" is already partitioned
-\set ON_ERROR_STOP 1
-CREATE UNLOGGED TABLE PUBLIC."Hypertable_unlogged" (
-  time BIGINT NOT NULL,
-  "Device_id" TEXT NOT NULL,
-  temp_c int NOT NULL DEFAULT -1
-);
-\set ON_ERROR_STOP 0
-SELECT * FROM create_hypertable('"public"."Hypertable_unlogged"', 'time', chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
-ERROR:  table "Hypertable_unlogged" has to be logged
-\set ON_ERROR_STOP 1
-ALTER TABLE PUBLIC."Hypertable_unlogged" SET LOGGED;
-SELECT * FROM create_hypertable('"public"."Hypertable_unlogged"', 'time', chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
- hypertable_id | schema_name |     table_name      | created 
----------------+-------------+---------------------+---------
-             2 | public      | Hypertable_unlogged | t
-(1 row)
-
-CREATE TEMP TABLE "Hypertable_temp" (
-  time BIGINT NOT NULL,
-  "Device_id" TEXT NOT NULL,
-  temp_c int NOT NULL DEFAULT -1
-);
-\set ON_ERROR_STOP 0
-SELECT * FROM create_hypertable('"Hypertable_temp"', 'time', chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
-ERROR:  table "Hypertable_temp" has to be logged
-ALTER TABLE "Hypertable_1" SET UNLOGGED;
-ERROR:  logging cannot be turned off for hypertables
-\set ON_ERROR_STOP 1
-ALTER TABLE "Hypertable_1" SET LOGGED;
-CREATE TABLE PUBLIC."Hypertable_1_replica_ident" (
-  time BIGINT NOT NULL,
-  "Device_id" TEXT NOT NULL,
-  temp_c int NOT NULL DEFAULT -1
-);
-ALTER TABLE "Hypertable_1_replica_ident" REPLICA IDENTITY FULL;
-\set ON_ERROR_STOP 0
-SELECT * FROM create_hypertable('"public"."Hypertable_1_replica_ident"', 'time', chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
-ERROR:  table "Hypertable_1_replica_ident" has replica identity set
-ALTER TABLE "Hypertable_1" REPLICA IDENTITY FULL;
-ERROR:  hypertables do not support logical replication
-\set ON_ERROR_STOP 1
-CREATE TABLE PUBLIC."Hypertable_1_rule" (
-  time BIGINT NOT NULL,
-  "Device_id" TEXT NOT NULL,
-  temp_c int NOT NULL DEFAULT -1
-);
-CREATE RULE notify_me AS ON UPDATE TO "Hypertable_1_rule" DO ALSO NOTIFY "Hypertable_1_rule";
-\set ON_ERROR_STOP 0
-SELECT * FROM create_hypertable('"public"."Hypertable_1_rule"', 'time', chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
-ERROR:  hypertables do not support rules
-\set ON_ERROR_STOP 1
-ALTER TABLE "Hypertable_1_rule" DISABLE RULE notify_me;
-\set ON_ERROR_STOP 0
-SELECT * FROM create_hypertable('"public"."Hypertable_1_rule"', 'time', chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
-ERROR:  hypertables do not support rules
-\set ON_ERROR_STOP 1
-DROP RULE notify_me ON "Hypertable_1_rule";
-SELECT * FROM create_hypertable('"public"."Hypertable_1_rule"', 'time', chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
- hypertable_id | schema_name |    table_name     | created 
----------------+-------------+-------------------+---------
-             3 | public      | Hypertable_1_rule | t
-(1 row)
-
-\set ON_ERROR_STOP 0
-CREATE RULE notify_me AS ON UPDATE TO "Hypertable_1_rule" DO ALSO NOTIFY "Hypertable_1_rule";
-ERROR:  hypertables do not support rules
-\set ON_ERROR_STOP 1
-\set ON_ERROR_STOP 0
-SELECT add_dimension(NULL,NULL);
-ERROR:  hypertable cannot be NULL
-\set ON_ERROR_STOP 1
-\set ON_ERROR_STOP 0
-SELECT attach_tablespace(NULL,NULL);
-ERROR:  invalid tablespace name
-\set ON_ERROR_STOP 1
-\set ON_ERROR_STOP 0
-select set_number_partitions(NULL,NULL);
-ERROR:  hypertable cannot be NULL
-\set ON_ERROR_STOP 1
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/ddl_extra.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/ddl_extra.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/ddl_extra.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/ddl_extra.out	2023-11-25 05:27:33.781052806 +0000
@@ -1,88 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-CREATE OR REPLACE FUNCTION show_columns_ext(rel regclass)
-RETURNS TABLE("Column" name,
-              "Type" text,
-              "NotNull" boolean,
-              "Compression" text) LANGUAGE SQL STABLE AS
-$BODY$
-    SELECT a.attname,
-    format_type(t.oid, t.typtypmod),
-    a.attnotnull,
-    (CASE WHEN a.attcompression = 'l' THEN 'lz4' WHEN a.attcompression = 'p' THEN 'pglz' ELSE '' END)
-    FROM pg_attribute a, pg_type t
-    WHERE a.attrelid = rel
-    AND a.atttypid = t.oid
-    AND a.attnum >= 0
-    ORDER BY a.attnum;
-$BODY$;
-CREATE TABLE conditions (
-  time TIMESTAMP NOT NULL,
-  location TEXT NOT NULL,
-  temperature DOUBLE PRECISION NULL,
-  humidity DOUBLE PRECISION NULL
-);
-SELECT create_hypertable('conditions', 'time', chunk_time_interval := '1 day'::interval);
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-    create_hypertable    
--------------------------
- (1,public,conditions,t)
-(1 row)
-
-INSERT INTO conditions
-SELECT generate_series('2021-10-10 00:00'::timestamp, '2021-10-11 00:00'::timestamp, '1 day'), 'POR', 55, 75;
-CREATE VIEW t AS
-    SELECT 'conditions'::regclass AS r
-    UNION ALL
-    SELECT * FROM show_chunks('conditions');
-SELECT * FROM t, LATERAL show_columns_ext(r) WHERE "Column" = 'location' ORDER BY 1, 2;
-                   r                    |  Column  | Type | NotNull | Compression 
-----------------------------------------+----------+------+---------+-------------
- conditions                             | location | text | t       | 
- _timescaledb_internal._hyper_1_1_chunk | location | text | t       | 
- _timescaledb_internal._hyper_1_2_chunk | location | text | t       | 
-(3 rows)
-
-ALTER TABLE conditions ALTER COLUMN location SET COMPRESSION pglz;
-SELECT * FROM t, LATERAL show_columns_ext(r) WHERE "Column" = 'location' ORDER BY 1, 2;
-                   r                    |  Column  | Type | NotNull | Compression 
-----------------------------------------+----------+------+---------+-------------
- conditions                             | location | text | t       | pglz
- _timescaledb_internal._hyper_1_1_chunk | location | text | t       | pglz
- _timescaledb_internal._hyper_1_2_chunk | location | text | t       | pglz
-(3 rows)
-
-INSERT INTO conditions VALUES ('2021-10-12 00:00'::timestamp, 'BRA', 66, 77);
-SELECT * FROM t, LATERAL show_columns_ext(r) WHERE "Column" = 'location' ORDER BY 1, 2;
-                   r                    |  Column  | Type | NotNull | Compression 
-----------------------------------------+----------+------+---------+-------------
- conditions                             | location | text | t       | pglz
- _timescaledb_internal._hyper_1_1_chunk | location | text | t       | pglz
- _timescaledb_internal._hyper_1_2_chunk | location | text | t       | pglz
- _timescaledb_internal._hyper_1_3_chunk | location | text | t       | pglz
-(4 rows)
-
-ALTER TABLE conditions ALTER COLUMN location SET COMPRESSION default;
-SELECT * FROM t, LATERAL show_columns_ext(r) WHERE "Column" = 'location' ORDER BY 1, 2;
-                   r                    |  Column  | Type | NotNull | Compression 
-----------------------------------------+----------+------+---------+-------------
- conditions                             | location | text | t       | 
- _timescaledb_internal._hyper_1_1_chunk | location | text | t       | 
- _timescaledb_internal._hyper_1_2_chunk | location | text | t       | 
- _timescaledb_internal._hyper_1_3_chunk | location | text | t       | 
-(4 rows)
-
-\set ON_ERROR_STOP 0
--- failing test because compression is not allowed in "non-TOASTable" datatypes
-ALTER TABLE conditions ALTER COLUMN temperature SET COMPRESSION pglz;
-ERROR:  column data type double precision does not support compression
-SELECT * FROM t, LATERAL show_columns_ext(r) WHERE "Column" = 'temperature' ORDER BY 1, 2;
-                   r                    |   Column    |       Type       | NotNull | Compression 
-----------------------------------------+-------------+------------------+---------+-------------
- conditions                             | temperature | double precision | f       | 
- _timescaledb_internal._hyper_1_1_chunk | temperature | double precision | f       | 
- _timescaledb_internal._hyper_1_2_chunk | temperature | double precision | f       | 
- _timescaledb_internal._hyper_1_3_chunk | temperature | double precision | f       | 
-(4 rows)
-
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/delete-15.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/delete-15.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/delete-15.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/delete-15.out	2023-11-25 05:27:33.769052841 +0000
@@ -1,185 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-\o /dev/null
-\ir include/insert_two_partitions.sql
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-CREATE TABLE PUBLIC."two_Partitions" (
-  "timeCustom" BIGINT NOT NULL,
-  device_id TEXT NOT NULL,
-  series_0 DOUBLE PRECISION NULL,
-  series_1 DOUBLE PRECISION NULL,
-  series_2 DOUBLE PRECISION NULL,
-  series_bool BOOLEAN NULL
-);
-CREATE INDEX ON PUBLIC."two_Partitions" (device_id, "timeCustom" DESC NULLS LAST) WHERE device_id IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, series_0) WHERE series_0 IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, series_1)  WHERE series_1 IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, series_2) WHERE series_2 IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, series_bool) WHERE series_bool IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, device_id);
-SELECT * FROM create_hypertable('"public"."two_Partitions"'::regclass, 'timeCustom'::name, 'device_id'::name, associated_schema_name=>'_timescaledb_internal'::text, number_partitions => 2, chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
-\set QUIET off
-BEGIN;
-\COPY public."two_Partitions" FROM 'data/ds1_dev1_1.tsv' NULL AS '';
-COMMIT;
-INSERT INTO public."two_Partitions"("timeCustom", device_id, series_0, series_1) VALUES
-(1257987600000000000, 'dev1', 1.5, 1),
-(1257987600000000000, 'dev1', 1.5, 2),
-(1257894000000000000, 'dev2', 1.5, 1),
-(1257894002000000000, 'dev1', 2.5, 3);
-INSERT INTO "two_Partitions"("timeCustom", device_id, series_0, series_1) VALUES
-(1257894000000000000, 'dev2', 1.5, 2);
-\set QUIET on
-\o
-SELECT * FROM "two_Partitions" ORDER BY "timeCustom", device_id, series_0, series_1;
-     timeCustom      | device_id | series_0 | series_1 | series_2 | series_bool 
----------------------+-----------+----------+----------+----------+-------------
- 1257894000000000000 | dev1      |      1.5 |        1 |        2 | t
- 1257894000000000000 | dev1      |      1.5 |        2 |          | 
- 1257894000000000000 | dev2      |      1.5 |        1 |          | 
- 1257894000000000000 | dev2      |      1.5 |        2 |          | 
- 1257894000000001000 | dev1      |      2.5 |        3 |          | 
- 1257894001000000000 | dev1      |      3.5 |        4 |          | 
- 1257894002000000000 | dev1      |      2.5 |        3 |          | 
- 1257894002000000000 | dev1      |      5.5 |        6 |          | t
- 1257894002000000000 | dev1      |      5.5 |        7 |          | f
- 1257897600000000000 | dev1      |      4.5 |        5 |          | f
- 1257987600000000000 | dev1      |      1.5 |        1 |          | 
- 1257987600000000000 | dev1      |      1.5 |        2 |          | 
-(12 rows)
-
-DELETE FROM "two_Partitions" WHERE series_0 = 1.5;
-DELETE FROM "two_Partitions" WHERE series_0 = 100;
-SELECT * FROM "two_Partitions" ORDER BY "timeCustom", device_id, series_0, series_1;
-     timeCustom      | device_id | series_0 | series_1 | series_2 | series_bool 
----------------------+-----------+----------+----------+----------+-------------
- 1257894000000001000 | dev1      |      2.5 |        3 |          | 
- 1257894001000000000 | dev1      |      3.5 |        4 |          | 
- 1257894002000000000 | dev1      |      2.5 |        3 |          | 
- 1257894002000000000 | dev1      |      5.5 |        6 |          | t
- 1257894002000000000 | dev1      |      5.5 |        7 |          | f
- 1257897600000000000 | dev1      |      4.5 |        5 |          | f
-(6 rows)
-
--- Make sure DELETE isn't optimized if it includes Append plans
--- Need to turn of nestloop to make append appear the same on PG96 and PG10
-set enable_nestloop = 'off';
-CREATE OR REPLACE FUNCTION series_val()
-RETURNS integer LANGUAGE PLPGSQL STABLE AS
-$BODY$
-BEGIN
-    RETURN 5;
-END;
-$BODY$;
--- ConstraintAwareAppend applied for SELECT
-EXPLAIN (costs off)
-SELECT FROM "two_Partitions"
-WHERE series_1 IN (SELECT series_1 FROM "two_Partitions" WHERE series_1 > series_val());
-                                                                   QUERY PLAN                                                                   
-------------------------------------------------------------------------------------------------------------------------------------------------
- Hash Join
-   Hash Cond: ("two_Partitions".series_1 = "two_Partitions_1".series_1)
-   ->  Custom Scan (ChunkAppend) on "two_Partitions"
-         Chunks excluded during startup: 0
-         ->  Index Only Scan using "_hyper_1_1_chunk_two_Partitions_timeCustom_series_1_idx" on _hyper_1_1_chunk
-               Index Cond: (series_1 > (series_val())::double precision)
-         ->  Index Only Scan using "_hyper_1_2_chunk_two_Partitions_timeCustom_series_1_idx" on _hyper_1_2_chunk
-               Index Cond: (series_1 > (series_val())::double precision)
-         ->  Index Only Scan using "_hyper_1_3_chunk_two_Partitions_timeCustom_series_1_idx" on _hyper_1_3_chunk
-               Index Cond: (series_1 > (series_val())::double precision)
-         ->  Index Only Scan using "_hyper_1_4_chunk_two_Partitions_timeCustom_series_1_idx" on _hyper_1_4_chunk
-               Index Cond: (series_1 > (series_val())::double precision)
-   ->  Hash
-         ->  HashAggregate
-               Group Key: "two_Partitions_1".series_1
-               ->  Custom Scan (ChunkAppend) on "two_Partitions" "two_Partitions_1"
-                     Chunks excluded during startup: 0
-                     ->  Index Only Scan using "_hyper_1_1_chunk_two_Partitions_timeCustom_series_1_idx" on _hyper_1_1_chunk _hyper_1_1_chunk_1
-                           Index Cond: (series_1 > (series_val())::double precision)
-                     ->  Index Only Scan using "_hyper_1_2_chunk_two_Partitions_timeCustom_series_1_idx" on _hyper_1_2_chunk _hyper_1_2_chunk_1
-                           Index Cond: (series_1 > (series_val())::double precision)
-                     ->  Index Only Scan using "_hyper_1_3_chunk_two_Partitions_timeCustom_series_1_idx" on _hyper_1_3_chunk _hyper_1_3_chunk_1
-                           Index Cond: (series_1 > (series_val())::double precision)
-                     ->  Index Only Scan using "_hyper_1_4_chunk_two_Partitions_timeCustom_series_1_idx" on _hyper_1_4_chunk _hyper_1_4_chunk_1
-                           Index Cond: (series_1 > (series_val())::double precision)
-(25 rows)
-
--- ConstraintAwareAppend NOT applied for DELETE
-EXPLAIN (costs off)
-DELETE FROM "two_Partitions"
-WHERE series_1 IN (SELECT series_1 FROM "two_Partitions" WHERE series_1 > series_val());
-                                                                      QUERY PLAN                                                                       
--------------------------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (HypertableModify)
-   ->  Delete on "two_Partitions"
-         Delete on _hyper_1_1_chunk "two_Partitions_2"
-         Delete on _hyper_1_2_chunk "two_Partitions_3"
-         Delete on _hyper_1_3_chunk "two_Partitions_4"
-         Delete on _hyper_1_4_chunk "two_Partitions_5"
-         ->  Hash Join
-               Hash Cond: ("two_Partitions".series_1 = "two_Partitions_1".series_1)
-               ->  Append
-                     ->  Seq Scan on _hyper_1_1_chunk "two_Partitions_2"
-                     ->  Seq Scan on _hyper_1_2_chunk "two_Partitions_3"
-                     ->  Seq Scan on _hyper_1_3_chunk "two_Partitions_4"
-                     ->  Seq Scan on _hyper_1_4_chunk "two_Partitions_5"
-               ->  Hash
-                     ->  HashAggregate
-                           Group Key: "two_Partitions_1".series_1
-                           ->  Append
-                                 ->  Index Scan using "_hyper_1_1_chunk_two_Partitions_timeCustom_series_1_idx" on _hyper_1_1_chunk "two_Partitions_6"
-                                       Index Cond: (series_1 > (series_val())::double precision)
-                                 ->  Index Scan using "_hyper_1_2_chunk_two_Partitions_timeCustom_series_1_idx" on _hyper_1_2_chunk "two_Partitions_7"
-                                       Index Cond: (series_1 > (series_val())::double precision)
-                                 ->  Index Scan using "_hyper_1_3_chunk_two_Partitions_timeCustom_series_1_idx" on _hyper_1_3_chunk "two_Partitions_8"
-                                       Index Cond: (series_1 > (series_val())::double precision)
-                                 ->  Index Scan using "_hyper_1_4_chunk_two_Partitions_timeCustom_series_1_idx" on _hyper_1_4_chunk "two_Partitions_9"
-                                       Index Cond: (series_1 > (series_val())::double precision)
-(25 rows)
-
-SELECT * FROM "two_Partitions" ORDER BY "timeCustom", device_id, series_0, series_1;
-     timeCustom      | device_id | series_0 | series_1 | series_2 | series_bool 
----------------------+-----------+----------+----------+----------+-------------
- 1257894000000001000 | dev1      |      2.5 |        3 |          | 
- 1257894001000000000 | dev1      |      3.5 |        4 |          | 
- 1257894002000000000 | dev1      |      2.5 |        3 |          | 
- 1257894002000000000 | dev1      |      5.5 |        6 |          | t
- 1257894002000000000 | dev1      |      5.5 |        7 |          | f
- 1257897600000000000 | dev1      |      4.5 |        5 |          | f
-(6 rows)
-
-BEGIN;
-DELETE FROM "two_Partitions"
-WHERE series_1 IN (SELECT series_1 FROM "two_Partitions" WHERE series_1 > series_val());
-SELECT * FROM "two_Partitions" ORDER BY "timeCustom", device_id, series_0, series_1;
-     timeCustom      | device_id | series_0 | series_1 | series_2 | series_bool 
----------------------+-----------+----------+----------+----------+-------------
- 1257894000000001000 | dev1      |      2.5 |        3 |          | 
- 1257894001000000000 | dev1      |      3.5 |        4 |          | 
- 1257894002000000000 | dev1      |      2.5 |        3 |          | 
- 1257897600000000000 | dev1      |      4.5 |        5 |          | f
-(4 rows)
-
-ROLLBACK;
-BEGIN;
-DELETE FROM "two_Partitions"
-WHERE series_1 IN (SELECT series_1 FROM "two_Partitions" WHERE series_1 > series_val()) RETURNING "timeCustom";
-     timeCustom      
----------------------
- 1257894002000000000
- 1257894002000000000
-(2 rows)
-
-SELECT * FROM "two_Partitions" ORDER BY "timeCustom", device_id, series_0, series_1;
-     timeCustom      | device_id | series_0 | series_1 | series_2 | series_bool 
----------------------+-----------+----------+----------+----------+-------------
- 1257894000000001000 | dev1      |      2.5 |        3 |          | 
- 1257894001000000000 | dev1      |      3.5 |        4 |          | 
- 1257894002000000000 | dev1      |      2.5 |        3 |          | 
- 1257897600000000000 | dev1      |      4.5 |        5 |          | f
-(4 rows)
-
-ROLLBACK;
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/drop_extension.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/drop_extension.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/drop_extension.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/drop_extension.out	2023-11-25 05:27:33.781052806 +0000
@@ -1,82 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-CREATE TABLE drop_test(time timestamp, temp float8, device text);
-SELECT create_hypertable('drop_test', 'time', 'device', 2);
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-   create_hypertable    
-------------------------
- (1,public,drop_test,t)
-(1 row)
-
-SELECT * FROM _timescaledb_catalog.hypertable;
- id | schema_name | table_name | associated_schema_name | associated_table_prefix | num_dimensions | chunk_sizing_func_schema |  chunk_sizing_func_name  | chunk_target_size | compression_state | compressed_hypertable_id | replication_factor 
-----+-------------+------------+------------------------+-------------------------+----------------+--------------------------+--------------------------+-------------------+-------------------+--------------------------+--------------------
-  1 | public      | drop_test  | _timescaledb_internal  | _hyper_1                |              2 | _timescaledb_internal    | calculate_chunk_interval |                 0 |                 0 |                          |                   
-(1 row)
-
-INSERT INTO drop_test VALUES('Mon Mar 20 09:17:00.936242 2017', 23.4, 'dev1');
-SELECT * FROM drop_test;
-              time               | temp | device 
----------------------------------+------+--------
- Mon Mar 20 09:17:00.936242 2017 | 23.4 | dev1
-(1 row)
-
-\c :TEST_DBNAME :ROLE_SUPERUSER
-DROP EXTENSION timescaledb CASCADE;
-NOTICE:  drop cascades to 2 other objects
-\c :TEST_DBNAME :ROLE_DEFAULT_PERM_USER
--- Querying the original table should not return any rows since all of
--- them actually existed in chunks that are now gone
-SELECT * FROM drop_test;
- time | temp | device 
-------+------+--------
-(0 rows)
-
-\c :TEST_DBNAME :ROLE_SUPERUSER
--- Recreate the extension
-SET client_min_messages=error;
-CREATE EXTENSION timescaledb;
-RESET client_min_messages;
--- Test that calling twice generates proper error
-\set ON_ERROR_STOP 0
-CREATE EXTENSION timescaledb;
-ERROR:  extension "timescaledb" has already been loaded with another version
-\set ON_ERROR_STOP 1
-\c :TEST_DBNAME :ROLE_DEFAULT_PERM_USER
--- CREATE twice with IF NOT EXISTS should be OK
-CREATE EXTENSION IF NOT EXISTS timescaledb;
-NOTICE:  extension "timescaledb" already exists, skipping
--- Make the table a hypertable again
-SELECT create_hypertable('drop_test', 'time', 'device', 2);
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-   create_hypertable    
-------------------------
- (1,public,drop_test,t)
-(1 row)
-
-SELECT * FROM _timescaledb_catalog.hypertable;
- id | schema_name | table_name | associated_schema_name | associated_table_prefix | num_dimensions | chunk_sizing_func_schema |  chunk_sizing_func_name  | chunk_target_size | compression_state | compressed_hypertable_id | replication_factor 
-----+-------------+------------+------------------------+-------------------------+----------------+--------------------------+--------------------------+-------------------+-------------------+--------------------------+--------------------
-  1 | public      | drop_test  | _timescaledb_internal  | _hyper_1                |              2 | _timescaledb_internal    | calculate_chunk_interval |                 0 |                 0 |                          |                   
-(1 row)
-
-INSERT INTO drop_test VALUES('Mon Mar 20 09:18:19.100462 2017', 22.1, 'dev1');
-SELECT * FROM drop_test;
-              time               | temp | device 
----------------------------------+------+--------
- Mon Mar 20 09:18:19.100462 2017 | 22.1 | dev1
-(1 row)
-
---test drops thru cascades of other objects
-\c :TEST_DBNAME :ROLE_SUPERUSER
-drop schema public cascade;
-NOTICE:  drop cascades to 3 other objects
-\dn
-  List of schemas
- Name |   Owner    
-------+------------
- test | super_user
-(1 row)
-
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/drop_hypertable.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/drop_hypertable.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/drop_hypertable.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/drop_hypertable.out	2023-11-25 05:27:33.765052852 +0000
@@ -1,112 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-SELECT * from _timescaledb_catalog.hypertable;
- id | schema_name | table_name | associated_schema_name | associated_table_prefix | num_dimensions | chunk_sizing_func_schema | chunk_sizing_func_name | chunk_target_size | compression_state | compressed_hypertable_id | replication_factor 
-----+-------------+------------+------------------------+-------------------------+----------------+--------------------------+------------------------+-------------------+-------------------+--------------------------+--------------------
-(0 rows)
-
-SELECT * from _timescaledb_catalog.dimension;
- id | hypertable_id | column_name | column_type | aligned | num_slices | partitioning_func_schema | partitioning_func | interval_length | compress_interval_length | integer_now_func_schema | integer_now_func 
-----+---------------+-------------+-------------+---------+------------+--------------------------+-------------------+-----------------+--------------------------+-------------------------+------------------
-(0 rows)
-
-CREATE TABLE should_drop (time timestamp, temp float8);
-SELECT create_hypertable('should_drop', 'time');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-    create_hypertable     
---------------------------
- (1,public,should_drop,t)
-(1 row)
-
-CREATE TABLE hyper_with_dependencies (time timestamp, temp float8);
-SELECT create_hypertable('hyper_with_dependencies', 'time');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-          create_hypertable           
---------------------------------------
- (2,public,hyper_with_dependencies,t)
-(1 row)
-
-CREATE VIEW dependent_view AS SELECT * FROM hyper_with_dependencies;
-INSERT INTO hyper_with_dependencies VALUES (now(), 1.0);
-\set ON_ERROR_STOP 0
-DROP TABLE hyper_with_dependencies;
-ERROR:  cannot drop table hyper_with_dependencies because other objects depend on it
-\set ON_ERROR_STOP 1
-DROP TABLE hyper_with_dependencies CASCADE;
-NOTICE:  drop cascades to view dependent_view
-\dv
-      List of relations
- Schema | Name | Type | Owner 
---------+------+------+-------
-(0 rows)
-
-CREATE TABLE chunk_with_dependencies (time timestamp, temp float8);
-SELECT create_hypertable('chunk_with_dependencies', 'time');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-          create_hypertable           
---------------------------------------
- (3,public,chunk_with_dependencies,t)
-(1 row)
-
-INSERT INTO chunk_with_dependencies VALUES (now(), 1.0);
-CREATE VIEW dependent_view_chunk AS SELECT * FROM _timescaledb_internal._hyper_3_2_chunk;
-\set ON_ERROR_STOP 0
-DROP TABLE chunk_with_dependencies;
-ERROR:  cannot drop table _timescaledb_internal._hyper_3_2_chunk because other objects depend on it
-\set ON_ERROR_STOP 1
-DROP TABLE chunk_with_dependencies CASCADE;
-NOTICE:  drop cascades to view dependent_view_chunk
-\dv
-      List of relations
- Schema | Name | Type | Owner 
---------+------+------+-------
-(0 rows)
-
--- Calling create hypertable again will increment hypertable ID
--- although no new hypertable is created. Make sure we can handle this.
-SELECT create_hypertable('should_drop', 'time', if_not_exists => true);
-NOTICE:  table "should_drop" is already a hypertable, skipping
-    create_hypertable     
---------------------------
- (1,public,should_drop,f)
-(1 row)
-
-SELECT * from _timescaledb_catalog.hypertable;
- id | schema_name | table_name  | associated_schema_name | associated_table_prefix | num_dimensions | chunk_sizing_func_schema |  chunk_sizing_func_name  | chunk_target_size | compression_state | compressed_hypertable_id | replication_factor 
-----+-------------+-------------+------------------------+-------------------------+----------------+--------------------------+--------------------------+-------------------+-------------------+--------------------------+--------------------
-  1 | public      | should_drop | _timescaledb_internal  | _hyper_1                |              1 | _timescaledb_internal    | calculate_chunk_interval |                 0 |                 0 |                          |                   
-(1 row)
-
-SELECT * from _timescaledb_catalog.dimension;
- id | hypertable_id | column_name |         column_type         | aligned | num_slices | partitioning_func_schema | partitioning_func | interval_length | compress_interval_length | integer_now_func_schema | integer_now_func 
-----+---------------+-------------+-----------------------------+---------+------------+--------------------------+-------------------+-----------------+--------------------------+-------------------------+------------------
-  1 |             1 | time        | timestamp without time zone | t       |            |                          |                   |    604800000000 |                          |                         | 
-(1 row)
-
-DROP TABLE should_drop;
-CREATE TABLE should_drop (time timestamp, temp float8);
-SELECT create_hypertable('should_drop', 'time');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-    create_hypertable     
---------------------------
- (4,public,should_drop,t)
-(1 row)
-
-INSERT INTO should_drop VALUES (now(), 1.0);
-SELECT * from _timescaledb_catalog.hypertable;
- id | schema_name | table_name  | associated_schema_name | associated_table_prefix | num_dimensions | chunk_sizing_func_schema |  chunk_sizing_func_name  | chunk_target_size | compression_state | compressed_hypertable_id | replication_factor 
-----+-------------+-------------+------------------------+-------------------------+----------------+--------------------------+--------------------------+-------------------+-------------------+--------------------------+--------------------
-  4 | public      | should_drop | _timescaledb_internal  | _hyper_4                |              1 | _timescaledb_internal    | calculate_chunk_interval |                 0 |                 0 |                          |                   
-(1 row)
-
-SELECT * from _timescaledb_catalog.dimension;
- id | hypertable_id | column_name |         column_type         | aligned | num_slices | partitioning_func_schema | partitioning_func | interval_length | compress_interval_length | integer_now_func_schema | integer_now_func 
-----+---------------+-------------+-----------------------------+---------+------------+--------------------------+-------------------+-----------------+--------------------------+-------------------------+------------------
-  4 |             4 | time        | timestamp without time zone | t       |            |                          |                   |    604800000000 |                          |                         | 
-(1 row)
-
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/drop_owned.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/drop_owned.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/drop_owned.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/drop_owned.out	2023-11-25 05:27:33.781052806 +0000
@@ -1,194 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-\c :TEST_DBNAME :ROLE_SUPERUSER
-CREATE SCHEMA hypertable_schema;
-GRANT ALL ON SCHEMA hypertable_schema TO :ROLE_DEFAULT_PERM_USER;
-SET ROLE :ROLE_DEFAULT_PERM_USER;
-CREATE TABLE hypertable_schema.default_perm_user (time timestamptz, temp float, location int);
-SELECT create_hypertable('hypertable_schema.default_perm_user', 'time', 'location', 2);
-NOTICE:  adding not-null constraint to column "time"
-             create_hypertable             
--------------------------------------------
- (1,hypertable_schema,default_perm_user,t)
-(1 row)
-
-INSERT INTO hypertable_schema.default_perm_user VALUES ('2001-01-01 01:01:01', 23.3, 1);
-RESET ROLE;
-CREATE TABLE hypertable_schema.superuser (time timestamptz, temp float, location int);
-SELECT create_hypertable('hypertable_schema.superuser', 'time', 'location', 2);
-NOTICE:  adding not-null constraint to column "time"
-         create_hypertable         
------------------------------------
- (2,hypertable_schema,superuser,t)
-(1 row)
-
-INSERT INTO hypertable_schema.superuser VALUES ('2001-01-01 01:01:01', 23.3, 1);
-SELECT * FROM _timescaledb_catalog.hypertable ORDER BY id;
- id |    schema_name    |    table_name     | associated_schema_name | associated_table_prefix | num_dimensions | chunk_sizing_func_schema |  chunk_sizing_func_name  | chunk_target_size | compression_state | compressed_hypertable_id | replication_factor 
-----+-------------------+-------------------+------------------------+-------------------------+----------------+--------------------------+--------------------------+-------------------+-------------------+--------------------------+--------------------
-  1 | hypertable_schema | default_perm_user | _timescaledb_internal  | _hyper_1                |              2 | _timescaledb_internal    | calculate_chunk_interval |                 0 |                 0 |                          |                   
-  2 | hypertable_schema | superuser         | _timescaledb_internal  | _hyper_2                |              2 | _timescaledb_internal    | calculate_chunk_interval |                 0 |                 0 |                          |                   
-(2 rows)
-
-SELECT * FROM _timescaledb_catalog.chunk;
- id | hypertable_id |      schema_name      |    table_name    | compressed_chunk_id | dropped | status | osm_chunk 
-----+---------------+-----------------------+------------------+---------------------+---------+--------+-----------
-  1 |             1 | _timescaledb_internal | _hyper_1_1_chunk |                     | f       |      0 | f
-  2 |             2 | _timescaledb_internal | _hyper_2_2_chunk |                     | f       |      0 | f
-(2 rows)
-
-DROP OWNED BY :ROLE_DEFAULT_PERM_USER;
-SELECT * FROM _timescaledb_catalog.hypertable ORDER BY id;
- id |    schema_name    | table_name | associated_schema_name | associated_table_prefix | num_dimensions | chunk_sizing_func_schema |  chunk_sizing_func_name  | chunk_target_size | compression_state | compressed_hypertable_id | replication_factor 
-----+-------------------+------------+------------------------+-------------------------+----------------+--------------------------+--------------------------+-------------------+-------------------+--------------------------+--------------------
-  2 | hypertable_schema | superuser  | _timescaledb_internal  | _hyper_2                |              2 | _timescaledb_internal    | calculate_chunk_interval |                 0 |                 0 |                          |                   
-(1 row)
-
-SELECT * FROM _timescaledb_catalog.chunk;
- id | hypertable_id |      schema_name      |    table_name    | compressed_chunk_id | dropped | status | osm_chunk 
-----+---------------+-----------------------+------------------+---------------------+---------+--------+-----------
-  2 |             2 | _timescaledb_internal | _hyper_2_2_chunk |                     | f       |      0 | f
-(1 row)
-
-DROP TABLE  hypertable_schema.superuser;
---everything should be cleaned up
-SELECT * FROM _timescaledb_catalog.hypertable GROUP BY id;
- id | schema_name | table_name | associated_schema_name | associated_table_prefix | num_dimensions | chunk_sizing_func_schema | chunk_sizing_func_name | chunk_target_size | compression_state | compressed_hypertable_id | replication_factor 
-----+-------------+------------+------------------------+-------------------------+----------------+--------------------------+------------------------+-------------------+-------------------+--------------------------+--------------------
-(0 rows)
-
-SELECT * FROM _timescaledb_catalog.chunk;
- id | hypertable_id | schema_name | table_name | compressed_chunk_id | dropped | status | osm_chunk 
-----+---------------+-------------+------------+---------------------+---------+--------+-----------
-(0 rows)
-
-SELECT * FROM _timescaledb_catalog.dimension;
- id | hypertable_id | column_name | column_type | aligned | num_slices | partitioning_func_schema | partitioning_func | interval_length | compress_interval_length | integer_now_func_schema | integer_now_func 
-----+---------------+-------------+-------------+---------+------------+--------------------------+-------------------+-----------------+--------------------------+-------------------------+------------------
-(0 rows)
-
-SELECT * FROM _timescaledb_catalog.dimension_slice;
- id | dimension_id | range_start | range_end 
-----+--------------+-------------+-----------
-(0 rows)
-
-SELECT * FROM _timescaledb_catalog.chunk_index;
- chunk_id | index_name | hypertable_id | hypertable_index_name 
-----------+------------+---------------+-----------------------
-(0 rows)
-
-SELECT * FROM _timescaledb_catalog.chunk_constraint;
- chunk_id | dimension_slice_id | constraint_name | hypertable_constraint_name 
-----------+--------------------+-----------------+----------------------------
-(0 rows)
-
--- test drop owned in database without extension installed
-\c :TEST_DBNAME :ROLE_SUPERUSER
-CREATE database test_drop_owned;
-\c test_drop_owned
-DROP OWNED BY :ROLE_SUPERUSER;
-\c :TEST_DBNAME :ROLE_SUPERUSER
-DROP DATABASE test_drop_owned;
--- Test that dependencies on roles are added to chunks when creating
--- new chunks. If that is not done, DROP OWNED BY will not revoke the
--- privilege on the chunk.
-CREATE TABLE sensor_data(time timestamptz not null, cpu double precision null);
-SELECT * FROM create_hypertable('sensor_data','time');
- hypertable_id | schema_name | table_name  | created 
----------------+-------------+-------------+---------
-             3 | public      | sensor_data | t
-(1 row)
-
-INSERT INTO sensor_data
-SELECT time,
-       random() AS cpu
-FROM generate_series('2020-01-01'::timestamptz, '2020-01-24'::timestamptz, INTERVAL '10 minute') AS g1(time);
-\dp sensor_data
-                                Access privileges
- Schema |    Name     | Type  | Access privileges | Column privileges | Policies 
---------+-------------+-------+-------------------+-------------------+----------
- public | sensor_data | table |                   |                   | 
-(1 row)
-
-\dp _timescaledb_internal._hyper_3*
-                                          Access privileges
-        Schema         |       Name       | Type  | Access privileges | Column privileges | Policies 
------------------------+------------------+-------+-------------------+-------------------+----------
- _timescaledb_internal | _hyper_3_3_chunk | table |                   |                   | 
- _timescaledb_internal | _hyper_3_4_chunk | table |                   |                   | 
- _timescaledb_internal | _hyper_3_5_chunk | table |                   |                   | 
- _timescaledb_internal | _hyper_3_6_chunk | table |                   |                   | 
- _timescaledb_internal | _hyper_3_7_chunk | table |                   |                   | 
-(5 rows)
-
-GRANT SELECT ON sensor_data TO :ROLE_DEFAULT_PERM_USER;
-\dp sensor_data
-                                      Access privileges
- Schema |    Name     | Type  |       Access privileges        | Column privileges | Policies 
---------+-------------+-------+--------------------------------+-------------------+----------
- public | sensor_data | table | super_user=arwdDxt/super_user +|                   | 
-        |             |       | default_perm_user=r/super_user |                   | 
-(1 row)
-
-\dp _timescaledb_internal._hyper_3*
-                                                Access privileges
-        Schema         |       Name       | Type  |       Access privileges        | Column privileges | Policies 
------------------------+------------------+-------+--------------------------------+-------------------+----------
- _timescaledb_internal | _hyper_3_3_chunk | table | super_user=arwdDxt/super_user +|                   | 
-                       |                  |       | default_perm_user=r/super_user |                   | 
- _timescaledb_internal | _hyper_3_4_chunk | table | super_user=arwdDxt/super_user +|                   | 
-                       |                  |       | default_perm_user=r/super_user |                   | 
- _timescaledb_internal | _hyper_3_5_chunk | table | super_user=arwdDxt/super_user +|                   | 
-                       |                  |       | default_perm_user=r/super_user |                   | 
- _timescaledb_internal | _hyper_3_6_chunk | table | super_user=arwdDxt/super_user +|                   | 
-                       |                  |       | default_perm_user=r/super_user |                   | 
- _timescaledb_internal | _hyper_3_7_chunk | table | super_user=arwdDxt/super_user +|                   | 
-                       |                  |       | default_perm_user=r/super_user |                   | 
-(5 rows)
-
--- Insert more chunks after adding the user to the hypertable. These
--- will now get the privileges of the hypertable.
-INSERT INTO sensor_data
-SELECT time,
-       random() AS cpu
-FROM generate_series('2020-01-20'::timestamptz, '2020-02-05'::timestamptz, INTERVAL '10 minute') AS g1(time);
-\dp _timescaledb_internal._hyper_3*
-                                                Access privileges
-        Schema         |       Name       | Type  |       Access privileges        | Column privileges | Policies 
------------------------+------------------+-------+--------------------------------+-------------------+----------
- _timescaledb_internal | _hyper_3_3_chunk | table | super_user=arwdDxt/super_user +|                   | 
-                       |                  |       | default_perm_user=r/super_user |                   | 
- _timescaledb_internal | _hyper_3_4_chunk | table | super_user=arwdDxt/super_user +|                   | 
-                       |                  |       | default_perm_user=r/super_user |                   | 
- _timescaledb_internal | _hyper_3_5_chunk | table | super_user=arwdDxt/super_user +|                   | 
-                       |                  |       | default_perm_user=r/super_user |                   | 
- _timescaledb_internal | _hyper_3_6_chunk | table | super_user=arwdDxt/super_user +|                   | 
-                       |                  |       | default_perm_user=r/super_user |                   | 
- _timescaledb_internal | _hyper_3_7_chunk | table | super_user=arwdDxt/super_user +|                   | 
-                       |                  |       | default_perm_user=r/super_user |                   | 
- _timescaledb_internal | _hyper_3_8_chunk | table | super_user=arwdDxt/super_user +|                   | 
-                       |                  |       | default_perm_user=r/super_user |                   | 
-(6 rows)
-
--- This should revoke the privileges on both the hypertable and the chunks.
-DROP OWNED BY :ROLE_DEFAULT_PERM_USER;
-\dp sensor_data
-                                      Access privileges
- Schema |    Name     | Type  |       Access privileges       | Column privileges | Policies 
---------+-------------+-------+-------------------------------+-------------------+----------
- public | sensor_data | table | super_user=arwdDxt/super_user |                   | 
-(1 row)
-
-\dp _timescaledb_internal._hyper_3*
-                                                Access privileges
-        Schema         |       Name       | Type  |       Access privileges       | Column privileges | Policies 
------------------------+------------------+-------+-------------------------------+-------------------+----------
- _timescaledb_internal | _hyper_3_3_chunk | table | super_user=arwdDxt/super_user |                   | 
- _timescaledb_internal | _hyper_3_4_chunk | table | super_user=arwdDxt/super_user |                   | 
- _timescaledb_internal | _hyper_3_5_chunk | table | super_user=arwdDxt/super_user |                   | 
- _timescaledb_internal | _hyper_3_6_chunk | table | super_user=arwdDxt/super_user |                   | 
- _timescaledb_internal | _hyper_3_7_chunk | table | super_user=arwdDxt/super_user |                   | 
- _timescaledb_internal | _hyper_3_8_chunk | table | super_user=arwdDxt/super_user |                   | 
-(6 rows)
-
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/drop_rename_hypertable.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/drop_rename_hypertable.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/drop_rename_hypertable.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/drop_rename_hypertable.out	2023-11-25 05:27:38.957037758 +0000
@@ -1,240 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-\o /dev/null
-\ir include/insert_two_partitions.sql
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-CREATE TABLE PUBLIC."two_Partitions" (
-  "timeCustom" BIGINT NOT NULL,
-  device_id TEXT NOT NULL,
-  series_0 DOUBLE PRECISION NULL,
-  series_1 DOUBLE PRECISION NULL,
-  series_2 DOUBLE PRECISION NULL,
-  series_bool BOOLEAN NULL
-);
-CREATE INDEX ON PUBLIC."two_Partitions" (device_id, "timeCustom" DESC NULLS LAST) WHERE device_id IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, series_0) WHERE series_0 IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, series_1)  WHERE series_1 IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, series_2) WHERE series_2 IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, series_bool) WHERE series_bool IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, device_id);
-SELECT * FROM create_hypertable('"public"."two_Partitions"'::regclass, 'timeCustom'::name, 'device_id'::name, associated_schema_name=>'_timescaledb_internal'::text, number_partitions => 2, chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
-\set QUIET off
-BEGIN;
-\COPY public."two_Partitions" FROM 'data/ds1_dev1_1.tsv' NULL AS '';
-COMMIT;
-INSERT INTO public."two_Partitions"("timeCustom", device_id, series_0, series_1) VALUES
-(1257987600000000000, 'dev1', 1.5, 1),
-(1257987600000000000, 'dev1', 1.5, 2),
-(1257894000000000000, 'dev2', 1.5, 1),
-(1257894002000000000, 'dev1', 2.5, 3);
-INSERT INTO "two_Partitions"("timeCustom", device_id, series_0, series_1) VALUES
-(1257894000000000000, 'dev2', 1.5, 2);
-\set QUIET on
-\o
-SELECT * FROM test.show_columnsp('_timescaledb_internal.%_hyper%');
-                                      Relation                                      | Kind |   Column    |   Column type    | NotNull 
-------------------------------------------------------------------------------------+------+-------------+------------------+---------
- _timescaledb_internal._hyper_1_1_chunk                                             | r    | timeCustom  | bigint           | t
- _timescaledb_internal._hyper_1_1_chunk                                             | r    | device_id   | text             | t
- _timescaledb_internal._hyper_1_1_chunk                                             | r    | series_0    | double precision | f
- _timescaledb_internal._hyper_1_1_chunk                                             | r    | series_1    | double precision | f
- _timescaledb_internal._hyper_1_1_chunk                                             | r    | series_2    | double precision | f
- _timescaledb_internal._hyper_1_1_chunk                                             | r    | series_bool | boolean          | f
- _timescaledb_internal."_hyper_1_1_chunk_two_Partitions_device_id_timeCustom_idx"   | i    | device_id   | text             | f
- _timescaledb_internal."_hyper_1_1_chunk_two_Partitions_device_id_timeCustom_idx"   | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_1_chunk_two_Partitions_timeCustom_device_id_idx"   | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_1_chunk_two_Partitions_timeCustom_device_id_idx"   | i    | device_id   | text             | f
- _timescaledb_internal."_hyper_1_1_chunk_two_Partitions_timeCustom_idx"             | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_1_chunk_two_Partitions_timeCustom_series_0_idx"    | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_1_chunk_two_Partitions_timeCustom_series_0_idx"    | i    | series_0    | double precision | f
- _timescaledb_internal."_hyper_1_1_chunk_two_Partitions_timeCustom_series_1_idx"    | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_1_chunk_two_Partitions_timeCustom_series_1_idx"    | i    | series_1    | double precision | f
- _timescaledb_internal."_hyper_1_1_chunk_two_Partitions_timeCustom_series_2_idx"    | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_1_chunk_two_Partitions_timeCustom_series_2_idx"    | i    | series_2    | double precision | f
- _timescaledb_internal."_hyper_1_1_chunk_two_Partitions_timeCustom_series_bool_idx" | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_1_chunk_two_Partitions_timeCustom_series_bool_idx" | i    | series_bool | boolean          | f
- _timescaledb_internal._hyper_1_2_chunk                                             | r    | timeCustom  | bigint           | t
- _timescaledb_internal._hyper_1_2_chunk                                             | r    | device_id   | text             | t
- _timescaledb_internal._hyper_1_2_chunk                                             | r    | series_0    | double precision | f
- _timescaledb_internal._hyper_1_2_chunk                                             | r    | series_1    | double precision | f
- _timescaledb_internal._hyper_1_2_chunk                                             | r    | series_2    | double precision | f
- _timescaledb_internal._hyper_1_2_chunk                                             | r    | series_bool | boolean          | f
- _timescaledb_internal."_hyper_1_2_chunk_two_Partitions_device_id_timeCustom_idx"   | i    | device_id   | text             | f
- _timescaledb_internal."_hyper_1_2_chunk_two_Partitions_device_id_timeCustom_idx"   | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_2_chunk_two_Partitions_timeCustom_device_id_idx"   | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_2_chunk_two_Partitions_timeCustom_device_id_idx"   | i    | device_id   | text             | f
- _timescaledb_internal."_hyper_1_2_chunk_two_Partitions_timeCustom_idx"             | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_2_chunk_two_Partitions_timeCustom_series_0_idx"    | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_2_chunk_two_Partitions_timeCustom_series_0_idx"    | i    | series_0    | double precision | f
- _timescaledb_internal."_hyper_1_2_chunk_two_Partitions_timeCustom_series_1_idx"    | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_2_chunk_two_Partitions_timeCustom_series_1_idx"    | i    | series_1    | double precision | f
- _timescaledb_internal."_hyper_1_2_chunk_two_Partitions_timeCustom_series_2_idx"    | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_2_chunk_two_Partitions_timeCustom_series_2_idx"    | i    | series_2    | double precision | f
- _timescaledb_internal."_hyper_1_2_chunk_two_Partitions_timeCustom_series_bool_idx" | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_2_chunk_two_Partitions_timeCustom_series_bool_idx" | i    | series_bool | boolean          | f
- _timescaledb_internal._hyper_1_3_chunk                                             | r    | timeCustom  | bigint           | t
- _timescaledb_internal._hyper_1_3_chunk                                             | r    | device_id   | text             | t
- _timescaledb_internal._hyper_1_3_chunk                                             | r    | series_0    | double precision | f
- _timescaledb_internal._hyper_1_3_chunk                                             | r    | series_1    | double precision | f
- _timescaledb_internal._hyper_1_3_chunk                                             | r    | series_2    | double precision | f
- _timescaledb_internal._hyper_1_3_chunk                                             | r    | series_bool | boolean          | f
- _timescaledb_internal."_hyper_1_3_chunk_two_Partitions_device_id_timeCustom_idx"   | i    | device_id   | text             | f
- _timescaledb_internal."_hyper_1_3_chunk_two_Partitions_device_id_timeCustom_idx"   | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_3_chunk_two_Partitions_timeCustom_device_id_idx"   | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_3_chunk_two_Partitions_timeCustom_device_id_idx"   | i    | device_id   | text             | f
- _timescaledb_internal."_hyper_1_3_chunk_two_Partitions_timeCustom_idx"             | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_3_chunk_two_Partitions_timeCustom_series_0_idx"    | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_3_chunk_two_Partitions_timeCustom_series_0_idx"    | i    | series_0    | double precision | f
- _timescaledb_internal."_hyper_1_3_chunk_two_Partitions_timeCustom_series_1_idx"    | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_3_chunk_two_Partitions_timeCustom_series_1_idx"    | i    | series_1    | double precision | f
- _timescaledb_internal."_hyper_1_3_chunk_two_Partitions_timeCustom_series_2_idx"    | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_3_chunk_two_Partitions_timeCustom_series_2_idx"    | i    | series_2    | double precision | f
- _timescaledb_internal."_hyper_1_3_chunk_two_Partitions_timeCustom_series_bool_idx" | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_3_chunk_two_Partitions_timeCustom_series_bool_idx" | i    | series_bool | boolean          | f
- _timescaledb_internal._hyper_1_4_chunk                                             | r    | timeCustom  | bigint           | t
- _timescaledb_internal._hyper_1_4_chunk                                             | r    | device_id   | text             | t
- _timescaledb_internal._hyper_1_4_chunk                                             | r    | series_0    | double precision | f
- _timescaledb_internal._hyper_1_4_chunk                                             | r    | series_1    | double precision | f
- _timescaledb_internal._hyper_1_4_chunk                                             | r    | series_2    | double precision | f
- _timescaledb_internal._hyper_1_4_chunk                                             | r    | series_bool | boolean          | f
- _timescaledb_internal."_hyper_1_4_chunk_two_Partitions_device_id_timeCustom_idx"   | i    | device_id   | text             | f
- _timescaledb_internal."_hyper_1_4_chunk_two_Partitions_device_id_timeCustom_idx"   | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_4_chunk_two_Partitions_timeCustom_device_id_idx"   | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_4_chunk_two_Partitions_timeCustom_device_id_idx"   | i    | device_id   | text             | f
- _timescaledb_internal."_hyper_1_4_chunk_two_Partitions_timeCustom_idx"             | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_4_chunk_two_Partitions_timeCustom_series_0_idx"    | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_4_chunk_two_Partitions_timeCustom_series_0_idx"    | i    | series_0    | double precision | f
- _timescaledb_internal."_hyper_1_4_chunk_two_Partitions_timeCustom_series_1_idx"    | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_4_chunk_two_Partitions_timeCustom_series_1_idx"    | i    | series_1    | double precision | f
- _timescaledb_internal."_hyper_1_4_chunk_two_Partitions_timeCustom_series_2_idx"    | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_4_chunk_two_Partitions_timeCustom_series_2_idx"    | i    | series_2    | double precision | f
- _timescaledb_internal."_hyper_1_4_chunk_two_Partitions_timeCustom_series_bool_idx" | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_4_chunk_two_Partitions_timeCustom_series_bool_idx" | i    | series_bool | boolean          | f
-(76 rows)
-
--- Test that renaming hypertable works
-SELECT * FROM test.show_columns('_timescaledb_internal._hyper_1_1_chunk');
-   Column    |       Type       | NotNull 
--------------+------------------+---------
- timeCustom  | bigint           | t
- device_id   | text             | t
- series_0    | double precision | f
- series_1    | double precision | f
- series_2    | double precision | f
- series_bool | boolean          | f
-(6 rows)
-
-ALTER TABLE "two_Partitions" RENAME TO "newname";
-SELECT * FROM "newname";
-     timeCustom      | device_id | series_0 | series_1 | series_2 | series_bool 
----------------------+-----------+----------+----------+----------+-------------
- 1257894000000000000 | dev1      |      1.5 |        1 |        2 | t
- 1257894000000000000 | dev1      |      1.5 |        2 |          | 
- 1257894000000001000 | dev1      |      2.5 |        3 |          | 
- 1257894001000000000 | dev1      |      3.5 |        4 |          | 
- 1257894002000000000 | dev1      |      5.5 |        6 |          | t
- 1257894002000000000 | dev1      |      5.5 |        7 |          | f
- 1257894002000000000 | dev1      |      2.5 |        3 |          | 
- 1257897600000000000 | dev1      |      4.5 |        5 |          | f
- 1257987600000000000 | dev1      |      1.5 |        1 |          | 
- 1257987600000000000 | dev1      |      1.5 |        2 |          | 
- 1257894000000000000 | dev2      |      1.5 |        1 |          | 
- 1257894000000000000 | dev2      |      1.5 |        2 |          | 
-(12 rows)
-
-SELECT * FROM _timescaledb_catalog.hypertable;
- id | schema_name | table_name | associated_schema_name | associated_table_prefix | num_dimensions | chunk_sizing_func_schema |  chunk_sizing_func_name  | chunk_target_size | compression_state | compressed_hypertable_id | replication_factor 
-----+-------------+------------+------------------------+-------------------------+----------------+--------------------------+--------------------------+-------------------+-------------------+--------------------------+--------------------
-  1 | public      | newname    | _timescaledb_internal  | _hyper_1                |              2 | _timescaledb_internal    | calculate_chunk_interval |                 0 |                 0 |                          |                   
-(1 row)
-
-\c :TEST_DBNAME :ROLE_SUPERUSER
-CREATE SCHEMA "newschema" AUTHORIZATION :ROLE_DEFAULT_PERM_USER;
-\c :TEST_DBNAME :ROLE_DEFAULT_PERM_USER
-ALTER TABLE "newname" SET SCHEMA "newschema";
-SELECT * FROM "newschema"."newname";
-     timeCustom      | device_id | series_0 | series_1 | series_2 | series_bool 
----------------------+-----------+----------+----------+----------+-------------
- 1257894000000000000 | dev1      |      1.5 |        1 |        2 | t
- 1257894000000000000 | dev1      |      1.5 |        2 |          | 
- 1257894000000001000 | dev1      |      2.5 |        3 |          | 
- 1257894001000000000 | dev1      |      3.5 |        4 |          | 
- 1257894002000000000 | dev1      |      5.5 |        6 |          | t
- 1257894002000000000 | dev1      |      5.5 |        7 |          | f
- 1257894002000000000 | dev1      |      2.5 |        3 |          | 
- 1257897600000000000 | dev1      |      4.5 |        5 |          | f
- 1257987600000000000 | dev1      |      1.5 |        1 |          | 
- 1257987600000000000 | dev1      |      1.5 |        2 |          | 
- 1257894000000000000 | dev2      |      1.5 |        1 |          | 
- 1257894000000000000 | dev2      |      1.5 |        2 |          | 
-(12 rows)
-
-SELECT * FROM _timescaledb_catalog.hypertable;
- id | schema_name | table_name | associated_schema_name | associated_table_prefix | num_dimensions | chunk_sizing_func_schema |  chunk_sizing_func_name  | chunk_target_size | compression_state | compressed_hypertable_id | replication_factor 
-----+-------------+------------+------------------------+-------------------------+----------------+--------------------------+--------------------------+-------------------+-------------------+--------------------------+--------------------
-  1 | newschema   | newname    | _timescaledb_internal  | _hyper_1                |              2 | _timescaledb_internal    | calculate_chunk_interval |                 0 |                 0 |                          |                   
-(1 row)
-
-DROP TABLE "newschema"."newname";
-SELECT * FROM _timescaledb_catalog.hypertable;
- id | schema_name | table_name | associated_schema_name | associated_table_prefix | num_dimensions | chunk_sizing_func_schema | chunk_sizing_func_name | chunk_target_size | compression_state | compressed_hypertable_id | replication_factor 
-----+-------------+------------+------------------------+-------------------------+----------------+--------------------------+------------------------+-------------------+-------------------+--------------------------+--------------------
-(0 rows)
-
-\dt  "public".*
-      List of relations
- Schema | Name | Type | Owner 
---------+------+------+-------
-(0 rows)
-
-\dt  "_timescaledb_catalog".*
-                                      List of relations
-        Schema        |                       Name                       | Type  |   Owner    
-----------------------+--------------------------------------------------+-------+------------
- _timescaledb_catalog | chunk                                            | table | super_user
- _timescaledb_catalog | chunk_constraint                                 | table | super_user
- _timescaledb_catalog | chunk_copy_operation                             | table | super_user
- _timescaledb_catalog | chunk_data_node                                  | table | super_user
- _timescaledb_catalog | chunk_index                                      | table | super_user
- _timescaledb_catalog | compression_algorithm                            | table | super_user
- _timescaledb_catalog | compression_chunk_size                           | table | super_user
- _timescaledb_catalog | continuous_agg                                   | table | super_user
- _timescaledb_catalog | continuous_agg_migrate_plan                      | table | super_user
- _timescaledb_catalog | continuous_agg_migrate_plan_step                 | table | super_user
- _timescaledb_catalog | continuous_aggs_bucket_function                  | table | super_user
- _timescaledb_catalog | continuous_aggs_hypertable_invalidation_log      | table | super_user
- _timescaledb_catalog | continuous_aggs_invalidation_threshold           | table | super_user
- _timescaledb_catalog | continuous_aggs_materialization_invalidation_log | table | super_user
- _timescaledb_catalog | continuous_aggs_watermark                        | table | super_user
- _timescaledb_catalog | dimension                                        | table | super_user
- _timescaledb_catalog | dimension_partition                              | table | super_user
- _timescaledb_catalog | dimension_slice                                  | table | super_user
- _timescaledb_catalog | hypertable                                       | table | super_user
- _timescaledb_catalog | hypertable_compression                           | table | super_user
- _timescaledb_catalog | hypertable_data_node                             | table | super_user
- _timescaledb_catalog | metadata                                         | table | super_user
- _timescaledb_catalog | remote_txn                                       | table | super_user
- _timescaledb_catalog | tablespace                                       | table | super_user
- _timescaledb_catalog | telemetry_event                                  | table | super_user
-(25 rows)
-
-\dt "_timescaledb_internal".*
-                          List of relations
-        Schema         |          Name          | Type  |   Owner    
------------------------+------------------------+-------+------------
- _timescaledb_internal | bgw_job_stat           | table | super_user
- _timescaledb_internal | bgw_policy_chunk_stats | table | super_user
- _timescaledb_internal | job_errors             | table | super_user
-(3 rows)
-
--- Test that renaming ordinary table works
-CREATE TABLE renametable (foo int);
-ALTER TABLE "renametable" RENAME TO "newname_none_ht";
-SELECT * FROM "newname_none_ht";
- foo 
------
-(0 rows)
-
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/drop_schema.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/drop_schema.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/drop_schema.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/drop_schema.out	2023-11-25 05:27:38.949037781 +0000
@@ -1,117 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-\c :TEST_DBNAME :ROLE_SUPERUSER
-CREATE SCHEMA chunk_schema1;
-CREATE SCHEMA chunk_schema2;
-CREATE SCHEMA hypertable_schema;
-CREATE SCHEMA extra_schema;
-GRANT ALL ON SCHEMA hypertable_schema TO :ROLE_DEFAULT_PERM_USER;
-GRANT ALL ON SCHEMA chunk_schema1 TO :ROLE_DEFAULT_PERM_USER;
-GRANT ALL ON SCHEMA chunk_schema2 TO :ROLE_DEFAULT_PERM_USER;
-SET ROLE :ROLE_DEFAULT_PERM_USER;
-CREATE TABLE hypertable_schema.test1 (time timestamptz, temp float, location int);
-CREATE TABLE hypertable_schema.test2 (time timestamptz, temp float, location int);
---create two identical tables with their own chunk schemas
-SELECT create_hypertable('hypertable_schema.test1', 'time', 'location', 2, associated_schema_name => 'chunk_schema1');
-NOTICE:  adding not-null constraint to column "time"
-       create_hypertable       
--------------------------------
- (1,hypertable_schema,test1,t)
-(1 row)
-
-SELECT create_hypertable('hypertable_schema.test2', 'time', 'location', 2, associated_schema_name => 'chunk_schema2');
-NOTICE:  adding not-null constraint to column "time"
-       create_hypertable       
--------------------------------
- (2,hypertable_schema,test2,t)
-(1 row)
-
-INSERT INTO hypertable_schema.test1 VALUES ('2001-01-01 01:01:01', 23.3, 1);
-INSERT INTO hypertable_schema.test2 VALUES ('2001-01-01 01:01:01', 23.3, 1);
-SELECT * FROM _timescaledb_catalog.hypertable ORDER BY id;
- id |    schema_name    | table_name | associated_schema_name | associated_table_prefix | num_dimensions | chunk_sizing_func_schema |  chunk_sizing_func_name  | chunk_target_size | compression_state | compressed_hypertable_id | replication_factor 
-----+-------------------+------------+------------------------+-------------------------+----------------+--------------------------+--------------------------+-------------------+-------------------+--------------------------+--------------------
-  1 | hypertable_schema | test1      | chunk_schema1          | _hyper_1                |              2 | _timescaledb_internal    | calculate_chunk_interval |                 0 |                 0 |                          |                   
-  2 | hypertable_schema | test2      | chunk_schema2          | _hyper_2                |              2 | _timescaledb_internal    | calculate_chunk_interval |                 0 |                 0 |                          |                   
-(2 rows)
-
-SELECT * FROM _timescaledb_catalog.chunk;
- id | hypertable_id |  schema_name  |    table_name    | compressed_chunk_id | dropped | status | osm_chunk 
-----+---------------+---------------+------------------+---------------------+---------+--------+-----------
-  1 |             1 | chunk_schema1 | _hyper_1_1_chunk |                     | f       |      0 | f
-  2 |             2 | chunk_schema2 | _hyper_2_2_chunk |                     | f       |      0 | f
-(2 rows)
-
-RESET ROLE;
---drop the associated schema. We drop the extra schema to show we can
---handle multi-schema drops
-DROP SCHEMA chunk_schema1, extra_schema CASCADE;
-NOTICE:  drop cascades to table chunk_schema1._hyper_1_1_chunk
-NOTICE:  the chunk storage schema changed to "_timescaledb_internal" for 1 hypertable
-SET ROLE :ROLE_DEFAULT_PERM_USER;
---show that the metadata for the table using the dropped schema is
---changed. The other table is not affected.
-SELECT * FROM _timescaledb_catalog.hypertable ORDER BY id;
- id |    schema_name    | table_name | associated_schema_name | associated_table_prefix | num_dimensions | chunk_sizing_func_schema |  chunk_sizing_func_name  | chunk_target_size | compression_state | compressed_hypertable_id | replication_factor 
-----+-------------------+------------+------------------------+-------------------------+----------------+--------------------------+--------------------------+-------------------+-------------------+--------------------------+--------------------
-  1 | hypertable_schema | test1      | _timescaledb_internal  | _hyper_1                |              2 | _timescaledb_internal    | calculate_chunk_interval |                 0 |                 0 |                          |                   
-  2 | hypertable_schema | test2      | chunk_schema2          | _hyper_2                |              2 | _timescaledb_internal    | calculate_chunk_interval |                 0 |                 0 |                          |                   
-(2 rows)
-
-SELECT * FROM _timescaledb_catalog.chunk;
- id | hypertable_id |  schema_name  |    table_name    | compressed_chunk_id | dropped | status | osm_chunk 
-----+---------------+---------------+------------------+---------------------+---------+--------+-----------
-  2 |             2 | chunk_schema2 | _hyper_2_2_chunk |                     | f       |      0 | f
-(1 row)
-
---new chunk should be created in the internal associated schema
-INSERT INTO hypertable_schema.test1 VALUES ('2001-01-01 01:01:01', 23.3, 1);
-SELECT * FROM _timescaledb_catalog.chunk;
- id | hypertable_id |      schema_name      |    table_name    | compressed_chunk_id | dropped | status | osm_chunk 
-----+---------------+-----------------------+------------------+---------------------+---------+--------+-----------
-  2 |             2 | chunk_schema2         | _hyper_2_2_chunk |                     | f       |      0 | f
-  3 |             1 | _timescaledb_internal | _hyper_1_3_chunk |                     | f       |      0 | f
-(2 rows)
-
-RESET ROLE;
---dropping the internal schema should not work
-\set ON_ERROR_STOP 0
-DROP SCHEMA _timescaledb_internal CASCADE;
-ERROR:  cannot drop schema _timescaledb_internal because extension timescaledb requires it
-\set ON_ERROR_STOP 1
---dropping the hypertable schema should delete everything
-DROP SCHEMA hypertable_schema CASCADE;
-NOTICE:  drop cascades to 4 other objects
-SET ROLE :ROLE_DEFAULT_PERM_USER;
---everything should be cleaned up
-SELECT * FROM _timescaledb_catalog.hypertable GROUP BY id;
- id | schema_name | table_name | associated_schema_name | associated_table_prefix | num_dimensions | chunk_sizing_func_schema | chunk_sizing_func_name | chunk_target_size | compression_state | compressed_hypertable_id | replication_factor 
-----+-------------+------------+------------------------+-------------------------+----------------+--------------------------+------------------------+-------------------+-------------------+--------------------------+--------------------
-(0 rows)
-
-SELECT * FROM _timescaledb_catalog.chunk;
- id | hypertable_id | schema_name | table_name | compressed_chunk_id | dropped | status | osm_chunk 
-----+---------------+-------------+------------+---------------------+---------+--------+-----------
-(0 rows)
-
-SELECT * FROM _timescaledb_catalog.dimension;
- id | hypertable_id | column_name | column_type | aligned | num_slices | partitioning_func_schema | partitioning_func | interval_length | compress_interval_length | integer_now_func_schema | integer_now_func 
-----+---------------+-------------+-------------+---------+------------+--------------------------+-------------------+-----------------+--------------------------+-------------------------+------------------
-(0 rows)
-
-SELECT * FROM _timescaledb_catalog.dimension_slice;
- id | dimension_id | range_start | range_end 
-----+--------------+-------------+-----------
-(0 rows)
-
-SELECT * FROM _timescaledb_catalog.chunk_index;
- chunk_id | index_name | hypertable_id | hypertable_index_name 
-----------+------------+---------------+-----------------------
-(0 rows)
-
-SELECT * FROM _timescaledb_catalog.chunk_constraint;
- chunk_id | dimension_slice_id | constraint_name | hypertable_constraint_name 
-----------+--------------------+-----------------+----------------------------
-(0 rows)
-
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/dump_meta.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/dump_meta.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/dump_meta.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/dump_meta.out	2023-11-25 05:27:38.957037758 +0000
@@ -1,225 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-\ir include/insert_two_partitions.sql
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-CREATE TABLE PUBLIC."two_Partitions" (
-  "timeCustom" BIGINT NOT NULL,
-  device_id TEXT NOT NULL,
-  series_0 DOUBLE PRECISION NULL,
-  series_1 DOUBLE PRECISION NULL,
-  series_2 DOUBLE PRECISION NULL,
-  series_bool BOOLEAN NULL
-);
-CREATE INDEX ON PUBLIC."two_Partitions" (device_id, "timeCustom" DESC NULLS LAST) WHERE device_id IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, series_0) WHERE series_0 IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, series_1)  WHERE series_1 IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, series_2) WHERE series_2 IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, series_bool) WHERE series_bool IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, device_id);
-SELECT * FROM create_hypertable('"public"."two_Partitions"'::regclass, 'timeCustom'::name, 'device_id'::name, associated_schema_name=>'_timescaledb_internal'::text, number_partitions => 2, chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
- hypertable_id | schema_name |   table_name   | created 
----------------+-------------+----------------+---------
-             1 | public      | two_Partitions | t
-(1 row)
-
-\set QUIET off
-BEGIN;
-BEGIN
-\COPY public."two_Partitions" FROM 'data/ds1_dev1_1.tsv' NULL AS '';
-COPY 7
-COMMIT;
-COMMIT
-INSERT INTO public."two_Partitions"("timeCustom", device_id, series_0, series_1) VALUES
-(1257987600000000000, 'dev1', 1.5, 1),
-(1257987600000000000, 'dev1', 1.5, 2),
-(1257894000000000000, 'dev2', 1.5, 1),
-(1257894002000000000, 'dev1', 2.5, 3);
-INSERT 0 4
-INSERT INTO "two_Partitions"("timeCustom", device_id, series_0, series_1) VALUES
-(1257894000000000000, 'dev2', 1.5, 2);
-INSERT 0 1
-\set QUIET on
-\ir ../../scripts/dump_meta_data.sql
---
--- This file is licensed under the Apache License, see LICENSE-APACHE
--- at the top level directory of the TimescaleDB distribution.
--- This script will dump relevant meta data from internal TimescaleDB tables
--- that can help our engineers trouble shoot.
---
--- usage:
--- psql [your connect flags] -d your_timescale_db < dump_meta_data.sql > dumpfile.txt
-\echo 'TimescaleDB meta data dump'
-TimescaleDB meta data dump
-</exclude_from_test>
-\echo 'List of tables'
-List of tables
-\dt
-                  List of relations
- Schema |      Name      | Type  |       Owner       
---------+----------------+-------+-------------------
- public | two_Partitions | table | default_perm_user
-(1 row)
-
-\echo 'List of hypertables'
-List of hypertables
-SELECT * FROM _timescaledb_catalog.hypertable;
- id | schema_name |   table_name   | associated_schema_name | associated_table_prefix | num_dimensions | chunk_sizing_func_schema |  chunk_sizing_func_name  | chunk_target_size | compression_state | compressed_hypertable_id | replication_factor 
-----+-------------+----------------+------------------------+-------------------------+----------------+--------------------------+--------------------------+-------------------+-------------------+--------------------------+--------------------
-  1 | public      | two_Partitions | _timescaledb_internal  | _hyper_1                |              2 | _timescaledb_internal    | calculate_chunk_interval |                 0 |                 0 |                          |                   
-(1 row)
-
-\echo 'List of chunk indexes'
-List of chunk indexes
-SELECT * FROM _timescaledb_catalog.chunk_index;
- chunk_id |                         index_name                         | hypertable_id |           hypertable_index_name           
-----------+------------------------------------------------------------+---------------+-------------------------------------------
-        1 | _hyper_1_1_chunk_two_Partitions_device_id_timeCustom_idx   |             1 | two_Partitions_device_id_timeCustom_idx
-        1 | _hyper_1_1_chunk_two_Partitions_timeCustom_series_0_idx    |             1 | two_Partitions_timeCustom_series_0_idx
-        1 | _hyper_1_1_chunk_two_Partitions_timeCustom_series_1_idx    |             1 | two_Partitions_timeCustom_series_1_idx
-        1 | _hyper_1_1_chunk_two_Partitions_timeCustom_series_2_idx    |             1 | two_Partitions_timeCustom_series_2_idx
-        1 | _hyper_1_1_chunk_two_Partitions_timeCustom_series_bool_idx |             1 | two_Partitions_timeCustom_series_bool_idx
-        1 | _hyper_1_1_chunk_two_Partitions_timeCustom_device_id_idx   |             1 | two_Partitions_timeCustom_device_id_idx
-        1 | _hyper_1_1_chunk_two_Partitions_timeCustom_idx             |             1 | two_Partitions_timeCustom_idx
-        2 | _hyper_1_2_chunk_two_Partitions_device_id_timeCustom_idx   |             1 | two_Partitions_device_id_timeCustom_idx
-        2 | _hyper_1_2_chunk_two_Partitions_timeCustom_series_0_idx    |             1 | two_Partitions_timeCustom_series_0_idx
-        2 | _hyper_1_2_chunk_two_Partitions_timeCustom_series_1_idx    |             1 | two_Partitions_timeCustom_series_1_idx
-        2 | _hyper_1_2_chunk_two_Partitions_timeCustom_series_2_idx    |             1 | two_Partitions_timeCustom_series_2_idx
-        2 | _hyper_1_2_chunk_two_Partitions_timeCustom_series_bool_idx |             1 | two_Partitions_timeCustom_series_bool_idx
-        2 | _hyper_1_2_chunk_two_Partitions_timeCustom_device_id_idx   |             1 | two_Partitions_timeCustom_device_id_idx
-        2 | _hyper_1_2_chunk_two_Partitions_timeCustom_idx             |             1 | two_Partitions_timeCustom_idx
-        3 | _hyper_1_3_chunk_two_Partitions_device_id_timeCustom_idx   |             1 | two_Partitions_device_id_timeCustom_idx
-        3 | _hyper_1_3_chunk_two_Partitions_timeCustom_series_0_idx    |             1 | two_Partitions_timeCustom_series_0_idx
-        3 | _hyper_1_3_chunk_two_Partitions_timeCustom_series_1_idx    |             1 | two_Partitions_timeCustom_series_1_idx
-        3 | _hyper_1_3_chunk_two_Partitions_timeCustom_series_2_idx    |             1 | two_Partitions_timeCustom_series_2_idx
-        3 | _hyper_1_3_chunk_two_Partitions_timeCustom_series_bool_idx |             1 | two_Partitions_timeCustom_series_bool_idx
-        3 | _hyper_1_3_chunk_two_Partitions_timeCustom_device_id_idx   |             1 | two_Partitions_timeCustom_device_id_idx
-        3 | _hyper_1_3_chunk_two_Partitions_timeCustom_idx             |             1 | two_Partitions_timeCustom_idx
-        4 | _hyper_1_4_chunk_two_Partitions_device_id_timeCustom_idx   |             1 | two_Partitions_device_id_timeCustom_idx
-        4 | _hyper_1_4_chunk_two_Partitions_timeCustom_series_0_idx    |             1 | two_Partitions_timeCustom_series_0_idx
-        4 | _hyper_1_4_chunk_two_Partitions_timeCustom_series_1_idx    |             1 | two_Partitions_timeCustom_series_1_idx
-        4 | _hyper_1_4_chunk_two_Partitions_timeCustom_series_2_idx    |             1 | two_Partitions_timeCustom_series_2_idx
-        4 | _hyper_1_4_chunk_two_Partitions_timeCustom_series_bool_idx |             1 | two_Partitions_timeCustom_series_bool_idx
-        4 | _hyper_1_4_chunk_two_Partitions_timeCustom_device_id_idx   |             1 | two_Partitions_timeCustom_device_id_idx
-        4 | _hyper_1_4_chunk_two_Partitions_timeCustom_idx             |             1 | two_Partitions_timeCustom_idx
-(28 rows)
-
-\echo 'Size of hypertables'
-Size of hypertables
-SELECT hypertable,
-       table_bytes,
-       index_bytes,
-       toast_bytes,
-       total_bytes
-       FROM (
-       SELECT *, total_bytes-index_bytes-COALESCE(toast_bytes,0) AS table_bytes FROM (
-              SELECT
-              pgc.oid::regclass::text as hypertable,
-              sum(pg_total_relation_size('"' || c.schema_name || '"."' || c.table_name || '"'))::bigint as total_bytes,
-              sum(pg_indexes_size('"' || c.schema_name || '"."' || c.table_name || '"'))::bigint AS index_bytes,
-              sum(pg_total_relation_size(reltoastrelid))::bigint AS toast_bytes
-              FROM
-              _timescaledb_catalog.hypertable h,
-              _timescaledb_catalog.chunk c,
-              pg_class pgc,
-              pg_namespace pns
-              WHERE c.hypertable_id = h.id
-              AND pgc.relname = h.table_name
-              AND pns.oid = pgc.relnamespace
-              AND pns.nspname = h.schema_name
-              AND relkind = 'r'
-              AND c.dropped is false
-              GROUP BY pgc.oid
-              ) sub1
-       ) sub2;
-    hypertable    | table_bytes | index_bytes | toast_bytes | total_bytes 
-------------------+-------------+-------------+-------------+-------------
- "two_Partitions" |       32768 |      417792 |       32768 |      483328
-(1 row)
-
-\echo 'Chunk sizes:'
-Chunk sizes:
-SELECT chunk_id,
-chunk_table,
-partitioning_columns,
-partitioning_column_types,
-partitioning_hash_functions,
-ranges,
-table_bytes,
-index_bytes,
-toast_bytes,
-total_bytes
-FROM (
-SELECT *,
-      total_bytes-index_bytes-COALESCE(toast_bytes,0) AS table_bytes
-      FROM (
-       SELECT c.id as chunk_id,
-       '"' || c.schema_name || '"."' || c.table_name || '"' as chunk_table,
-       pg_total_relation_size('"' || c.schema_name || '"."' || c.table_name || '"') AS total_bytes,
-       pg_indexes_size('"' || c.schema_name || '"."' || c.table_name || '"') AS index_bytes,
-       pg_total_relation_size(reltoastrelid) AS toast_bytes,
-       array_agg(d.column_name ORDER BY d.interval_length, d.column_name ASC) as partitioning_columns,
-       array_agg(d.column_type ORDER BY d.interval_length, d.column_name ASC) as partitioning_column_types,
-       array_agg(d.partitioning_func_schema || '.' || d.partitioning_func ORDER BY d.interval_length, d.column_name ASC) as partitioning_hash_functions,
-       array_agg('[' || _timescaledb_internal.range_value_to_pretty(range_start, column_type) ||
-                 ',' ||
-                 _timescaledb_internal.range_value_to_pretty(range_end, column_type) || ')' ORDER BY d.interval_length, d.column_name ASC) as ranges
-       FROM
-       _timescaledb_catalog.hypertable h,
-       _timescaledb_catalog.chunk c,
-       _timescaledb_catalog.chunk_constraint cc,
-       _timescaledb_catalog.dimension d,
-       _timescaledb_catalog.dimension_slice ds,
-       pg_class pgc,
-       pg_namespace pns
-       WHERE pgc.relname = h.table_name
-             AND pns.oid = pgc.relnamespace
-             AND pns.nspname = h.schema_name
-             AND relkind = 'r'
-             AND c.hypertable_id = h.id
-             AND c.id = cc.chunk_id
-             AND cc.dimension_slice_id = ds.id
-             AND ds.dimension_id = d.id
-             AND c.dropped is false
-       GROUP BY c.id, pgc.reltoastrelid, pgc.oid ORDER BY c.id
-       ) sub1
-) sub2;
- chunk_id |                chunk_table                 |  partitioning_columns  | partitioning_column_types |           partitioning_hash_functions           |                              ranges                               | table_bytes | index_bytes | toast_bytes | total_bytes 
-----------+--------------------------------------------+------------------------+---------------------------+-------------------------------------------------+-------------------------------------------------------------------+-------------+-------------+-------------+-------------
-        1 | "_timescaledb_internal"."_hyper_1_1_chunk" | {timeCustom,device_id} | {bigint,text}             | {NULL,_timescaledb_internal.get_partition_hash} | {"['1257892416000000000','1257895008000000000')","[1073741823,)"} |        8192 |      114688 |        8192 |      131072
-        2 | "_timescaledb_internal"."_hyper_1_2_chunk" | {timeCustom,device_id} | {bigint,text}             | {NULL,_timescaledb_internal.get_partition_hash} | {"['1257897600000000000','1257900192000000000')","[1073741823,)"} |        8192 |      106496 |        8192 |      122880
-        3 | "_timescaledb_internal"."_hyper_1_3_chunk" | {timeCustom,device_id} | {bigint,text}             | {NULL,_timescaledb_internal.get_partition_hash} | {"['1257985728000000000','1257988320000000000')","[1073741823,)"} |        8192 |       98304 |        8192 |      114688
-        4 | "_timescaledb_internal"."_hyper_1_4_chunk" | {timeCustom,device_id} | {bigint,text}             | {NULL,_timescaledb_internal.get_partition_hash} | {"['1257892416000000000','1257895008000000000')","[,1073741823)"} |        8192 |       98304 |        8192 |      114688
-(4 rows)
-
-\echo 'Hypertable index sizes'
-Hypertable index sizes
-SELECT h.schema_name || '.' || h.table_name AS hypertable,
-       h.schema_name || '.' || ci.hypertable_index_name AS index_name,
-       sum(pg_relation_size(c.oid))::bigint AS index_bytes
-FROM
-pg_class c,
-pg_namespace n,
-_timescaledb_catalog.hypertable h,
-_timescaledb_catalog.chunk ch,
-_timescaledb_catalog.chunk_index ci
-WHERE ch.schema_name = n.nspname
-      AND c.relnamespace = n.oid
-      AND c.relname = ci.index_name
-      AND ch.id = ci.chunk_id
-      AND h.id = ci.hypertable_id
-GROUP BY h.schema_name, h.table_name, ci.hypertable_index_name
-ORDER BY h.schema_name, h.table_name, ci.hypertable_index_name;
-      hypertable       |                    index_name                    | index_bytes 
------------------------+--------------------------------------------------+-------------
- public.two_Partitions | public.two_Partitions_device_id_timeCustom_idx   |       65536
- public.two_Partitions | public.two_Partitions_timeCustom_device_id_idx   |       65536
- public.two_Partitions | public.two_Partitions_timeCustom_idx             |       65536
- public.two_Partitions | public.two_Partitions_timeCustom_series_0_idx    |       65536
- public.two_Partitions | public.two_Partitions_timeCustom_series_1_idx    |       65536
- public.two_Partitions | public.two_Partitions_timeCustom_series_2_idx    |       40960
- public.two_Partitions | public.two_Partitions_timeCustom_series_bool_idx |       49152
-(7 rows)
-
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/extension_scripts.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/extension_scripts.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/extension_scripts.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/extension_scripts.out	2023-11-25 05:27:38.957037758 +0000
@@ -1,58 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-\c :TEST_DBNAME :ROLE_SUPERUSER
-DROP EXTENSION timescaledb;
--- test that installation script errors when any of our internal schemas already exists
-\set ON_ERROR_STOP 0
-CREATE SCHEMA _timescaledb_catalog;
-CREATE EXTENSION timescaledb;
-ERROR:  schema "_timescaledb_catalog" already exists
-DROP SCHEMA _timescaledb_catalog;
-\c :TEST_DBNAME :ROLE_SUPERUSER
-CREATE SCHEMA _timescaledb_internal;
-CREATE EXTENSION timescaledb;
-ERROR:  schema "_timescaledb_internal" already exists
-DROP SCHEMA _timescaledb_internal;
-\c :TEST_DBNAME :ROLE_SUPERUSER
-CREATE SCHEMA _timescaledb_cache;
-CREATE EXTENSION timescaledb;
-ERROR:  schema "_timescaledb_cache" already exists
-DROP SCHEMA _timescaledb_cache;
-\c :TEST_DBNAME :ROLE_SUPERUSER
-CREATE SCHEMA _timescaledb_config;
-CREATE EXTENSION timescaledb;
-ERROR:  schema "_timescaledb_config" already exists
-DROP SCHEMA _timescaledb_config;
-\c :TEST_DBNAME :ROLE_SUPERUSER
-CREATE SCHEMA timescaledb_experimental;
-CREATE EXTENSION timescaledb;
-ERROR:  schema "timescaledb_experimental" already exists
-DROP SCHEMA timescaledb_experimental;
-\c :TEST_DBNAME :ROLE_SUPERUSER
-CREATE SCHEMA timescaledb_information;
-CREATE EXTENSION timescaledb;
-ERROR:  schema "timescaledb_information" already exists
-DROP SCHEMA timescaledb_information;
--- test that installation script errors when any of the function in public schema already exists
--- we don't test every public function but just a few common ones
-\c :TEST_DBNAME :ROLE_SUPERUSER
-CREATE FUNCTION time_bucket(int,int) RETURNS int LANGUAGE SQL AS $$ SELECT 1::int; $$;
-CREATE EXTENSION timescaledb;
-ERROR:  function "time_bucket" already exists with same argument types
-DROP FUNCTION time_bucket;
-\c :TEST_DBNAME :ROLE_SUPERUSER
-CREATE OR REPLACE FUNCTION show_chunks(relation regclass, older_than "any" DEFAULT NULL, newer_than "any" DEFAULT NULL) RETURNS SETOF regclass language internal as 'pg_partition_ancestors';
-CREATE EXTENSION timescaledb;
-ERROR:  function "show_chunks" already exists with same argument types
-DROP FUNCTION show_chunks;
--- Create a user that is not all-lowercase
-CREATE USER "FooBar" WITH SUPERUSER;
-\c :TEST_DBNAME "FooBar"
-SET client_min_messages TO error;
-CREATE EXTENSION timescaledb;
-DROP EXTENSION timescaledb;
-RESET client_min_messages;
-\c :TEST_DBNAME :ROLE_SUPERUSER
-SET client_min_messages TO ERROR;
-CREATE EXTENSION timescaledb;
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/generated_as_identity.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/generated_as_identity.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/generated_as_identity.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/generated_as_identity.out	2023-11-25 05:27:38.949037781 +0000
@@ -1,113 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-CREATE table test_gen (
-    id int generated by default AS IDENTITY primary key,
-    payload text
-);
- SELECT create_hypertable('test_gen', 'id', chunk_time_interval=>10);
-   create_hypertable   
------------------------
- (1,public,test_gen,t)
-(1 row)
-
-insert into test_gen (payload) select generate_series(1,15) returning *;
- id | payload 
-----+---------
-  1 | 1
-  2 | 2
-  3 | 3
-  4 | 4
-  5 | 5
-  6 | 6
-  7 | 7
-  8 | 8
-  9 | 9
- 10 | 10
- 11 | 11
- 12 | 12
- 13 | 13
- 14 | 14
- 15 | 15
-(15 rows)
-
-select * from test_gen;
- id | payload 
-----+---------
-  1 | 1
-  2 | 2
-  3 | 3
-  4 | 4
-  5 | 5
-  6 | 6
-  7 | 7
-  8 | 8
-  9 | 9
- 10 | 10
- 11 | 11
- 12 | 12
- 13 | 13
- 14 | 14
- 15 | 15
-(15 rows)
-
-\set ON_ERROR_STOP 0
-insert into test_gen values('1', 'a');
-ERROR:  duplicate key value violates unique constraint "1_1_test_gen_pkey"
-\set ON_ERROR_STOP 1
-ALTER TABLE test_gen ALTER COLUMN id DROP IDENTITY;
-\set ON_ERROR_STOP 0
-insert into test_gen (payload) select generate_series(15,20) returning *;
-ERROR:  NULL value in column "id" violates not-null constraint
-\set ON_ERROR_STOP 1
-ALTER TABLE test_gen ALTER COLUMN id ADD GENERATED BY DEFAULT AS IDENTITY;
-\set ON_ERROR_STOP 0
-insert into test_gen (payload) select generate_series(15,20) returning *;
-ERROR:  duplicate key value violates unique constraint "1_1_test_gen_pkey"
-\set ON_ERROR_STOP 1
-ALTER TABLE test_gen ALTER COLUMN id SET GENERATED BY DEFAULT RESTART 100;
-insert into test_gen (payload) select generate_series(15,20) returning *;
- id  | payload 
------+---------
- 100 | 15
- 101 | 16
- 102 | 17
- 103 | 18
- 104 | 19
- 105 | 20
-(6 rows)
-
-select * from test_gen;
- id  | payload 
------+---------
-   1 | 1
-   2 | 2
-   3 | 3
-   4 | 4
-   5 | 5
-   6 | 6
-   7 | 7
-   8 | 8
-   9 | 9
-  10 | 10
-  11 | 11
-  12 | 12
-  13 | 13
-  14 | 14
-  15 | 15
- 100 | 15
- 101 | 16
- 102 | 17
- 103 | 18
- 104 | 19
- 105 | 20
-(21 rows)
-
-SELECT * FROM test.show_subtables('test_gen');
-                 Child                  | Tablespace 
-----------------------------------------+------------
- _timescaledb_internal._hyper_1_1_chunk | 
- _timescaledb_internal._hyper_1_2_chunk | 
- _timescaledb_internal._hyper_1_3_chunk | 
-(3 rows)
-
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/grant_hypertable.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/grant_hypertable.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/grant_hypertable.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/grant_hypertable.out	2023-11-25 05:27:38.973037712 +0000
@@ -1,394 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-\c :TEST_DBNAME :ROLE_SUPERUSER
-CREATE TABLE conditions(
-    time TIMESTAMPTZ NOT NULL,
-    device INTEGER,
-    temperature FLOAT
-);
--- Create a hypertable and show that it does not have any privileges
-SELECT * FROM create_hypertable('conditions', 'time', chunk_time_interval => '5 days'::interval);
- hypertable_id | schema_name | table_name | created 
----------------+-------------+------------+---------
-             1 | public      | conditions | t
-(1 row)
-
-INSERT INTO conditions
-SELECT time, (random()*30)::int, random()*80 - 40
-FROM generate_series('2018-12-01 00:00'::timestamp, '2018-12-10 00:00'::timestamp, '1h') AS time;
-\z conditions
-                               Access privileges
- Schema |    Name    | Type  | Access privileges | Column privileges | Policies 
---------+------------+-------+-------------------+-------------------+----------
- public | conditions | table |                   |                   | 
-(1 row)
-
-\z _timescaledb_internal.*chunk
-                                          Access privileges
-        Schema         |       Name       | Type  | Access privileges | Column privileges | Policies 
------------------------+------------------+-------+-------------------+-------------------+----------
- _timescaledb_internal | _hyper_1_1_chunk | table |                   |                   | 
- _timescaledb_internal | _hyper_1_2_chunk | table |                   |                   | 
- _timescaledb_internal | _hyper_1_3_chunk | table |                   |                   | 
-(3 rows)
-
--- Add privileges and show that they propagate to the chunks
-GRANT SELECT, INSERT ON conditions TO PUBLIC;
-\z conditions
-                                     Access privileges
- Schema |    Name    | Type  |       Access privileges       | Column privileges | Policies 
---------+------------+-------+-------------------------------+-------------------+----------
- public | conditions | table | super_user=arwdDxt/super_user+|                   | 
-        |            |       | =ar/super_user                |                   | 
-(1 row)
-
-\z _timescaledb_internal.*chunk
-                                                Access privileges
-        Schema         |       Name       | Type  |       Access privileges       | Column privileges | Policies 
------------------------+------------------+-------+-------------------------------+-------------------+----------
- _timescaledb_internal | _hyper_1_1_chunk | table | super_user=arwdDxt/super_user+|                   | 
-                       |                  |       | =ar/super_user                |                   | 
- _timescaledb_internal | _hyper_1_2_chunk | table | super_user=arwdDxt/super_user+|                   | 
-                       |                  |       | =ar/super_user                |                   | 
- _timescaledb_internal | _hyper_1_3_chunk | table | super_user=arwdDxt/super_user+|                   | 
-                       |                  |       | =ar/super_user                |                   | 
-(3 rows)
-
--- Create some more chunks and show that they also get the privileges.
-INSERT INTO conditions
-SELECT time, (random()*30)::int, random()*80 - 40
-FROM generate_series('2018-12-10 00:00'::timestamp, '2018-12-20 00:00'::timestamp, '1h') AS time;
-\z conditions
-                                     Access privileges
- Schema |    Name    | Type  |       Access privileges       | Column privileges | Policies 
---------+------------+-------+-------------------------------+-------------------+----------
- public | conditions | table | super_user=arwdDxt/super_user+|                   | 
-        |            |       | =ar/super_user                |                   | 
-(1 row)
-
-\z _timescaledb_internal.*chunk
-                                                Access privileges
-        Schema         |       Name       | Type  |       Access privileges       | Column privileges | Policies 
------------------------+------------------+-------+-------------------------------+-------------------+----------
- _timescaledb_internal | _hyper_1_1_chunk | table | super_user=arwdDxt/super_user+|                   | 
-                       |                  |       | =ar/super_user                |                   | 
- _timescaledb_internal | _hyper_1_2_chunk | table | super_user=arwdDxt/super_user+|                   | 
-                       |                  |       | =ar/super_user                |                   | 
- _timescaledb_internal | _hyper_1_3_chunk | table | super_user=arwdDxt/super_user+|                   | 
-                       |                  |       | =ar/super_user                |                   | 
- _timescaledb_internal | _hyper_1_4_chunk | table | super_user=arwdDxt/super_user+|                   | 
-                       |                  |       | =ar/super_user                |                   | 
- _timescaledb_internal | _hyper_1_5_chunk | table | super_user=arwdDxt/super_user+|                   | 
-                       |                  |       | =ar/super_user                |                   | 
-(5 rows)
-
--- Revoke one of the privileges and show that it propagate to the
--- chunks.
-REVOKE INSERT ON conditions FROM PUBLIC;
-\z conditions
-                                     Access privileges
- Schema |    Name    | Type  |       Access privileges       | Column privileges | Policies 
---------+------------+-------+-------------------------------+-------------------+----------
- public | conditions | table | super_user=arwdDxt/super_user+|                   | 
-        |            |       | =r/super_user                 |                   | 
-(1 row)
-
-\z _timescaledb_internal.*chunk
-                                                Access privileges
-        Schema         |       Name       | Type  |       Access privileges       | Column privileges | Policies 
------------------------+------------------+-------+-------------------------------+-------------------+----------
- _timescaledb_internal | _hyper_1_1_chunk | table | super_user=arwdDxt/super_user+|                   | 
-                       |                  |       | =r/super_user                 |                   | 
- _timescaledb_internal | _hyper_1_2_chunk | table | super_user=arwdDxt/super_user+|                   | 
-                       |                  |       | =r/super_user                 |                   | 
- _timescaledb_internal | _hyper_1_3_chunk | table | super_user=arwdDxt/super_user+|                   | 
-                       |                  |       | =r/super_user                 |                   | 
- _timescaledb_internal | _hyper_1_4_chunk | table | super_user=arwdDxt/super_user+|                   | 
-                       |                  |       | =r/super_user                 |                   | 
- _timescaledb_internal | _hyper_1_5_chunk | table | super_user=arwdDxt/super_user+|                   | 
-                       |                  |       | =r/super_user                 |                   | 
-(5 rows)
-
--- Add some more chunks and show that it inherits the grants from the
--- hypertable.
-INSERT INTO conditions
-SELECT time, (random()*30)::int, random()*80 - 40
-FROM generate_series('2018-12-20 00:00'::timestamp, '2018-12-30 00:00'::timestamp, '1h') AS time;
-\z conditions
-                                     Access privileges
- Schema |    Name    | Type  |       Access privileges       | Column privileges | Policies 
---------+------------+-------+-------------------------------+-------------------+----------
- public | conditions | table | super_user=arwdDxt/super_user+|                   | 
-        |            |       | =r/super_user                 |                   | 
-(1 row)
-
-\z _timescaledb_internal.*chunk
-                                                Access privileges
-        Schema         |       Name       | Type  |       Access privileges       | Column privileges | Policies 
------------------------+------------------+-------+-------------------------------+-------------------+----------
- _timescaledb_internal | _hyper_1_1_chunk | table | super_user=arwdDxt/super_user+|                   | 
-                       |                  |       | =r/super_user                 |                   | 
- _timescaledb_internal | _hyper_1_2_chunk | table | super_user=arwdDxt/super_user+|                   | 
-                       |                  |       | =r/super_user                 |                   | 
- _timescaledb_internal | _hyper_1_3_chunk | table | super_user=arwdDxt/super_user+|                   | 
-                       |                  |       | =r/super_user                 |                   | 
- _timescaledb_internal | _hyper_1_4_chunk | table | super_user=arwdDxt/super_user+|                   | 
-                       |                  |       | =r/super_user                 |                   | 
- _timescaledb_internal | _hyper_1_5_chunk | table | super_user=arwdDxt/super_user+|                   | 
-                       |                  |       | =r/super_user                 |                   | 
- _timescaledb_internal | _hyper_1_6_chunk | table | super_user=arwdDxt/super_user+|                   | 
-                       |                  |       | =r/super_user                 |                   | 
- _timescaledb_internal | _hyper_1_7_chunk | table | super_user=arwdDxt/super_user+|                   | 
-                       |                  |       | =r/super_user                 |                   | 
-(7 rows)
-
--- Change grants of one chunk explicitly and check that it is possible
-\z _timescaledb_internal._hyper_1_1_chunk
-                                                Access privileges
-        Schema         |       Name       | Type  |       Access privileges       | Column privileges | Policies 
------------------------+------------------+-------+-------------------------------+-------------------+----------
- _timescaledb_internal | _hyper_1_1_chunk | table | super_user=arwdDxt/super_user+|                   | 
-                       |                  |       | =r/super_user                 |                   | 
-(1 row)
-
-GRANT UPDATE ON _timescaledb_internal._hyper_1_1_chunk TO PUBLIC;
-\z _timescaledb_internal._hyper_1_1_chunk
-                                                Access privileges
-        Schema         |       Name       | Type  |       Access privileges       | Column privileges | Policies 
------------------------+------------------+-------+-------------------------------+-------------------+----------
- _timescaledb_internal | _hyper_1_1_chunk | table | super_user=arwdDxt/super_user+|                   | 
-                       |                  |       | =rw/super_user                |                   | 
-(1 row)
-
-REVOKE SELECT ON _timescaledb_internal._hyper_1_1_chunk FROM PUBLIC;
-\z _timescaledb_internal._hyper_1_1_chunk
-                                                Access privileges
-        Schema         |       Name       | Type  |       Access privileges       | Column privileges | Policies 
------------------------+------------------+-------+-------------------------------+-------------------+----------
- _timescaledb_internal | _hyper_1_1_chunk | table | super_user=arwdDxt/super_user+|                   | 
-                       |                  |       | =w/super_user                 |                   | 
-(1 row)
-
--- Check that revoking a permission first on the chunk and then on the
--- hypertable that was added through the hypertable (INSERT and
--- SELECT, in this case) still do not copy permissions from the
--- hypertable (so there should not be a select permission to public on
--- the chunk but there should be one on the hypertable).
-GRANT INSERT ON conditions TO PUBLIC;
-\z conditions
-                                     Access privileges
- Schema |    Name    | Type  |       Access privileges       | Column privileges | Policies 
---------+------------+-------+-------------------------------+-------------------+----------
- public | conditions | table | super_user=arwdDxt/super_user+|                   | 
-        |            |       | =ar/super_user                |                   | 
-(1 row)
-
-\z _timescaledb_internal._hyper_1_2_chunk
-                                                Access privileges
-        Schema         |       Name       | Type  |       Access privileges       | Column privileges | Policies 
------------------------+------------------+-------+-------------------------------+-------------------+----------
- _timescaledb_internal | _hyper_1_2_chunk | table | super_user=arwdDxt/super_user+|                   | 
-                       |                  |       | =ar/super_user                |                   | 
-(1 row)
-
-REVOKE SELECT ON _timescaledb_internal._hyper_1_2_chunk FROM PUBLIC;
-REVOKE INSERT ON conditions FROM PUBLIC;
-\z conditions
-                                     Access privileges
- Schema |    Name    | Type  |       Access privileges       | Column privileges | Policies 
---------+------------+-------+-------------------------------+-------------------+----------
- public | conditions | table | super_user=arwdDxt/super_user+|                   | 
-        |            |       | =r/super_user                 |                   | 
-(1 row)
-
-\z _timescaledb_internal._hyper_1_2_chunk
-                                                Access privileges
-        Schema         |       Name       | Type  |       Access privileges       | Column privileges | Policies 
------------------------+------------------+-------+-------------------------------+-------------------+----------
- _timescaledb_internal | _hyper_1_2_chunk | table | super_user=arwdDxt/super_user |                   | 
-(1 row)
-
--- Check that granting permissions through hypertable does not remove
--- separate grants on chunk.
-GRANT UPDATE ON _timescaledb_internal._hyper_1_3_chunk TO PUBLIC;
-\z conditions
-                                     Access privileges
- Schema |    Name    | Type  |       Access privileges       | Column privileges | Policies 
---------+------------+-------+-------------------------------+-------------------+----------
- public | conditions | table | super_user=arwdDxt/super_user+|                   | 
-        |            |       | =r/super_user                 |                   | 
-(1 row)
-
-\z _timescaledb_internal._hyper_1_3_chunk
-                                                Access privileges
-        Schema         |       Name       | Type  |       Access privileges       | Column privileges | Policies 
------------------------+------------------+-------+-------------------------------+-------------------+----------
- _timescaledb_internal | _hyper_1_3_chunk | table | super_user=arwdDxt/super_user+|                   | 
-                       |                  |       | =rw/super_user                |                   | 
-(1 row)
-
-GRANT INSERT ON conditions TO PUBLIC;
-REVOKE INSERT ON conditions FROM PUBLIC;
-\z conditions
-                                     Access privileges
- Schema |    Name    | Type  |       Access privileges       | Column privileges | Policies 
---------+------------+-------+-------------------------------+-------------------+----------
- public | conditions | table | super_user=arwdDxt/super_user+|                   | 
-        |            |       | =r/super_user                 |                   | 
-(1 row)
-
-\z _timescaledb_internal._hyper_1_3_chunk
-                                                Access privileges
-        Schema         |       Name       | Type  |       Access privileges       | Column privileges | Policies 
------------------------+------------------+-------+-------------------------------+-------------------+----------
- _timescaledb_internal | _hyper_1_3_chunk | table | super_user=arwdDxt/super_user+|                   | 
-                       |                  |       | =rw/super_user                |                   | 
-(1 row)
-
--- Check that GRANT ALL IN SCHEMA adds privileges to the parent
--- and also goes to chunks in another schema
-GRANT ALL ON ALL TABLES  IN SCHEMA public TO :ROLE_DEFAULT_PERM_USER_2;
-\z conditions
-                                          Access privileges
- Schema |    Name    | Type  |           Access privileges            | Column privileges | Policies 
---------+------------+-------+----------------------------------------+-------------------+----------
- public | conditions | table | super_user=arwdDxt/super_user         +|                   | 
-        |            |       | =r/super_user                         +|                   | 
-        |            |       | default_perm_user_2=arwdDxt/super_user |                   | 
-(1 row)
-
-\z _timescaledb_internal.*chunk
-                                                    Access privileges
-        Schema         |       Name       | Type  |           Access privileges            | Column privileges | Policies 
------------------------+------------------+-------+----------------------------------------+-------------------+----------
- _timescaledb_internal | _hyper_1_1_chunk | table | super_user=arwdDxt/super_user         +|                   | 
-                       |                  |       | =w/super_user                         +|                   | 
-                       |                  |       | default_perm_user_2=arwdDxt/super_user |                   | 
- _timescaledb_internal | _hyper_1_2_chunk | table | super_user=arwdDxt/super_user         +|                   | 
-                       |                  |       | default_perm_user_2=arwdDxt/super_user |                   | 
- _timescaledb_internal | _hyper_1_3_chunk | table | super_user=arwdDxt/super_user         +|                   | 
-                       |                  |       | =rw/super_user                        +|                   | 
-                       |                  |       | default_perm_user_2=arwdDxt/super_user |                   | 
- _timescaledb_internal | _hyper_1_4_chunk | table | super_user=arwdDxt/super_user         +|                   | 
-                       |                  |       | =r/super_user                         +|                   | 
-                       |                  |       | default_perm_user_2=arwdDxt/super_user |                   | 
- _timescaledb_internal | _hyper_1_5_chunk | table | super_user=arwdDxt/super_user         +|                   | 
-                       |                  |       | =r/super_user                         +|                   | 
-                       |                  |       | default_perm_user_2=arwdDxt/super_user |                   | 
- _timescaledb_internal | _hyper_1_6_chunk | table | super_user=arwdDxt/super_user         +|                   | 
-                       |                  |       | =r/super_user                         +|                   | 
-                       |                  |       | default_perm_user_2=arwdDxt/super_user |                   | 
- _timescaledb_internal | _hyper_1_7_chunk | table | super_user=arwdDxt/super_user         +|                   | 
-                       |                  |       | =r/super_user                         +|                   | 
-                       |                  |       | default_perm_user_2=arwdDxt/super_user |                   | 
-(7 rows)
-
--- Check that REVOKE ALL IN SCHEMA removes privileges of the parent
--- and also goes to chunks in another schema
-REVOKE ALL ON ALL TABLES  IN SCHEMA public FROM :ROLE_DEFAULT_PERM_USER_2;
-\z conditions
-                                     Access privileges
- Schema |    Name    | Type  |       Access privileges       | Column privileges | Policies 
---------+------------+-------+-------------------------------+-------------------+----------
- public | conditions | table | super_user=arwdDxt/super_user+|                   | 
-        |            |       | =r/super_user                 |                   | 
-(1 row)
-
-\z _timescaledb_internal.*chunk
-                                                Access privileges
-        Schema         |       Name       | Type  |       Access privileges       | Column privileges | Policies 
------------------------+------------------+-------+-------------------------------+-------------------+----------
- _timescaledb_internal | _hyper_1_1_chunk | table | super_user=arwdDxt/super_user+|                   | 
-                       |                  |       | =w/super_user                 |                   | 
- _timescaledb_internal | _hyper_1_2_chunk | table | super_user=arwdDxt/super_user |                   | 
- _timescaledb_internal | _hyper_1_3_chunk | table | super_user=arwdDxt/super_user+|                   | 
-                       |                  |       | =rw/super_user                |                   | 
- _timescaledb_internal | _hyper_1_4_chunk | table | super_user=arwdDxt/super_user+|                   | 
-                       |                  |       | =r/super_user                 |                   | 
- _timescaledb_internal | _hyper_1_5_chunk | table | super_user=arwdDxt/super_user+|                   | 
-                       |                  |       | =r/super_user                 |                   | 
- _timescaledb_internal | _hyper_1_6_chunk | table | super_user=arwdDxt/super_user+|                   | 
-                       |                  |       | =r/super_user                 |                   | 
- _timescaledb_internal | _hyper_1_7_chunk | table | super_user=arwdDxt/super_user+|                   | 
-                       |                  |       | =r/super_user                 |                   | 
-(7 rows)
-
--- Create chunks in the same schema as the hypertable and check that
--- they also get the same privileges as the hypertable
-CREATE TABLE measurements(
-    time TIMESTAMPTZ NOT NULL,
-    device INTEGER,
-    temperature FLOAT
-);
--- Create a hypertable with chunks in the same schema
-SELECT * FROM create_hypertable('public.measurements', 'time', chunk_time_interval => '5 days'::interval, associated_schema_name => 'public');
- hypertable_id | schema_name |  table_name  | created 
----------------+-------------+--------------+---------
-             2 | public      | measurements | t
-(1 row)
-
-INSERT INTO measurements
-SELECT time, (random()*30)::int, random()*80 - 40
-FROM generate_series('2018-12-01 00:00'::timestamp, '2018-12-10 00:00'::timestamp, '1h') AS time;
--- GRANT ALL and check privileges
-GRANT ALL ON ALL TABLES  IN SCHEMA public TO :ROLE_DEFAULT_PERM_USER_2;
-\z measurements
-                                           Access privileges
- Schema |     Name     | Type  |           Access privileges            | Column privileges | Policies 
---------+--------------+-------+----------------------------------------+-------------------+----------
- public | measurements | table | super_user=arwdDxt/super_user         +|                   | 
-        |              |       | default_perm_user_2=arwdDxt/super_user |                   | 
-(1 row)
-
-\z conditions
-                                          Access privileges
- Schema |    Name    | Type  |           Access privileges            | Column privileges | Policies 
---------+------------+-------+----------------------------------------+-------------------+----------
- public | conditions | table | super_user=arwdDxt/super_user         +|                   | 
-        |            |       | =r/super_user                         +|                   | 
-        |            |       | default_perm_user_2=arwdDxt/super_user |                   | 
-(1 row)
-
-\z public.*chunk
-                                             Access privileges
- Schema |       Name        | Type  |           Access privileges            | Column privileges | Policies 
---------+-------------------+-------+----------------------------------------+-------------------+----------
- public | _hyper_2_10_chunk | table | super_user=arwdDxt/super_user         +|                   | 
-        |                   |       | default_perm_user_2=arwdDxt/super_user |                   | 
- public | _hyper_2_8_chunk  | table | super_user=arwdDxt/super_user         +|                   | 
-        |                   |       | default_perm_user_2=arwdDxt/super_user |                   | 
- public | _hyper_2_9_chunk  | table | super_user=arwdDxt/super_user         +|                   | 
-        |                   |       | default_perm_user_2=arwdDxt/super_user |                   | 
-(3 rows)
-
--- REVOKE ALL and check privileges
-REVOKE ALL ON ALL TABLES  IN SCHEMA public FROM :ROLE_DEFAULT_PERM_USER_2;
-\z measurements
-                                      Access privileges
- Schema |     Name     | Type  |       Access privileges       | Column privileges | Policies 
---------+--------------+-------+-------------------------------+-------------------+----------
- public | measurements | table | super_user=arwdDxt/super_user |                   | 
-(1 row)
-
-\z conditions
-                                     Access privileges
- Schema |    Name    | Type  |       Access privileges       | Column privileges | Policies 
---------+------------+-------+-------------------------------+-------------------+----------
- public | conditions | table | super_user=arwdDxt/super_user+|                   | 
-        |            |       | =r/super_user                 |                   | 
-(1 row)
-
-\z public.*chunk
-                                         Access privileges
- Schema |       Name        | Type  |       Access privileges       | Column privileges | Policies 
---------+-------------------+-------+-------------------------------+-------------------+----------
- public | _hyper_2_10_chunk | table | super_user=arwdDxt/super_user |                   | 
- public | _hyper_2_8_chunk  | table | super_user=arwdDxt/super_user |                   | 
- public | _hyper_2_9_chunk  | table | super_user=arwdDxt/super_user |                   | 
-(3 rows)
-
--- GRANT/REVOKE in an empty schema (Issue #4581)
-CREATE SCHEMA test_grant;
-GRANT ALL ON ALL TABLES IN SCHEMA test_grant TO :ROLE_DEFAULT_PERM_USER_2;
-REVOKE ALL ON ALL TABLES IN SCHEMA test_grant FROM :ROLE_DEFAULT_PERM_USER_2;
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/hash.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/hash.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/hash.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/hash.out	2023-11-25 05:27:38.957037758 +0000
@@ -1,323 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
--- Test hashing Const values. We should expect the same hash value for
--- all integer types when values are compatible
-SELECT _timescaledb_internal.get_partition_hash(1::int);
- get_partition_hash 
---------------------
-          242423622
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_hash(1::bigint);
- get_partition_hash 
---------------------
-          242423622
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_hash(1::smallint);
- get_partition_hash 
---------------------
-          242423622
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_hash(true);
- get_partition_hash 
---------------------
-          242423622
-(1 row)
-
--- Floating point types should also hash the same for compatible values
-SELECT _timescaledb_internal.get_partition_hash(1.0::real);
- get_partition_hash 
---------------------
-          376496956
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_hash(1.0::double precision);
- get_partition_hash 
---------------------
-          376496956
-(1 row)
-
--- Float aliases
-SELECT _timescaledb_internal.get_partition_hash(1.0::float);
- get_partition_hash 
---------------------
-          376496956
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_hash(1.0::float4);
- get_partition_hash 
---------------------
-          376496956
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_hash(1.0::float8);
- get_partition_hash 
---------------------
-          376496956
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_hash(1.0::numeric);
- get_partition_hash 
---------------------
-         1324868424
-(1 row)
-
--- 'name' and '"char"' are internal PostgreSQL types, which are not
--- intended for use by the general user. They are included here only
--- for completeness
--- https://www.postgresql.org/docs/10/static/datatype-character.html#datatype-character-special-table
-SELECT _timescaledb_internal.get_partition_hash('c'::name);
- get_partition_hash 
---------------------
-         1903644986
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_hash('c'::"char");
- get_partition_hash 
---------------------
-          203891234
-(1 row)
-
--- String and character hashes should also have the same output for
--- compatible values
-SELECT _timescaledb_internal.get_partition_hash('c'::char);
- get_partition_hash 
---------------------
-         1903644986
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_hash('c'::varchar(2));
- get_partition_hash 
---------------------
-         1903644986
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_hash('c'::text);
- get_partition_hash 
---------------------
-         1903644986
-(1 row)
-
--- 'c' is 0x63 in ASCII
-SELECT _timescaledb_internal.get_partition_hash(E'\\x63'::bytea);
- get_partition_hash 
---------------------
-         1903644986
-(1 row)
-
--- Time and date types
-SELECT _timescaledb_internal.get_partition_hash(interval '1 day');
- get_partition_hash 
---------------------
-           93502988
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_hash('2017-03-22T09:18:23'::timestamp);
- get_partition_hash 
---------------------
-          307315039
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_hash('2017-03-22T09:18:23'::timestamptz);
- get_partition_hash 
---------------------
-         1195163597
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_hash('2017-03-22'::date);
- get_partition_hash 
---------------------
-          693590295
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_hash('10:00:00'::time);
- get_partition_hash 
---------------------
-         1380652790
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_hash('10:00:00-1'::timetz);
- get_partition_hash 
---------------------
-          769387140
-(1 row)
-
--- Other types
-SELECT _timescaledb_internal.get_partition_hash(ARRAY[1,2,3]);
- get_partition_hash 
---------------------
-         1822090118
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_hash('08002b:010203'::macaddr);
- get_partition_hash 
---------------------
-          294987870
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_hash('192.168.100.128/25'::cidr);
- get_partition_hash 
---------------------
-         1612896565
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_hash('192.168.100.128'::inet);
- get_partition_hash 
---------------------
-         1952516432
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_hash('2001:4f8:3:ba:2e0:81ff:fe22:d1f1'::inet);
- get_partition_hash 
---------------------
-          933321588
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_hash('2001:4f8:3:ba:2e0:81ff:fe22:d1f1/128'::cidr);
- get_partition_hash 
---------------------
-          933321588
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_hash('{ "foo": "bar" }'::jsonb);
- get_partition_hash 
---------------------
-          208840587
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_hash('4b6a5eec-b344-11e7-abc4-cec278b6b50a'::uuid);
- get_partition_hash 
---------------------
-          504202548
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_hash(1::regclass);
- get_partition_hash 
---------------------
-          242423622
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_hash(int4range(10, 20));
- get_partition_hash 
---------------------
-         1202375768
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_hash(int8range(10, 20));
- get_partition_hash 
---------------------
-         1202375768
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_hash(numrange(10, 20));
- get_partition_hash 
---------------------
-         1083987536
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_hash(tsrange('2017-03-22T09:18:23', '2017-03-23T09:18:23'));
- get_partition_hash 
---------------------
-         2079608838
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_hash(tstzrange('2017-03-22T09:18:23+01', '2017-03-23T09:18:23+00'));
- get_partition_hash 
---------------------
-         1255083771
-(1 row)
-
--- Test hashing Var values
-CREATE TABLE hash_test(id int, value text);
-INSERT INTO hash_test VALUES (1, 'test');
--- Test Vars
-SELECT _timescaledb_internal.get_partition_hash(id) FROM hash_test;
- get_partition_hash 
---------------------
-          242423622
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_hash(value) FROM hash_test;
- get_partition_hash 
---------------------
-         1771415073
-(1 row)
-
--- Test coerced value
-SELECT _timescaledb_internal.get_partition_hash(id::text) FROM hash_test;
- get_partition_hash 
---------------------
-         1516350201
-(1 row)
-
--- Test legacy function that converts values to text first
-SELECT _timescaledb_internal.get_partition_for_key('4b6a5eec-b344-11e7-abc4-cec278b6b50a'::text);
- get_partition_for_key 
------------------------
-             934882099
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_for_key('4b6a5eec-b344-11e7-abc4-cec278b6b50a'::varchar);
- get_partition_for_key 
------------------------
-             934882099
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_for_key(187);
- get_partition_for_key 
------------------------
-            1161071810
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_for_key(187::bigint);
- get_partition_for_key 
------------------------
-            1161071810
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_for_key(187::numeric);
- get_partition_for_key 
------------------------
-            1161071810
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_for_key(187::double precision);
- get_partition_for_key 
------------------------
-            1161071810
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_for_key(int4range(10, 20));
- get_partition_for_key 
------------------------
-             505239042
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_hash('08002b:010203'::macaddr);
- get_partition_hash 
---------------------
-          294987870
-(1 row)
-
--- Test inside IMMUTABLE function (Issue #4575)
-CREATE FUNCTION my_get_partition_hash(INTEGER) RETURNS INTEGER
-AS 'SELECT _timescaledb_internal.get_partition_hash($1);'
-LANGUAGE SQL IMMUTABLE;
-CREATE FUNCTION my_get_partition_for_key(INTEGER) RETURNS INTEGER
-AS 'SELECT _timescaledb_internal.get_partition_for_key($1);'
-LANGUAGE SQL IMMUTABLE;
-SELECT my_get_partition_hash(1);
- my_get_partition_hash 
------------------------
-             242423622
-(1 row)
-
-SELECT my_get_partition_for_key(1);
- my_get_partition_for_key 
---------------------------
-               1516350201
-(1 row)
-
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/histogram_test.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/histogram_test.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/histogram_test.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/histogram_test.out	2023-11-25 05:27:38.957037758 +0000
@@ -1,127 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
--- table 1
-CREATE TABLE "hitest1"(key real, val varchar(40));
--- insertions
-INSERT INTO "hitest1" VALUES(0, 'hi');
-INSERT INTO "hitest1" VALUES(1, 'sup');
-INSERT INTO "hitest1" VALUES(2, 'hello');
-INSERT INTO "hitest1" VALUES(3, 'yo');
-INSERT INTO "hitest1" VALUES(4, 'howdy');
-INSERT INTO "hitest1" VALUES(5, 'hola');
-INSERT INTO "hitest1" VALUES(6, 'ya');
-INSERT INTO "hitest1" VALUES(1, 'sup');
-INSERT INTO "hitest1" VALUES(2, 'hello');
-INSERT INTO "hitest1" VALUES(1, 'sup');
--- table 2
-CREATE TABLE "hitest2"(name varchar(30), score integer, qualify boolean);
--- insertions
-INSERT INTO "hitest2" VALUES('Tom', 6, TRUE);
-INSERT INTO "hitest2" VALUES('Mary', 4, FALSE);
-INSERT INTO "hitest2" VALUES('Jaq', 3, FALSE);
-INSERT INTO "hitest2" VALUES('Jane', 10, TRUE);
--- standard 2 bucket
-SELECT histogram(key, 0, 9, 2) FROM hitest1;
- histogram 
------------
- {0,8,2,0}
-(1 row)
-
--- standard multi-bucket
-SELECT histogram(key, 0, 9, 5) FROM hitest1;
-    histogram    
------------------
- {0,4,3,2,1,0,0}
-(1 row)
-
--- standard 3 bucket
-SELECT val, histogram(key, 0, 7, 3) FROM hitest1 GROUP BY val ORDER BY val;
-  val  |  histogram  
--------+-------------
- hello | {0,2,0,0,0}
- hi    | {0,1,0,0,0}
- hola  | {0,0,0,1,0}
- howdy | {0,0,1,0,0}
- sup   | {0,3,0,0,0}
- ya    | {0,0,0,1,0}
- yo    | {0,0,1,0,0}
-(7 rows)
-
--- standard element beneath lb
-SELECT histogram(key, 1, 7, 3) FROM hitest1;
-  histogram  
--------------
- {1,5,2,2,0}
-(1 row)
-
--- standard element above ub
-SELECT histogram(key, 0, 3, 3) FROM hitest1;
-  histogram  
--------------
- {0,1,3,2,4}
-(1 row)
-
--- standard element beneath and above lb and ub, respectively
-SELECT histogram(key, 1, 3, 2) FROM hitest1;
- histogram 
------------
- {1,3,2,4}
-(1 row)
-
--- standard 1 bucket
-SELECT histogram(key, 1, 3, 1) FROM hitest1;
- histogram 
------------
- {1,5,4}
-(1 row)
-
--- standard 2 bucket
-SELECT qualify, histogram(score, 0, 10, 2) FROM hitest2 GROUP BY qualify ORDER BY qualify;
- qualify | histogram 
----------+-----------
- f       | {0,2,0,0}
- t       | {0,0,1,1}
-(2 rows)
-
--- standard multi-bucket
-SELECT qualify, histogram(score, 0, 10, 5) FROM hitest2 GROUP BY qualify ORDER BY qualify;
- qualify |    histogram    
----------+-----------------
- f       | {0,0,1,1,0,0,0}
- t       | {0,0,0,0,1,0,1}
-(2 rows)
-
--- check number of buckets is constant
-\set ON_ERROR_STOP 0
-select histogram(i,10,90,case when i=1 then 1 else 1000000 end) FROM generate_series(1,100) i;
-ERROR:  number of buckets must not change between calls
-\set ON_ERROR_STOP 1
-CREATE TABLE weather (
-       time TIMESTAMPTZ NOT NULL,
-       city TEXT,
-       temperature FLOAT,
-       PRIMARY KEY(time, city)
-);
--- There is a bug in width_bucket() causing a NaN as a result, so we
--- check that it is not causing a crash in histogram().
-SELECT * FROM create_hypertable('weather', 'time', 'city', 3);
- hypertable_id | schema_name | table_name | created 
----------------+-------------+------------+---------
-             1 | public      | weather    | t
-(1 row)
-
-INSERT INTO weather VALUES
-       ('2023-02-10 09:16:51.133584+00','city1',10.4),
-       ('2023-02-10 11:16:51.611618+00','city1',10.3),
-       ('2023-02-10 06:58:59.999999+00','city1',10.3),
-       ('2023-02-10 01:58:59.999999+00','city1',10.3),
-       ('2023-02-09 01:58:59.999999+00','city1',10.3),
-       ('2023-02-10 08:58:59.999999+00','city1',10.3),
-       ('2023-03-23 06:12:02.73765+00 ','city1', 9.7),
-       ('2023-03-23 06:12:06.990998+00','city1',11.7);
--- This will currently generate an error.
-\set ON_ERROR_STOP 0
-SELECT histogram(temperature, -1.79769e+308, 1.79769e+308,10) FROM weather GROUP BY city;
-ERROR:  index -2147483648 from "width_bucket" out of range
-\set ON_ERROR_STOP 1
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/information_views.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/information_views.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/information_views.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/information_views.out	2023-11-25 05:27:38.961037746 +0000
@@ -1,268 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-SELECT * FROM timescaledb_information.hypertables;
- hypertable_schema | hypertable_name | owner | num_dimensions | num_chunks | compression_enabled | is_distributed | replication_factor | data_nodes | tablespaces 
--------------------+-----------------+-------+----------------+------------+---------------------+----------------+--------------------+------------+-------------
-(0 rows)
-
--- create simple hypertable with 1 chunk
-CREATE TABLE ht1(time TIMESTAMPTZ NOT NULL);
-SELECT create_hypertable('ht1','time');
- create_hypertable 
--------------------
- (1,public,ht1,t)
-(1 row)
-
-INSERT INTO ht1 SELECT '2000-01-01'::TIMESTAMPTZ;
--- create simple hypertable with 1 chunk and toasted data
-CREATE TABLE ht2(time TIMESTAMPTZ NOT NULL, data TEXT);
-SELECT create_hypertable('ht2','time');
- create_hypertable 
--------------------
- (2,public,ht2,t)
-(1 row)
-
-INSERT INTO ht2 SELECT '2000-01-01'::TIMESTAMPTZ, repeat('8k',4096);
-SELECT * FROM timescaledb_information.hypertables
-ORDER BY hypertable_schema, hypertable_name;
- hypertable_schema | hypertable_name |       owner       | num_dimensions | num_chunks | compression_enabled | is_distributed | replication_factor | data_nodes | tablespaces 
--------------------+-----------------+-------------------+----------------+------------+---------------------+----------------+--------------------+------------+-------------
- public            | ht1             | default_perm_user |              1 |          1 | f                   | f              |                    |            | 
- public            | ht2             | default_perm_user |              1 |          1 | f                   | f              |                    |            | 
-(2 rows)
-
-\c :TEST_DBNAME :ROLE_SUPERUSER
--- create schema open and hypertable with 3 chunks
-CREATE SCHEMA open;
-GRANT USAGE ON SCHEMA open TO :ROLE_DEFAULT_PERM_USER;
-CREATE TABLE open.open_ht(time TIMESTAMPTZ NOT NULL);
-SELECT create_hypertable('open.open_ht','time');
- create_hypertable  
---------------------
- (3,open,open_ht,t)
-(1 row)
-
-INSERT INTO open.open_ht SELECT '2000-01-01'::TIMESTAMPTZ;
-INSERT INTO open.open_ht SELECT '2001-01-01'::TIMESTAMPTZ;
-INSERT INTO open.open_ht SELECT '2002-01-01'::TIMESTAMPTZ;
--- create schema closed and hypertable
-CREATE SCHEMA closed;
-CREATE TABLE closed.closed_ht(time TIMESTAMPTZ NOT NULL);
-SELECT create_hypertable('closed.closed_ht','time');
-   create_hypertable    
-------------------------
- (4,closed,closed_ht,t)
-(1 row)
-
-INSERT INTO closed.closed_ht SELECT '2000-01-01'::TIMESTAMPTZ;
-SELECT * FROM timescaledb_information.hypertables
-ORDER BY hypertable_schema, hypertable_name;
- hypertable_schema | hypertable_name |       owner       | num_dimensions | num_chunks | compression_enabled | is_distributed | replication_factor | data_nodes | tablespaces 
--------------------+-----------------+-------------------+----------------+------------+---------------------+----------------+--------------------+------------+-------------
- closed            | closed_ht       | super_user        |              1 |          1 | f                   | f              |                    |            | 
- open              | open_ht         | super_user        |              1 |          3 | f                   | f              |                    |            | 
- public            | ht1             | default_perm_user |              1 |          1 | f                   | f              |                    |            | 
- public            | ht2             | default_perm_user |              1 |          1 | f                   | f              |                    |            | 
-(4 rows)
-
-\c :TEST_DBNAME :ROLE_DEFAULT_PERM_USER
-\set ON_ERROR_STOP 0
-\x
-SELECT * FROM timescaledb_information.hypertables
-ORDER BY hypertable_schema, hypertable_name;
--[ RECORD 1 ]-------+------------------
-hypertable_schema   | closed
-hypertable_name     | closed_ht
-owner               | super_user
-num_dimensions      | 1
-num_chunks          | 1
-compression_enabled | f
-is_distributed      | f
-replication_factor  | 
-data_nodes          | 
-tablespaces         | 
--[ RECORD 2 ]-------+------------------
-hypertable_schema   | open
-hypertable_name     | open_ht
-owner               | super_user
-num_dimensions      | 1
-num_chunks          | 3
-compression_enabled | f
-is_distributed      | f
-replication_factor  | 
-data_nodes          | 
-tablespaces         | 
--[ RECORD 3 ]-------+------------------
-hypertable_schema   | public
-hypertable_name     | ht1
-owner               | default_perm_user
-num_dimensions      | 1
-num_chunks          | 1
-compression_enabled | f
-is_distributed      | f
-replication_factor  | 
-data_nodes          | 
-tablespaces         | 
--[ RECORD 4 ]-------+------------------
-hypertable_schema   | public
-hypertable_name     | ht2
-owner               | default_perm_user
-num_dimensions      | 1
-num_chunks          | 1
-compression_enabled | f
-is_distributed      | f
-replication_factor  | 
-data_nodes          | 
-tablespaces         | 
-
--- filter by schema
-SELECT * FROM timescaledb_information.hypertables
-WHERE hypertable_schema = 'closed'
-ORDER BY hypertable_schema, hypertable_name;
--[ RECORD 1 ]-------+-----------
-hypertable_schema   | closed
-hypertable_name     | closed_ht
-owner               | super_user
-num_dimensions      | 1
-num_chunks          | 1
-compression_enabled | f
-is_distributed      | f
-replication_factor  | 
-data_nodes          | 
-tablespaces         | 
-
--- filter by table name
-SELECT * FROM timescaledb_information.hypertables
-WHERE hypertable_name = 'ht1'
-ORDER BY hypertable_schema, hypertable_name;
--[ RECORD 1 ]-------+------------------
-hypertable_schema   | public
-hypertable_name     | ht1
-owner               | default_perm_user
-num_dimensions      | 1
-num_chunks          | 1
-compression_enabled | f
-is_distributed      | f
-replication_factor  | 
-data_nodes          | 
-tablespaces         | 
-
--- filter by owner
-SELECT * FROM timescaledb_information.hypertables
-WHERE owner = 'super_user'
-ORDER BY hypertable_schema, hypertable_name;
--[ RECORD 1 ]-------+-----------
-hypertable_schema   | closed
-hypertable_name     | closed_ht
-owner               | super_user
-num_dimensions      | 1
-num_chunks          | 1
-compression_enabled | f
-is_distributed      | f
-replication_factor  | 
-data_nodes          | 
-tablespaces         | 
--[ RECORD 2 ]-------+-----------
-hypertable_schema   | open
-hypertable_name     | open_ht
-owner               | super_user
-num_dimensions      | 1
-num_chunks          | 3
-compression_enabled | f
-is_distributed      | f
-replication_factor  | 
-data_nodes          | 
-tablespaces         | 
-
-\x
----Add integer table --
-CREATE TABLE test_table_int(time bigint, junk int);
-SELECT create_hypertable('test_table_int', 'time', chunk_time_interval => 10);
-NOTICE:  adding not-null constraint to column "time"
-      create_hypertable      
------------------------------
- (5,public,test_table_int,t)
-(1 row)
-
-CREATE OR REPLACE function table_int_now() returns BIGINT LANGUAGE SQL IMMUTABLE as  'SELECT 1::BIGINT';
-SELECT set_integer_now_func('test_table_int', 'table_int_now');
- set_integer_now_func 
-----------------------
- 
-(1 row)
-
-INSERT into test_table_int SELECT generate_series( 1, 20), 100;
- SELECT * FROM timescaledb_information.chunks WHERE hypertable_name = 'ht1' ORDER BY chunk_name;
- hypertable_schema | hypertable_name |     chunk_schema      |    chunk_name    | primary_dimension |  primary_dimension_type  |         range_start          |          range_end           | range_start_integer | range_end_integer | is_compressed | chunk_tablespace | data_nodes 
--------------------+-----------------+-----------------------+------------------+-------------------+--------------------------+------------------------------+------------------------------+---------------------+-------------------+---------------+------------------+------------
- public            | ht1             | _timescaledb_internal | _hyper_1_1_chunk | time              | timestamp with time zone | Wed Dec 29 16:00:00 1999 PST | Wed Jan 05 16:00:00 2000 PST |                     |                   | f             |                  | 
-(1 row)
-
- SELECT * FROM timescaledb_information.chunks WHERE hypertable_name = 'test_table_int' ORDER BY chunk_name;
- hypertable_schema | hypertable_name |     chunk_schema      |    chunk_name    | primary_dimension | primary_dimension_type | range_start | range_end | range_start_integer | range_end_integer | is_compressed | chunk_tablespace | data_nodes 
--------------------+-----------------+-----------------------+------------------+-------------------+------------------------+-------------+-----------+---------------------+-------------------+---------------+------------------+------------
- public            | test_table_int  | _timescaledb_internal | _hyper_5_7_chunk | time              | bigint                 |             |           |                   0 |                10 | f             |                  | 
- public            | test_table_int  | _timescaledb_internal | _hyper_5_8_chunk | time              | bigint                 |             |           |                  10 |                20 | f             |                  | 
- public            | test_table_int  | _timescaledb_internal | _hyper_5_9_chunk | time              | bigint                 |             |           |                  20 |                30 | f             |                  | 
-(3 rows)
-
-\x
-SELECT * FROM timescaledb_information.dimensions ORDER BY hypertable_name, dimension_number;
--[ RECORD 1 ]-----+-------------------------
-hypertable_schema | closed
-hypertable_name   | closed_ht
-dimension_number  | 1
-column_name       | time
-column_type       | timestamp with time zone
-dimension_type    | Time
-time_interval     | @ 7 days
-integer_interval  | 
-integer_now_func  | 
-num_partitions    | 
--[ RECORD 2 ]-----+-------------------------
-hypertable_schema | public
-hypertable_name   | ht1
-dimension_number  | 1
-column_name       | time
-column_type       | timestamp with time zone
-dimension_type    | Time
-time_interval     | @ 7 days
-integer_interval  | 
-integer_now_func  | 
-num_partitions    | 
--[ RECORD 3 ]-----+-------------------------
-hypertable_schema | public
-hypertable_name   | ht2
-dimension_number  | 1
-column_name       | time
-column_type       | timestamp with time zone
-dimension_type    | Time
-time_interval     | @ 7 days
-integer_interval  | 
-integer_now_func  | 
-num_partitions    | 
--[ RECORD 4 ]-----+-------------------------
-hypertable_schema | open
-hypertable_name   | open_ht
-dimension_number  | 1
-column_name       | time
-column_type       | timestamp with time zone
-dimension_type    | Time
-time_interval     | @ 7 days
-integer_interval  | 
-integer_now_func  | 
-num_partitions    | 
--[ RECORD 5 ]-----+-------------------------
-hypertable_schema | public
-hypertable_name   | test_table_int
-dimension_number  | 1
-column_name       | time
-column_type       | bigint
-dimension_type    | Time
-time_interval     | 
-integer_interval  | 10
-integer_now_func  | table_int_now
-num_partitions    | 
-
-\x
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/insert-15.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/insert-15.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/insert-15.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/insert-15.out	2023-11-25 05:27:38.969037723 +0000
@@ -1,677 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-\ir include/insert_two_partitions.sql
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-CREATE TABLE PUBLIC."two_Partitions" (
-  "timeCustom" BIGINT NOT NULL,
-  device_id TEXT NOT NULL,
-  series_0 DOUBLE PRECISION NULL,
-  series_1 DOUBLE PRECISION NULL,
-  series_2 DOUBLE PRECISION NULL,
-  series_bool BOOLEAN NULL
-);
-CREATE INDEX ON PUBLIC."two_Partitions" (device_id, "timeCustom" DESC NULLS LAST) WHERE device_id IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, series_0) WHERE series_0 IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, series_1)  WHERE series_1 IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, series_2) WHERE series_2 IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, series_bool) WHERE series_bool IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, device_id);
-SELECT * FROM create_hypertable('"public"."two_Partitions"'::regclass, 'timeCustom'::name, 'device_id'::name, associated_schema_name=>'_timescaledb_internal'::text, number_partitions => 2, chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
- hypertable_id | schema_name |   table_name   | created 
----------------+-------------+----------------+---------
-             1 | public      | two_Partitions | t
-(1 row)
-
-\set QUIET off
-BEGIN;
-BEGIN
-\COPY public."two_Partitions" FROM 'data/ds1_dev1_1.tsv' NULL AS '';
-COPY 7
-COMMIT;
-COMMIT
-INSERT INTO public."two_Partitions"("timeCustom", device_id, series_0, series_1) VALUES
-(1257987600000000000, 'dev1', 1.5, 1),
-(1257987600000000000, 'dev1', 1.5, 2),
-(1257894000000000000, 'dev2', 1.5, 1),
-(1257894002000000000, 'dev1', 2.5, 3);
-INSERT 0 4
-INSERT INTO "two_Partitions"("timeCustom", device_id, series_0, series_1) VALUES
-(1257894000000000000, 'dev2', 1.5, 2);
-INSERT 0 1
-\set QUIET on
-SELECT * FROM test.show_columnsp('_timescaledb_internal.%_hyper%');
-                                      Relation                                      | Kind |   Column    |   Column type    | NotNull 
-------------------------------------------------------------------------------------+------+-------------+------------------+---------
- _timescaledb_internal._hyper_1_1_chunk                                             | r    | timeCustom  | bigint           | t
- _timescaledb_internal._hyper_1_1_chunk                                             | r    | device_id   | text             | t
- _timescaledb_internal._hyper_1_1_chunk                                             | r    | series_0    | double precision | f
- _timescaledb_internal._hyper_1_1_chunk                                             | r    | series_1    | double precision | f
- _timescaledb_internal._hyper_1_1_chunk                                             | r    | series_2    | double precision | f
- _timescaledb_internal._hyper_1_1_chunk                                             | r    | series_bool | boolean          | f
- _timescaledb_internal."_hyper_1_1_chunk_two_Partitions_device_id_timeCustom_idx"   | i    | device_id   | text             | f
- _timescaledb_internal."_hyper_1_1_chunk_two_Partitions_device_id_timeCustom_idx"   | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_1_chunk_two_Partitions_timeCustom_device_id_idx"   | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_1_chunk_two_Partitions_timeCustom_device_id_idx"   | i    | device_id   | text             | f
- _timescaledb_internal."_hyper_1_1_chunk_two_Partitions_timeCustom_idx"             | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_1_chunk_two_Partitions_timeCustom_series_0_idx"    | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_1_chunk_two_Partitions_timeCustom_series_0_idx"    | i    | series_0    | double precision | f
- _timescaledb_internal."_hyper_1_1_chunk_two_Partitions_timeCustom_series_1_idx"    | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_1_chunk_two_Partitions_timeCustom_series_1_idx"    | i    | series_1    | double precision | f
- _timescaledb_internal."_hyper_1_1_chunk_two_Partitions_timeCustom_series_2_idx"    | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_1_chunk_two_Partitions_timeCustom_series_2_idx"    | i    | series_2    | double precision | f
- _timescaledb_internal."_hyper_1_1_chunk_two_Partitions_timeCustom_series_bool_idx" | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_1_chunk_two_Partitions_timeCustom_series_bool_idx" | i    | series_bool | boolean          | f
- _timescaledb_internal._hyper_1_2_chunk                                             | r    | timeCustom  | bigint           | t
- _timescaledb_internal._hyper_1_2_chunk                                             | r    | device_id   | text             | t
- _timescaledb_internal._hyper_1_2_chunk                                             | r    | series_0    | double precision | f
- _timescaledb_internal._hyper_1_2_chunk                                             | r    | series_1    | double precision | f
- _timescaledb_internal._hyper_1_2_chunk                                             | r    | series_2    | double precision | f
- _timescaledb_internal._hyper_1_2_chunk                                             | r    | series_bool | boolean          | f
- _timescaledb_internal."_hyper_1_2_chunk_two_Partitions_device_id_timeCustom_idx"   | i    | device_id   | text             | f
- _timescaledb_internal."_hyper_1_2_chunk_two_Partitions_device_id_timeCustom_idx"   | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_2_chunk_two_Partitions_timeCustom_device_id_idx"   | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_2_chunk_two_Partitions_timeCustom_device_id_idx"   | i    | device_id   | text             | f
- _timescaledb_internal."_hyper_1_2_chunk_two_Partitions_timeCustom_idx"             | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_2_chunk_two_Partitions_timeCustom_series_0_idx"    | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_2_chunk_two_Partitions_timeCustom_series_0_idx"    | i    | series_0    | double precision | f
- _timescaledb_internal."_hyper_1_2_chunk_two_Partitions_timeCustom_series_1_idx"    | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_2_chunk_two_Partitions_timeCustom_series_1_idx"    | i    | series_1    | double precision | f
- _timescaledb_internal."_hyper_1_2_chunk_two_Partitions_timeCustom_series_2_idx"    | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_2_chunk_two_Partitions_timeCustom_series_2_idx"    | i    | series_2    | double precision | f
- _timescaledb_internal."_hyper_1_2_chunk_two_Partitions_timeCustom_series_bool_idx" | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_2_chunk_two_Partitions_timeCustom_series_bool_idx" | i    | series_bool | boolean          | f
- _timescaledb_internal._hyper_1_3_chunk                                             | r    | timeCustom  | bigint           | t
- _timescaledb_internal._hyper_1_3_chunk                                             | r    | device_id   | text             | t
- _timescaledb_internal._hyper_1_3_chunk                                             | r    | series_0    | double precision | f
- _timescaledb_internal._hyper_1_3_chunk                                             | r    | series_1    | double precision | f
- _timescaledb_internal._hyper_1_3_chunk                                             | r    | series_2    | double precision | f
- _timescaledb_internal._hyper_1_3_chunk                                             | r    | series_bool | boolean          | f
- _timescaledb_internal."_hyper_1_3_chunk_two_Partitions_device_id_timeCustom_idx"   | i    | device_id   | text             | f
- _timescaledb_internal."_hyper_1_3_chunk_two_Partitions_device_id_timeCustom_idx"   | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_3_chunk_two_Partitions_timeCustom_device_id_idx"   | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_3_chunk_two_Partitions_timeCustom_device_id_idx"   | i    | device_id   | text             | f
- _timescaledb_internal."_hyper_1_3_chunk_two_Partitions_timeCustom_idx"             | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_3_chunk_two_Partitions_timeCustom_series_0_idx"    | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_3_chunk_two_Partitions_timeCustom_series_0_idx"    | i    | series_0    | double precision | f
- _timescaledb_internal."_hyper_1_3_chunk_two_Partitions_timeCustom_series_1_idx"    | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_3_chunk_two_Partitions_timeCustom_series_1_idx"    | i    | series_1    | double precision | f
- _timescaledb_internal."_hyper_1_3_chunk_two_Partitions_timeCustom_series_2_idx"    | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_3_chunk_two_Partitions_timeCustom_series_2_idx"    | i    | series_2    | double precision | f
- _timescaledb_internal."_hyper_1_3_chunk_two_Partitions_timeCustom_series_bool_idx" | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_3_chunk_two_Partitions_timeCustom_series_bool_idx" | i    | series_bool | boolean          | f
- _timescaledb_internal._hyper_1_4_chunk                                             | r    | timeCustom  | bigint           | t
- _timescaledb_internal._hyper_1_4_chunk                                             | r    | device_id   | text             | t
- _timescaledb_internal._hyper_1_4_chunk                                             | r    | series_0    | double precision | f
- _timescaledb_internal._hyper_1_4_chunk                                             | r    | series_1    | double precision | f
- _timescaledb_internal._hyper_1_4_chunk                                             | r    | series_2    | double precision | f
- _timescaledb_internal._hyper_1_4_chunk                                             | r    | series_bool | boolean          | f
- _timescaledb_internal."_hyper_1_4_chunk_two_Partitions_device_id_timeCustom_idx"   | i    | device_id   | text             | f
- _timescaledb_internal."_hyper_1_4_chunk_two_Partitions_device_id_timeCustom_idx"   | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_4_chunk_two_Partitions_timeCustom_device_id_idx"   | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_4_chunk_two_Partitions_timeCustom_device_id_idx"   | i    | device_id   | text             | f
- _timescaledb_internal."_hyper_1_4_chunk_two_Partitions_timeCustom_idx"             | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_4_chunk_two_Partitions_timeCustom_series_0_idx"    | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_4_chunk_two_Partitions_timeCustom_series_0_idx"    | i    | series_0    | double precision | f
- _timescaledb_internal."_hyper_1_4_chunk_two_Partitions_timeCustom_series_1_idx"    | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_4_chunk_two_Partitions_timeCustom_series_1_idx"    | i    | series_1    | double precision | f
- _timescaledb_internal."_hyper_1_4_chunk_two_Partitions_timeCustom_series_2_idx"    | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_4_chunk_two_Partitions_timeCustom_series_2_idx"    | i    | series_2    | double precision | f
- _timescaledb_internal."_hyper_1_4_chunk_two_Partitions_timeCustom_series_bool_idx" | i    | timeCustom  | bigint           | f
- _timescaledb_internal."_hyper_1_4_chunk_two_Partitions_timeCustom_series_bool_idx" | i    | series_bool | boolean          | f
-(76 rows)
-
-SELECT * FROM test.show_indexesp('_timescaledb_internal._hyper%');
-                 Table                  |                                       Index                                        |         Columns          | Expr | Unique | Primary | Exclusion | Tablespace 
-----------------------------------------+------------------------------------------------------------------------------------+--------------------------+------+--------+---------+-----------+------------
- _timescaledb_internal._hyper_1_1_chunk | _timescaledb_internal."_hyper_1_1_chunk_two_Partitions_device_id_timeCustom_idx"   | {device_id,timeCustom}   |      | f      | f       | f         | 
- _timescaledb_internal._hyper_1_1_chunk | _timescaledb_internal."_hyper_1_1_chunk_two_Partitions_timeCustom_series_0_idx"    | {timeCustom,series_0}    |      | f      | f       | f         | 
- _timescaledb_internal._hyper_1_1_chunk | _timescaledb_internal."_hyper_1_1_chunk_two_Partitions_timeCustom_series_1_idx"    | {timeCustom,series_1}    |      | f      | f       | f         | 
- _timescaledb_internal._hyper_1_1_chunk | _timescaledb_internal."_hyper_1_1_chunk_two_Partitions_timeCustom_series_2_idx"    | {timeCustom,series_2}    |      | f      | f       | f         | 
- _timescaledb_internal._hyper_1_1_chunk | _timescaledb_internal."_hyper_1_1_chunk_two_Partitions_timeCustom_series_bool_idx" | {timeCustom,series_bool} |      | f      | f       | f         | 
- _timescaledb_internal._hyper_1_1_chunk | _timescaledb_internal."_hyper_1_1_chunk_two_Partitions_timeCustom_device_id_idx"   | {timeCustom,device_id}   |      | f      | f       | f         | 
- _timescaledb_internal._hyper_1_1_chunk | _timescaledb_internal."_hyper_1_1_chunk_two_Partitions_timeCustom_idx"             | {timeCustom}             |      | f      | f       | f         | 
- _timescaledb_internal._hyper_1_2_chunk | _timescaledb_internal."_hyper_1_2_chunk_two_Partitions_device_id_timeCustom_idx"   | {device_id,timeCustom}   |      | f      | f       | f         | 
- _timescaledb_internal._hyper_1_2_chunk | _timescaledb_internal."_hyper_1_2_chunk_two_Partitions_timeCustom_series_0_idx"    | {timeCustom,series_0}    |      | f      | f       | f         | 
- _timescaledb_internal._hyper_1_2_chunk | _timescaledb_internal."_hyper_1_2_chunk_two_Partitions_timeCustom_series_1_idx"    | {timeCustom,series_1}    |      | f      | f       | f         | 
- _timescaledb_internal._hyper_1_2_chunk | _timescaledb_internal."_hyper_1_2_chunk_two_Partitions_timeCustom_series_2_idx"    | {timeCustom,series_2}    |      | f      | f       | f         | 
- _timescaledb_internal._hyper_1_2_chunk | _timescaledb_internal."_hyper_1_2_chunk_two_Partitions_timeCustom_series_bool_idx" | {timeCustom,series_bool} |      | f      | f       | f         | 
- _timescaledb_internal._hyper_1_2_chunk | _timescaledb_internal."_hyper_1_2_chunk_two_Partitions_timeCustom_device_id_idx"   | {timeCustom,device_id}   |      | f      | f       | f         | 
- _timescaledb_internal._hyper_1_2_chunk | _timescaledb_internal."_hyper_1_2_chunk_two_Partitions_timeCustom_idx"             | {timeCustom}             |      | f      | f       | f         | 
- _timescaledb_internal._hyper_1_3_chunk | _timescaledb_internal."_hyper_1_3_chunk_two_Partitions_device_id_timeCustom_idx"   | {device_id,timeCustom}   |      | f      | f       | f         | 
- _timescaledb_internal._hyper_1_3_chunk | _timescaledb_internal."_hyper_1_3_chunk_two_Partitions_timeCustom_series_0_idx"    | {timeCustom,series_0}    |      | f      | f       | f         | 
- _timescaledb_internal._hyper_1_3_chunk | _timescaledb_internal."_hyper_1_3_chunk_two_Partitions_timeCustom_series_1_idx"    | {timeCustom,series_1}    |      | f      | f       | f         | 
- _timescaledb_internal._hyper_1_3_chunk | _timescaledb_internal."_hyper_1_3_chunk_two_Partitions_timeCustom_series_2_idx"    | {timeCustom,series_2}    |      | f      | f       | f         | 
- _timescaledb_internal._hyper_1_3_chunk | _timescaledb_internal."_hyper_1_3_chunk_two_Partitions_timeCustom_series_bool_idx" | {timeCustom,series_bool} |      | f      | f       | f         | 
- _timescaledb_internal._hyper_1_3_chunk | _timescaledb_internal."_hyper_1_3_chunk_two_Partitions_timeCustom_device_id_idx"   | {timeCustom,device_id}   |      | f      | f       | f         | 
- _timescaledb_internal._hyper_1_3_chunk | _timescaledb_internal."_hyper_1_3_chunk_two_Partitions_timeCustom_idx"             | {timeCustom}             |      | f      | f       | f         | 
- _timescaledb_internal._hyper_1_4_chunk | _timescaledb_internal."_hyper_1_4_chunk_two_Partitions_device_id_timeCustom_idx"   | {device_id,timeCustom}   |      | f      | f       | f         | 
- _timescaledb_internal._hyper_1_4_chunk | _timescaledb_internal."_hyper_1_4_chunk_two_Partitions_timeCustom_series_0_idx"    | {timeCustom,series_0}    |      | f      | f       | f         | 
- _timescaledb_internal._hyper_1_4_chunk | _timescaledb_internal."_hyper_1_4_chunk_two_Partitions_timeCustom_series_1_idx"    | {timeCustom,series_1}    |      | f      | f       | f         | 
- _timescaledb_internal._hyper_1_4_chunk | _timescaledb_internal."_hyper_1_4_chunk_two_Partitions_timeCustom_series_2_idx"    | {timeCustom,series_2}    |      | f      | f       | f         | 
- _timescaledb_internal._hyper_1_4_chunk | _timescaledb_internal."_hyper_1_4_chunk_two_Partitions_timeCustom_series_bool_idx" | {timeCustom,series_bool} |      | f      | f       | f         | 
- _timescaledb_internal._hyper_1_4_chunk | _timescaledb_internal."_hyper_1_4_chunk_two_Partitions_timeCustom_device_id_idx"   | {timeCustom,device_id}   |      | f      | f       | f         | 
- _timescaledb_internal._hyper_1_4_chunk | _timescaledb_internal."_hyper_1_4_chunk_two_Partitions_timeCustom_idx"             | {timeCustom}             |      | f      | f       | f         | 
-(28 rows)
-
-SELECT * FROM _timescaledb_catalog.chunk;
- id | hypertable_id |      schema_name      |    table_name    | compressed_chunk_id | dropped | status | osm_chunk 
-----+---------------+-----------------------+------------------+---------------------+---------+--------+-----------
-  1 |             1 | _timescaledb_internal | _hyper_1_1_chunk |                     | f       |      0 | f
-  2 |             1 | _timescaledb_internal | _hyper_1_2_chunk |                     | f       |      0 | f
-  3 |             1 | _timescaledb_internal | _hyper_1_3_chunk |                     | f       |      0 | f
-  4 |             1 | _timescaledb_internal | _hyper_1_4_chunk |                     | f       |      0 | f
-(4 rows)
-
-SELECT * FROM "two_Partitions" ORDER BY "timeCustom", device_id, series_0, series_1;
-     timeCustom      | device_id | series_0 | series_1 | series_2 | series_bool 
----------------------+-----------+----------+----------+----------+-------------
- 1257894000000000000 | dev1      |      1.5 |        1 |        2 | t
- 1257894000000000000 | dev1      |      1.5 |        2 |          | 
- 1257894000000000000 | dev2      |      1.5 |        1 |          | 
- 1257894000000000000 | dev2      |      1.5 |        2 |          | 
- 1257894000000001000 | dev1      |      2.5 |        3 |          | 
- 1257894001000000000 | dev1      |      3.5 |        4 |          | 
- 1257894002000000000 | dev1      |      2.5 |        3 |          | 
- 1257894002000000000 | dev1      |      5.5 |        6 |          | t
- 1257894002000000000 | dev1      |      5.5 |        7 |          | f
- 1257897600000000000 | dev1      |      4.5 |        5 |          | f
- 1257987600000000000 | dev1      |      1.5 |        1 |          | 
- 1257987600000000000 | dev1      |      1.5 |        2 |          | 
-(12 rows)
-
-SELECT * FROM ONLY "two_Partitions";
- timeCustom | device_id | series_0 | series_1 | series_2 | series_bool 
-------------+-----------+----------+----------+----------+-------------
-(0 rows)
-
-CREATE TABLE error_test(time timestamp, temp float8, device text NOT NULL);
-SELECT create_hypertable('error_test', 'time', 'device', 2);
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-    create_hypertable    
--------------------------
- (2,public,error_test,t)
-(1 row)
-
-\set QUIET off
-INSERT INTO error_test VALUES ('Mon Mar 20 09:18:20.1 2017', 21.3, 'dev1');
-INSERT 0 1
-\set ON_ERROR_STOP 0
--- generate insert error
-INSERT INTO error_test VALUES ('Mon Mar 20 09:18:22.3 2017', 21.1, NULL);
-ERROR:  null value in column "device" of relation "_hyper_2_6_chunk" violates not-null constraint
-\set ON_ERROR_STOP 1
-INSERT INTO error_test VALUES ('Mon Mar 20 09:18:25.7 2017', 22.4, 'dev2');
-INSERT 0 1
-\set QUIET on
-SELECT * FROM error_test;
-            time            | temp | device 
-----------------------------+------+--------
- Mon Mar 20 09:18:20.1 2017 | 21.3 | dev1
- Mon Mar 20 09:18:25.7 2017 | 22.4 | dev2
-(2 rows)
-
---test character(9) partition keys since there were issues with padding causing partitioning errors
-CREATE TABLE tick_character (
-    symbol      character(9) NOT NULL,
-    mid       REAL NOT NULL,
-    spread      REAL NOT NULL,
-    time        TIMESTAMPTZ       NOT NULL
-);
-SELECT create_hypertable ('tick_character', 'time', 'symbol', 2);
-      create_hypertable      
------------------------------
- (3,public,tick_character,t)
-(1 row)
-
-INSERT INTO tick_character ( symbol, mid, spread, time ) VALUES ( 'GBPJPY', 142.639000, 5.80, 'Mon Mar 20 09:18:22.3 2017') RETURNING time, symbol, mid;
-              time              |  symbol   |   mid   
---------------------------------+-----------+---------
- Mon Mar 20 09:18:22.3 2017 PDT | GBPJPY    | 142.639
-(1 row)
-
-SELECT * FROM tick_character;
-  symbol   |   mid   | spread |              time              
------------+---------+--------+--------------------------------
- GBPJPY    | 142.639 |    5.8 | Mon Mar 20 09:18:22.3 2017 PDT
-(1 row)
-
-CREATE TABLE  date_col_test(time date, temp float8, device text NOT NULL);
-SELECT create_hypertable('date_col_test', 'time', 'device', 1000, chunk_time_interval => INTERVAL '1 Day');
-NOTICE:  adding not-null constraint to column "time"
-     create_hypertable      
-----------------------------
- (4,public,date_col_test,t)
-(1 row)
-
-INSERT INTO date_col_test
-VALUES ('2001-02-01', 98, 'dev1'),
-('2001-03-02', 98, 'dev1');
-SELECT * FROM date_col_test WHERE time > '2001-01-01';
-    time    | temp | device 
-------------+------+--------
- 02-01-2001 |   98 | dev1
- 03-02-2001 |   98 | dev1
-(2 rows)
-
--- Out-of-order insertion regression test.
--- this used to trip an assert in subspace_store.c checking that
--- max_open_chunks_per_insert was obeyed
-set timescaledb.max_open_chunks_per_insert=1;
-CREATE TABLE chunk_assert_fail(i bigint, j bigint);
-SELECT create_hypertable('chunk_assert_fail', 'i', 'j', 1000, chunk_time_interval=>1);
-NOTICE:  adding not-null constraint to column "i"
-       create_hypertable        
---------------------------------
- (5,public,chunk_assert_fail,t)
-(1 row)
-
-insert into chunk_assert_fail values (1, 1), (1, 2), (2,1);
-select * from chunk_assert_fail;
- i | j 
----+---
- 1 | 1
- 1 | 2
- 2 | 1
-(3 rows)
-
-CREATE TABLE one_space_test(time timestamp, temp float8, device text NOT NULL);
-SELECT create_hypertable('one_space_test', 'time', 'device', 1);
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-      create_hypertable      
------------------------------
- (6,public,one_space_test,t)
-(1 row)
-
-INSERT INTO one_space_test VALUES
-('2001-01-01 01:01:01', 1.0, 'device'),
-('2002-01-01 01:02:01', 1.0, 'device');
-SELECT * FROM one_space_test;
-           time           | temp | device 
---------------------------+------+--------
- Mon Jan 01 01:01:01 2001 |    1 | device
- Tue Jan 01 01:02:01 2002 |    1 | device
-(2 rows)
-
---CTE & EXPLAIN ANALYZE TESTS
-WITH insert_cte as (
-	INSERT INTO one_space_test VALUES
-		('2001-01-01 01:02:01', 1.0, 'device')
-	RETURNING *)
-SELECT * FROM insert_cte;
-           time           | temp | device 
---------------------------+------+--------
- Mon Jan 01 01:02:01 2001 |    1 | device
-(1 row)
-
-EXPLAIN (analyze, costs off, timing off) --can't turn summary off in 9.6 so instead grep it away at end.
-WITH insert_cte as (
-	INSERT INTO one_space_test VALUES
-		('2001-01-01 01:03:01', 1.0, 'device')
-	)
-SELECT 1 \g | grep -v "Planning" | grep -v "Execution"
-                               QUERY PLAN                                
--------------------------------------------------------------------------
- Result (actual rows=1 loops=1)
-   CTE insert_cte
-     ->  Custom Scan (HypertableModify) (actual rows=0 loops=1)
-           ->  Insert on one_space_test (actual rows=0 loops=1)
-                 ->  Custom Scan (ChunkDispatch) (actual rows=1 loops=1)
-                       ->  Result (actual rows=1 loops=1)
-(8 rows)
-
--- INSERTs can exclude chunks based on constraints
-EXPLAIN (costs off) INSERT INTO chunk_assert_fail SELECT i, j FROM chunk_assert_fail;
-                      QUERY PLAN                       
--------------------------------------------------------
- Custom Scan (HypertableModify)
-   ->  Insert on chunk_assert_fail
-         ->  Custom Scan (ChunkDispatch)
-               ->  Append
-                     ->  Seq Scan on _hyper_5_11_chunk
-                     ->  Seq Scan on _hyper_5_12_chunk
-                     ->  Seq Scan on _hyper_5_13_chunk
-(7 rows)
-
-EXPLAIN (costs off) INSERT INTO chunk_assert_fail SELECT i, j FROM chunk_assert_fail WHERE i < 1;
-                 QUERY PLAN                 
---------------------------------------------
- Custom Scan (HypertableModify)
-   ->  Insert on chunk_assert_fail
-         ->  Custom Scan (ChunkDispatch)
-               ->  Result
-                     One-Time Filter: false
-(5 rows)
-
-EXPLAIN (costs off) INSERT INTO chunk_assert_fail SELECT i, j FROM chunk_assert_fail WHERE i = 1;
-                                               QUERY PLAN                                                
----------------------------------------------------------------------------------------------------------
- Custom Scan (HypertableModify)
-   ->  Insert on chunk_assert_fail
-         ->  Custom Scan (ChunkDispatch)
-               ->  Append
-                     ->  Index Scan using _hyper_5_11_chunk_chunk_assert_fail_i_idx on _hyper_5_11_chunk
-                           Index Cond: (i = 1)
-                     ->  Index Scan using _hyper_5_12_chunk_chunk_assert_fail_i_idx on _hyper_5_12_chunk
-                           Index Cond: (i = 1)
-(8 rows)
-
-EXPLAIN (costs off) INSERT INTO chunk_assert_fail SELECT i, j FROM chunk_assert_fail WHERE i > 1;
-                                            QUERY PLAN                                             
----------------------------------------------------------------------------------------------------
- Custom Scan (HypertableModify)
-   ->  Insert on chunk_assert_fail
-         ->  Custom Scan (ChunkDispatch)
-               ->  Index Scan using _hyper_5_13_chunk_chunk_assert_fail_i_idx on _hyper_5_13_chunk
-                     Index Cond: (i > 1)
-(5 rows)
-
-INSERT INTO chunk_assert_fail SELECT i, j FROM chunk_assert_fail WHERE i > 1;
-EXPLAIN (costs off) INSERT INTO one_space_test SELECT * FROM one_space_test WHERE time < 'infinity' LIMIT 1;
-                                                  QUERY PLAN                                                   
----------------------------------------------------------------------------------------------------------------
- Custom Scan (HypertableModify)
-   ->  Insert on one_space_test
-         ->  Custom Scan (ChunkDispatch)
-               ->  Limit
-                     ->  Append
-                           ->  Index Scan using _hyper_6_14_chunk_one_space_test_time_idx on _hyper_6_14_chunk
-                                 Index Cond: ("time" < 'infinity'::timestamp without time zone)
-                           ->  Index Scan using _hyper_6_15_chunk_one_space_test_time_idx on _hyper_6_15_chunk
-                                 Index Cond: ("time" < 'infinity'::timestamp without time zone)
-(9 rows)
-
-EXPLAIN (costs off) INSERT INTO one_space_test SELECT * FROM one_space_test WHERE time >= 'infinity' LIMIT 1;
-                    QUERY PLAN                    
---------------------------------------------------
- Custom Scan (HypertableModify)
-   ->  Insert on one_space_test
-         ->  Custom Scan (ChunkDispatch)
-               ->  Limit
-                     ->  Result
-                           One-Time Filter: false
-(6 rows)
-
-EXPLAIN (costs off) INSERT INTO one_space_test SELECT * FROM one_space_test WHERE time <= '-infinity' LIMIT 1;
-                    QUERY PLAN                    
---------------------------------------------------
- Custom Scan (HypertableModify)
-   ->  Insert on one_space_test
-         ->  Custom Scan (ChunkDispatch)
-               ->  Limit
-                     ->  Result
-                           One-Time Filter: false
-(6 rows)
-
-EXPLAIN (costs off) INSERT INTO one_space_test SELECT * FROM one_space_test WHERE time > '-infinity' LIMIT 1;
-                                                  QUERY PLAN                                                   
----------------------------------------------------------------------------------------------------------------
- Custom Scan (HypertableModify)
-   ->  Insert on one_space_test
-         ->  Custom Scan (ChunkDispatch)
-               ->  Limit
-                     ->  Append
-                           ->  Index Scan using _hyper_6_14_chunk_one_space_test_time_idx on _hyper_6_14_chunk
-                                 Index Cond: ("time" > '-infinity'::timestamp without time zone)
-                           ->  Index Scan using _hyper_6_15_chunk_one_space_test_time_idx on _hyper_6_15_chunk
-                                 Index Cond: ("time" > '-infinity'::timestamp without time zone)
-(9 rows)
-
-INSERT INTO one_space_test SELECT * FROM one_space_test WHERE time < 'infinity' LIMIT 1;
-INSERT INTO one_space_test SELECT * FROM one_space_test WHERE time >= 'infinity' LIMIT 1;
-INSERT INTO one_space_test SELECT * FROM one_space_test WHERE time <= '-infinity' LIMIT 1;
-INSERT INTO one_space_test SELECT * FROM one_space_test WHERE time > '-infinity' LIMIT 1;
-CREATE TABLE timestamp_inf(time TIMESTAMP);
-SELECT create_hypertable('timestamp_inf', 'time');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-     create_hypertable      
-----------------------------
- (7,public,timestamp_inf,t)
-(1 row)
-
-INSERT INTO timestamp_inf VALUES ('2018/01/02'), ('2019/01/02');
-EXPLAIN (costs off) INSERT INTO timestamp_inf SELECT * FROM timestamp_inf
-    WHERE time < 'infinity' LIMIT 1;
-                                                    QUERY PLAN                                                     
--------------------------------------------------------------------------------------------------------------------
- Custom Scan (HypertableModify)
-   ->  Insert on timestamp_inf
-         ->  Custom Scan (ChunkDispatch)
-               ->  Limit
-                     ->  Append
-                           ->  Index Only Scan using _hyper_7_16_chunk_timestamp_inf_time_idx on _hyper_7_16_chunk
-                                 Index Cond: ("time" < 'infinity'::timestamp without time zone)
-                           ->  Index Only Scan using _hyper_7_17_chunk_timestamp_inf_time_idx on _hyper_7_17_chunk
-                                 Index Cond: ("time" < 'infinity'::timestamp without time zone)
-(9 rows)
-
-EXPLAIN (costs off) INSERT INTO timestamp_inf SELECT * FROM timestamp_inf
-    WHERE time >= 'infinity' LIMIT 1;
-                    QUERY PLAN                    
---------------------------------------------------
- Custom Scan (HypertableModify)
-   ->  Insert on timestamp_inf
-         ->  Custom Scan (ChunkDispatch)
-               ->  Limit
-                     ->  Result
-                           One-Time Filter: false
-(6 rows)
-
-EXPLAIN (costs off) INSERT INTO timestamp_inf SELECT * FROM timestamp_inf
-    WHERE time <= '-infinity' LIMIT 1;
-                    QUERY PLAN                    
---------------------------------------------------
- Custom Scan (HypertableModify)
-   ->  Insert on timestamp_inf
-         ->  Custom Scan (ChunkDispatch)
-               ->  Limit
-                     ->  Result
-                           One-Time Filter: false
-(6 rows)
-
-EXPLAIN (costs off) INSERT INTO timestamp_inf SELECT * FROM timestamp_inf
-    WHERE time > '-infinity' LIMIT 1;
-                                                    QUERY PLAN                                                     
--------------------------------------------------------------------------------------------------------------------
- Custom Scan (HypertableModify)
-   ->  Insert on timestamp_inf
-         ->  Custom Scan (ChunkDispatch)
-               ->  Limit
-                     ->  Append
-                           ->  Index Only Scan using _hyper_7_16_chunk_timestamp_inf_time_idx on _hyper_7_16_chunk
-                                 Index Cond: ("time" > '-infinity'::timestamp without time zone)
-                           ->  Index Only Scan using _hyper_7_17_chunk_timestamp_inf_time_idx on _hyper_7_17_chunk
-                                 Index Cond: ("time" > '-infinity'::timestamp without time zone)
-(9 rows)
-
-CREATE TABLE date_inf(time DATE);
-SELECT create_hypertable('date_inf', 'time');
-NOTICE:  adding not-null constraint to column "time"
-   create_hypertable   
------------------------
- (8,public,date_inf,t)
-(1 row)
-
-INSERT INTO date_inf VALUES ('2018/01/02'), ('2019/01/02');
-EXPLAIN (costs off) INSERT INTO date_inf SELECT * FROM date_inf
-    WHERE time < 'infinity' LIMIT 1;
-                                                  QUERY PLAN                                                  
---------------------------------------------------------------------------------------------------------------
- Custom Scan (HypertableModify)
-   ->  Insert on date_inf
-         ->  Custom Scan (ChunkDispatch)
-               ->  Limit
-                     ->  Append
-                           ->  Index Only Scan using _hyper_8_18_chunk_date_inf_time_idx on _hyper_8_18_chunk
-                                 Index Cond: ("time" < 'infinity'::date)
-                           ->  Index Only Scan using _hyper_8_19_chunk_date_inf_time_idx on _hyper_8_19_chunk
-                                 Index Cond: ("time" < 'infinity'::date)
-(9 rows)
-
-EXPLAIN (costs off) INSERT INTO date_inf SELECT * FROM date_inf
-    WHERE time >= 'infinity' LIMIT 1;
-                    QUERY PLAN                    
---------------------------------------------------
- Custom Scan (HypertableModify)
-   ->  Insert on date_inf
-         ->  Custom Scan (ChunkDispatch)
-               ->  Limit
-                     ->  Result
-                           One-Time Filter: false
-(6 rows)
-
-EXPLAIN (costs off) INSERT INTO date_inf SELECT * FROM date_inf
-    WHERE time <= '-infinity' LIMIT 1;
-                    QUERY PLAN                    
---------------------------------------------------
- Custom Scan (HypertableModify)
-   ->  Insert on date_inf
-         ->  Custom Scan (ChunkDispatch)
-               ->  Limit
-                     ->  Result
-                           One-Time Filter: false
-(6 rows)
-
-EXPLAIN (costs off) INSERT INTO date_inf SELECT * FROM date_inf
-    WHERE time > '-infinity' LIMIT 1;
-                                                  QUERY PLAN                                                  
---------------------------------------------------------------------------------------------------------------
- Custom Scan (HypertableModify)
-   ->  Insert on date_inf
-         ->  Custom Scan (ChunkDispatch)
-               ->  Limit
-                     ->  Append
-                           ->  Index Only Scan using _hyper_8_18_chunk_date_inf_time_idx on _hyper_8_18_chunk
-                                 Index Cond: ("time" > '-infinity'::date)
-                           ->  Index Only Scan using _hyper_8_19_chunk_date_inf_time_idx on _hyper_8_19_chunk
-                                 Index Cond: ("time" > '-infinity'::date)
-(9 rows)
-
--- test INSERT with cached plans / plpgsql functions
--- https://github.com/timescale/timescaledb/issues/1809
-CREATE TABLE status_table(a int, b int, last_ts timestamptz, UNIQUE(a,b));
-CREATE TABLE metrics(time timestamptz NOT NULL, value float);
-CREATE TABLE metrics2(time timestamptz NOT NULL, value float);
-SELECT (create_hypertable(t,'time')).table_name FROM (VALUES ('metrics'),('metrics2')) v(t);
- table_name 
-------------
- metrics
- metrics2
-(2 rows)
-
-INSERT INTO metrics VALUES ('2000-01-01',random()), ('2000-02-01',random()), ('2000-03-01',random());
-CREATE OR REPLACE FUNCTION insert_test() RETURNS VOID LANGUAGE plpgsql AS
-$$
-  DECLARE
-    r RECORD;
-  BEGIN
-    FOR r IN
-      SELECT * FROM metrics
-    LOOP
-      WITH foo AS (
-        INSERT INTO metrics2 SELECT * FROM metrics RETURNING *
-      )
-      INSERT INTO status_table (a,b, last_ts)
-        VALUES (1,1, now())
-        ON CONFLICT (a,b) DO UPDATE SET last_ts=(SELECT max(time) FROM metrics);
-    END LOOP;
-  END;
-$$;
-SELECT insert_test(), insert_test(), insert_test();
- insert_test | insert_test | insert_test 
--------------+-------------+-------------
-             |             | 
-(1 row)
-
--- test Postgres crashes on INSERT ... SELECT ... WHERE NOT EXISTS with empty table
--- https://github.com/timescale/timescaledb/issues/1883
-CREATE TABLE readings (
-	toe TIMESTAMPTZ NOT NULL,
-	sensor_id INT NOT NULL,
-	value INT NOT NULL
-);
-SELECT create_hypertable(
-	'readings',
-	'toe',
-	chunk_time_interval => interval '1 day',
-	if_not_exists => TRUE,
-	migrate_data => TRUE
-);
-   create_hypertable    
-------------------------
- (11,public,readings,t)
-(1 row)
-
-EXPLAIN (costs off)
-INSERT INTO readings
-SELECT '2020-05-09 10:34:35.296288+00', 1, 0
-WHERE NOT EXISTS (
-	SELECT 1
-	FROM readings
-	WHERE sensor_id = 1
-		AND toe = '2020-05-09 10:34:35.296288+00'
-);
-                     QUERY PLAN                      
------------------------------------------------------
- Custom Scan (HypertableModify)
-   InitPlan 1 (returns $0)
-     ->  Result
-           One-Time Filter: false
-   ->  Insert on readings
-         ->  Result
-               One-Time Filter: (NOT $0)
-               ->  Custom Scan (ChunkDispatch)
-                     ->  Result
-                           One-Time Filter: (NOT $0)
-(10 rows)
-
-INSERT INTO readings
-SELECT '2020-05-09 10:34:35.296288+00', 1, 0
-WHERE NOT EXISTS (
-	SELECT 1
-	FROM readings
-	WHERE sensor_id = 1
-		AND toe = '2020-05-09 10:34:35.296288+00'
-);
-DROP TABLE readings;
-CREATE TABLE sample_table (
-       sequence INTEGER NOT NULL,
-       time TIMESTAMP WITHOUT TIME ZONE NOT NULL,
-       value NUMERIC NOT NULL,
-       UNIQUE (sequence, time)
-);
-SELECT * FROM create_hypertable('sample_table', 'time',
-       chunk_time_interval => INTERVAL '1 day');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
- hypertable_id | schema_name |  table_name  | created 
----------------+-------------+--------------+---------
-            12 | public      | sample_table | t
-(1 row)
-
-INSERT INTO sample_table (sequence,time,value) VALUES
-       (7, generate_series(TIMESTAMP '2019-08-01', TIMESTAMP '2019-08-10', INTERVAL '10 minutes'), ROUND(RANDOM()*10)::int);
-\set ON_ERROR_STOP 0
-INSERT INTO sample_table (sequence,time,value) VALUES
-       (7, generate_series(TIMESTAMP '2019-07-21', TIMESTAMP '2019-08-01', INTERVAL '10 minutes'), ROUND(RANDOM()*10)::int);
-ERROR:  duplicate key value violates unique constraint "27_1_sample_table_sequence_time_key"
-\set ON_ERROR_STOP 1
-INSERT INTO sample_table (sequence,time,value) VALUES
-       (7,generate_series(TIMESTAMP '2019-01-01', TIMESTAMP '2019-07-01', '10 minutes'), ROUND(RANDOM()*10)::int);
-DROP TABLE sample_table;
--- test on conflict clause on columns with default value
--- issue #3037
-CREATE TABLE i3037(time timestamptz PRIMARY KEY);
-SELECT create_hypertable('i3037','time');
-  create_hypertable  
----------------------
- (13,public,i3037,t)
-(1 row)
-
-ALTER TABLE i3037 ADD COLUMN value float DEFAULT 0;
-INSERT INTO i3037 VALUES ('2000-01-01');
-INSERT INTO i3037 VALUES ('2000-01-01') ON CONFLICT(time) DO UPDATE SET value = EXCLUDED.value;
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/insert_many.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/insert_many.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/insert_many.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/insert_many.out	2023-11-25 05:27:38.957037758 +0000
@@ -1,435 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-CREATE TABLE  many_partitions_test(time timestamp, temp float8, device text NOT NULL);
-SELECT create_hypertable('many_partitions_test', 'time', 'device', 1000);
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-         create_hypertable         
------------------------------------
- (1,public,many_partitions_test,t)
-(1 row)
-
---NOTE: how much slower the first two queries are -- they are creating chunks
-INSERT INTO many_partitions_test
-    SELECT to_timestamp(ser), ser, ser::text FROM generate_series(1,100) ser;
-INSERT INTO many_partitions_test
-    SELECT to_timestamp(ser), ser, ser::text FROM generate_series(101,200) ser;
-INSERT INTO many_partitions_test
-    SELECT to_timestamp(ser), ser, (ser-201)::text FROM generate_series(201,300) ser;
-SELECT * FROM  many_partitions_test ORDER BY time DESC LIMIT 2;
-           time           | temp | device 
---------------------------+------+--------
- Wed Dec 31 16:05:00 1969 |  300 | 99
- Wed Dec 31 16:04:59 1969 |  299 | 98
-(2 rows)
-
-SELECT count(*) FROM  many_partitions_test;
- count 
--------
-   300
-(1 row)
-
-CREATE TABLE many_partitions_test_1m (time timestamp, temp float8, device text NOT NULL);
-SELECT create_hypertable('many_partitions_test_1m', 'time', 'device', 1000);
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-          create_hypertable           
---------------------------------------
- (2,public,many_partitions_test_1m,t)
-(1 row)
-
-EXPLAIN (verbose on, costs off)
-INSERT INTO many_partitions_test_1m(time, temp, device)
-SELECT time_bucket('1 minute', time) AS period, avg(temp), device
-FROM many_partitions_test
-GROUP BY period, device;
-                                                                  QUERY PLAN                                                                  
-----------------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (HypertableModify)
-   ->  Insert on public.many_partitions_test_1m
-         ->  Custom Scan (ChunkDispatch)
-               Output: "*SELECT*".period, "*SELECT*".avg, "*SELECT*".device
-               ->  HashAggregate
-                     Output: (time_bucket('@ 1 min'::interval, _hyper_1_1_chunk."time")), avg(_hyper_1_1_chunk.temp), _hyper_1_1_chunk.device
-                     Group Key: time_bucket('@ 1 min'::interval, _hyper_1_1_chunk."time"), _hyper_1_1_chunk.device
-                     ->  Result
-                           Output: time_bucket('@ 1 min'::interval, _hyper_1_1_chunk."time"), _hyper_1_1_chunk.device, _hyper_1_1_chunk.temp
-                           ->  Append
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_1_chunk
-                                       Output: _hyper_1_1_chunk."time", _hyper_1_1_chunk.temp, _hyper_1_1_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_2_chunk
-                                       Output: _hyper_1_2_chunk."time", _hyper_1_2_chunk.temp, _hyper_1_2_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_3_chunk
-                                       Output: _hyper_1_3_chunk."time", _hyper_1_3_chunk.temp, _hyper_1_3_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_4_chunk
-                                       Output: _hyper_1_4_chunk."time", _hyper_1_4_chunk.temp, _hyper_1_4_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_5_chunk
-                                       Output: _hyper_1_5_chunk."time", _hyper_1_5_chunk.temp, _hyper_1_5_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_6_chunk
-                                       Output: _hyper_1_6_chunk."time", _hyper_1_6_chunk.temp, _hyper_1_6_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_7_chunk
-                                       Output: _hyper_1_7_chunk."time", _hyper_1_7_chunk.temp, _hyper_1_7_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_8_chunk
-                                       Output: _hyper_1_8_chunk."time", _hyper_1_8_chunk.temp, _hyper_1_8_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_9_chunk
-                                       Output: _hyper_1_9_chunk."time", _hyper_1_9_chunk.temp, _hyper_1_9_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_10_chunk
-                                       Output: _hyper_1_10_chunk."time", _hyper_1_10_chunk.temp, _hyper_1_10_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_11_chunk
-                                       Output: _hyper_1_11_chunk."time", _hyper_1_11_chunk.temp, _hyper_1_11_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_12_chunk
-                                       Output: _hyper_1_12_chunk."time", _hyper_1_12_chunk.temp, _hyper_1_12_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_13_chunk
-                                       Output: _hyper_1_13_chunk."time", _hyper_1_13_chunk.temp, _hyper_1_13_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_14_chunk
-                                       Output: _hyper_1_14_chunk."time", _hyper_1_14_chunk.temp, _hyper_1_14_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_15_chunk
-                                       Output: _hyper_1_15_chunk."time", _hyper_1_15_chunk.temp, _hyper_1_15_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_16_chunk
-                                       Output: _hyper_1_16_chunk."time", _hyper_1_16_chunk.temp, _hyper_1_16_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_17_chunk
-                                       Output: _hyper_1_17_chunk."time", _hyper_1_17_chunk.temp, _hyper_1_17_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_18_chunk
-                                       Output: _hyper_1_18_chunk."time", _hyper_1_18_chunk.temp, _hyper_1_18_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_19_chunk
-                                       Output: _hyper_1_19_chunk."time", _hyper_1_19_chunk.temp, _hyper_1_19_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_20_chunk
-                                       Output: _hyper_1_20_chunk."time", _hyper_1_20_chunk.temp, _hyper_1_20_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_21_chunk
-                                       Output: _hyper_1_21_chunk."time", _hyper_1_21_chunk.temp, _hyper_1_21_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_22_chunk
-                                       Output: _hyper_1_22_chunk."time", _hyper_1_22_chunk.temp, _hyper_1_22_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_23_chunk
-                                       Output: _hyper_1_23_chunk."time", _hyper_1_23_chunk.temp, _hyper_1_23_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_24_chunk
-                                       Output: _hyper_1_24_chunk."time", _hyper_1_24_chunk.temp, _hyper_1_24_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_25_chunk
-                                       Output: _hyper_1_25_chunk."time", _hyper_1_25_chunk.temp, _hyper_1_25_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_26_chunk
-                                       Output: _hyper_1_26_chunk."time", _hyper_1_26_chunk.temp, _hyper_1_26_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_27_chunk
-                                       Output: _hyper_1_27_chunk."time", _hyper_1_27_chunk.temp, _hyper_1_27_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_28_chunk
-                                       Output: _hyper_1_28_chunk."time", _hyper_1_28_chunk.temp, _hyper_1_28_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_29_chunk
-                                       Output: _hyper_1_29_chunk."time", _hyper_1_29_chunk.temp, _hyper_1_29_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_30_chunk
-                                       Output: _hyper_1_30_chunk."time", _hyper_1_30_chunk.temp, _hyper_1_30_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_31_chunk
-                                       Output: _hyper_1_31_chunk."time", _hyper_1_31_chunk.temp, _hyper_1_31_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_32_chunk
-                                       Output: _hyper_1_32_chunk."time", _hyper_1_32_chunk.temp, _hyper_1_32_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_33_chunk
-                                       Output: _hyper_1_33_chunk."time", _hyper_1_33_chunk.temp, _hyper_1_33_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_34_chunk
-                                       Output: _hyper_1_34_chunk."time", _hyper_1_34_chunk.temp, _hyper_1_34_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_35_chunk
-                                       Output: _hyper_1_35_chunk."time", _hyper_1_35_chunk.temp, _hyper_1_35_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_36_chunk
-                                       Output: _hyper_1_36_chunk."time", _hyper_1_36_chunk.temp, _hyper_1_36_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_37_chunk
-                                       Output: _hyper_1_37_chunk."time", _hyper_1_37_chunk.temp, _hyper_1_37_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_38_chunk
-                                       Output: _hyper_1_38_chunk."time", _hyper_1_38_chunk.temp, _hyper_1_38_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_39_chunk
-                                       Output: _hyper_1_39_chunk."time", _hyper_1_39_chunk.temp, _hyper_1_39_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_40_chunk
-                                       Output: _hyper_1_40_chunk."time", _hyper_1_40_chunk.temp, _hyper_1_40_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_41_chunk
-                                       Output: _hyper_1_41_chunk."time", _hyper_1_41_chunk.temp, _hyper_1_41_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_42_chunk
-                                       Output: _hyper_1_42_chunk."time", _hyper_1_42_chunk.temp, _hyper_1_42_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_43_chunk
-                                       Output: _hyper_1_43_chunk."time", _hyper_1_43_chunk.temp, _hyper_1_43_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_44_chunk
-                                       Output: _hyper_1_44_chunk."time", _hyper_1_44_chunk.temp, _hyper_1_44_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_45_chunk
-                                       Output: _hyper_1_45_chunk."time", _hyper_1_45_chunk.temp, _hyper_1_45_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_46_chunk
-                                       Output: _hyper_1_46_chunk."time", _hyper_1_46_chunk.temp, _hyper_1_46_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_47_chunk
-                                       Output: _hyper_1_47_chunk."time", _hyper_1_47_chunk.temp, _hyper_1_47_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_48_chunk
-                                       Output: _hyper_1_48_chunk."time", _hyper_1_48_chunk.temp, _hyper_1_48_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_49_chunk
-                                       Output: _hyper_1_49_chunk."time", _hyper_1_49_chunk.temp, _hyper_1_49_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_50_chunk
-                                       Output: _hyper_1_50_chunk."time", _hyper_1_50_chunk.temp, _hyper_1_50_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_51_chunk
-                                       Output: _hyper_1_51_chunk."time", _hyper_1_51_chunk.temp, _hyper_1_51_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_52_chunk
-                                       Output: _hyper_1_52_chunk."time", _hyper_1_52_chunk.temp, _hyper_1_52_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_53_chunk
-                                       Output: _hyper_1_53_chunk."time", _hyper_1_53_chunk.temp, _hyper_1_53_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_54_chunk
-                                       Output: _hyper_1_54_chunk."time", _hyper_1_54_chunk.temp, _hyper_1_54_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_55_chunk
-                                       Output: _hyper_1_55_chunk."time", _hyper_1_55_chunk.temp, _hyper_1_55_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_56_chunk
-                                       Output: _hyper_1_56_chunk."time", _hyper_1_56_chunk.temp, _hyper_1_56_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_57_chunk
-                                       Output: _hyper_1_57_chunk."time", _hyper_1_57_chunk.temp, _hyper_1_57_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_58_chunk
-                                       Output: _hyper_1_58_chunk."time", _hyper_1_58_chunk.temp, _hyper_1_58_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_59_chunk
-                                       Output: _hyper_1_59_chunk."time", _hyper_1_59_chunk.temp, _hyper_1_59_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_60_chunk
-                                       Output: _hyper_1_60_chunk."time", _hyper_1_60_chunk.temp, _hyper_1_60_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_61_chunk
-                                       Output: _hyper_1_61_chunk."time", _hyper_1_61_chunk.temp, _hyper_1_61_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_62_chunk
-                                       Output: _hyper_1_62_chunk."time", _hyper_1_62_chunk.temp, _hyper_1_62_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_63_chunk
-                                       Output: _hyper_1_63_chunk."time", _hyper_1_63_chunk.temp, _hyper_1_63_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_64_chunk
-                                       Output: _hyper_1_64_chunk."time", _hyper_1_64_chunk.temp, _hyper_1_64_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_65_chunk
-                                       Output: _hyper_1_65_chunk."time", _hyper_1_65_chunk.temp, _hyper_1_65_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_66_chunk
-                                       Output: _hyper_1_66_chunk."time", _hyper_1_66_chunk.temp, _hyper_1_66_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_67_chunk
-                                       Output: _hyper_1_67_chunk."time", _hyper_1_67_chunk.temp, _hyper_1_67_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_68_chunk
-                                       Output: _hyper_1_68_chunk."time", _hyper_1_68_chunk.temp, _hyper_1_68_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_69_chunk
-                                       Output: _hyper_1_69_chunk."time", _hyper_1_69_chunk.temp, _hyper_1_69_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_70_chunk
-                                       Output: _hyper_1_70_chunk."time", _hyper_1_70_chunk.temp, _hyper_1_70_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_71_chunk
-                                       Output: _hyper_1_71_chunk."time", _hyper_1_71_chunk.temp, _hyper_1_71_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_72_chunk
-                                       Output: _hyper_1_72_chunk."time", _hyper_1_72_chunk.temp, _hyper_1_72_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_73_chunk
-                                       Output: _hyper_1_73_chunk."time", _hyper_1_73_chunk.temp, _hyper_1_73_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_74_chunk
-                                       Output: _hyper_1_74_chunk."time", _hyper_1_74_chunk.temp, _hyper_1_74_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_75_chunk
-                                       Output: _hyper_1_75_chunk."time", _hyper_1_75_chunk.temp, _hyper_1_75_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_76_chunk
-                                       Output: _hyper_1_76_chunk."time", _hyper_1_76_chunk.temp, _hyper_1_76_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_77_chunk
-                                       Output: _hyper_1_77_chunk."time", _hyper_1_77_chunk.temp, _hyper_1_77_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_78_chunk
-                                       Output: _hyper_1_78_chunk."time", _hyper_1_78_chunk.temp, _hyper_1_78_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_79_chunk
-                                       Output: _hyper_1_79_chunk."time", _hyper_1_79_chunk.temp, _hyper_1_79_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_80_chunk
-                                       Output: _hyper_1_80_chunk."time", _hyper_1_80_chunk.temp, _hyper_1_80_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_81_chunk
-                                       Output: _hyper_1_81_chunk."time", _hyper_1_81_chunk.temp, _hyper_1_81_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_82_chunk
-                                       Output: _hyper_1_82_chunk."time", _hyper_1_82_chunk.temp, _hyper_1_82_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_83_chunk
-                                       Output: _hyper_1_83_chunk."time", _hyper_1_83_chunk.temp, _hyper_1_83_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_84_chunk
-                                       Output: _hyper_1_84_chunk."time", _hyper_1_84_chunk.temp, _hyper_1_84_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_85_chunk
-                                       Output: _hyper_1_85_chunk."time", _hyper_1_85_chunk.temp, _hyper_1_85_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_86_chunk
-                                       Output: _hyper_1_86_chunk."time", _hyper_1_86_chunk.temp, _hyper_1_86_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_87_chunk
-                                       Output: _hyper_1_87_chunk."time", _hyper_1_87_chunk.temp, _hyper_1_87_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_88_chunk
-                                       Output: _hyper_1_88_chunk."time", _hyper_1_88_chunk.temp, _hyper_1_88_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_89_chunk
-                                       Output: _hyper_1_89_chunk."time", _hyper_1_89_chunk.temp, _hyper_1_89_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_90_chunk
-                                       Output: _hyper_1_90_chunk."time", _hyper_1_90_chunk.temp, _hyper_1_90_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_91_chunk
-                                       Output: _hyper_1_91_chunk."time", _hyper_1_91_chunk.temp, _hyper_1_91_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_92_chunk
-                                       Output: _hyper_1_92_chunk."time", _hyper_1_92_chunk.temp, _hyper_1_92_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_93_chunk
-                                       Output: _hyper_1_93_chunk."time", _hyper_1_93_chunk.temp, _hyper_1_93_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_94_chunk
-                                       Output: _hyper_1_94_chunk."time", _hyper_1_94_chunk.temp, _hyper_1_94_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_95_chunk
-                                       Output: _hyper_1_95_chunk."time", _hyper_1_95_chunk.temp, _hyper_1_95_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_96_chunk
-                                       Output: _hyper_1_96_chunk."time", _hyper_1_96_chunk.temp, _hyper_1_96_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_97_chunk
-                                       Output: _hyper_1_97_chunk."time", _hyper_1_97_chunk.temp, _hyper_1_97_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_98_chunk
-                                       Output: _hyper_1_98_chunk."time", _hyper_1_98_chunk.temp, _hyper_1_98_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_99_chunk
-                                       Output: _hyper_1_99_chunk."time", _hyper_1_99_chunk.temp, _hyper_1_99_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_100_chunk
-                                       Output: _hyper_1_100_chunk."time", _hyper_1_100_chunk.temp, _hyper_1_100_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_101_chunk
-                                       Output: _hyper_1_101_chunk."time", _hyper_1_101_chunk.temp, _hyper_1_101_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_102_chunk
-                                       Output: _hyper_1_102_chunk."time", _hyper_1_102_chunk.temp, _hyper_1_102_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_103_chunk
-                                       Output: _hyper_1_103_chunk."time", _hyper_1_103_chunk.temp, _hyper_1_103_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_104_chunk
-                                       Output: _hyper_1_104_chunk."time", _hyper_1_104_chunk.temp, _hyper_1_104_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_105_chunk
-                                       Output: _hyper_1_105_chunk."time", _hyper_1_105_chunk.temp, _hyper_1_105_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_106_chunk
-                                       Output: _hyper_1_106_chunk."time", _hyper_1_106_chunk.temp, _hyper_1_106_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_107_chunk
-                                       Output: _hyper_1_107_chunk."time", _hyper_1_107_chunk.temp, _hyper_1_107_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_108_chunk
-                                       Output: _hyper_1_108_chunk."time", _hyper_1_108_chunk.temp, _hyper_1_108_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_109_chunk
-                                       Output: _hyper_1_109_chunk."time", _hyper_1_109_chunk.temp, _hyper_1_109_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_110_chunk
-                                       Output: _hyper_1_110_chunk."time", _hyper_1_110_chunk.temp, _hyper_1_110_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_111_chunk
-                                       Output: _hyper_1_111_chunk."time", _hyper_1_111_chunk.temp, _hyper_1_111_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_112_chunk
-                                       Output: _hyper_1_112_chunk."time", _hyper_1_112_chunk.temp, _hyper_1_112_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_113_chunk
-                                       Output: _hyper_1_113_chunk."time", _hyper_1_113_chunk.temp, _hyper_1_113_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_114_chunk
-                                       Output: _hyper_1_114_chunk."time", _hyper_1_114_chunk.temp, _hyper_1_114_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_115_chunk
-                                       Output: _hyper_1_115_chunk."time", _hyper_1_115_chunk.temp, _hyper_1_115_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_116_chunk
-                                       Output: _hyper_1_116_chunk."time", _hyper_1_116_chunk.temp, _hyper_1_116_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_117_chunk
-                                       Output: _hyper_1_117_chunk."time", _hyper_1_117_chunk.temp, _hyper_1_117_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_118_chunk
-                                       Output: _hyper_1_118_chunk."time", _hyper_1_118_chunk.temp, _hyper_1_118_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_119_chunk
-                                       Output: _hyper_1_119_chunk."time", _hyper_1_119_chunk.temp, _hyper_1_119_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_120_chunk
-                                       Output: _hyper_1_120_chunk."time", _hyper_1_120_chunk.temp, _hyper_1_120_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_121_chunk
-                                       Output: _hyper_1_121_chunk."time", _hyper_1_121_chunk.temp, _hyper_1_121_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_122_chunk
-                                       Output: _hyper_1_122_chunk."time", _hyper_1_122_chunk.temp, _hyper_1_122_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_123_chunk
-                                       Output: _hyper_1_123_chunk."time", _hyper_1_123_chunk.temp, _hyper_1_123_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_124_chunk
-                                       Output: _hyper_1_124_chunk."time", _hyper_1_124_chunk.temp, _hyper_1_124_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_125_chunk
-                                       Output: _hyper_1_125_chunk."time", _hyper_1_125_chunk.temp, _hyper_1_125_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_126_chunk
-                                       Output: _hyper_1_126_chunk."time", _hyper_1_126_chunk.temp, _hyper_1_126_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_127_chunk
-                                       Output: _hyper_1_127_chunk."time", _hyper_1_127_chunk.temp, _hyper_1_127_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_128_chunk
-                                       Output: _hyper_1_128_chunk."time", _hyper_1_128_chunk.temp, _hyper_1_128_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_129_chunk
-                                       Output: _hyper_1_129_chunk."time", _hyper_1_129_chunk.temp, _hyper_1_129_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_130_chunk
-                                       Output: _hyper_1_130_chunk."time", _hyper_1_130_chunk.temp, _hyper_1_130_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_131_chunk
-                                       Output: _hyper_1_131_chunk."time", _hyper_1_131_chunk.temp, _hyper_1_131_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_132_chunk
-                                       Output: _hyper_1_132_chunk."time", _hyper_1_132_chunk.temp, _hyper_1_132_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_133_chunk
-                                       Output: _hyper_1_133_chunk."time", _hyper_1_133_chunk.temp, _hyper_1_133_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_134_chunk
-                                       Output: _hyper_1_134_chunk."time", _hyper_1_134_chunk.temp, _hyper_1_134_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_135_chunk
-                                       Output: _hyper_1_135_chunk."time", _hyper_1_135_chunk.temp, _hyper_1_135_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_136_chunk
-                                       Output: _hyper_1_136_chunk."time", _hyper_1_136_chunk.temp, _hyper_1_136_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_137_chunk
-                                       Output: _hyper_1_137_chunk."time", _hyper_1_137_chunk.temp, _hyper_1_137_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_138_chunk
-                                       Output: _hyper_1_138_chunk."time", _hyper_1_138_chunk.temp, _hyper_1_138_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_139_chunk
-                                       Output: _hyper_1_139_chunk."time", _hyper_1_139_chunk.temp, _hyper_1_139_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_140_chunk
-                                       Output: _hyper_1_140_chunk."time", _hyper_1_140_chunk.temp, _hyper_1_140_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_141_chunk
-                                       Output: _hyper_1_141_chunk."time", _hyper_1_141_chunk.temp, _hyper_1_141_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_142_chunk
-                                       Output: _hyper_1_142_chunk."time", _hyper_1_142_chunk.temp, _hyper_1_142_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_143_chunk
-                                       Output: _hyper_1_143_chunk."time", _hyper_1_143_chunk.temp, _hyper_1_143_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_144_chunk
-                                       Output: _hyper_1_144_chunk."time", _hyper_1_144_chunk.temp, _hyper_1_144_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_145_chunk
-                                       Output: _hyper_1_145_chunk."time", _hyper_1_145_chunk.temp, _hyper_1_145_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_146_chunk
-                                       Output: _hyper_1_146_chunk."time", _hyper_1_146_chunk.temp, _hyper_1_146_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_147_chunk
-                                       Output: _hyper_1_147_chunk."time", _hyper_1_147_chunk.temp, _hyper_1_147_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_148_chunk
-                                       Output: _hyper_1_148_chunk."time", _hyper_1_148_chunk.temp, _hyper_1_148_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_149_chunk
-                                       Output: _hyper_1_149_chunk."time", _hyper_1_149_chunk.temp, _hyper_1_149_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_150_chunk
-                                       Output: _hyper_1_150_chunk."time", _hyper_1_150_chunk.temp, _hyper_1_150_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_151_chunk
-                                       Output: _hyper_1_151_chunk."time", _hyper_1_151_chunk.temp, _hyper_1_151_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_152_chunk
-                                       Output: _hyper_1_152_chunk."time", _hyper_1_152_chunk.temp, _hyper_1_152_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_153_chunk
-                                       Output: _hyper_1_153_chunk."time", _hyper_1_153_chunk.temp, _hyper_1_153_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_154_chunk
-                                       Output: _hyper_1_154_chunk."time", _hyper_1_154_chunk.temp, _hyper_1_154_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_155_chunk
-                                       Output: _hyper_1_155_chunk."time", _hyper_1_155_chunk.temp, _hyper_1_155_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_156_chunk
-                                       Output: _hyper_1_156_chunk."time", _hyper_1_156_chunk.temp, _hyper_1_156_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_157_chunk
-                                       Output: _hyper_1_157_chunk."time", _hyper_1_157_chunk.temp, _hyper_1_157_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_158_chunk
-                                       Output: _hyper_1_158_chunk."time", _hyper_1_158_chunk.temp, _hyper_1_158_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_159_chunk
-                                       Output: _hyper_1_159_chunk."time", _hyper_1_159_chunk.temp, _hyper_1_159_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_160_chunk
-                                       Output: _hyper_1_160_chunk."time", _hyper_1_160_chunk.temp, _hyper_1_160_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_161_chunk
-                                       Output: _hyper_1_161_chunk."time", _hyper_1_161_chunk.temp, _hyper_1_161_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_162_chunk
-                                       Output: _hyper_1_162_chunk."time", _hyper_1_162_chunk.temp, _hyper_1_162_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_163_chunk
-                                       Output: _hyper_1_163_chunk."time", _hyper_1_163_chunk.temp, _hyper_1_163_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_164_chunk
-                                       Output: _hyper_1_164_chunk."time", _hyper_1_164_chunk.temp, _hyper_1_164_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_165_chunk
-                                       Output: _hyper_1_165_chunk."time", _hyper_1_165_chunk.temp, _hyper_1_165_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_166_chunk
-                                       Output: _hyper_1_166_chunk."time", _hyper_1_166_chunk.temp, _hyper_1_166_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_167_chunk
-                                       Output: _hyper_1_167_chunk."time", _hyper_1_167_chunk.temp, _hyper_1_167_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_168_chunk
-                                       Output: _hyper_1_168_chunk."time", _hyper_1_168_chunk.temp, _hyper_1_168_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_169_chunk
-                                       Output: _hyper_1_169_chunk."time", _hyper_1_169_chunk.temp, _hyper_1_169_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_170_chunk
-                                       Output: _hyper_1_170_chunk."time", _hyper_1_170_chunk.temp, _hyper_1_170_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_171_chunk
-                                       Output: _hyper_1_171_chunk."time", _hyper_1_171_chunk.temp, _hyper_1_171_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_172_chunk
-                                       Output: _hyper_1_172_chunk."time", _hyper_1_172_chunk.temp, _hyper_1_172_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_173_chunk
-                                       Output: _hyper_1_173_chunk."time", _hyper_1_173_chunk.temp, _hyper_1_173_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_174_chunk
-                                       Output: _hyper_1_174_chunk."time", _hyper_1_174_chunk.temp, _hyper_1_174_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_175_chunk
-                                       Output: _hyper_1_175_chunk."time", _hyper_1_175_chunk.temp, _hyper_1_175_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_176_chunk
-                                       Output: _hyper_1_176_chunk."time", _hyper_1_176_chunk.temp, _hyper_1_176_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_177_chunk
-                                       Output: _hyper_1_177_chunk."time", _hyper_1_177_chunk.temp, _hyper_1_177_chunk.device
-                                 ->  Seq Scan on _timescaledb_internal._hyper_1_178_chunk
-                                       Output: _hyper_1_178_chunk."time", _hyper_1_178_chunk.temp, _hyper_1_178_chunk.device
-(366 rows)
-
-INSERT INTO many_partitions_test_1m(time, temp, device)
-SELECT time_bucket('1 minute', time) AS period, avg(temp), device
-FROM many_partitions_test
-GROUP BY period, device;
-SELECT * FROM many_partitions_test_1m ORDER BY time, device LIMIT 10;
-           time           | temp | device 
---------------------------+------+--------
- Wed Dec 31 16:00:00 1969 |    1 | 1
- Wed Dec 31 16:00:00 1969 |   10 | 10
- Wed Dec 31 16:00:00 1969 |   11 | 11
- Wed Dec 31 16:00:00 1969 |   12 | 12
- Wed Dec 31 16:00:00 1969 |   13 | 13
- Wed Dec 31 16:00:00 1969 |   14 | 14
- Wed Dec 31 16:00:00 1969 |   15 | 15
- Wed Dec 31 16:00:00 1969 |   16 | 16
- Wed Dec 31 16:00:00 1969 |   17 | 17
- Wed Dec 31 16:00:00 1969 |   18 | 18
-(10 rows)
-
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/insert_returning.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/insert_returning.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/insert_returning.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/insert_returning.out	2023-11-25 05:27:38.965037735 +0000
@@ -1,58 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-CREATE TABLE standard_table (
-    standard_id integer PRIMARY KEY,
-    name text not null
-);
-CREATE TABLE hypertable (
-    time timestamptz not null,
-    name text not null,
-    standard_id integer not null
-);
-select * from create_hypertable('hypertable', 'time');
- hypertable_id | schema_name | table_name | created 
----------------+-------------+------------+---------
-             1 | public      | hypertable | t
-(1 row)
-
-INSERT INTO standard_table (standard_id, name)
-VALUES (1, 'standard_1');
-INSERT INTO hypertable (time, name, standard_id)
-VALUES ('2021-01-01 01:01:01+00', 'hypertable_1', 1);
-INSERT INTO hypertable (time, name, standard_id)
-VALUES ('2022-02-02 02:02:02+00', 'hypertable_2', 1)
-RETURNING *, EXISTS (
-  SELECT *
-  FROM standard_table
-  WHERE standard_table.standard_id = hypertable.standard_id
-);
-             time             |     name     | standard_id | exists 
-------------------------------+--------------+-------------+--------
- Tue Feb 01 18:02:02 2022 PST | hypertable_2 |           1 | t
-(1 row)
-
-INSERT INTO hypertable (time, name, standard_id)
-VALUES ('2023-03-03 03:03:03+00', 'hypertable_3', 1)
-RETURNING *, EXISTS (
-  SELECT *
-  FROM standard_table
-  WHERE standard_table.standard_id = hypertable.standard_id
-);
-             time             |     name     | standard_id | exists 
-------------------------------+--------------+-------------+--------
- Thu Mar 02 19:03:03 2023 PST | hypertable_3 |           1 | t
-(1 row)
-
-INSERT INTO hypertable (time, name, standard_id)
-VALUES ('2024-04-04 04:04:04+00', 'hypertable_4', 2)
-RETURNING *, EXISTS (
-  SELECT *
-  FROM standard_table
-  WHERE standard_table.standard_id = hypertable.standard_id
-);
-             time             |     name     | standard_id | exists 
-------------------------------+--------------+-------------+--------
- Wed Apr 03 21:04:04 2024 PDT | hypertable_4 |           2 | f
-(1 row)
-
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/insert_single.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/insert_single.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/insert_single.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/insert_single.out	2023-11-25 05:27:38.957037758 +0000
@@ -1,563 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-\ir include/insert_single.sql
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-CREATE TABLE PUBLIC."one_Partition" (
-  "timeCustom" BIGINT NOT NULL,
-  device_id TEXT NOT NULL,
-  series_0 DOUBLE PRECISION NULL,
-  series_1 DOUBLE PRECISION NULL,
-  series_2 DOUBLE PRECISION NULL,
-  series_bool BOOLEAN NULL
-);
-CREATE INDEX ON PUBLIC."one_Partition" (device_id, "timeCustom" DESC NULLS LAST) WHERE device_id IS NOT NULL;
-CREATE INDEX ON PUBLIC."one_Partition" ("timeCustom" DESC NULLS LAST, series_0) WHERE series_0 IS NOT NULL;
-CREATE INDEX ON PUBLIC."one_Partition" ("timeCustom" DESC NULLS LAST, series_1)  WHERE series_1 IS NOT NULL;
-CREATE INDEX ON PUBLIC."one_Partition" ("timeCustom" DESC NULLS LAST, series_2) WHERE series_2 IS NOT NULL;
-CREATE INDEX ON PUBLIC."one_Partition" ("timeCustom" DESC NULLS LAST, series_bool) WHERE series_bool IS NOT NULL;
-\c :DBNAME :ROLE_SUPERUSER
-CREATE SCHEMA "one_Partition" AUTHORIZATION :ROLE_DEFAULT_PERM_USER;
-\c :DBNAME :ROLE_DEFAULT_PERM_USER;
-SELECT * FROM create_hypertable('"public"."one_Partition"', 'timeCustom', associated_schema_name=>'one_Partition', chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
- hypertable_id | schema_name |  table_name   | created 
----------------+-------------+---------------+---------
-             1 | public      | one_Partition | t
-(1 row)
-
---output command tags
-\set QUIET off
-BEGIN;
-BEGIN
-\COPY "one_Partition" FROM 'data/ds1_dev1_1.tsv' NULL AS '';
-COPY 7
-COMMIT;
-COMMIT
-INSERT INTO "one_Partition"("timeCustom", device_id, series_0, series_1) VALUES
-(1257987600000000000, 'dev1', 1.5, 1),
-(1257987600000000000, 'dev1', 1.5, 2),
-(1257894000000000000, 'dev2', 1.5, 1),
-(1257894002000000000, 'dev1', 2.5, 3);
-INSERT 0 4
-INSERT INTO "one_Partition"("timeCustom", device_id, series_0, series_1) VALUES
-(1257894000000000000, 'dev2', 1.5, 2);
-INSERT 0 1
-\set QUIET on
-SELECT * FROM test.show_columnsp('"one_Partition".%');
-                                  Relation                                   | Kind |   Column    |   Column type    | NotNull 
------------------------------------------------------------------------------+------+-------------+------------------+---------
- "one_Partition"._hyper_1_1_chunk                                            | r    | timeCustom  | bigint           | t
- "one_Partition"._hyper_1_1_chunk                                            | r    | device_id   | text             | t
- "one_Partition"._hyper_1_1_chunk                                            | r    | series_0    | double precision | f
- "one_Partition"._hyper_1_1_chunk                                            | r    | series_1    | double precision | f
- "one_Partition"._hyper_1_1_chunk                                            | r    | series_2    | double precision | f
- "one_Partition"._hyper_1_1_chunk                                            | r    | series_bool | boolean          | f
- "one_Partition"."_hyper_1_1_chunk_one_Partition_device_id_timeCustom_idx"   | i    | device_id   | text             | f
- "one_Partition"."_hyper_1_1_chunk_one_Partition_device_id_timeCustom_idx"   | i    | timeCustom  | bigint           | f
- "one_Partition"."_hyper_1_1_chunk_one_Partition_timeCustom_idx"             | i    | timeCustom  | bigint           | f
- "one_Partition"."_hyper_1_1_chunk_one_Partition_timeCustom_series_0_idx"    | i    | timeCustom  | bigint           | f
- "one_Partition"."_hyper_1_1_chunk_one_Partition_timeCustom_series_0_idx"    | i    | series_0    | double precision | f
- "one_Partition"."_hyper_1_1_chunk_one_Partition_timeCustom_series_1_idx"    | i    | timeCustom  | bigint           | f
- "one_Partition"."_hyper_1_1_chunk_one_Partition_timeCustom_series_1_idx"    | i    | series_1    | double precision | f
- "one_Partition"."_hyper_1_1_chunk_one_Partition_timeCustom_series_2_idx"    | i    | timeCustom  | bigint           | f
- "one_Partition"."_hyper_1_1_chunk_one_Partition_timeCustom_series_2_idx"    | i    | series_2    | double precision | f
- "one_Partition"."_hyper_1_1_chunk_one_Partition_timeCustom_series_bool_idx" | i    | timeCustom  | bigint           | f
- "one_Partition"."_hyper_1_1_chunk_one_Partition_timeCustom_series_bool_idx" | i    | series_bool | boolean          | f
- "one_Partition"._hyper_1_2_chunk                                            | r    | timeCustom  | bigint           | t
- "one_Partition"._hyper_1_2_chunk                                            | r    | device_id   | text             | t
- "one_Partition"._hyper_1_2_chunk                                            | r    | series_0    | double precision | f
- "one_Partition"._hyper_1_2_chunk                                            | r    | series_1    | double precision | f
- "one_Partition"._hyper_1_2_chunk                                            | r    | series_2    | double precision | f
- "one_Partition"._hyper_1_2_chunk                                            | r    | series_bool | boolean          | f
- "one_Partition"."_hyper_1_2_chunk_one_Partition_device_id_timeCustom_idx"   | i    | device_id   | text             | f
- "one_Partition"."_hyper_1_2_chunk_one_Partition_device_id_timeCustom_idx"   | i    | timeCustom  | bigint           | f
- "one_Partition"."_hyper_1_2_chunk_one_Partition_timeCustom_idx"             | i    | timeCustom  | bigint           | f
- "one_Partition"."_hyper_1_2_chunk_one_Partition_timeCustom_series_0_idx"    | i    | timeCustom  | bigint           | f
- "one_Partition"."_hyper_1_2_chunk_one_Partition_timeCustom_series_0_idx"    | i    | series_0    | double precision | f
- "one_Partition"."_hyper_1_2_chunk_one_Partition_timeCustom_series_1_idx"    | i    | timeCustom  | bigint           | f
- "one_Partition"."_hyper_1_2_chunk_one_Partition_timeCustom_series_1_idx"    | i    | series_1    | double precision | f
- "one_Partition"."_hyper_1_2_chunk_one_Partition_timeCustom_series_2_idx"    | i    | timeCustom  | bigint           | f
- "one_Partition"."_hyper_1_2_chunk_one_Partition_timeCustom_series_2_idx"    | i    | series_2    | double precision | f
- "one_Partition"."_hyper_1_2_chunk_one_Partition_timeCustom_series_bool_idx" | i    | timeCustom  | bigint           | f
- "one_Partition"."_hyper_1_2_chunk_one_Partition_timeCustom_series_bool_idx" | i    | series_bool | boolean          | f
- "one_Partition"._hyper_1_3_chunk                                            | r    | timeCustom  | bigint           | t
- "one_Partition"._hyper_1_3_chunk                                            | r    | device_id   | text             | t
- "one_Partition"._hyper_1_3_chunk                                            | r    | series_0    | double precision | f
- "one_Partition"._hyper_1_3_chunk                                            | r    | series_1    | double precision | f
- "one_Partition"._hyper_1_3_chunk                                            | r    | series_2    | double precision | f
- "one_Partition"._hyper_1_3_chunk                                            | r    | series_bool | boolean          | f
- "one_Partition"."_hyper_1_3_chunk_one_Partition_device_id_timeCustom_idx"   | i    | device_id   | text             | f
- "one_Partition"."_hyper_1_3_chunk_one_Partition_device_id_timeCustom_idx"   | i    | timeCustom  | bigint           | f
- "one_Partition"."_hyper_1_3_chunk_one_Partition_timeCustom_idx"             | i    | timeCustom  | bigint           | f
- "one_Partition"."_hyper_1_3_chunk_one_Partition_timeCustom_series_0_idx"    | i    | timeCustom  | bigint           | f
- "one_Partition"."_hyper_1_3_chunk_one_Partition_timeCustom_series_0_idx"    | i    | series_0    | double precision | f
- "one_Partition"."_hyper_1_3_chunk_one_Partition_timeCustom_series_1_idx"    | i    | timeCustom  | bigint           | f
- "one_Partition"."_hyper_1_3_chunk_one_Partition_timeCustom_series_1_idx"    | i    | series_1    | double precision | f
- "one_Partition"."_hyper_1_3_chunk_one_Partition_timeCustom_series_2_idx"    | i    | timeCustom  | bigint           | f
- "one_Partition"."_hyper_1_3_chunk_one_Partition_timeCustom_series_2_idx"    | i    | series_2    | double precision | f
- "one_Partition"."_hyper_1_3_chunk_one_Partition_timeCustom_series_bool_idx" | i    | timeCustom  | bigint           | f
- "one_Partition"."_hyper_1_3_chunk_one_Partition_timeCustom_series_bool_idx" | i    | series_bool | boolean          | f
-(51 rows)
-
-SELECT * FROM "one_Partition" ORDER BY "timeCustom", device_id, series_0, series_1, series_2;
-     timeCustom      | device_id | series_0 | series_1 | series_2 | series_bool 
----------------------+-----------+----------+----------+----------+-------------
- 1257894000000000000 | dev1      |      1.5 |        1 |        2 | t
- 1257894000000000000 | dev1      |      1.5 |        2 |          | 
- 1257894000000000000 | dev2      |      1.5 |        1 |          | 
- 1257894000000000000 | dev2      |      1.5 |        2 |          | 
- 1257894000000001000 | dev1      |      2.5 |        3 |          | 
- 1257894001000000000 | dev1      |      3.5 |        4 |          | 
- 1257894002000000000 | dev1      |      2.5 |        3 |          | 
- 1257894002000000000 | dev1      |      5.5 |        6 |          | t
- 1257894002000000000 | dev1      |      5.5 |        7 |          | f
- 1257897600000000000 | dev1      |      4.5 |        5 |          | f
- 1257987600000000000 | dev1      |      1.5 |        1 |          | 
- 1257987600000000000 | dev1      |      1.5 |        2 |          | 
-(12 rows)
-
---test that we can insert data into a 1-dimensional table (only time partitioning)
-CREATE TABLE "1dim"(time timestamp PRIMARY KEY, temp float);
-SELECT create_hypertable('"1dim"', 'time');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
- create_hypertable 
--------------------
- (2,public,1dim,t)
-(1 row)
-
-INSERT INTO "1dim" VALUES('2017-01-20T09:00:01', 22.5) RETURNING *;
-           time           | temp 
---------------------------+------
- Fri Jan 20 09:00:01 2017 | 22.5
-(1 row)
-
-INSERT INTO "1dim" VALUES('2017-01-20T09:00:21', 21.2);
-INSERT INTO "1dim" VALUES('2017-01-20T09:00:47', 25.1);
-SELECT * FROM "1dim";
-           time           | temp 
---------------------------+------
- Fri Jan 20 09:00:01 2017 | 22.5
- Fri Jan 20 09:00:21 2017 | 21.2
- Fri Jan 20 09:00:47 2017 | 25.1
-(3 rows)
-
-CREATE TABLE regular_table (time timestamp, temp float);
-INSERT INTO regular_table SELECT * FROM "1dim";
-SELECT * FROM regular_table;
-           time           | temp 
---------------------------+------
- Fri Jan 20 09:00:01 2017 | 22.5
- Fri Jan 20 09:00:21 2017 | 21.2
- Fri Jan 20 09:00:47 2017 | 25.1
-(3 rows)
-
-TRUNCATE TABLE regular_table;
-INSERT INTO regular_table VALUES('2017-01-20T09:00:59', 29.2);
-INSERT INTO "1dim" SELECT * FROM regular_table;
-SELECT * FROM "1dim";
-           time           | temp 
---------------------------+------
- Fri Jan 20 09:00:01 2017 | 22.5
- Fri Jan 20 09:00:21 2017 | 21.2
- Fri Jan 20 09:00:47 2017 | 25.1
- Fri Jan 20 09:00:59 2017 | 29.2
-(4 rows)
-
-SELECT "1dim" FROM "1dim";
-               1dim                
------------------------------------
- ("Fri Jan 20 09:00:01 2017",22.5)
- ("Fri Jan 20 09:00:21 2017",21.2)
- ("Fri Jan 20 09:00:47 2017",25.1)
- ("Fri Jan 20 09:00:59 2017",29.2)
-(4 rows)
-
---test that we can insert pre-1970 dates
-CREATE TABLE "1dim_pre1970"(time timestamp PRIMARY KEY, temp float);
-SELECT create_hypertable('"1dim_pre1970"', 'time', chunk_time_interval=> INTERVAL '1 Month');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-     create_hypertable     
----------------------------
- (3,public,1dim_pre1970,t)
-(1 row)
-
-INSERT INTO "1dim_pre1970" VALUES('1969-12-01T19:00:00', 21.2);
-INSERT INTO "1dim_pre1970" VALUES('1969-12-20T09:00:00', 25.1);
-INSERT INTO "1dim_pre1970" VALUES('1970-01-20T09:00:00', 26.6);
-INSERT INTO "1dim_pre1970" VALUES('1969-02-20T09:00:00', 29.9);
---should show warning
-BEGIN;
-CREATE TABLE "1dim_usec_interval"(time timestamp PRIMARY KEY, temp float);
-SELECT create_hypertable('"1dim_usec_interval"', 'time', chunk_time_interval=> 10);
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-WARNING:  unexpected interval: smaller than one second
-        create_hypertable        
----------------------------------
- (4,public,1dim_usec_interval,t)
-(1 row)
-
-INSERT INTO "1dim_usec_interval" VALUES('1969-12-01T19:00:00', 21.2);
-ROLLBACK;
-CREATE TABLE "1dim_usec_interval"(time timestamp PRIMARY KEY, temp float);
-SELECT create_hypertable('"1dim_usec_interval"', 'time', chunk_time_interval=> 1000000);
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-        create_hypertable        
----------------------------------
- (5,public,1dim_usec_interval,t)
-(1 row)
-
-INSERT INTO "1dim_usec_interval" VALUES('1969-12-01T19:00:00', 21.2);
-CREATE TABLE "1dim_neg"(time INTEGER, temp float);
-SELECT create_hypertable('"1dim_neg"', 'time', chunk_time_interval=>10);
-NOTICE:  adding not-null constraint to column "time"
-   create_hypertable   
------------------------
- (6,public,1dim_neg,t)
-(1 row)
-
-INSERT INTO "1dim_neg" VALUES (-20, 21.2);
-INSERT INTO "1dim_neg" VALUES (-19, 21.2);
-INSERT INTO "1dim_neg" VALUES (-1, 21.2);
-INSERT INTO "1dim_neg" VALUES (0, 21.2);
-INSERT INTO "1dim_neg" VALUES (1, 21.2);
-INSERT INTO "1dim_neg" VALUES (19, 21.2);
-INSERT INTO "1dim_neg" VALUES (20, 21.2);
-SELECT * FROM "1dim_pre1970";
-           time           | temp 
---------------------------+------
- Mon Dec 01 19:00:00 1969 | 21.2
- Sat Dec 20 09:00:00 1969 | 25.1
- Tue Jan 20 09:00:00 1970 | 26.6
- Thu Feb 20 09:00:00 1969 | 29.9
-(4 rows)
-
-SELECT * FROM "1dim_neg";
- time | temp 
-------+------
-  -20 | 21.2
-  -19 | 21.2
-   -1 | 21.2
-    0 | 21.2
-    1 | 21.2
-   19 | 21.2
-   20 | 21.2
-(7 rows)
-
-SELECT * FROM _timescaledb_catalog.chunk;
- id | hypertable_id |      schema_name      |    table_name     | compressed_chunk_id | dropped | status | osm_chunk 
-----+---------------+-----------------------+-------------------+---------------------+---------+--------+-----------
-  1 |             1 | one_Partition         | _hyper_1_1_chunk  |                     | f       |      0 | f
-  2 |             1 | one_Partition         | _hyper_1_2_chunk  |                     | f       |      0 | f
-  3 |             1 | one_Partition         | _hyper_1_3_chunk  |                     | f       |      0 | f
-  4 |             2 | _timescaledb_internal | _hyper_2_4_chunk  |                     | f       |      0 | f
-  5 |             3 | _timescaledb_internal | _hyper_3_5_chunk  |                     | f       |      0 | f
-  6 |             3 | _timescaledb_internal | _hyper_3_6_chunk  |                     | f       |      0 | f
-  7 |             3 | _timescaledb_internal | _hyper_3_7_chunk  |                     | f       |      0 | f
-  8 |             3 | _timescaledb_internal | _hyper_3_8_chunk  |                     | f       |      0 | f
- 10 |             5 | _timescaledb_internal | _hyper_5_10_chunk |                     | f       |      0 | f
- 11 |             6 | _timescaledb_internal | _hyper_6_11_chunk |                     | f       |      0 | f
- 12 |             6 | _timescaledb_internal | _hyper_6_12_chunk |                     | f       |      0 | f
- 13 |             6 | _timescaledb_internal | _hyper_6_13_chunk |                     | f       |      0 | f
- 14 |             6 | _timescaledb_internal | _hyper_6_14_chunk |                     | f       |      0 | f
- 15 |             6 | _timescaledb_internal | _hyper_6_15_chunk |                     | f       |      0 | f
-(14 rows)
-
-SELECT * FROM _timescaledb_catalog.dimension_slice;
- id | dimension_id |     range_start     |      range_end      
-----+--------------+---------------------+---------------------
-  1 |            1 | 1257892416000000000 | 1257895008000000000
-  2 |            1 | 1257897600000000000 | 1257900192000000000
-  3 |            1 | 1257985728000000000 | 1257988320000000000
-  4 |            2 |    1484784000000000 |    1485388800000000
-  5 |            3 |      -5184000000000 |      -2592000000000
-  6 |            3 |      -2592000000000 |                   0
-  7 |            3 |                   0 |       2592000000000
-  8 |            3 |     -28512000000000 |     -25920000000000
- 10 |            5 |      -2610000000000 |      -2609999000000
- 11 |            6 |                 -20 |                 -10
- 12 |            6 |                 -10 |                   0
- 13 |            6 |                   0 |                  10
- 14 |            6 |                  10 |                  20
- 15 |            6 |                  20 |                  30
-(14 rows)
-
--- Create a three-dimensional table
-CREATE TABLE "3dim" (time timestamp, temp float, device text, location text);
-SELECT create_hypertable('"3dim"', 'time', 'device', 2);
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
- create_hypertable 
--------------------
- (7,public,3dim,t)
-(1 row)
-
-SELECT add_dimension('"3dim"', 'location', 2);
-       add_dimension        
-----------------------------
- (9,public,3dim,location,t)
-(1 row)
-
-INSERT INTO "3dim" VALUES('2017-01-20T09:00:01', 22.5, 'blue', 'nyc');
-INSERT INTO "3dim" VALUES('2017-01-20T09:00:21', 21.2, 'brown', 'sthlm');
-INSERT INTO "3dim" VALUES('2017-01-20T09:00:47', 25.1, 'yellow', 'la');
---show the constraints on the three-dimensional chunk
-SELECT * FROM test.show_constraints('_timescaledb_internal._hyper_7_16_chunk');
-  Constraint   | Type |  Columns   | Index |                                                                     Expr                                                                     | Deferrable | Deferred | Validated 
----------------+------+------------+-------+----------------------------------------------------------------------------------------------------------------------------------------------+------------+----------+-----------
- constraint_16 | c    | {time}     | -     | (("time" >= 'Thu Jan 19 00:00:00 2017'::timestamp without time zone) AND ("time" < 'Thu Jan 26 00:00:00 2017'::timestamp without time zone)) | f          | f        | t
- constraint_17 | c    | {device}   | -     | (_timescaledb_internal.get_partition_hash(device) < 1073741823)                                                                              | f          | f        | t
- constraint_18 | c    | {location} | -     | (_timescaledb_internal.get_partition_hash(location) >= 1073741823)                                                                           | f          | f        | t
-(3 rows)
-
---queries should work in three dimensions
-SELECT * FROM "3dim";
-           time           | temp | device | location 
---------------------------+------+--------+----------
- Fri Jan 20 09:00:01 2017 | 22.5 | blue   | nyc
- Fri Jan 20 09:00:47 2017 | 25.1 | yellow | la
- Fri Jan 20 09:00:21 2017 | 21.2 | brown  | sthlm
-(3 rows)
-
--- test that explain works
-EXPLAIN (COSTS FALSE)
-INSERT INTO "3dim" VALUES('2017-01-21T09:00:01', 32.9, 'green', 'nyc'),
-                         ('2017-01-21T09:00:47', 27.3, 'purple', 'la') RETURNING *;
-                 QUERY PLAN                  
----------------------------------------------
- Custom Scan (HypertableModify)
-   ->  Insert on "3dim"
-         ->  Custom Scan (ChunkDispatch)
-               ->  Values Scan on "*VALUES*"
-(4 rows)
-
-EXPLAIN (COSTS FALSE)
-WITH "3dim_insert" AS (
-     INSERT INTO "3dim" VALUES('2017-01-21T09:01:44', 19.3, 'black', 'la') RETURNING time, temp
-), regular_insert AS (
-   INSERT INTO regular_table VALUES('2017-01-21T10:00:51', 14.3) RETURNING time, temp
-) INSERT INTO "1dim" (SELECT time, temp FROM "3dim_insert" UNION SELECT time, temp FROM regular_insert);
-                                  QUERY PLAN                                  
-------------------------------------------------------------------------------
- Custom Scan (HypertableModify)
-   CTE 3dim_insert
-     ->  Custom Scan (HypertableModify)
-           ->  Insert on "3dim"
-                 ->  Custom Scan (ChunkDispatch)
-                       ->  Result
-   CTE regular_insert
-     ->  Insert on regular_table
-           ->  Result
-   ->  Insert on "1dim"
-         ->  Custom Scan (ChunkDispatch)
-               ->  Unique
-                     ->  Sort
-                           Sort Key: "3dim_insert"."time", "3dim_insert".temp
-                           ->  Append
-                                 ->  CTE Scan on "3dim_insert"
-                                 ->  CTE Scan on regular_insert
-(17 rows)
-
--- test prepared statement INSERT
-PREPARE "1dim_plan" (timestamp, float) AS
-INSERT INTO "1dim" VALUES($1, $2) ON CONFLICT (time) DO NOTHING;
-EXECUTE "1dim_plan" ('2017-04-17 23:35', 31.4);
-EXECUTE "1dim_plan" ('2017-04-17 23:35', 32.6);
--- test prepared statement with generic plan (forced when no parameters)
-PREPARE "1dim_plan_generic" AS
-INSERT INTO "1dim" VALUES('2017-05-18 17:24', 18.3);
-EXECUTE "1dim_plan_generic";
-SELECT * FROM "1dim" ORDER BY time;
-           time           | temp 
---------------------------+------
- Fri Jan 20 09:00:01 2017 | 22.5
- Fri Jan 20 09:00:21 2017 | 21.2
- Fri Jan 20 09:00:47 2017 | 25.1
- Fri Jan 20 09:00:59 2017 | 29.2
- Mon Apr 17 23:35:00 2017 | 31.4
- Thu May 18 17:24:00 2017 | 18.3
-(6 rows)
-
-SELECT * FROM "3dim" ORDER BY (time, device);
-           time           | temp | device | location 
---------------------------+------+--------+----------
- Fri Jan 20 09:00:01 2017 | 22.5 | blue   | nyc
- Fri Jan 20 09:00:21 2017 | 21.2 | brown  | sthlm
- Fri Jan 20 09:00:47 2017 | 25.1 | yellow | la
-(3 rows)
-
--- Test that large intervals and no interval fail for INTEGER
-\set ON_ERROR_STOP 0
-CREATE TABLE "inttime_err"(time INTEGER PRIMARY KEY, temp float);
-SELECT create_hypertable('"inttime_err"', 'time', chunk_time_interval=>2147483648);
-ERROR:  invalid interval: must be between 1 and 2147483647
-SELECT create_hypertable('"inttime_err"', 'time');
-ERROR:  integer dimensions require an explicit interval
-\set ON_ERROR_STOP 1
-SELECT create_hypertable('"inttime_err"', 'time', chunk_time_interval=>2147483647);
-    create_hypertable     
---------------------------
- (8,public,inttime_err,t)
-(1 row)
-
--- Test that large intervals and no interval fail for SMALLINT
-\set ON_ERROR_STOP 0
-CREATE TABLE "smallinttime_err"(time SMALLINT PRIMARY KEY, temp float);
-SELECT create_hypertable('"smallinttime_err"', 'time', chunk_time_interval=>32768);
-ERROR:  invalid interval: must be between 1 and 32767
-SELECT create_hypertable('"smallinttime_err"', 'time');
-ERROR:  integer dimensions require an explicit interval
-\set ON_ERROR_STOP 1
-SELECT create_hypertable('"smallinttime_err"', 'time', chunk_time_interval=>32767);
-       create_hypertable       
--------------------------------
- (9,public,smallinttime_err,t)
-(1 row)
-
---make sure date inserts work even when the timezone changes the
-CREATE TABLE hyper_date(time date, temp float);
-SELECT create_hypertable('"hyper_date"', 'time');
-NOTICE:  adding not-null constraint to column "time"
-    create_hypertable     
---------------------------
- (10,public,hyper_date,t)
-(1 row)
-
-SET timezone=+1;
-INSERT INTO "hyper_date" VALUES('2011-01-26', 22.5);
-RESET timezone;
---make sure timestamp inserts work even when the timezone changes the
-SET timezone = 'UTC';
-CREATE TABLE "test_tz"(time timestamp PRIMARY KEY, temp float);
-SELECT create_hypertable('"test_tz"', 'time', chunk_time_interval=> INTERVAL '1 day');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-   create_hypertable   
------------------------
- (11,public,test_tz,t)
-(1 row)
-
-INSERT INTO "test_tz" VALUES('2017-09-22 10:00:00', 21.2);
-INSERT INTO "test_tz" VALUES('2017-09-21 19:00:00', 21.2);
-SET timezone = 'US/central';
-INSERT INTO "test_tz" VALUES('2017-09-21 19:01:00', 21.2);
-SELECT * FROM test.show_constraints('_timescaledb_internal._hyper_10_20_chunk');
-  Constraint   | Type | Columns | Index |                                Expr                                | Deferrable | Deferred | Validated 
----------------+------+---------+-------+--------------------------------------------------------------------+------------+----------+-----------
- constraint_23 | c    | {time}  | -     | (("time" >= '01-20-2011'::date) AND ("time" < '01-27-2011'::date)) | f          | f        | t
-(1 row)
-
-SELECT * FROM test_tz;
-           time           | temp 
---------------------------+------
- Fri Sep 22 10:00:00 2017 | 21.2
- Thu Sep 21 19:00:00 2017 | 21.2
- Thu Sep 21 19:01:00 2017 | 21.2
-(3 rows)
-
--- test various memory settings --
-SET timescaledb.max_open_chunks_per_insert = 10;
-SET timescaledb.max_cached_chunks_per_hypertable = 10;
-CREATE TABLE "nondefault_mem_settings"(time timestamp PRIMARY KEY, temp float);
-SELECT create_hypertable('"nondefault_mem_settings"', 'time', chunk_time_interval=> INTERVAL '1 Month');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-           create_hypertable           
----------------------------------------
- (12,public,nondefault_mem_settings,t)
-(1 row)
-
-INSERT INTO "nondefault_mem_settings" VALUES('2000-12-01T19:00:00', 21.2);
-INSERT INTO "nondefault_mem_settings" VALUES('2001-12-20T09:00:00', 25.1);
---lowest possible
-SET timescaledb.max_open_chunks_per_insert = 1;
-SET timescaledb.max_cached_chunks_per_hypertable = 1;
-INSERT INTO "nondefault_mem_settings" VALUES
-('2001-01-20T09:00:00', 26.6),
-('2002-02-20T09:00:00', 27.9),
-('2003-02-20T09:00:00', 28.9);
-INSERT INTO "nondefault_mem_settings" VALUES
-('2001-03-20T09:00:00', 30.6),
-('2002-03-20T09:00:00', 31.9),
-('2003-03-20T09:00:00', 32.9);
---warning about mismatched cache sizes
-SET timescaledb.max_open_chunks_per_insert = 100;
-WARNING:  insert cache size is larger than hypertable chunk cache size
-SET timescaledb.max_cached_chunks_per_hypertable = 10;
-WARNING:  insert cache size is larger than hypertable chunk cache size
-INSERT INTO "nondefault_mem_settings" VALUES
-('2001-05-20T09:00:00', 36.6),
-('2002-05-20T09:00:00', 37.9),
-('2003-05-20T09:00:00', 38.9);
---unlimited
-SET timescaledb.max_open_chunks_per_insert = 0;
-SET timescaledb.max_cached_chunks_per_hypertable = 0;
-INSERT INTO "nondefault_mem_settings" VALUES
-('2001-04-20T09:00:00', 33.6),
-('2002-04-20T09:00:00', 34.9),
-('2003-04-20T09:00:00', 35.9);
-SELECT * FROM "nondefault_mem_settings";
-           time           | temp 
---------------------------+------
- Fri Dec 01 19:00:00 2000 | 21.2
- Thu Dec 20 09:00:00 2001 | 25.1
- Sat Jan 20 09:00:00 2001 | 26.6
- Wed Feb 20 09:00:00 2002 | 27.9
- Thu Feb 20 09:00:00 2003 | 28.9
- Tue Mar 20 09:00:00 2001 | 30.6
- Wed Mar 20 09:00:00 2002 | 31.9
- Thu Mar 20 09:00:00 2003 | 32.9
- Sun May 20 09:00:00 2001 | 36.6
- Mon May 20 09:00:00 2002 | 37.9
- Tue May 20 09:00:00 2003 | 38.9
- Fri Apr 20 09:00:00 2001 | 33.6
- Sat Apr 20 09:00:00 2002 | 34.9
- Sun Apr 20 09:00:00 2003 | 35.9
-(14 rows)
-
---test rollback
-BEGIN;
-\set QUIET off
-CREATE TABLE "data_records" ("time" bigint NOT NULL, "value" integer CHECK (VALUE >= 0));
-CREATE TABLE
-SELECT create_hypertable('data_records', 'time', chunk_time_interval => 2592000000);
-     create_hypertable      
-----------------------------
- (13,public,data_records,t)
-(1 row)
-
-INSERT INTO "data_records" ("time", "value") VALUES (0, 1);
-INSERT 0 1
-SAVEPOINT savepoint_1;
-SAVEPOINT
-INSERT INTO "data_records" ("time", "value") VALUES (1, 0);
-INSERT 0 1
-ROLLBACK TO SAVEPOINT savepoint_1;
-ROLLBACK
-INSERT INTO "data_records" ("time", "value") VALUES (2, 1);
-INSERT 0 1
-SAVEPOINT savepoint_2;
-SAVEPOINT
-\set ON_ERROR_STOP 0
-INSERT INTO "data_records" ("time", "value") VALUES (3, -1);
-ERROR:  new row for relation "_hyper_13_37_chunk" violates check constraint "data_records_value_check"
-\set ON_ERROR_STOP 1
-ROLLBACK TO SAVEPOINT savepoint_2;
-ROLLBACK
-INSERT INTO "data_records" ("time", "value") VALUES (4, 1);
-INSERT 0 1
-SAVEPOINT savepoint_3;
-SAVEPOINT
-INSERT INTO "data_records" ("time", "value") VALUES (5, 0);
-INSERT 0 1
-ROLLBACK TO SAVEPOINT savepoint_3;
-ROLLBACK
-SELECT * FROM data_records;
- time | value 
-------+-------
-    0 |     1
-    2 |     1
-    4 |     1
-(3 rows)
-
-\set QUIET on
-ROLLBACK;
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/lateral.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/lateral.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/lateral.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/lateral.out	2023-11-25 05:27:38.957037758 +0000
@@ -1,189 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-CREATE TABLE regular_table(name text, junk text);
-CREATE TABLE ht(time timestamptz NOT NULL, location text);
-SELECT create_hypertable('ht', 'time');
- create_hypertable 
--------------------
- (1,public,ht,t)
-(1 row)
-
-INSERT INTO ht(time) select timestamp 'epoch' + (i * interval '1 second') from generate_series(1, 100) as T(i);
-INSERT INTO regular_table values('name', 'junk');
-SELECT * FROM regular_table ik LEFT JOIN LATERAL (select max(time::timestamptz) from ht s where ik.name='name' and s.time < now()) s on true;
- name | junk |             max              
-------+------+------------------------------
- name | junk | Thu Jan 01 00:01:40 1970 PST
-(1 row)
-
-select * from regular_table ik LEFT JOIN LATERAL (select max(time::timestamptz) from ht s where ik.name='name' and s.time > now()) s on true;
- name | junk | max 
-------+------+-----
- name | junk | 
-(1 row)
-
-DROP TABLE regular_table;
-DROP TABLE ht;
-CREATE TABLE orders(id int, user_id int, time TIMESTAMPTZ NOT NULL);
-SELECT create_hypertable('orders', 'time');
-  create_hypertable  
----------------------
- (2,public,orders,t)
-(1 row)
-
-INSERT INTO orders values(1,1,timestamp 'epoch' + '1 second');
-INSERT INTO orders values(2,1,timestamp 'epoch' + '2 second');
-INSERT INTO orders values(3,1,timestamp 'epoch' + '3 second');
-INSERT INTO orders values(4,2,timestamp 'epoch' + '4 second');
-INSERT INTO orders values(5,1,timestamp 'epoch' + '5 second');
-INSERT INTO orders values(6,3,timestamp 'epoch' + '6 second');
-INSERT INTO orders values(7,1,timestamp 'epoch' + '7 second');
-INSERT INTO orders values(8,4,timestamp 'epoch' + '8 second');
-INSERT INTO orders values(9,2,timestamp 'epoch' + '9 second');
--- Need a LATERAL query with a reference to the upper-level table and
--- with a restriction on time
--- Upper-level table constraint should be a constant in order to trigger
--- creation of a one-time filter in the planner
-SELECT user_id, first_order_time, max_time FROM
-(SELECT user_id, min(time) AS first_order_time FROM orders GROUP BY user_id) o1
-LEFT JOIN LATERAL
-(SELECT max(time) AS max_time FROM orders WHERE o1.user_id = '2' AND time > now()) o2 ON true
-ORDER BY user_id, first_order_time, max_time;
- user_id |       first_order_time       | max_time 
----------+------------------------------+----------
-       1 | Thu Jan 01 00:00:01 1970 PST | 
-       2 | Thu Jan 01 00:00:04 1970 PST | 
-       3 | Thu Jan 01 00:00:06 1970 PST | 
-       4 | Thu Jan 01 00:00:08 1970 PST | 
-(4 rows)
-
-SELECT user_id, first_order_time, max_time FROM
-(SELECT user_id, min(time) AS first_order_time FROM orders GROUP BY user_id) o1
-LEFT JOIN LATERAL
-(SELECT max(time) AS max_time FROM orders WHERE o1.user_id = '2' AND time < now()) o2 ON true
-ORDER BY user_id, first_order_time, max_time;
- user_id |       first_order_time       |           max_time           
----------+------------------------------+------------------------------
-       1 | Thu Jan 01 00:00:01 1970 PST | 
-       2 | Thu Jan 01 00:00:04 1970 PST | Thu Jan 01 00:00:09 1970 PST
-       3 | Thu Jan 01 00:00:06 1970 PST | 
-       4 | Thu Jan 01 00:00:08 1970 PST | 
-(4 rows)
-
--- Nested LATERALs
-SELECT user_id, first_order_time, time1, min_time FROM
-(SELECT user_id, min(time) AS first_order_time FROM orders GROUP BY user_id) o1
-LEFT JOIN LATERAL
-(SELECT user_id as o2user_id, time AS time1 FROM orders WHERE o1.user_id = '2' AND time < now()) o2 ON true
-LEFT JOIN LATERAL
-(SELECT min(time) as min_time FROM orders WHERE o2.o2user_id = '1' AND time < now()) o3 ON true
-ORDER BY user_id, first_order_time, time1, min_time;
- user_id |       first_order_time       |            time1             |           min_time           
----------+------------------------------+------------------------------+------------------------------
-       1 | Thu Jan 01 00:00:01 1970 PST |                              | 
-       2 | Thu Jan 01 00:00:04 1970 PST | Thu Jan 01 00:00:01 1970 PST | Thu Jan 01 00:00:01 1970 PST
-       2 | Thu Jan 01 00:00:04 1970 PST | Thu Jan 01 00:00:02 1970 PST | Thu Jan 01 00:00:01 1970 PST
-       2 | Thu Jan 01 00:00:04 1970 PST | Thu Jan 01 00:00:03 1970 PST | Thu Jan 01 00:00:01 1970 PST
-       2 | Thu Jan 01 00:00:04 1970 PST | Thu Jan 01 00:00:04 1970 PST | 
-       2 | Thu Jan 01 00:00:04 1970 PST | Thu Jan 01 00:00:05 1970 PST | Thu Jan 01 00:00:01 1970 PST
-       2 | Thu Jan 01 00:00:04 1970 PST | Thu Jan 01 00:00:06 1970 PST | 
-       2 | Thu Jan 01 00:00:04 1970 PST | Thu Jan 01 00:00:07 1970 PST | Thu Jan 01 00:00:01 1970 PST
-       2 | Thu Jan 01 00:00:04 1970 PST | Thu Jan 01 00:00:08 1970 PST | 
-       2 | Thu Jan 01 00:00:04 1970 PST | Thu Jan 01 00:00:09 1970 PST | 
-       3 | Thu Jan 01 00:00:06 1970 PST |                              | 
-       4 | Thu Jan 01 00:00:08 1970 PST |                              | 
-(12 rows)
-
--- Cleanup
-DROP TABLE orders;
----- OUTER JOIN tests ---
---github issue 2500
-CREATE TABLE t1_timescale (a int, b int);
-CREATE TABLE t2 (a int, b int);
-SELECT create_hypertable('t1_timescale', 'a', chunk_time_interval=>1000);
-NOTICE:  adding not-null constraint to column "a"
-     create_hypertable     
----------------------------
- (3,public,t1_timescale,t)
-(1 row)
-
-INSERT into t2 values (3, 3), (15 , 15);
-INSERT into t1_timescale select generate_series(5, 25, 1), 77;
-UPDATE t1_timescale SET b = 15 WHERE a = 15;
-SELECT * FROM t1_timescale
-FULL OUTER JOIN  t2 on t1_timescale.b=t2.b and t2.b between 10 and 20
-ORDER BY 1, 2, 3, 4;
- a  | b  | a  | b  
-----+----+----+----
-  5 | 77 |    |   
-  6 | 77 |    |   
-  7 | 77 |    |   
-  8 | 77 |    |   
-  9 | 77 |    |   
- 10 | 77 |    |   
- 11 | 77 |    |   
- 12 | 77 |    |   
- 13 | 77 |    |   
- 14 | 77 |    |   
- 15 | 15 | 15 | 15
- 16 | 77 |    |   
- 17 | 77 |    |   
- 18 | 77 |    |   
- 19 | 77 |    |   
- 20 | 77 |    |   
- 21 | 77 |    |   
- 22 | 77 |    |   
- 23 | 77 |    |   
- 24 | 77 |    |   
- 25 | 77 |    |   
-    |    |  3 |  3
-(22 rows)
-
-SELECT * FROM t1_timescale
-LEFT OUTER JOIN  t2 on t1_timescale.b=t2.b and t2.b between 10 and 20
-WHERE t1_timescale.a=5
-ORDER BY 1, 2, 3, 4;
- a | b  | a | b 
----+----+---+---
- 5 | 77 |   |  
-(1 row)
-
-SELECT * FROM t1_timescale
-RIGHT JOIN  t2 on t1_timescale.b=t2.b and t2.b between 10 and 20
-ORDER BY 1, 2, 3, 4;
- a  | b  | a  | b  
-----+----+----+----
- 15 | 15 | 15 | 15
-    |    |  3 |  3
-(2 rows)
-
-SELECT * FROM t1_timescale
-RIGHT JOIN  t2 on t1_timescale.b=t2.b and t2.b between 10 and 20
-WHERE t1_timescale.a=5
-ORDER BY 1, 2, 3, 4;
- a | b | a | b 
----+---+---+---
-(0 rows)
-
-SELECT * FROM t1_timescale
-LEFT OUTER JOIN  t2 on t1_timescale.a=t2.a and t2.b between 10 and 20
-WHERE t1_timescale.a IN ( 10, 15, 20, 25)
-ORDER BY 1, 2, 3, 4;
- a  | b  | a  | b  
-----+----+----+----
- 10 | 77 |    |   
- 15 | 15 | 15 | 15
- 20 | 77 |    |   
- 25 | 77 |    |   
-(4 rows)
-
-SELECT * FROM t1_timescale
-RIGHT OUTER JOIN  t2 on t1_timescale.a=t2.a and t2.b between 10 and 20
-ORDER BY 1, 2, 3, 4;
- a  | b  | a  | b  
-----+----+----+----
- 15 | 15 | 15 | 15
-    |    |  3 |  3
-(2 rows)
-
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/license.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/license.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/license.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/license.out	2023-11-25 05:27:38.957037758 +0000
@@ -1,89 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-\c :TEST_DBNAME :ROLE_SUPERUSER
-\set ECHO queries
-SHOW timescaledb.license;
- timescaledb.license 
----------------------
- apache
-(1 row)
-
-SELECT _timescaledb_internal.tsl_loaded();
- tsl_loaded 
-------------
- f
-(1 row)
-
-SET timescaledb.license='apache';
-ERROR:  invalid value for parameter "timescaledb.license": "apache"
-DETAIL:  Cannot change a license in a running session.
-HINT:  Change the license in the configuration file or server command line.
-SET timescaledb.license='timescale';
-ERROR:  invalid value for parameter "timescaledb.license": "timescale"
-DETAIL:  Cannot change a license in a running session.
-HINT:  Change the license in the configuration file or server command line.
-SET timescaledb.license='something_else';
-ERROR:  invalid value for parameter "timescaledb.license": "something_else"
-DETAIL:  Unrecognized license type.
-HINT:  Supported license types are 'timescale' or 'apache'.
-SELECT locf(1);
-ERROR:  function "locf" is not supported under the current "apache" license
-HINT:  Upgrade your license to 'timescale' to use this free community feature.
-SELECT interpolate(1);
-ERROR:  function "interpolate" is not supported under the current "apache" license
-HINT:  Upgrade your license to 'timescale' to use this free community feature.
-SELECT time_bucket_gapfill(1,1,1,1);
-ERROR:  function "time_bucket_gapfill" is not supported under the current "apache" license
-HINT:  Upgrade your license to 'timescale' to use this free community feature.
-CREATE OR REPLACE FUNCTION custom_func(jobid int, args jsonb) RETURNS VOID AS $$
-DECLARE
-BEGIN
-END;
-$$ LANGUAGE plpgsql;
-SELECT add_job('custom_func','1h', config:='{"type":"function"}'::jsonb);
-ERROR:  function "add_job" is not supported under the current "apache" license
-HINT:  Upgrade your license to 'timescale' to use this free community feature.
-DROP FUNCTION custom_func;
-CREATE TABLE metrics(time timestamptz NOT NULL, value float);
-SELECT create_hypertable('metrics', 'time');
-  create_hypertable   
-----------------------
- (1,public,metrics,t)
-(1 row)
-
-ALTER TABLE metrics SET (timescaledb.compress);
-ERROR:  functionality not supported under the current "apache" license. Learn more at https://timescale.com/.
-HINT:  To access all features and the best time-series experience, try out Timescale Cloud.
-INSERT INTO metrics
-VALUES ('2022-01-01 00:00:00', 1), ('2022-01-01 01:00:00', 2), ('2022-01-01 02:00:00', 3);
-CREATE MATERIALIZED VIEW metrics_hourly
-WITH (timescaledb.continuous) AS
-SELECT time_bucket(INTERVAL '1 hour', time) AS bucket,
-   AVG(value),
-   MAX(value),
-   MIN(value)
-FROM metrics
-GROUP BY bucket
-WITH NO DATA;
-ERROR:  functionality not supported under the current "apache" license. Learn more at https://timescale.com/.
-HINT:  To access all features and the best time-series experience, try out Timescale Cloud.
-CREATE MATERIALIZED VIEW metrics_hourly
-AS
-SELECT time_bucket(INTERVAL '1 hour', time) AS bucket,
-   AVG(value),
-   MAX(value),
-   MIN(value)
-FROM metrics
-GROUP BY bucket;
-CALL refresh_continuous_aggregate('metrics_hourly', NULL, NULL);
-ERROR:  function "refresh_continuous_aggregate" is not supported under the current "apache" license
-HINT:  Upgrade your license to 'timescale' to use this free community feature.
-SELECT _timescaledb_internal.invalidation_hyper_log_add_entry(0, 0, 0);
-ERROR:  function "invalidation_hyper_log_add_entry" is not supported under the current "apache" license
-HINT:  Upgrade your license to 'timescale' to use this free community feature.
-SELECT _timescaledb_internal.invalidation_cagg_log_add_entry(0, 0, 0);
-ERROR:  function "invalidation_cagg_log_add_entry" is not supported under the current "apache" license
-HINT:  Upgrade your license to 'timescale' to use this free community feature.
-DROP MATERIALIZED VIEW metrics_hourly;
-DROP TABLE metrics;
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/merge.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/merge.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/merge.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/merge.out	2023-11-25 05:27:38.965037735 +0000
@@ -1,159 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
--- Create conditions table with location and temperature
-CREATE TABLE conditions (
-   time        TIMESTAMPTZ       NOT NULL,
-   location    SMALLINT          NOT NULL,
-   temperature DOUBLE PRECISION  NULL
-);
-SELECT create_hypertable(
-  'conditions',
-  'time',
-  chunk_time_interval => INTERVAL '5 seconds');
-    create_hypertable    
--------------------------
- (1,public,conditions,t)
-(1 row)
-
-INSERT INTO conditions
-SELECT time, location, 14 as temperature
-FROM generate_series(
-	'2021-01-01 00:00:00',
-    '2021-01-01 00:00:09',
-    INTERVAL '5 seconds'
-  ) as time,
-generate_series(1,4) as location;
--- Create conditions_updated table with location and temperature
-CREATE TABLE conditions_updated (
-   time        TIMESTAMPTZ       NOT NULL,
-   location    SMALLINT          NOT NULL,
-   temperature DOUBLE PRECISION  NULL
-);
-SELECT create_hypertable(
-  'conditions_updated',
-  'time',
-  chunk_time_interval => INTERVAL '5 seconds');
-        create_hypertable        
----------------------------------
- (2,public,conditions_updated,t)
-(1 row)
-
--- Generate data that overlaps with conditions table
-INSERT INTO conditions_updated
-SELECT time, location, 80 as temperature
-FROM generate_series(
-	'2021-01-01 00:00:05',
-    '2021-01-01 00:00:14',
-    INTERVAL '5 seconds'
-  ) as time,
-generate_series(1,4) as location;
--- Print table/rows/num of chunks
-select * from conditions order by time, location asc;
-             time             | location | temperature 
-------------------------------+----------+-------------
- Fri Jan 01 00:00:00 2021 PST |        1 |          14
- Fri Jan 01 00:00:00 2021 PST |        2 |          14
- Fri Jan 01 00:00:00 2021 PST |        3 |          14
- Fri Jan 01 00:00:00 2021 PST |        4 |          14
- Fri Jan 01 00:00:05 2021 PST |        1 |          14
- Fri Jan 01 00:00:05 2021 PST |        2 |          14
- Fri Jan 01 00:00:05 2021 PST |        3 |          14
- Fri Jan 01 00:00:05 2021 PST |        4 |          14
-(8 rows)
-
-select * from conditions_updated order by time, location asc;
-             time             | location | temperature 
-------------------------------+----------+-------------
- Fri Jan 01 00:00:05 2021 PST |        1 |          80
- Fri Jan 01 00:00:05 2021 PST |        2 |          80
- Fri Jan 01 00:00:05 2021 PST |        3 |          80
- Fri Jan 01 00:00:05 2021 PST |        4 |          80
- Fri Jan 01 00:00:10 2021 PST |        1 |          80
- Fri Jan 01 00:00:10 2021 PST |        2 |          80
- Fri Jan 01 00:00:10 2021 PST |        3 |          80
- Fri Jan 01 00:00:10 2021 PST |        4 |          80
-(8 rows)
-
-select hypertable_name, count(*) as num_of_chunks from timescaledb_information.chunks group by hypertable_name;
-  hypertable_name   | num_of_chunks 
---------------------+---------------
- conditions         |             2
- conditions_updated |             2
-(2 rows)
-
--- Print expected values in the conditions table once conditions_updated is merged into it
--- If a key exists in both tables, we take average of the temperature measured
--- average logic here is a mess but it works
-SELECT COALESCE(c.time, cu.time) as time,
-       COALESCE(c.location, cu.location) as location,
-       (COALESCE(c.temperature, cu.temperature) + COALESCE(cu.temperature, c.temperature))/2 as temperature
-FROM conditions AS c FULL JOIN conditions_updated AS cu
-ON c.time = cu.time AND c.location = cu.location;
-             time             | location | temperature 
-------------------------------+----------+-------------
- Fri Jan 01 00:00:00 2021 PST |        1 |          14
- Fri Jan 01 00:00:00 2021 PST |        2 |          14
- Fri Jan 01 00:00:00 2021 PST |        3 |          14
- Fri Jan 01 00:00:00 2021 PST |        4 |          14
- Fri Jan 01 00:00:05 2021 PST |        1 |          47
- Fri Jan 01 00:00:05 2021 PST |        2 |          47
- Fri Jan 01 00:00:05 2021 PST |        3 |          47
- Fri Jan 01 00:00:05 2021 PST |        4 |          47
- Fri Jan 01 00:00:10 2021 PST |        1 |          80
- Fri Jan 01 00:00:10 2021 PST |        2 |          80
- Fri Jan 01 00:00:10 2021 PST |        3 |          80
- Fri Jan 01 00:00:10 2021 PST |        4 |          80
-(12 rows)
-
--- Test that normal PostgreSQL tables can merge without exceptions
-CREATE TABLE conditions_pg AS SELECT * FROM conditions;
-CREATE TABLE conditions_updated_pg AS SELECT * FROM conditions_updated;
-MERGE INTO conditions_pg c
-USING conditions_updated_pg cu
-ON c.time = cu.time AND c.location = cu.location
-WHEN MATCHED THEN
-UPDATE SET temperature = (c.temperature + cu.temperature)/2
-WHEN NOT MATCHED THEN
-INSERT (time, location, temperature) VALUES (cu.time, cu.location, cu.temperature);
-SELECT * FROM conditions_pg ORDER BY time, location ASC;
-             time             | location | temperature 
-------------------------------+----------+-------------
- Fri Jan 01 00:00:00 2021 PST |        1 |          14
- Fri Jan 01 00:00:00 2021 PST |        2 |          14
- Fri Jan 01 00:00:00 2021 PST |        3 |          14
- Fri Jan 01 00:00:00 2021 PST |        4 |          14
- Fri Jan 01 00:00:05 2021 PST |        1 |          47
- Fri Jan 01 00:00:05 2021 PST |        2 |          47
- Fri Jan 01 00:00:05 2021 PST |        3 |          47
- Fri Jan 01 00:00:05 2021 PST |        4 |          47
- Fri Jan 01 00:00:10 2021 PST |        1 |          80
- Fri Jan 01 00:00:10 2021 PST |        2 |          80
- Fri Jan 01 00:00:10 2021 PST |        3 |          80
- Fri Jan 01 00:00:10 2021 PST |        4 |          80
-(12 rows)
-
--- Merge conditions_updated into conditions
-\set ON_ERROR_STOP 0
-MERGE INTO conditions c
-USING conditions_updated cu
-ON c.time = cu.time AND c.location = cu.location
-WHEN MATCHED THEN
-UPDATE SET temperature = (c.temperature + cu.temperature)/2
-WHEN NOT MATCHED THEN
-INSERT (time, location, temperature) VALUES (cu.time, cu.location, cu.temperature);
-ERROR:  The MERGE command does not support hypertables in this version
-SELECT * FROM conditions ORDER BY time, location ASC;
-             time             | location | temperature 
-------------------------------+----------+-------------
- Fri Jan 01 00:00:00 2021 PST |        1 |          14
- Fri Jan 01 00:00:00 2021 PST |        2 |          14
- Fri Jan 01 00:00:00 2021 PST |        3 |          14
- Fri Jan 01 00:00:00 2021 PST |        4 |          14
- Fri Jan 01 00:00:05 2021 PST |        1 |          14
- Fri Jan 01 00:00:05 2021 PST |        2 |          14
- Fri Jan 01 00:00:05 2021 PST |        3 |          14
- Fri Jan 01 00:00:05 2021 PST |        4 |          14
-(8 rows)
-
-\set ON_ERROR_STOP 1
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/misc.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/misc.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/misc.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/misc.out	2023-11-25 05:27:38.973037712 +0000
@@ -1,106 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
--- This file contains tests for functionality introduced in PG12
-------- TEST 1: Restrictive copy from file
-CREATE TABLE "copy_golden" (
-    "time" bigint NOT NULL,
-    "value" double precision NOT NULL
-);
-\COPY copy_golden (time, value) FROM data/copy_data.csv WITH CSV HEADER
-SELECT * FROM copy_golden ORDER BY TIME;
- time |       value        
-------+--------------------
-    1 |  0.951734602451324
-    2 |  0.717823888640851
-    3 |  0.543408489786088
-    4 |  0.641131274402142
-    5 |   0.12689296528697
-    6 | 0.0126486560329795
-    7 |  0.213605496101081
-    8 |  0.132784110959619
-    9 |  0.381155731156468
-   10 |  0.284836102742702
-   11 |  0.795640022493899
-   12 |  0.631451691035181
-   13 | 0.0958626130595803
-   14 |  0.929304684977978
-   15 |  0.524866581428796
-   16 |  0.919249163009226
-   17 |  0.878917074296623
-   18 |   0.68551931809634
-   19 |  0.594833800103515
-   20 |  0.819584367796779
-   21 |  0.474171321373433
-   22 |  0.938535195309669
-   23 |  0.333933369256556
-   24 |  0.274582070298493
-   25 |  0.602348630782217
-(25 rows)
-
-CREATE TABLE "copy_control" (
-    "time" bigint NOT NULL,
-    "value" double precision NOT NULL
-);
-\COPY copy_control (time, value) FROM data/copy_data.csv WITH CSV HEADER WHERE time > 10;
-SELECT * FROM copy_control ORDER BY TIME;
- time |       value        
-------+--------------------
-   11 |  0.795640022493899
-   12 |  0.631451691035181
-   13 | 0.0958626130595803
-   14 |  0.929304684977978
-   15 |  0.524866581428796
-   16 |  0.919249163009226
-   17 |  0.878917074296623
-   18 |   0.68551931809634
-   19 |  0.594833800103515
-   20 |  0.819584367796779
-   21 |  0.474171321373433
-   22 |  0.938535195309669
-   23 |  0.333933369256556
-   24 |  0.274582070298493
-   25 |  0.602348630782217
-(15 rows)
-
-CREATE TABLE "copy_test" (
-    "time" bigint NOT NULL,
-    "value" double precision NOT NULL
-);
-SELECT create_hypertable('copy_test', 'time', chunk_time_interval => 10);
-   create_hypertable    
-------------------------
- (1,public,copy_test,t)
-(1 row)
-
-\COPY copy_test (time, value) FROM data/copy_data.csv WITH CSV HEADER WHERE time > 10;
-SELECT * FROM copy_test ORDER BY TIME;
- time |       value        
-------+--------------------
-   11 |  0.795640022493899
-   12 |  0.631451691035181
-   13 | 0.0958626130595803
-   14 |  0.929304684977978
-   15 |  0.524866581428796
-   16 |  0.919249163009226
-   17 |  0.878917074296623
-   18 |   0.68551931809634
-   19 |  0.594833800103515
-   20 |  0.819584367796779
-   21 |  0.474171321373433
-   22 |  0.938535195309669
-   23 |  0.333933369256556
-   24 |  0.274582070298493
-   25 |  0.602348630782217
-(15 rows)
-
--- Verify attempting to use subqueries fails the same as non-hypertables
-\set ON_ERROR_STOP 0
-\COPY copy_control (time, value) FROM data/copy_data.csv WITH CSV HEADER WHERE time IN (SELECT time FROM copy_golden);
-ERROR:  cannot use subquery in COPY FROM WHERE condition at character 74
-\COPY copy_test (time, value) FROM data/copy_data.csv WITH CSV HEADER WHERE time IN (SELECT time FROM copy_golden);
-ERROR:  cannot use subquery in COPY FROM WHERE condition at character 71
-\set ON_ERROR_STOP 1
-DROP TABLE copy_golden;
-DROP TABLE copy_control;
-DROP TABLE copy_test;
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/null_exclusion.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/null_exclusion.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/null_exclusion.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/null_exclusion.out	2023-11-25 05:27:38.949037781 +0000
@@ -1,126 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-create table metrics(ts timestamp, id int, value float);
-select create_hypertable('metrics', 'ts');
-WARNING:  column type "timestamp without time zone" used for "ts" does not follow best practices
-NOTICE:  adding not-null constraint to column "ts"
-  create_hypertable   
-----------------------
- (1,public,metrics,t)
-(1 row)
-
-insert into metrics values ('2022-02-02 02:02:02', 2, 2.),
-    ('2023-03-03 03:03:03', 3, 3.);
-analyze metrics;
--- non-const condition
-explain (analyze, costs off, summary off, timing off)
-select * from metrics
-where ts >= (select max(ts) from metrics);
-                                                                    QUERY PLAN                                                                    
---------------------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics (actual rows=1 loops=1)
-   Chunks excluded during runtime: 1
-   InitPlan 2 (returns $1)
-     ->  Result (actual rows=1 loops=1)
-           InitPlan 1 (returns $0)
-             ->  Limit (actual rows=1 loops=1)
-                   ->  Custom Scan (ChunkAppend) on metrics metrics_1 (actual rows=1 loops=1)
-                         Order: metrics_1.ts DESC
-                         ->  Index Only Scan using _hyper_1_2_chunk_metrics_ts_idx on _hyper_1_2_chunk _hyper_1_2_chunk_1 (actual rows=1 loops=1)
-                               Index Cond: (ts IS NOT NULL)
-                               Heap Fetches: 1
-                         ->  Index Only Scan using _hyper_1_1_chunk_metrics_ts_idx on _hyper_1_1_chunk _hyper_1_1_chunk_1 (never executed)
-                               Index Cond: (ts IS NOT NULL)
-                               Heap Fetches: 0
-   ->  Seq Scan on _hyper_1_1_chunk (never executed)
-         Filter: (ts >= $1)
-   ->  Seq Scan on _hyper_1_2_chunk (actual rows=1 loops=1)
-         Filter: (ts >= $1)
-(18 rows)
-
--- two non-const conditions
-explain (analyze, costs off, summary off, timing off)
-select * from metrics
-where ts >= (select max(ts) from metrics)
-    and id = 1;
-                                                                    QUERY PLAN                                                                    
---------------------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics (actual rows=0 loops=1)
-   Chunks excluded during runtime: 1
-   InitPlan 2 (returns $1)
-     ->  Result (actual rows=1 loops=1)
-           InitPlan 1 (returns $0)
-             ->  Limit (actual rows=1 loops=1)
-                   ->  Custom Scan (ChunkAppend) on metrics metrics_1 (actual rows=1 loops=1)
-                         Order: metrics_1.ts DESC
-                         ->  Index Only Scan using _hyper_1_2_chunk_metrics_ts_idx on _hyper_1_2_chunk _hyper_1_2_chunk_1 (actual rows=1 loops=1)
-                               Index Cond: (ts IS NOT NULL)
-                               Heap Fetches: 1
-                         ->  Index Only Scan using _hyper_1_1_chunk_metrics_ts_idx on _hyper_1_1_chunk _hyper_1_1_chunk_1 (never executed)
-                               Index Cond: (ts IS NOT NULL)
-                               Heap Fetches: 0
-   ->  Seq Scan on _hyper_1_1_chunk (never executed)
-         Filter: ((ts >= $1) AND (id = 1))
-   ->  Seq Scan on _hyper_1_2_chunk (actual rows=0 loops=1)
-         Filter: ((ts >= $1) AND (id = 1))
-         Rows Removed by Filter: 1
-(19 rows)
-
--- condition that becomes const null after evaluating the param
-explain (analyze, costs off, summary off, timing off)
-select * from metrics
-where ts >= (select max(ts) from metrics where id = -1);
-                                                                 QUERY PLAN                                                                  
----------------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics (actual rows=0 loops=1)
-   Chunks excluded during runtime: 2
-   InitPlan 2 (returns $1)
-     ->  Result (actual rows=1 loops=1)
-           InitPlan 1 (returns $0)
-             ->  Limit (actual rows=0 loops=1)
-                   ->  Custom Scan (ChunkAppend) on metrics metrics_1 (actual rows=0 loops=1)
-                         Order: metrics_1.ts DESC
-                         ->  Index Scan using _hyper_1_2_chunk_metrics_ts_idx on _hyper_1_2_chunk _hyper_1_2_chunk_1 (actual rows=0 loops=1)
-                               Index Cond: (ts IS NOT NULL)
-                               Filter: (id = '-1'::integer)
-                               Rows Removed by Filter: 1
-                         ->  Index Scan using _hyper_1_1_chunk_metrics_ts_idx on _hyper_1_1_chunk _hyper_1_1_chunk_1 (actual rows=0 loops=1)
-                               Index Cond: (ts IS NOT NULL)
-                               Filter: (id = '-1'::integer)
-                               Rows Removed by Filter: 1
-   ->  Seq Scan on _hyper_1_1_chunk (never executed)
-         Filter: (ts >= $1)
-   ->  Seq Scan on _hyper_1_2_chunk (never executed)
-         Filter: (ts >= $1)
-(20 rows)
-
--- const null condition and some other condition
-explain (analyze, costs off, summary off, timing off)
-select * from metrics
-where ts >= (select max(ts) from metrics where id = -1)
-    and id = 1;
-                                                                 QUERY PLAN                                                                  
----------------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics (actual rows=0 loops=1)
-   Chunks excluded during runtime: 2
-   InitPlan 2 (returns $1)
-     ->  Result (actual rows=1 loops=1)
-           InitPlan 1 (returns $0)
-             ->  Limit (actual rows=0 loops=1)
-                   ->  Custom Scan (ChunkAppend) on metrics metrics_1 (actual rows=0 loops=1)
-                         Order: metrics_1.ts DESC
-                         ->  Index Scan using _hyper_1_2_chunk_metrics_ts_idx on _hyper_1_2_chunk _hyper_1_2_chunk_1 (actual rows=0 loops=1)
-                               Index Cond: (ts IS NOT NULL)
-                               Filter: (id = '-1'::integer)
-                               Rows Removed by Filter: 1
-                         ->  Index Scan using _hyper_1_1_chunk_metrics_ts_idx on _hyper_1_1_chunk _hyper_1_1_chunk_1 (actual rows=0 loops=1)
-                               Index Cond: (ts IS NOT NULL)
-                               Filter: (id = '-1'::integer)
-                               Rows Removed by Filter: 1
-   ->  Seq Scan on _hyper_1_1_chunk (never executed)
-         Filter: ((ts >= $1) AND (id = 1))
-   ->  Seq Scan on _hyper_1_2_chunk (never executed)
-         Filter: ((ts >= $1) AND (id = 1))
-(20 rows)
-
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/parallel-15.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/parallel-15.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/parallel-15.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/parallel-15.out	2023-11-25 05:27:38.949037781 +0000
@@ -1,447 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
---parallel queries require big-ish tables so collect them all here
---so that we need to generate queries only once.
--- output with analyze is not stable because it depends on worker assignment
-\set PREFIX 'EXPLAIN (costs off)'
-\set CHUNK1 _timescaledb_internal._hyper_1_1_chunk
-\set CHUNK2 _timescaledb_internal._hyper_1_2_chunk
-CREATE TABLE test (i int, j double precision, ts timestamp);
-SELECT create_hypertable('test','i',chunk_time_interval:=500000);
-WARNING:  column type "timestamp without time zone" used for "ts" does not follow best practices
-NOTICE:  adding not-null constraint to column "i"
- create_hypertable 
--------------------
- (1,public,test,t)
-(1 row)
-
-INSERT INTO test SELECT x, x+0.1, _timescaledb_internal.to_timestamp(x*1000)  FROM generate_series(0,1000000-1,10) AS x;
-ANALYZE test;
-ALTER TABLE :CHUNK1 SET (parallel_workers=2);
-ALTER TABLE :CHUNK2 SET (parallel_workers=2);
-SET work_mem TO '50MB';
-SET force_parallel_mode = 'on';
-SET max_parallel_workers_per_gather = 4;
-SET parallel_setup_cost TO 0;
-EXPLAIN (costs off) SELECT first(i, j) FROM "test";
-                          QUERY PLAN                           
----------------------------------------------------------------
- Finalize Aggregate
-   ->  Gather
-         Workers Planned: 2
-         ->  Partial Aggregate
-               ->  Parallel Append
-                     ->  Parallel Seq Scan on _hyper_1_1_chunk
-                     ->  Parallel Seq Scan on _hyper_1_2_chunk
-(7 rows)
-
-SELECT first(i, j) FROM "test";
- first 
--------
-     0
-(1 row)
-
-EXPLAIN (costs off) SELECT last(i, j) FROM "test";
-                          QUERY PLAN                           
----------------------------------------------------------------
- Finalize Aggregate
-   ->  Gather
-         Workers Planned: 2
-         ->  Partial Aggregate
-               ->  Parallel Append
-                     ->  Parallel Seq Scan on _hyper_1_1_chunk
-                     ->  Parallel Seq Scan on _hyper_1_2_chunk
-(7 rows)
-
-SELECT last(i, j) FROM "test";
-  last  
---------
- 999990
-(1 row)
-
-EXPLAIN (costs off) SELECT time_bucket('1 second', ts) sec, last(i, j)
-FROM "test"
-GROUP BY sec
-ORDER BY sec
-LIMIT 5;
-                                      QUERY PLAN                                      
---------------------------------------------------------------------------------------
- Gather
-   Workers Planned: 1
-   Single Copy: true
-   ->  Limit
-         ->  Sort
-               Sort Key: (time_bucket('@ 1 sec'::interval, _hyper_1_1_chunk.ts))
-               ->  HashAggregate
-                     Group Key: time_bucket('@ 1 sec'::interval, _hyper_1_1_chunk.ts)
-                     ->  Result
-                           ->  Append
-                                 ->  Seq Scan on _hyper_1_1_chunk
-                                 ->  Seq Scan on _hyper_1_2_chunk
-(12 rows)
-
--- test single copy parallel plan with parallel chunk append
-:PREFIX SELECT time_bucket('1 second', ts) sec, last(i, j)
-FROM "test"
-WHERE length(version()) > 0
-GROUP BY sec
-ORDER BY sec
-LIMIT 5;
-                                      QUERY PLAN                                      
---------------------------------------------------------------------------------------
- Gather
-   Workers Planned: 1
-   Single Copy: true
-   ->  Limit
-         ->  Sort
-               Sort Key: (time_bucket('@ 1 sec'::interval, test.ts))
-               ->  HashAggregate
-                     Group Key: time_bucket('@ 1 sec'::interval, test.ts)
-                     ->  Result
-                           ->  Result
-                                 One-Time Filter: (length(version()) > 0)
-                                 ->  Custom Scan (ChunkAppend) on test
-                                       Chunks excluded during startup: 0
-                                       ->  Result
-                                             One-Time Filter: (length(version()) > 0)
-                                             ->  Seq Scan on _hyper_1_1_chunk
-                                       ->  Result
-                                             One-Time Filter: (length(version()) > 0)
-                                             ->  Seq Scan on _hyper_1_2_chunk
-(19 rows)
-
-SELECT time_bucket('1 second', ts) sec, last(i, j)
-FROM "test"
-GROUP BY sec
-ORDER BY sec
-LIMIT 5;
-           sec            | last 
---------------------------+------
- Wed Dec 31 16:00:00 1969 |  990
- Wed Dec 31 16:00:01 1969 | 1990
- Wed Dec 31 16:00:02 1969 | 2990
- Wed Dec 31 16:00:03 1969 | 3990
- Wed Dec 31 16:00:04 1969 | 4990
-(5 rows)
-
---test variants of histogram
-EXPLAIN (costs off) SELECT histogram(i, 1, 1000000, 2) FROM "test";
-                          QUERY PLAN                           
----------------------------------------------------------------
- Finalize Aggregate
-   ->  Gather
-         Workers Planned: 2
-         ->  Partial Aggregate
-               ->  Parallel Append
-                     ->  Parallel Seq Scan on _hyper_1_1_chunk
-                     ->  Parallel Seq Scan on _hyper_1_2_chunk
-(7 rows)
-
-SELECT histogram(i, 1, 1000000, 2) FROM "test";
-     histogram     
--------------------
- {1,50000,49999,0}
-(1 row)
-
-EXPLAIN (costs off) SELECT histogram(i, 1,1000001,10) FROM "test";
-                          QUERY PLAN                           
----------------------------------------------------------------
- Finalize Aggregate
-   ->  Gather
-         Workers Planned: 2
-         ->  Partial Aggregate
-               ->  Parallel Append
-                     ->  Parallel Seq Scan on _hyper_1_1_chunk
-                     ->  Parallel Seq Scan on _hyper_1_2_chunk
-(7 rows)
-
-SELECT histogram(i, 1, 1000001, 10) FROM "test";
-                            histogram                             
-------------------------------------------------------------------
- {1,10000,10000,10000,10000,10000,10000,10000,10000,10000,9999,0}
-(1 row)
-
-EXPLAIN (costs off) SELECT histogram(i, 0,100000,5) FROM "test";
-                          QUERY PLAN                           
----------------------------------------------------------------
- Finalize Aggregate
-   ->  Gather
-         Workers Planned: 2
-         ->  Partial Aggregate
-               ->  Parallel Append
-                     ->  Parallel Seq Scan on _hyper_1_1_chunk
-                     ->  Parallel Seq Scan on _hyper_1_2_chunk
-(7 rows)
-
-SELECT histogram(i, 0, 100000, 5) FROM "test";
-             histogram              
-------------------------------------
- {0,2000,2000,2000,2000,2000,90000}
-(1 row)
-
-EXPLAIN (costs off) SELECT histogram(i, 10,100000,5) FROM "test";
-                          QUERY PLAN                           
----------------------------------------------------------------
- Finalize Aggregate
-   ->  Gather
-         Workers Planned: 2
-         ->  Partial Aggregate
-               ->  Parallel Append
-                     ->  Parallel Seq Scan on _hyper_1_1_chunk
-                     ->  Parallel Seq Scan on _hyper_1_2_chunk
-(7 rows)
-
-SELECT histogram(i, 10, 100000, 5) FROM "test";
-             histogram              
-------------------------------------
- {1,2000,2000,2000,2000,1999,90000}
-(1 row)
-
-EXPLAIN (costs off) SELECT histogram(NULL, 10,100000,5) FROM "test" WHERE  i = coalesce(-1,j);
-                                     QUERY PLAN                                     
-------------------------------------------------------------------------------------
- Finalize Aggregate
-   ->  Gather
-         Workers Planned: 2
-         ->  Partial Aggregate
-               ->  Parallel Append
-                     ->  Parallel Seq Scan on _hyper_1_1_chunk
-                           Filter: ((i)::double precision = '-1'::double precision)
-                     ->  Parallel Seq Scan on _hyper_1_2_chunk
-                           Filter: ((i)::double precision = '-1'::double precision)
-(9 rows)
-
-SELECT histogram(NULL, 10,100000,5) FROM "test" WHERE  i = coalesce(-1,j);
- histogram 
------------
- 
-(1 row)
-
--- test parallel ChunkAppend
-:PREFIX SELECT i FROM "test" WHERE length(version()) > 0;
-                          QUERY PLAN                          
---------------------------------------------------------------
- Gather
-   Workers Planned: 1
-   Single Copy: true
-   ->  Result
-         One-Time Filter: (length(version()) > 0)
-         ->  Custom Scan (ChunkAppend) on test
-               Chunks excluded during startup: 0
-               ->  Result
-                     One-Time Filter: (length(version()) > 0)
-                     ->  Seq Scan on _hyper_1_1_chunk
-               ->  Result
-                     One-Time Filter: (length(version()) > 0)
-                     ->  Seq Scan on _hyper_1_2_chunk
-(13 rows)
-
--- test worker assignment
--- first chunk should have 1 worker and second chunk should have 2
-SET max_parallel_workers_per_gather TO 2;
-:PREFIX SELECT count(*) FROM "test" WHERE i >= 400000 AND length(version()) > 0;
-                                                     QUERY PLAN                                                     
---------------------------------------------------------------------------------------------------------------------
- Finalize Aggregate
-   ->  Gather
-         Workers Planned: 2
-         ->  Partial Aggregate
-               ->  Result
-                     One-Time Filter: (length(version()) > 0)
-                     ->  Parallel Custom Scan (ChunkAppend) on test
-                           Chunks excluded during startup: 0
-                           ->  Result
-                                 One-Time Filter: (length(version()) > 0)
-                                 ->  Parallel Index Only Scan using _hyper_1_1_chunk_test_i_idx on _hyper_1_1_chunk
-                                       Index Cond: (i >= 400000)
-                           ->  Result
-                                 One-Time Filter: (length(version()) > 0)
-                                 ->  Parallel Seq Scan on _hyper_1_2_chunk
-                                       Filter: (i >= 400000)
-(16 rows)
-
-SELECT count(*) FROM "test" WHERE i >= 400000 AND length(version()) > 0;
- count 
--------
- 60000
-(1 row)
-
--- test worker assignment
--- first chunk should have 2 worker and second chunk should have 1
-:PREFIX SELECT count(*) FROM "test" WHERE i < 600000 AND length(version()) > 0;
-                                                     QUERY PLAN                                                     
---------------------------------------------------------------------------------------------------------------------
- Finalize Aggregate
-   ->  Gather
-         Workers Planned: 2
-         ->  Partial Aggregate
-               ->  Result
-                     One-Time Filter: (length(version()) > 0)
-                     ->  Parallel Custom Scan (ChunkAppend) on test
-                           Chunks excluded during startup: 0
-                           ->  Result
-                                 One-Time Filter: (length(version()) > 0)
-                                 ->  Parallel Index Only Scan using _hyper_1_2_chunk_test_i_idx on _hyper_1_2_chunk
-                                       Index Cond: (i < 600000)
-                           ->  Result
-                                 One-Time Filter: (length(version()) > 0)
-                                 ->  Parallel Seq Scan on _hyper_1_1_chunk
-                                       Filter: (i < 600000)
-(16 rows)
-
-SELECT count(*) FROM "test" WHERE i < 600000 AND length(version()) > 0;
- count 
--------
- 60000
-(1 row)
-
--- test ChunkAppend with # workers < # childs
-SET max_parallel_workers_per_gather TO 1;
-:PREFIX SELECT count(*) FROM "test" WHERE length(version()) > 0;
-                                QUERY PLAN                                 
----------------------------------------------------------------------------
- Finalize Aggregate
-   ->  Gather
-         Workers Planned: 1
-         ->  Partial Aggregate
-               ->  Result
-                     One-Time Filter: (length(version()) > 0)
-                     ->  Parallel Custom Scan (ChunkAppend) on test
-                           Chunks excluded during startup: 0
-                           ->  Result
-                                 One-Time Filter: (length(version()) > 0)
-                                 ->  Parallel Seq Scan on _hyper_1_1_chunk
-                           ->  Result
-                                 One-Time Filter: (length(version()) > 0)
-                                 ->  Parallel Seq Scan on _hyper_1_2_chunk
-(14 rows)
-
-SELECT count(*) FROM "test" WHERE length(version()) > 0;
- count  
---------
- 100000
-(1 row)
-
--- test ChunkAppend with # workers > # childs
-SET max_parallel_workers_per_gather TO 2;
-:PREFIX SELECT count(*) FROM "test" WHERE i >= 500000 AND length(version()) > 0;
-                                QUERY PLAN                                 
----------------------------------------------------------------------------
- Finalize Aggregate
-   ->  Gather
-         Workers Planned: 2
-         ->  Partial Aggregate
-               ->  Result
-                     One-Time Filter: (length(version()) > 0)
-                     ->  Parallel Custom Scan (ChunkAppend) on test
-                           Chunks excluded during startup: 0
-                           ->  Result
-                                 One-Time Filter: (length(version()) > 0)
-                                 ->  Parallel Seq Scan on _hyper_1_2_chunk
-                                       Filter: (i >= 500000)
-(12 rows)
-
-SELECT count(*) FROM "test" WHERE i >= 500000 AND length(version()) > 0;
- count 
--------
- 50000
-(1 row)
-
-RESET max_parallel_workers_per_gather;
--- test partial and non-partial plans
--- these will not be parallel on PG < 11
-ALTER TABLE :CHUNK1 SET (parallel_workers=0);
-ALTER TABLE :CHUNK2 SET (parallel_workers=2);
-:PREFIX SELECT count(*) FROM "test" WHERE i > 400000 AND length(version()) > 0;
-                                                QUERY PLAN                                                 
------------------------------------------------------------------------------------------------------------
- Finalize Aggregate
-   ->  Gather
-         Workers Planned: 2
-         ->  Partial Aggregate
-               ->  Result
-                     One-Time Filter: (length(version()) > 0)
-                     ->  Parallel Custom Scan (ChunkAppend) on test
-                           Chunks excluded during startup: 0
-                           ->  Result
-                                 One-Time Filter: (length(version()) > 0)
-                                 ->  Index Only Scan using _hyper_1_1_chunk_test_i_idx on _hyper_1_1_chunk
-                                       Index Cond: (i > 400000)
-                           ->  Result
-                                 One-Time Filter: (length(version()) > 0)
-                                 ->  Parallel Seq Scan on _hyper_1_2_chunk
-                                       Filter: (i > 400000)
-(16 rows)
-
-ALTER TABLE :CHUNK1 SET (parallel_workers=2);
-ALTER TABLE :CHUNK2 SET (parallel_workers=0);
-:PREFIX SELECT count(*) FROM "test" WHERE i < 600000 AND length(version()) > 0;
-                                                QUERY PLAN                                                 
------------------------------------------------------------------------------------------------------------
- Finalize Aggregate
-   ->  Gather
-         Workers Planned: 2
-         ->  Partial Aggregate
-               ->  Result
-                     One-Time Filter: (length(version()) > 0)
-                     ->  Parallel Custom Scan (ChunkAppend) on test
-                           Chunks excluded during startup: 0
-                           ->  Result
-                                 One-Time Filter: (length(version()) > 0)
-                                 ->  Index Only Scan using _hyper_1_2_chunk_test_i_idx on _hyper_1_2_chunk
-                                       Index Cond: (i < 600000)
-                           ->  Result
-                                 One-Time Filter: (length(version()) > 0)
-                                 ->  Parallel Seq Scan on _hyper_1_1_chunk
-                                       Filter: (i < 600000)
-(16 rows)
-
-ALTER TABLE :CHUNK1 RESET (parallel_workers);
-ALTER TABLE :CHUNK2 RESET (parallel_workers);
--- now() is not marked parallel safe in PostgreSQL < 12 so using now()
--- in a query will prevent parallelism but CURRENT_TIMESTAMP and
--- transaction_timestamp() are marked parallel safe
-:PREFIX SELECT i FROM "test" WHERE ts < CURRENT_TIMESTAMP;
-                   QUERY PLAN                   
-------------------------------------------------
- Gather
-   Workers Planned: 1
-   Single Copy: true
-   ->  Custom Scan (ChunkAppend) on test
-         Chunks excluded during startup: 0
-         ->  Seq Scan on _hyper_1_1_chunk
-               Filter: (ts < CURRENT_TIMESTAMP)
-         ->  Seq Scan on _hyper_1_2_chunk
-               Filter: (ts < CURRENT_TIMESTAMP)
-(9 rows)
-
-:PREFIX SELECT i FROM "test" WHERE ts < transaction_timestamp();
-                      QUERY PLAN                      
-------------------------------------------------------
- Gather
-   Workers Planned: 1
-   Single Copy: true
-   ->  Custom Scan (ChunkAppend) on test
-         Chunks excluded during startup: 0
-         ->  Seq Scan on _hyper_1_1_chunk
-               Filter: (ts < transaction_timestamp())
-         ->  Seq Scan on _hyper_1_2_chunk
-               Filter: (ts < transaction_timestamp())
-(9 rows)
-
--- this won't be parallel query because now() is parallel restricted in PG < 12
-:PREFIX SELECT i FROM "test" WHERE ts < now();
-                QUERY PLAN                 
--------------------------------------------
- Gather
-   Workers Planned: 1
-   Single Copy: true
-   ->  Custom Scan (ChunkAppend) on test
-         Chunks excluded during startup: 0
-         ->  Seq Scan on _hyper_1_1_chunk
-               Filter: (ts < now())
-         ->  Seq Scan on _hyper_1_2_chunk
-               Filter: (ts < now())
-(9 rows)
-
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/partition.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/partition.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/partition.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/partition.out	2023-11-25 05:27:38.961037746 +0000
@@ -1,524 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-CREATE TABLE part_legacy(time timestamptz, temp float, device int);
-SELECT create_hypertable('part_legacy', 'time', 'device', 2, partitioning_func => '_timescaledb_internal.get_partition_for_key');
-NOTICE:  adding not-null constraint to column "time"
-    create_hypertable     
---------------------------
- (1,public,part_legacy,t)
-(1 row)
-
--- Show legacy partitioning function is used
-SELECT * FROM _timescaledb_catalog.dimension;
- id | hypertable_id | column_name |       column_type        | aligned | num_slices | partitioning_func_schema |   partitioning_func   | interval_length | compress_interval_length | integer_now_func_schema | integer_now_func 
-----+---------------+-------------+--------------------------+---------+------------+--------------------------+-----------------------+-----------------+--------------------------+-------------------------+------------------
-  1 |             1 | time        | timestamp with time zone | t       |            |                          |                       |    604800000000 |                          |                         | 
-  2 |             1 | device      | integer                  | f       |          2 | _timescaledb_internal    | get_partition_for_key |                 |                          |                         | 
-(2 rows)
-
-INSERT INTO part_legacy VALUES ('2017-03-22T09:18:23', 23.4, 1);
-INSERT INTO part_legacy VALUES ('2017-03-22T09:18:23', 23.4, 76);
-VACUUM part_legacy;
--- Show two chunks and CHECK constraint with cast
-SELECT * FROM test.show_constraintsp('_timescaledb_internal._hyper_1_%_chunk');
-                 Table                  |  Constraint  | Type | Columns  | Index |                                                                      Expr                                                                      | Deferrable | Deferred | Validated 
-----------------------------------------+--------------+------+----------+-------+------------------------------------------------------------------------------------------------------------------------------------------------+------------+----------+-----------
- _timescaledb_internal._hyper_1_1_chunk | constraint_1 | c    | {time}   | -     | (("time" >= 'Wed Mar 15 17:00:00 2017 PDT'::timestamp with time zone) AND ("time" < 'Wed Mar 22 17:00:00 2017 PDT'::timestamp with time zone)) | f          | f        | t
- _timescaledb_internal._hyper_1_1_chunk | constraint_2 | c    | {device} | -     | (_timescaledb_internal.get_partition_for_key(device) >= 1073741823)                                                                            | f          | f        | t
- _timescaledb_internal._hyper_1_2_chunk | constraint_1 | c    | {time}   | -     | (("time" >= 'Wed Mar 15 17:00:00 2017 PDT'::timestamp with time zone) AND ("time" < 'Wed Mar 22 17:00:00 2017 PDT'::timestamp with time zone)) | f          | f        | t
- _timescaledb_internal._hyper_1_2_chunk | constraint_3 | c    | {device} | -     | (_timescaledb_internal.get_partition_for_key(device) < 1073741823)                                                                             | f          | f        | t
-(4 rows)
-
--- Make sure constraint exclusion works on device column
-BEGIN;
--- For plan stability between versions
-SET LOCAL enable_bitmapscan = false;
-SET LOCAL enable_indexscan = false;
-EXPLAIN (verbose, costs off)
-SELECT * FROM part_legacy WHERE device = 1;
-                                    QUERY PLAN                                     
------------------------------------------------------------------------------------
- Seq Scan on _timescaledb_internal._hyper_1_1_chunk
-   Output: _hyper_1_1_chunk."time", _hyper_1_1_chunk.temp, _hyper_1_1_chunk.device
-   Filter: (_hyper_1_1_chunk.device = 1)
-(3 rows)
-
-COMMIT;
-CREATE TABLE part_new(time timestamptz, temp float, device int);
-SELECT create_hypertable('part_new', 'time', 'device', 2);
-NOTICE:  adding not-null constraint to column "time"
-   create_hypertable   
------------------------
- (2,public,part_new,t)
-(1 row)
-
-SELECT * FROM _timescaledb_catalog.dimension;
- id | hypertable_id | column_name |       column_type        | aligned | num_slices | partitioning_func_schema |   partitioning_func   | interval_length | compress_interval_length | integer_now_func_schema | integer_now_func 
-----+---------------+-------------+--------------------------+---------+------------+--------------------------+-----------------------+-----------------+--------------------------+-------------------------+------------------
-  1 |             1 | time        | timestamp with time zone | t       |            |                          |                       |    604800000000 |                          |                         | 
-  2 |             1 | device      | integer                  | f       |          2 | _timescaledb_internal    | get_partition_for_key |                 |                          |                         | 
-  3 |             2 | time        | timestamp with time zone | t       |            |                          |                       |    604800000000 |                          |                         | 
-  4 |             2 | device      | integer                  | f       |          2 | _timescaledb_internal    | get_partition_hash    |                 |                          |                         | 
-(4 rows)
-
-INSERT INTO part_new VALUES ('2017-03-22T09:18:23', 23.4, 1);
-INSERT INTO part_new VALUES ('2017-03-22T09:18:23', 23.4, 2);
-VACUUM part_new;
--- Show two chunks and CHECK constraint without cast
-SELECT * FROM test.show_constraintsp('_timescaledb_internal._hyper_2_%_chunk');
-                 Table                  |  Constraint  | Type | Columns  | Index |                                                                      Expr                                                                      | Deferrable | Deferred | Validated 
-----------------------------------------+--------------+------+----------+-------+------------------------------------------------------------------------------------------------------------------------------------------------+------------+----------+-----------
- _timescaledb_internal._hyper_2_3_chunk | constraint_4 | c    | {time}   | -     | (("time" >= 'Wed Mar 15 17:00:00 2017 PDT'::timestamp with time zone) AND ("time" < 'Wed Mar 22 17:00:00 2017 PDT'::timestamp with time zone)) | f          | f        | t
- _timescaledb_internal._hyper_2_3_chunk | constraint_5 | c    | {device} | -     | (_timescaledb_internal.get_partition_hash(device) < 1073741823)                                                                                | f          | f        | t
- _timescaledb_internal._hyper_2_4_chunk | constraint_4 | c    | {time}   | -     | (("time" >= 'Wed Mar 15 17:00:00 2017 PDT'::timestamp with time zone) AND ("time" < 'Wed Mar 22 17:00:00 2017 PDT'::timestamp with time zone)) | f          | f        | t
- _timescaledb_internal._hyper_2_4_chunk | constraint_6 | c    | {device} | -     | (_timescaledb_internal.get_partition_hash(device) >= 1073741823)                                                                               | f          | f        | t
-(4 rows)
-
--- Make sure constraint exclusion works on device column
-BEGIN;
--- For plan stability between versions
-SET LOCAL enable_bitmapscan = false;
-SET LOCAL enable_indexscan = false;
-EXPLAIN (verbose, costs off)
-SELECT * FROM part_new WHERE device = 1;
-                                    QUERY PLAN                                     
------------------------------------------------------------------------------------
- Seq Scan on _timescaledb_internal._hyper_2_3_chunk
-   Output: _hyper_2_3_chunk."time", _hyper_2_3_chunk.temp, _hyper_2_3_chunk.device
-   Filter: (_hyper_2_3_chunk.device = 1)
-(3 rows)
-
-COMMIT;
-CREATE TABLE part_new_convert1(time timestamptz, temp float8, device int);
-SELECT create_hypertable('part_new_convert1', 'time', 'temp', 2);
-NOTICE:  adding not-null constraint to column "time"
-       create_hypertable        
---------------------------------
- (3,public,part_new_convert1,t)
-(1 row)
-
-INSERT INTO part_new_convert1 VALUES ('2017-03-22T09:18:23', 1.0, 2);
-\set ON_ERROR_STOP 0
--- Changing the type of a hash-partitioned column should not be supported
-ALTER TABLE part_new_convert1 ALTER COLUMN temp TYPE numeric;
-ERROR:  cannot change the type of a hash-partitioned column
-\set ON_ERROR_STOP 1
--- Should be able to change if not hash partitioned though
-ALTER TABLE part_new_convert1 ALTER COLUMN time TYPE timestamp;
-SELECT * FROM test.show_columnsp('_timescaledb_internal._hyper_3_%_chunk');
-                Relation                | Kind | Column |         Column type         | NotNull 
-----------------------------------------+------+--------+-----------------------------+---------
- _timescaledb_internal._hyper_3_5_chunk | r    | time   | timestamp without time zone | t
- _timescaledb_internal._hyper_3_5_chunk | r    | temp   | double precision            | f
- _timescaledb_internal._hyper_3_5_chunk | r    | device | integer                     | f
-(3 rows)
-
-CREATE TABLE part_add_dim(time timestamptz, temp float8, device int, location int);
-SELECT create_hypertable('part_add_dim', 'time', 'temp', 2);
-NOTICE:  adding not-null constraint to column "time"
-     create_hypertable     
----------------------------
- (4,public,part_add_dim,t)
-(1 row)
-
-\set ON_ERROR_STOP 0
-SELECT add_dimension('part_add_dim', 'location', 2, partitioning_func => 'bad_func');
-ERROR:  function "bad_func" does not exist at character 74
-\set ON_ERROR_STOP 1
-SELECT add_dimension('part_add_dim', 'location', 2, partitioning_func => '_timescaledb_internal.get_partition_for_key');
-           add_dimension            
-------------------------------------
- (9,public,part_add_dim,location,t)
-(1 row)
-
-SELECT * FROM _timescaledb_catalog.dimension;
- id | hypertable_id | column_name |         column_type         | aligned | num_slices | partitioning_func_schema |   partitioning_func   | interval_length | compress_interval_length | integer_now_func_schema | integer_now_func 
-----+---------------+-------------+-----------------------------+---------+------------+--------------------------+-----------------------+-----------------+--------------------------+-------------------------+------------------
-  1 |             1 | time        | timestamp with time zone    | t       |            |                          |                       |    604800000000 |                          |                         | 
-  2 |             1 | device      | integer                     | f       |          2 | _timescaledb_internal    | get_partition_for_key |                 |                          |                         | 
-  3 |             2 | time        | timestamp with time zone    | t       |            |                          |                       |    604800000000 |                          |                         | 
-  4 |             2 | device      | integer                     | f       |          2 | _timescaledb_internal    | get_partition_hash    |                 |                          |                         | 
-  6 |             3 | temp        | double precision            | f       |          2 | _timescaledb_internal    | get_partition_hash    |                 |                          |                         | 
-  5 |             3 | time        | timestamp without time zone | t       |            |                          |                       |    604800000000 |                          |                         | 
-  7 |             4 | time        | timestamp with time zone    | t       |            |                          |                       |    604800000000 |                          |                         | 
-  8 |             4 | temp        | double precision            | f       |          2 | _timescaledb_internal    | get_partition_hash    |                 |                          |                         | 
-  9 |             4 | location    | integer                     | f       |          2 | _timescaledb_internal    | get_partition_for_key |                 |                          |                         | 
-(9 rows)
-
--- Test that we support custom SQL-based partitioning functions and
--- that our native partitioning function handles function expressions
--- as argument
-CREATE OR REPLACE FUNCTION custom_partfunc(source anyelement)
-    RETURNS INTEGER LANGUAGE PLPGSQL IMMUTABLE AS
-$BODY$
-DECLARE
-    retval INTEGER;
-BEGIN
-    retval = _timescaledb_internal.get_partition_hash(substring(source::text FROM '[A-za-z0-9 ]+'));
-    RAISE NOTICE 'hash value for % is %', source, retval;
-    RETURN retval;
-END
-$BODY$;
-CREATE TABLE part_custom_func(time timestamptz, temp float8, device text);
-SELECT create_hypertable('part_custom_func', 'time', 'device', 2, partitioning_func => 'custom_partfunc');
-NOTICE:  adding not-null constraint to column "time"
-       create_hypertable       
--------------------------------
- (5,public,part_custom_func,t)
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_hash(substring('dev1' FROM '[A-za-z0-9 ]+'));
- get_partition_hash 
---------------------
-         1129986420
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_hash('dev1'::text);
- get_partition_hash 
---------------------
-         1129986420
-(1 row)
-
-SELECT _timescaledb_internal.get_partition_hash('dev7'::text);
- get_partition_hash 
---------------------
-          449729092
-(1 row)
-
-INSERT INTO part_custom_func VALUES ('2017-03-22T09:18:23', 23.4, 'dev1'),
-                                    ('2017-03-22T09:18:23', 23.4, 'dev7');
-NOTICE:  hash value for dev1 is 1129986420
-NOTICE:  hash value for dev1 is 1129986420
-NOTICE:  hash value for dev7 is 449729092
-NOTICE:  hash value for dev7 is 449729092
-SELECT * FROM test.show_subtables('part_custom_func');
-                 Child                  | Tablespace 
-----------------------------------------+------------
- _timescaledb_internal._hyper_5_6_chunk | 
- _timescaledb_internal._hyper_5_7_chunk | 
-(2 rows)
-
--- This first test is slightly trivial, but segfaulted in old versions
-CREATE TYPE simpl AS (val1 int4);
-CREATE OR REPLACE FUNCTION simpl_type_hash(ANYELEMENT) RETURNS int4 AS $$
-    SELECT $1.val1;
-$$ LANGUAGE SQL IMMUTABLE;
-CREATE TABLE simpl_partition ("timestamp" TIMESTAMPTZ, object simpl);
-SELECT create_hypertable(
-    'simpl_partition',
-    'timestamp',
-    'object',
-    1000,
-    chunk_time_interval => interval '1 day',
-    partitioning_func=>'simpl_type_hash');
-NOTICE:  adding not-null constraint to column "timestamp"
-      create_hypertable       
-------------------------------
- (6,public,simpl_partition,t)
-(1 row)
-
-INSERT INTO simpl_partition VALUES ('2017-03-22T09:18:23', ROW(1)::simpl);
-SELECT * from simpl_partition;
-          timestamp           | object 
-------------------------------+--------
- Wed Mar 22 09:18:23 2017 PDT | (1)
-(1 row)
-
--- Also test that the fix works when we have more chunks than allowed at once
-SET timescaledb.max_open_chunks_per_insert=1;
-INSERT INTO simpl_partition VALUES
-    ('2017-03-22T10:18:23', ROW(0)::simpl),
-    ('2017-03-22T10:18:23', ROW(1)::simpl),
-    ('2017-03-22T10:18:23', ROW(2)::simpl),
-    ('2017-03-22T10:18:23', ROW(3)::simpl),
-    ('2017-03-22T10:18:23', ROW(4)::simpl),
-    ('2017-03-22T10:18:23', ROW(5)::simpl);
-SET timescaledb.max_open_chunks_per_insert=default;
-SELECT * from simpl_partition;
-          timestamp           | object 
-------------------------------+--------
- Wed Mar 22 09:18:23 2017 PDT | (1)
- Wed Mar 22 10:18:23 2017 PDT | (0)
- Wed Mar 22 10:18:23 2017 PDT | (1)
- Wed Mar 22 10:18:23 2017 PDT | (2)
- Wed Mar 22 10:18:23 2017 PDT | (3)
- Wed Mar 22 10:18:23 2017 PDT | (4)
- Wed Mar 22 10:18:23 2017 PDT | (5)
-(7 rows)
-
--- Test that index creation is handled correctly.
-CREATE TABLE hyper_with_index(time timestamptz, temp float, device int);
-CREATE UNIQUE INDEX temp_index ON hyper_with_index(temp);
-\set ON_ERROR_STOP 0
-SELECT create_hypertable('hyper_with_index', 'time');
-NOTICE:  adding not-null constraint to column "time"
-ERROR:  cannot create a unique index without the column "time" (used in partitioning)
-SELECT create_hypertable('hyper_with_index', 'time', 'device', 2);
-NOTICE:  adding not-null constraint to column "time"
-ERROR:  cannot create a unique index without the column "time" (used in partitioning)
-SELECT create_hypertable('hyper_with_index', 'time', 'temp', 2);
-NOTICE:  adding not-null constraint to column "time"
-ERROR:  cannot create a unique index without the column "time" (used in partitioning)
-\set ON_ERROR_STOP 1
-DROP INDEX temp_index;
-CREATE UNIQUE INDEX time_index ON hyper_with_index(time);
-\set ON_ERROR_STOP 0
--- should error because device not in index
-SELECT create_hypertable('hyper_with_index', 'time', 'device', 4);
-NOTICE:  adding not-null constraint to column "time"
-ERROR:  cannot create a unique index without the column "device" (used in partitioning)
-\set ON_ERROR_STOP 1
-SELECT create_hypertable('hyper_with_index', 'time');
-NOTICE:  adding not-null constraint to column "time"
-       create_hypertable        
---------------------------------
- (11,public,hyper_with_index,t)
-(1 row)
-
--- make sure user created index is used.
--- not using \d or \d+ because output syntax differs
--- between postgres 9 and postgres 10.
-SELECT indexname FROM pg_indexes WHERE tablename = 'hyper_with_index';
- indexname  
-------------
- time_index
-(1 row)
-
-\set ON_ERROR_STOP 0
-SELECT add_dimension('hyper_with_index', 'device', 4);
-ERROR:  cannot create a unique index without the column "device" (used in partitioning)
-\set ON_ERROR_STOP 1
-DROP INDEX time_index;
-CREATE UNIQUE INDEX time_space_index ON hyper_with_index(time, device);
-SELECT add_dimension('hyper_with_index', 'device', 4);
-             add_dimension             
----------------------------------------
- (23,public,hyper_with_index,device,t)
-(1 row)
-
-CREATE TABLE hyper_with_primary(time TIMESTAMPTZ PRIMARY KEY, temp float, device int);
-\set ON_ERROR_STOP 0
-SELECT create_hypertable('hyper_with_primary', 'time', 'device', 4);
-ERROR:  cannot create a unique index without the column "device" (used in partitioning)
-\set ON_ERROR_STOP 1
-SELECT create_hypertable('hyper_with_primary', 'time');
-        create_hypertable         
-----------------------------------
- (13,public,hyper_with_primary,t)
-(1 row)
-
-\set ON_ERROR_STOP 0
-SELECT add_dimension('hyper_with_primary', 'device', 4);
-ERROR:  cannot create a unique index without the column "device" (used in partitioning)
-\set ON_ERROR_STOP 1
--- NON-unique indexes can still be created
-CREATE INDEX temp_index ON hyper_with_index(temp);
--- Make sure custom composite types are supported as dimensions
-CREATE TYPE TUPLE as (val1 int4, val2 int4);
-CREATE FUNCTION tuple_hash(value ANYELEMENT) RETURNS INT4
-LANGUAGE PLPGSQL IMMUTABLE AS
-$BODY$
-BEGIN
-    RAISE NOTICE 'custom hash value is: %', value.val1+value.val2;
-    RETURN value.val1+value.val2;
-END
-$BODY$;
-CREATE TABLE part_custom_dim (time TIMESTAMPTZ, combo TUPLE, device TEXT);
-SELECT create_hypertable('part_custom_dim', 'time', 'combo', 4, partitioning_func=>'tuple_hash');
-NOTICE:  adding not-null constraint to column "time"
-       create_hypertable       
--------------------------------
- (14,public,part_custom_dim,t)
-(1 row)
-
-INSERT INTO part_custom_dim(time, combo) VALUES (now(), (1,2));
-NOTICE:  custom hash value is: 3
-NOTICE:  custom hash value is: 3
-DROP TABLE part_custom_dim;
--- Now make sure that renaming partitioning_func_schema will get updated properly
-\c :TEST_DBNAME :ROLE_SUPERUSER
-CREATE SCHEMA IF NOT EXISTS my_partitioning_schema;
-CREATE FUNCTION my_partitioning_schema.tuple_hash(value ANYELEMENT) RETURNS INT4
-LANGUAGE PLPGSQL IMMUTABLE AS
-$BODY$
-BEGIN
-    RAISE NOTICE 'custom hash value is: %', value.val1+value.val2;
-    RETURN value.val1+value.val2;
-END
-$BODY$;
-CREATE TABLE part_custom_dim (time TIMESTAMPTZ, combo TUPLE, device TEXT);
-SELECT create_hypertable('part_custom_dim', 'time', 'combo', 4, partitioning_func=>'my_partitioning_schema.tuple_hash');
-NOTICE:  adding not-null constraint to column "time"
-       create_hypertable       
--------------------------------
- (15,public,part_custom_dim,t)
-(1 row)
-
-INSERT INTO part_custom_dim(time, combo) VALUES (now(), (1,2));
-NOTICE:  custom hash value is: 3
-NOTICE:  custom hash value is: 3
-ALTER SCHEMA my_partitioning_schema RENAME TO new_partitioning_schema;
--- Inserts should work even after we rename the schema
-INSERT INTO part_custom_dim(time, combo) VALUES (now(), (3,4));
-NOTICE:  custom hash value is: 7
-NOTICE:  custom hash value is: 7
--- Test partitioning function on an open (time) dimension
-CREATE OR REPLACE FUNCTION time_partfunc(unixtime float8)
-    RETURNS TIMESTAMPTZ LANGUAGE PLPGSQL IMMUTABLE AS
-$BODY$
-DECLARE
-    retval TIMESTAMPTZ;
-BEGIN
-
-    retval := to_timestamp(unixtime);
-    RAISE NOTICE 'time value for % is %', unixtime, timezone('UTC', retval);
-    RETURN retval;
-END
-$BODY$;
-CREATE OR REPLACE FUNCTION time_partfunc_bad_parameters(unixtime float8, extra text)
-    RETURNS TIMESTAMPTZ LANGUAGE SQL IMMUTABLE AS
-$BODY$
-    SELECT to_timestamp(unixtime);
-$BODY$;
-CREATE OR REPLACE FUNCTION time_partfunc_bad_return_type(unixtime float8)
-    RETURNS FLOAT8 LANGUAGE SQL IMMUTABLE AS
-$BODY$
-    SELECT unixtime;
-$BODY$;
-CREATE TABLE part_time_func(time float8, temp float8, device text);
-\set ON_ERROR_STOP 0
--- Should fail due to invalid time column
-SELECT create_hypertable('part_time_func', 'time');
-ERROR:  invalid type for dimension "time"
--- Should fail due to bad signature of time partitioning function
-SELECT create_hypertable('part_time_func', 'time', time_partitioning_func => 'time_partfunc_bad_parameters');
-ERROR:  invalid partitioning function
-SELECT create_hypertable('part_time_func', 'time', time_partitioning_func => 'time_partfunc_bad_return_type');
-ERROR:  invalid partitioning function
-\set ON_ERROR_STOP 1
--- Should work with time partitioning function that returns a valid time type
-SELECT create_hypertable('part_time_func', 'time', time_partitioning_func => 'time_partfunc');
-NOTICE:  adding not-null constraint to column "time"
-      create_hypertable       
-------------------------------
- (16,public,part_time_func,t)
-(1 row)
-
-INSERT INTO part_time_func VALUES (1530214157.134, 23.4, 'dev1'),
-                                  (1533214157.8734, 22.3, 'dev7');
-NOTICE:  time value for 1530214157.134 is Thu Jun 28 19:29:17.134 2018
-NOTICE:  time value for 1530214157.134 is Thu Jun 28 19:29:17.134 2018
-NOTICE:  time value for 1530214157.134 is Thu Jun 28 19:29:17.134 2018
-NOTICE:  time value for 1530214157.134 is Thu Jun 28 19:29:17.134 2018
-NOTICE:  time value for 1533214157.8734 is Thu Aug 02 12:49:17.8734 2018
-NOTICE:  time value for 1533214157.8734 is Thu Aug 02 12:49:17.8734 2018
-NOTICE:  time value for 1533214157.8734 is Thu Aug 02 12:49:17.8734 2018
-NOTICE:  time value for 1533214157.8734 is Thu Aug 02 12:49:17.8734 2018
-SELECT time, temp, device FROM part_time_func;
-      time       | temp | device 
------------------+------+--------
-  1530214157.134 | 23.4 | dev1
- 1533214157.8734 | 22.3 | dev7
-(2 rows)
-
-SELECT time_partfunc(time) at time zone 'UTC', temp, device FROM part_time_func;
-NOTICE:  time value for 1530214157.134 is Thu Jun 28 19:29:17.134 2018
-NOTICE:  time value for 1533214157.8734 is Thu Aug 02 12:49:17.8734 2018
-           timezone            | temp | device 
--------------------------------+------+--------
- Thu Jun 28 19:29:17.134 2018  | 23.4 | dev1
- Thu Aug 02 12:49:17.8734 2018 | 22.3 | dev7
-(2 rows)
-
-SELECT * FROM test.show_subtables('part_time_func');
-                  Child                   | Tablespace 
-------------------------------------------+------------
- _timescaledb_internal._hyper_16_11_chunk | 
- _timescaledb_internal._hyper_16_12_chunk | 
-(2 rows)
-
-SELECT (test.show_constraints("Child")).*
-FROM test.show_subtables('part_time_func');
-  Constraint   | Type | Columns | Index |                                                                                     Expr                                                                                     | Deferrable | Deferred | Validated 
----------------+------+---------+-------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+----------+-----------
- constraint_18 | c    | {time}  | -     | ((time_partfunc("time") >= 'Wed Jun 27 17:00:00 2018 PDT'::timestamp with time zone) AND (time_partfunc("time") < 'Wed Jul 04 17:00:00 2018 PDT'::timestamp with time zone)) | f          | f        | t
- constraint_19 | c    | {time}  | -     | ((time_partfunc("time") >= 'Wed Aug 01 17:00:00 2018 PDT'::timestamp with time zone) AND (time_partfunc("time") < 'Wed Aug 08 17:00:00 2018 PDT'::timestamp with time zone)) | f          | f        | t
-(2 rows)
-
-SELECT (test.show_indexes("Child")).*
-FROM test.show_subtables('part_time_func');
-                              Index                               | Columns |        Expr         | Unique | Primary | Exclusion | Tablespace 
-------------------------------------------------------------------+---------+---------------------+--------+---------+-----------+------------
- _timescaledb_internal._hyper_16_11_chunk_part_time_func_expr_idx | {expr}  | time_partfunc(expr) | f      | f       | f         | 
- _timescaledb_internal._hyper_16_12_chunk_part_time_func_expr_idx | {expr}  | time_partfunc(expr) | f      | f       | f         | 
-(2 rows)
-
--- Check that constraint exclusion works with time partitioning
--- function (scan only one chunk)
--- No exclusion
-EXPLAIN (verbose, costs off)
-SELECT * FROM part_time_func;
-                                          QUERY PLAN                                           
------------------------------------------------------------------------------------------------
- Append
-   ->  Seq Scan on _timescaledb_internal._hyper_16_11_chunk
-         Output: _hyper_16_11_chunk."time", _hyper_16_11_chunk.temp, _hyper_16_11_chunk.device
-   ->  Seq Scan on _timescaledb_internal._hyper_16_12_chunk
-         Output: _hyper_16_12_chunk."time", _hyper_16_12_chunk.temp, _hyper_16_12_chunk.device
-(5 rows)
-
--- Exclude using the function on time
-EXPLAIN (verbose, costs off)
-SELECT * FROM part_time_func WHERE time_partfunc(time) < '2018-07-01';
-                                                     QUERY PLAN                                                      
----------------------------------------------------------------------------------------------------------------------
- Index Scan using _hyper_16_11_chunk_part_time_func_expr_idx on _timescaledb_internal._hyper_16_11_chunk
-   Output: _hyper_16_11_chunk."time", _hyper_16_11_chunk.temp, _hyper_16_11_chunk.device
-   Index Cond: (time_partfunc(_hyper_16_11_chunk."time") < 'Sun Jul 01 00:00:00 2018 PDT'::timestamp with time zone)
-(3 rows)
-
--- Exclude using the same date but as a UNIX timestamp. Won't do an
--- index scan since the index is on the time function expression
-EXPLAIN (verbose, costs off)
-SELECT * FROM part_time_func WHERE time < 1530403200.0;
-NOTICE:  time value for 1530403200 is Sun Jul 01 00:00:00 2018
-                                       QUERY PLAN                                        
------------------------------------------------------------------------------------------
- Seq Scan on _timescaledb_internal._hyper_16_11_chunk
-   Output: _hyper_16_11_chunk."time", _hyper_16_11_chunk.temp, _hyper_16_11_chunk.device
-   Filter: (_hyper_16_11_chunk."time" < '1530403200'::double precision)
-(3 rows)
-
--- Check that inserts will fail if we use a time partitioning function
--- that returns NULL
-CREATE OR REPLACE FUNCTION time_partfunc_null_ret(unixtime float8)
-    RETURNS TIMESTAMPTZ LANGUAGE PLPGSQL IMMUTABLE AS
-$BODY$
-BEGIN
-    RETURN NULL;
-END
-$BODY$;
-CREATE TABLE part_time_func_null_ret(time float8, temp float8, device text);
-SELECT create_hypertable('part_time_func_null_ret', 'time', time_partitioning_func => 'time_partfunc_null_ret');
-NOTICE:  adding not-null constraint to column "time"
-           create_hypertable           
----------------------------------------
- (17,public,part_time_func_null_ret,t)
-(1 row)
-
-\set ON_ERROR_STOP 0
-INSERT INTO part_time_func_null_ret VALUES (1530214157.134, 23.4, 'dev1'),
-                                           (1533214157.8734, 22.3, 'dev7');
-ERROR:  partitioning function "public.time_partfunc_null_ret" returned NULL
-\set ON_ERROR_STOP 1
--- Test manually refreshing dimension partitions used in update
--- script. Mostly to keep codecov happy.
-CREATE FUNCTION _timescaledb_internal.update_dimension_partition(hypertable REGCLASS) RETURNS VOID AS :MODULE_PATHNAME, 'ts_dimension_partition_update' LANGUAGE C VOLATILE;
-SELECT _timescaledb_internal.update_dimension_partition('part_custom_dim');
- update_dimension_partition 
-----------------------------
- 
-(1 row)
-
-DROP FUNCTION _timescaledb_internal.update_dimension_partition;
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/partitioning.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/partitioning.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/partitioning.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/partitioning.out	2023-11-25 05:27:38.973037712 +0000
@@ -1,27 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
--- Should expect an error when creating a hypertable from a partition
-\set ON_ERROR_STOP 0
-CREATE TABLE partitioned_ht_create(time timestamptz, temp float, device int) PARTITION BY RANGE (time);
-SELECT create_hypertable('partitioned_ht_create', 'time');
-ERROR:  table "partitioned_ht_create" is already partitioned
-\set ON_ERROR_STOP 1
--- Should expect an error when attaching a hypertable to a partition
-\set ON_ERROR_STOP 0
-CREATE TABLE partitioned_attachment_vanilla(time timestamptz, temp float, device int) PARTITION BY RANGE (time);
-CREATE TABLE attachment_hypertable(time timestamptz, temp float, device int);
-SELECT create_hypertable('attachment_hypertable', 'time');
-NOTICE:  adding not-null constraint to column "time"
-         create_hypertable          
-------------------------------------
- (1,public,attachment_hypertable,t)
-(1 row)
-
-ALTER TABLE partitioned_attachment_vanilla ATTACH PARTITION attachment_hypertable FOR VALUES FROM ('2016-07-01') TO ('2016-08-01');
-ERROR:  hypertables do not support native postgres partitioning
-\set ON_ERROR_STOP 1
--- Should not expect an error when attaching a normal table to a partition
-CREATE TABLE partitioned_vanilla(time timestamptz, temp float, device int) PARTITION BY RANGE (time);
-CREATE TABLE attachment_vanilla(time timestamptz, temp float, device int);
-ALTER TABLE partitioned_vanilla ATTACH PARTITION attachment_vanilla FOR VALUES FROM ('2016-07-01') TO ('2016-08-01');
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/partitionwise-15.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/partitionwise-15.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/partitionwise-15.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/partitionwise-15.out	2023-11-25 05:27:44.121022730 +0000
@@ -1,884 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-\set PREFIX 'EXPLAIN (VERBOSE, COSTS OFF)'
--- Create a two dimensional hypertable
-CREATE TABLE hyper (time timestamptz, device int, temp float);
-SELECT * FROM create_hypertable('hyper', 'time', 'device', 2);
-NOTICE:  adding not-null constraint to column "time"
- hypertable_id | schema_name | table_name | created 
----------------+-------------+------------+---------
-             1 | public      | hyper      | t
-(1 row)
-
--- Create a similar PostgreSQL partitioned table
-CREATE TABLE pg2dim (time timestamptz, device int, temp float) PARTITION BY HASH (device);
-CREATE TABLE pg2dim_h1 PARTITION OF pg2dim FOR VALUES WITH (MODULUS 2, REMAINDER 0) PARTITION BY RANGE(time);
-CREATE TABLE pg2dim_h2 PARTITION OF pg2dim FOR VALUES WITH (MODULUS 2, REMAINDER 1) PARTITION BY RANGE(time);
-CREATE TABLE pg2dim_h1_t1 PARTITION OF pg2dim_h1 FOR VALUES FROM ('2018-01-01 00:00') TO ('2018-09-01 00:00');
-CREATE TABLE pg2dim_h1_t2 PARTITION OF pg2dim_h1 FOR VALUES FROM ('2018-09-01 00:00') TO ('2018-12-01 00:00');
-CREATE TABLE pg2dim_h2_t1 PARTITION OF pg2dim_h2 FOR VALUES FROM ('2018-01-01 00:00') TO ('2018-09-01 00:00');
-CREATE TABLE pg2dim_h2_t2 PARTITION OF pg2dim_h2 FOR VALUES FROM ('2018-09-01 00:00') TO ('2018-12-01 00:00');
--- Create a 1-dimensional partitioned table for comparison
-CREATE TABLE pg1dim (time timestamptz, device int, temp float) PARTITION BY HASH (device);
-CREATE TABLE pg1dim_h1 PARTITION OF pg1dim FOR VALUES WITH (MODULUS 2, REMAINDER 0);
-CREATE TABLE pg1dim_h2 PARTITION OF pg1dim FOR VALUES WITH (MODULUS 2, REMAINDER 1);
-INSERT INTO hyper VALUES
-       ('2018-02-19 13:01', 1, 2.3),
-       ('2018-02-19 13:02', 3, 3.1),
-       ('2018-10-19 13:01', 1, 7.6),
-       ('2018-10-19 13:02', 3, 9.0);
-INSERT INTO pg2dim VALUES
-       ('2018-02-19 13:01', 1, 2.3),
-       ('2018-02-19 13:02', 3, 3.1),
-       ('2018-10-19 13:01', 1, 7.6),
-       ('2018-10-19 13:02', 3, 9.0);
-INSERT INTO pg1dim VALUES
-       ('2018-02-19 13:01', 1, 2.3),
-       ('2018-02-19 13:02', 3, 3.1),
-       ('2018-10-19 13:01', 1, 7.6),
-       ('2018-10-19 13:02', 3, 9.0);
-SELECT * FROM test.show_subtables('hyper');
-                 Child                  | Tablespace 
-----------------------------------------+------------
- _timescaledb_internal._hyper_1_1_chunk | 
- _timescaledb_internal._hyper_1_2_chunk | 
- _timescaledb_internal._hyper_1_3_chunk | 
- _timescaledb_internal._hyper_1_4_chunk | 
-(4 rows)
-
-SELECT * FROM pg2dim_h1_t1;
-             time             | device | temp 
-------------------------------+--------+------
- Mon Feb 19 13:01:00 2018 PST |      1 |  2.3
-(1 row)
-
-SELECT * FROM pg2dim_h1_t2;
-             time             | device | temp 
-------------------------------+--------+------
- Fri Oct 19 13:01:00 2018 PDT |      1 |  7.6
-(1 row)
-
-SELECT * FROM pg2dim_h2_t1;
-             time             | device | temp 
-------------------------------+--------+------
- Mon Feb 19 13:02:00 2018 PST |      3 |  3.1
-(1 row)
-
-SELECT * FROM pg2dim_h2_t2;
-             time             | device | temp 
-------------------------------+--------+------
- Fri Oct 19 13:02:00 2018 PDT |      3 |    9
-(1 row)
-
--- Compare partitionwise aggreate enabled/disabled. First run queries
--- on PG partitioned tables for reference.
--- All partition keys covered by GROUP BY
-SET enable_partitionwise_aggregate = 'off';
-:PREFIX
-SELECT device, avg(temp)
-FROM pg1dim
-GROUP BY 1
-ORDER BY 1;
-                         QUERY PLAN                         
-------------------------------------------------------------
- Sort
-   Output: pg1dim.device, (avg(pg1dim.temp))
-   Sort Key: pg1dim.device
-   ->  HashAggregate
-         Output: pg1dim.device, avg(pg1dim.temp)
-         Group Key: pg1dim.device
-         ->  Append
-               ->  Seq Scan on public.pg1dim_h1 pg1dim_1
-                     Output: pg1dim_1.device, pg1dim_1.temp
-               ->  Seq Scan on public.pg1dim_h2 pg1dim_2
-                     Output: pg1dim_2.device, pg1dim_2.temp
-(11 rows)
-
-SET enable_partitionwise_aggregate = 'on';
-:PREFIX
-SELECT device, avg(temp)
-FROM pg1dim
-GROUP BY 1
-ORDER BY 1;
-                         QUERY PLAN                         
-------------------------------------------------------------
- Sort
-   Output: pg1dim.device, (avg(pg1dim.temp))
-   Sort Key: pg1dim.device
-   ->  Append
-         ->  HashAggregate
-               Output: pg1dim.device, avg(pg1dim.temp)
-               Group Key: pg1dim.device
-               ->  Seq Scan on public.pg1dim_h1 pg1dim
-                     Output: pg1dim.device, pg1dim.temp
-         ->  HashAggregate
-               Output: pg1dim_1.device, avg(pg1dim_1.temp)
-               Group Key: pg1dim_1.device
-               ->  Seq Scan on public.pg1dim_h2 pg1dim_1
-                     Output: pg1dim_1.device, pg1dim_1.temp
-(14 rows)
-
--- All partition keys not covered by GROUP BY (partial partitionwise)
-SET enable_partitionwise_aggregate = 'off';
-:PREFIX
-SELECT device, avg(temp)
-FROM pg2dim
-GROUP BY 1
-ORDER BY 1;
-                         QUERY PLAN                         
-------------------------------------------------------------
- Sort
-   Output: pg2dim.device, (avg(pg2dim.temp))
-   Sort Key: pg2dim.device
-   ->  HashAggregate
-         Output: pg2dim.device, avg(pg2dim.temp)
-         Group Key: pg2dim.device
-         ->  Append
-               ->  Seq Scan on public.pg2dim_h1_t1 pg2dim_1
-                     Output: pg2dim_1.device, pg2dim_1.temp
-               ->  Seq Scan on public.pg2dim_h1_t2 pg2dim_2
-                     Output: pg2dim_2.device, pg2dim_2.temp
-               ->  Seq Scan on public.pg2dim_h2_t1 pg2dim_3
-                     Output: pg2dim_3.device, pg2dim_3.temp
-               ->  Seq Scan on public.pg2dim_h2_t2 pg2dim_4
-                     Output: pg2dim_4.device, pg2dim_4.temp
-(15 rows)
-
-SET enable_partitionwise_aggregate = 'on';
-:PREFIX
-SELECT device, avg(temp)
-FROM pg2dim
-GROUP BY 1
-ORDER BY 1;
-                               QUERY PLAN                                
--------------------------------------------------------------------------
- Sort
-   Output: pg2dim.device, (avg(pg2dim.temp))
-   Sort Key: pg2dim.device
-   ->  Finalize HashAggregate
-         Output: pg2dim.device, avg(pg2dim.temp)
-         Group Key: pg2dim.device
-         ->  Append
-               ->  Partial HashAggregate
-                     Output: pg2dim.device, PARTIAL avg(pg2dim.temp)
-                     Group Key: pg2dim.device
-                     ->  Seq Scan on public.pg2dim_h1_t1 pg2dim
-                           Output: pg2dim.device, pg2dim.temp
-               ->  Partial HashAggregate
-                     Output: pg2dim_1.device, PARTIAL avg(pg2dim_1.temp)
-                     Group Key: pg2dim_1.device
-                     ->  Seq Scan on public.pg2dim_h1_t2 pg2dim_1
-                           Output: pg2dim_1.device, pg2dim_1.temp
-               ->  Partial HashAggregate
-                     Output: pg2dim_2.device, PARTIAL avg(pg2dim_2.temp)
-                     Group Key: pg2dim_2.device
-                     ->  Seq Scan on public.pg2dim_h2_t1 pg2dim_2
-                           Output: pg2dim_2.device, pg2dim_2.temp
-               ->  Partial HashAggregate
-                     Output: pg2dim_3.device, PARTIAL avg(pg2dim_3.temp)
-                     Group Key: pg2dim_3.device
-                     ->  Seq Scan on public.pg2dim_h2_t2 pg2dim_3
-                           Output: pg2dim_3.device, pg2dim_3.temp
-(27 rows)
-
--- All partition keys covered by GROUP BY (full partitionwise)
-SET enable_partitionwise_aggregate = 'off';
-:PREFIX
-SELECT time, device, avg(temp)
-FROM pg2dim
-GROUP BY 1, 2
-ORDER BY 1, 2;
-                                 QUERY PLAN                                  
------------------------------------------------------------------------------
- Sort
-   Output: pg2dim."time", pg2dim.device, (avg(pg2dim.temp))
-   Sort Key: pg2dim."time", pg2dim.device
-   ->  HashAggregate
-         Output: pg2dim."time", pg2dim.device, avg(pg2dim.temp)
-         Group Key: pg2dim."time", pg2dim.device
-         ->  Append
-               ->  Seq Scan on public.pg2dim_h1_t1 pg2dim_1
-                     Output: pg2dim_1."time", pg2dim_1.device, pg2dim_1.temp
-               ->  Seq Scan on public.pg2dim_h1_t2 pg2dim_2
-                     Output: pg2dim_2."time", pg2dim_2.device, pg2dim_2.temp
-               ->  Seq Scan on public.pg2dim_h2_t1 pg2dim_3
-                     Output: pg2dim_3."time", pg2dim_3.device, pg2dim_3.temp
-               ->  Seq Scan on public.pg2dim_h2_t2 pg2dim_4
-                     Output: pg2dim_4."time", pg2dim_4.device, pg2dim_4.temp
-(15 rows)
-
-SET enable_partitionwise_aggregate = 'on';
-:PREFIX
-SELECT time, device, avg(temp)
-FROM pg2dim
-GROUP BY 1, 2
-ORDER BY 1, 2;
-                                 QUERY PLAN                                  
------------------------------------------------------------------------------
- Sort
-   Output: pg2dim."time", pg2dim.device, (avg(pg2dim.temp))
-   Sort Key: pg2dim."time", pg2dim.device
-   ->  Append
-         ->  HashAggregate
-               Output: pg2dim."time", pg2dim.device, avg(pg2dim.temp)
-               Group Key: pg2dim."time", pg2dim.device
-               ->  Seq Scan on public.pg2dim_h1_t1 pg2dim
-                     Output: pg2dim."time", pg2dim.device, pg2dim.temp
-         ->  HashAggregate
-               Output: pg2dim_1."time", pg2dim_1.device, avg(pg2dim_1.temp)
-               Group Key: pg2dim_1."time", pg2dim_1.device
-               ->  Seq Scan on public.pg2dim_h1_t2 pg2dim_1
-                     Output: pg2dim_1."time", pg2dim_1.device, pg2dim_1.temp
-         ->  HashAggregate
-               Output: pg2dim_2."time", pg2dim_2.device, avg(pg2dim_2.temp)
-               Group Key: pg2dim_2."time", pg2dim_2.device
-               ->  Seq Scan on public.pg2dim_h2_t1 pg2dim_2
-                     Output: pg2dim_2."time", pg2dim_2.device, pg2dim_2.temp
-         ->  HashAggregate
-               Output: pg2dim_3."time", pg2dim_3.device, avg(pg2dim_3.temp)
-               Group Key: pg2dim_3."time", pg2dim_3.device
-               ->  Seq Scan on public.pg2dim_h2_t2 pg2dim_3
-                     Output: pg2dim_3."time", pg2dim_3.device, pg2dim_3.temp
-(24 rows)
-
--- All partition keys not covered by GROUP BY because of date_trunc
--- expression on time (partial partitionwise)
-SET enable_partitionwise_aggregate = 'off';
-:PREFIX
-SELECT date_trunc('month', time), device, avg(temp)
-FROM pg2dim
-GROUP BY 1, 2
-ORDER BY 1, 2;
-                                               QUERY PLAN                                               
---------------------------------------------------------------------------------------------------------
- Sort
-   Output: (date_trunc('month'::text, pg2dim."time")), pg2dim.device, (avg(pg2dim.temp))
-   Sort Key: (date_trunc('month'::text, pg2dim."time")), pg2dim.device
-   ->  HashAggregate
-         Output: (date_trunc('month'::text, pg2dim."time")), pg2dim.device, avg(pg2dim.temp)
-         Group Key: (date_trunc('month'::text, pg2dim."time")), pg2dim.device
-         ->  Append
-               ->  Seq Scan on public.pg2dim_h1_t1 pg2dim_1
-                     Output: date_trunc('month'::text, pg2dim_1."time"), pg2dim_1.device, pg2dim_1.temp
-               ->  Seq Scan on public.pg2dim_h1_t2 pg2dim_2
-                     Output: date_trunc('month'::text, pg2dim_2."time"), pg2dim_2.device, pg2dim_2.temp
-               ->  Seq Scan on public.pg2dim_h2_t1 pg2dim_3
-                     Output: date_trunc('month'::text, pg2dim_3."time"), pg2dim_3.device, pg2dim_3.temp
-               ->  Seq Scan on public.pg2dim_h2_t2 pg2dim_4
-                     Output: date_trunc('month'::text, pg2dim_4."time"), pg2dim_4.device, pg2dim_4.temp
-(15 rows)
-
-SET enable_partitionwise_aggregate = 'on';
-:PREFIX
-SELECT date_trunc('month', time), device, avg(temp)
-FROM pg2dim
-GROUP BY 1, 2
-ORDER BY 1, 2;
-                                                      QUERY PLAN                                                       
------------------------------------------------------------------------------------------------------------------------
- Sort
-   Output: (date_trunc('month'::text, pg2dim."time")), pg2dim.device, (avg(pg2dim.temp))
-   Sort Key: (date_trunc('month'::text, pg2dim."time")), pg2dim.device
-   ->  Finalize HashAggregate
-         Output: (date_trunc('month'::text, pg2dim."time")), pg2dim.device, avg(pg2dim.temp)
-         Group Key: (date_trunc('month'::text, pg2dim."time")), pg2dim.device
-         ->  Append
-               ->  Partial HashAggregate
-                     Output: (date_trunc('month'::text, pg2dim."time")), pg2dim.device, PARTIAL avg(pg2dim.temp)
-                     Group Key: date_trunc('month'::text, pg2dim."time"), pg2dim.device
-                     ->  Seq Scan on public.pg2dim_h1_t1 pg2dim
-                           Output: date_trunc('month'::text, pg2dim."time"), pg2dim.device, pg2dim.temp
-               ->  Partial HashAggregate
-                     Output: (date_trunc('month'::text, pg2dim_1."time")), pg2dim_1.device, PARTIAL avg(pg2dim_1.temp)
-                     Group Key: date_trunc('month'::text, pg2dim_1."time"), pg2dim_1.device
-                     ->  Seq Scan on public.pg2dim_h1_t2 pg2dim_1
-                           Output: date_trunc('month'::text, pg2dim_1."time"), pg2dim_1.device, pg2dim_1.temp
-               ->  Partial HashAggregate
-                     Output: (date_trunc('month'::text, pg2dim_2."time")), pg2dim_2.device, PARTIAL avg(pg2dim_2.temp)
-                     Group Key: date_trunc('month'::text, pg2dim_2."time"), pg2dim_2.device
-                     ->  Seq Scan on public.pg2dim_h2_t1 pg2dim_2
-                           Output: date_trunc('month'::text, pg2dim_2."time"), pg2dim_2.device, pg2dim_2.temp
-               ->  Partial HashAggregate
-                     Output: (date_trunc('month'::text, pg2dim_3."time")), pg2dim_3.device, PARTIAL avg(pg2dim_3.temp)
-                     Group Key: date_trunc('month'::text, pg2dim_3."time"), pg2dim_3.device
-                     ->  Seq Scan on public.pg2dim_h2_t2 pg2dim_3
-                           Output: date_trunc('month'::text, pg2dim_3."time"), pg2dim_3.device, pg2dim_3.temp
-(27 rows)
-
--- Now run on hypertable
--- All partition keys not covered by GROUP BY (partial partitionwise)
-SET enable_partitionwise_aggregate = 'off';
-:PREFIX
-SELECT device, avg(temp)
-FROM hyper
-GROUP BY 1
-ORDER BY 1;
-                                 QUERY PLAN                                 
-----------------------------------------------------------------------------
- Sort
-   Output: _hyper_1_1_chunk.device, (avg(_hyper_1_1_chunk.temp))
-   Sort Key: _hyper_1_1_chunk.device
-   ->  HashAggregate
-         Output: _hyper_1_1_chunk.device, avg(_hyper_1_1_chunk.temp)
-         Group Key: _hyper_1_1_chunk.device
-         ->  Append
-               ->  Seq Scan on _timescaledb_internal._hyper_1_1_chunk
-                     Output: _hyper_1_1_chunk.device, _hyper_1_1_chunk.temp
-               ->  Seq Scan on _timescaledb_internal._hyper_1_2_chunk
-                     Output: _hyper_1_2_chunk.device, _hyper_1_2_chunk.temp
-               ->  Seq Scan on _timescaledb_internal._hyper_1_3_chunk
-                     Output: _hyper_1_3_chunk.device, _hyper_1_3_chunk.temp
-               ->  Seq Scan on _timescaledb_internal._hyper_1_4_chunk
-                     Output: _hyper_1_4_chunk.device, _hyper_1_4_chunk.temp
-(15 rows)
-
-SET enable_partitionwise_aggregate = 'on';
-:PREFIX
-SELECT device, avg(temp)
-FROM hyper
-GROUP BY 1
-ORDER BY 1;
-                                       QUERY PLAN                                        
------------------------------------------------------------------------------------------
- Sort
-   Output: _hyper_1_1_chunk.device, (avg(_hyper_1_1_chunk.temp))
-   Sort Key: _hyper_1_1_chunk.device
-   ->  Finalize HashAggregate
-         Output: _hyper_1_1_chunk.device, avg(_hyper_1_1_chunk.temp)
-         Group Key: _hyper_1_1_chunk.device
-         ->  Append
-               ->  Partial HashAggregate
-                     Output: _hyper_1_1_chunk.device, PARTIAL avg(_hyper_1_1_chunk.temp)
-                     Group Key: _hyper_1_1_chunk.device
-                     ->  Seq Scan on _timescaledb_internal._hyper_1_1_chunk
-                           Output: _hyper_1_1_chunk.device, _hyper_1_1_chunk.temp
-               ->  Partial HashAggregate
-                     Output: _hyper_1_2_chunk.device, PARTIAL avg(_hyper_1_2_chunk.temp)
-                     Group Key: _hyper_1_2_chunk.device
-                     ->  Seq Scan on _timescaledb_internal._hyper_1_2_chunk
-                           Output: _hyper_1_2_chunk.device, _hyper_1_2_chunk.temp
-               ->  Partial HashAggregate
-                     Output: _hyper_1_3_chunk.device, PARTIAL avg(_hyper_1_3_chunk.temp)
-                     Group Key: _hyper_1_3_chunk.device
-                     ->  Seq Scan on _timescaledb_internal._hyper_1_3_chunk
-                           Output: _hyper_1_3_chunk.device, _hyper_1_3_chunk.temp
-               ->  Partial HashAggregate
-                     Output: _hyper_1_4_chunk.device, PARTIAL avg(_hyper_1_4_chunk.temp)
-                     Group Key: _hyper_1_4_chunk.device
-                     ->  Seq Scan on _timescaledb_internal._hyper_1_4_chunk
-                           Output: _hyper_1_4_chunk.device, _hyper_1_4_chunk.temp
-(27 rows)
-
--- All partition keys covered (full partitionwise)
-SET enable_partitionwise_aggregate = 'off';
-:PREFIX
-SELECT time, device, avg(temp)
-FROM hyper
-GROUP BY 1, 2
-ORDER BY 1, 2;
-                                             QUERY PLAN                                              
------------------------------------------------------------------------------------------------------
- Sort
-   Output: _hyper_1_1_chunk."time", _hyper_1_1_chunk.device, (avg(_hyper_1_1_chunk.temp))
-   Sort Key: _hyper_1_1_chunk."time", _hyper_1_1_chunk.device
-   ->  HashAggregate
-         Output: _hyper_1_1_chunk."time", _hyper_1_1_chunk.device, avg(_hyper_1_1_chunk.temp)
-         Group Key: _hyper_1_1_chunk."time", _hyper_1_1_chunk.device
-         ->  Append
-               ->  Seq Scan on _timescaledb_internal._hyper_1_1_chunk
-                     Output: _hyper_1_1_chunk."time", _hyper_1_1_chunk.device, _hyper_1_1_chunk.temp
-               ->  Seq Scan on _timescaledb_internal._hyper_1_2_chunk
-                     Output: _hyper_1_2_chunk."time", _hyper_1_2_chunk.device, _hyper_1_2_chunk.temp
-               ->  Seq Scan on _timescaledb_internal._hyper_1_3_chunk
-                     Output: _hyper_1_3_chunk."time", _hyper_1_3_chunk.device, _hyper_1_3_chunk.temp
-               ->  Seq Scan on _timescaledb_internal._hyper_1_4_chunk
-                     Output: _hyper_1_4_chunk."time", _hyper_1_4_chunk.device, _hyper_1_4_chunk.temp
-(15 rows)
-
-SET enable_partitionwise_aggregate = 'on';
-:PREFIX
-SELECT time, device, avg(temp)
-FROM hyper
-GROUP BY 1, 2
-ORDER BY 1, 2;
-                                             QUERY PLAN                                              
------------------------------------------------------------------------------------------------------
- Sort
-   Output: _hyper_1_1_chunk."time", _hyper_1_1_chunk.device, (avg(_hyper_1_1_chunk.temp))
-   Sort Key: _hyper_1_1_chunk."time", _hyper_1_1_chunk.device
-   ->  Append
-         ->  HashAggregate
-               Output: _hyper_1_1_chunk."time", _hyper_1_1_chunk.device, avg(_hyper_1_1_chunk.temp)
-               Group Key: _hyper_1_1_chunk."time", _hyper_1_1_chunk.device
-               ->  Seq Scan on _timescaledb_internal._hyper_1_1_chunk
-                     Output: _hyper_1_1_chunk."time", _hyper_1_1_chunk.device, _hyper_1_1_chunk.temp
-         ->  HashAggregate
-               Output: _hyper_1_2_chunk."time", _hyper_1_2_chunk.device, avg(_hyper_1_2_chunk.temp)
-               Group Key: _hyper_1_2_chunk."time", _hyper_1_2_chunk.device
-               ->  Seq Scan on _timescaledb_internal._hyper_1_2_chunk
-                     Output: _hyper_1_2_chunk."time", _hyper_1_2_chunk.device, _hyper_1_2_chunk.temp
-         ->  HashAggregate
-               Output: _hyper_1_3_chunk."time", _hyper_1_3_chunk.device, avg(_hyper_1_3_chunk.temp)
-               Group Key: _hyper_1_3_chunk."time", _hyper_1_3_chunk.device
-               ->  Seq Scan on _timescaledb_internal._hyper_1_3_chunk
-                     Output: _hyper_1_3_chunk."time", _hyper_1_3_chunk.device, _hyper_1_3_chunk.temp
-         ->  HashAggregate
-               Output: _hyper_1_4_chunk."time", _hyper_1_4_chunk.device, avg(_hyper_1_4_chunk.temp)
-               Group Key: _hyper_1_4_chunk."time", _hyper_1_4_chunk.device
-               ->  Seq Scan on _timescaledb_internal._hyper_1_4_chunk
-                     Output: _hyper_1_4_chunk."time", _hyper_1_4_chunk.device, _hyper_1_4_chunk.temp
-(24 rows)
-
--- Partial aggregation since date_trunc(time) is not a partition key
-SET enable_partitionwise_aggregate = 'off';
-:PREFIX
-SELECT date_trunc('month', time), device, avg(temp)
-FROM hyper
-GROUP BY 1, 2
-ORDER BY 1, 2;
-                                                        QUERY PLAN                                                         
----------------------------------------------------------------------------------------------------------------------------
- Sort
-   Output: (date_trunc('month'::text, _hyper_1_1_chunk."time")), _hyper_1_1_chunk.device, (avg(_hyper_1_1_chunk.temp))
-   Sort Key: (date_trunc('month'::text, _hyper_1_1_chunk."time")), _hyper_1_1_chunk.device
-   ->  HashAggregate
-         Output: (date_trunc('month'::text, _hyper_1_1_chunk."time")), _hyper_1_1_chunk.device, avg(_hyper_1_1_chunk.temp)
-         Group Key: date_trunc('month'::text, _hyper_1_1_chunk."time"), _hyper_1_1_chunk.device
-         ->  Result
-               Output: date_trunc('month'::text, _hyper_1_1_chunk."time"), _hyper_1_1_chunk.device, _hyper_1_1_chunk.temp
-               ->  Append
-                     ->  Seq Scan on _timescaledb_internal._hyper_1_1_chunk
-                           Output: _hyper_1_1_chunk."time", _hyper_1_1_chunk.device, _hyper_1_1_chunk.temp
-                     ->  Seq Scan on _timescaledb_internal._hyper_1_2_chunk
-                           Output: _hyper_1_2_chunk."time", _hyper_1_2_chunk.device, _hyper_1_2_chunk.temp
-                     ->  Seq Scan on _timescaledb_internal._hyper_1_3_chunk
-                           Output: _hyper_1_3_chunk."time", _hyper_1_3_chunk.device, _hyper_1_3_chunk.temp
-                     ->  Seq Scan on _timescaledb_internal._hyper_1_4_chunk
-                           Output: _hyper_1_4_chunk."time", _hyper_1_4_chunk.device, _hyper_1_4_chunk.temp
-(17 rows)
-
-SET enable_partitionwise_aggregate = 'on';
-:PREFIX
-SELECT date_trunc('month', time), device, avg(temp)
-FROM hyper
-GROUP BY 1, 2
-ORDER BY 1, 2;
-                                                                  QUERY PLAN                                                                   
------------------------------------------------------------------------------------------------------------------------------------------------
- Sort
-   Output: (date_trunc('month'::text, _hyper_1_1_chunk."time")), _hyper_1_1_chunk.device, (avg(_hyper_1_1_chunk.temp))
-   Sort Key: (date_trunc('month'::text, _hyper_1_1_chunk."time")), _hyper_1_1_chunk.device
-   ->  Finalize HashAggregate
-         Output: (date_trunc('month'::text, _hyper_1_1_chunk."time")), _hyper_1_1_chunk.device, avg(_hyper_1_1_chunk.temp)
-         Group Key: (date_trunc('month'::text, _hyper_1_1_chunk."time")), _hyper_1_1_chunk.device
-         ->  Append
-               ->  Partial HashAggregate
-                     Output: (date_trunc('month'::text, _hyper_1_1_chunk."time")), _hyper_1_1_chunk.device, PARTIAL avg(_hyper_1_1_chunk.temp)
-                     Group Key: date_trunc('month'::text, _hyper_1_1_chunk."time"), _hyper_1_1_chunk.device
-                     ->  Seq Scan on _timescaledb_internal._hyper_1_1_chunk
-                           Output: date_trunc('month'::text, _hyper_1_1_chunk."time"), _hyper_1_1_chunk.device, _hyper_1_1_chunk.temp
-               ->  Partial HashAggregate
-                     Output: (date_trunc('month'::text, _hyper_1_2_chunk."time")), _hyper_1_2_chunk.device, PARTIAL avg(_hyper_1_2_chunk.temp)
-                     Group Key: date_trunc('month'::text, _hyper_1_2_chunk."time"), _hyper_1_2_chunk.device
-                     ->  Seq Scan on _timescaledb_internal._hyper_1_2_chunk
-                           Output: date_trunc('month'::text, _hyper_1_2_chunk."time"), _hyper_1_2_chunk.device, _hyper_1_2_chunk.temp
-               ->  Partial HashAggregate
-                     Output: (date_trunc('month'::text, _hyper_1_3_chunk."time")), _hyper_1_3_chunk.device, PARTIAL avg(_hyper_1_3_chunk.temp)
-                     Group Key: date_trunc('month'::text, _hyper_1_3_chunk."time"), _hyper_1_3_chunk.device
-                     ->  Seq Scan on _timescaledb_internal._hyper_1_3_chunk
-                           Output: date_trunc('month'::text, _hyper_1_3_chunk."time"), _hyper_1_3_chunk.device, _hyper_1_3_chunk.temp
-               ->  Partial HashAggregate
-                     Output: (date_trunc('month'::text, _hyper_1_4_chunk."time")), _hyper_1_4_chunk.device, PARTIAL avg(_hyper_1_4_chunk.temp)
-                     Group Key: date_trunc('month'::text, _hyper_1_4_chunk."time"), _hyper_1_4_chunk.device
-                     ->  Seq Scan on _timescaledb_internal._hyper_1_4_chunk
-                           Output: date_trunc('month'::text, _hyper_1_4_chunk."time"), _hyper_1_4_chunk.device, _hyper_1_4_chunk.temp
-(27 rows)
-
--- Also test time_bucket
-SET enable_partitionwise_aggregate = 'off';
-:PREFIX
-SELECT time_bucket('1 month', time), device, avg(temp)
-FROM hyper
-GROUP BY 1, 2
-ORDER BY 1, 2;
-                                                            QUERY PLAN                                                            
-----------------------------------------------------------------------------------------------------------------------------------
- Sort
-   Output: (time_bucket('@ 1 mon'::interval, _hyper_1_1_chunk."time")), _hyper_1_1_chunk.device, (avg(_hyper_1_1_chunk.temp))
-   Sort Key: (time_bucket('@ 1 mon'::interval, _hyper_1_1_chunk."time")), _hyper_1_1_chunk.device
-   ->  HashAggregate
-         Output: (time_bucket('@ 1 mon'::interval, _hyper_1_1_chunk."time")), _hyper_1_1_chunk.device, avg(_hyper_1_1_chunk.temp)
-         Group Key: time_bucket('@ 1 mon'::interval, _hyper_1_1_chunk."time"), _hyper_1_1_chunk.device
-         ->  Result
-               Output: time_bucket('@ 1 mon'::interval, _hyper_1_1_chunk."time"), _hyper_1_1_chunk.device, _hyper_1_1_chunk.temp
-               ->  Append
-                     ->  Seq Scan on _timescaledb_internal._hyper_1_1_chunk
-                           Output: _hyper_1_1_chunk."time", _hyper_1_1_chunk.device, _hyper_1_1_chunk.temp
-                     ->  Seq Scan on _timescaledb_internal._hyper_1_2_chunk
-                           Output: _hyper_1_2_chunk."time", _hyper_1_2_chunk.device, _hyper_1_2_chunk.temp
-                     ->  Seq Scan on _timescaledb_internal._hyper_1_3_chunk
-                           Output: _hyper_1_3_chunk."time", _hyper_1_3_chunk.device, _hyper_1_3_chunk.temp
-                     ->  Seq Scan on _timescaledb_internal._hyper_1_4_chunk
-                           Output: _hyper_1_4_chunk."time", _hyper_1_4_chunk.device, _hyper_1_4_chunk.temp
-(17 rows)
-
-SET enable_partitionwise_aggregate = 'on';
-:PREFIX
-SELECT time_bucket('1 month', time), device, avg(temp)
-FROM hyper
-GROUP BY 1, 2
-ORDER BY 1, 2;
-                                                                      QUERY PLAN                                                                      
-------------------------------------------------------------------------------------------------------------------------------------------------------
- Sort
-   Output: (time_bucket('@ 1 mon'::interval, _hyper_1_1_chunk."time")), _hyper_1_1_chunk.device, (avg(_hyper_1_1_chunk.temp))
-   Sort Key: (time_bucket('@ 1 mon'::interval, _hyper_1_1_chunk."time")), _hyper_1_1_chunk.device
-   ->  Finalize HashAggregate
-         Output: (time_bucket('@ 1 mon'::interval, _hyper_1_1_chunk."time")), _hyper_1_1_chunk.device, avg(_hyper_1_1_chunk.temp)
-         Group Key: (time_bucket('@ 1 mon'::interval, _hyper_1_1_chunk."time")), _hyper_1_1_chunk.device
-         ->  Append
-               ->  Partial HashAggregate
-                     Output: (time_bucket('@ 1 mon'::interval, _hyper_1_1_chunk."time")), _hyper_1_1_chunk.device, PARTIAL avg(_hyper_1_1_chunk.temp)
-                     Group Key: time_bucket('@ 1 mon'::interval, _hyper_1_1_chunk."time"), _hyper_1_1_chunk.device
-                     ->  Seq Scan on _timescaledb_internal._hyper_1_1_chunk
-                           Output: time_bucket('@ 1 mon'::interval, _hyper_1_1_chunk."time"), _hyper_1_1_chunk.device, _hyper_1_1_chunk.temp
-               ->  Partial HashAggregate
-                     Output: (time_bucket('@ 1 mon'::interval, _hyper_1_2_chunk."time")), _hyper_1_2_chunk.device, PARTIAL avg(_hyper_1_2_chunk.temp)
-                     Group Key: time_bucket('@ 1 mon'::interval, _hyper_1_2_chunk."time"), _hyper_1_2_chunk.device
-                     ->  Seq Scan on _timescaledb_internal._hyper_1_2_chunk
-                           Output: time_bucket('@ 1 mon'::interval, _hyper_1_2_chunk."time"), _hyper_1_2_chunk.device, _hyper_1_2_chunk.temp
-               ->  Partial HashAggregate
-                     Output: (time_bucket('@ 1 mon'::interval, _hyper_1_3_chunk."time")), _hyper_1_3_chunk.device, PARTIAL avg(_hyper_1_3_chunk.temp)
-                     Group Key: time_bucket('@ 1 mon'::interval, _hyper_1_3_chunk."time"), _hyper_1_3_chunk.device
-                     ->  Seq Scan on _timescaledb_internal._hyper_1_3_chunk
-                           Output: time_bucket('@ 1 mon'::interval, _hyper_1_3_chunk."time"), _hyper_1_3_chunk.device, _hyper_1_3_chunk.temp
-               ->  Partial HashAggregate
-                     Output: (time_bucket('@ 1 mon'::interval, _hyper_1_4_chunk."time")), _hyper_1_4_chunk.device, PARTIAL avg(_hyper_1_4_chunk.temp)
-                     Group Key: time_bucket('@ 1 mon'::interval, _hyper_1_4_chunk."time"), _hyper_1_4_chunk.device
-                     ->  Seq Scan on _timescaledb_internal._hyper_1_4_chunk
-                           Output: time_bucket('@ 1 mon'::interval, _hyper_1_4_chunk."time"), _hyper_1_4_chunk.device, _hyper_1_4_chunk.temp
-(27 rows)
-
--- Test partitionwise joins, mostly to see that we do not break
--- anything
-CREATE TABLE hyper_meta (time timestamptz, device int, info text);
-SELECT * FROM create_hypertable('hyper_meta', 'time', 'device', 2);
-NOTICE:  adding not-null constraint to column "time"
- hypertable_id | schema_name | table_name | created 
----------------+-------------+------------+---------
-             2 | public      | hyper_meta | t
-(1 row)
-
-INSERT INTO hyper_meta VALUES
-       ('2018-02-19 13:01', 1, 'device_1'),
-       ('2018-02-19 13:02', 3, 'device_3');
-SET enable_partitionwise_join = 'off';
-:PREFIX
-SELECT h.time, h.device, h.temp, hm.info
-FROM hyper h, hyper_meta hm
-WHERE h.device = hm.device;
-                                                       QUERY PLAN                                                        
--------------------------------------------------------------------------------------------------------------------------
- Merge Join
-   Output: h_1."time", h_1.device, h_1.temp, hm_1.info
-   Merge Cond: (hm_1.device = h_1.device)
-   ->  Merge Append
-         Sort Key: hm_1.device
-         ->  Index Scan using _hyper_2_5_chunk_hyper_meta_device_time_idx on _timescaledb_internal._hyper_2_5_chunk hm_1
-               Output: hm_1.info, hm_1.device
-         ->  Index Scan using _hyper_2_6_chunk_hyper_meta_device_time_idx on _timescaledb_internal._hyper_2_6_chunk hm_2
-               Output: hm_2.info, hm_2.device
-   ->  Materialize
-         Output: h_1."time", h_1.device, h_1.temp
-         ->  Merge Append
-               Sort Key: h_1.device
-               ->  Index Scan using _hyper_1_1_chunk_hyper_device_time_idx on _timescaledb_internal._hyper_1_1_chunk h_1
-                     Output: h_1."time", h_1.device, h_1.temp
-               ->  Index Scan using _hyper_1_2_chunk_hyper_device_time_idx on _timescaledb_internal._hyper_1_2_chunk h_2
-                     Output: h_2."time", h_2.device, h_2.temp
-               ->  Index Scan using _hyper_1_3_chunk_hyper_device_time_idx on _timescaledb_internal._hyper_1_3_chunk h_3
-                     Output: h_3."time", h_3.device, h_3.temp
-               ->  Index Scan using _hyper_1_4_chunk_hyper_device_time_idx on _timescaledb_internal._hyper_1_4_chunk h_4
-                     Output: h_4."time", h_4.device, h_4.temp
-(21 rows)
-
-:PREFIX
-SELECT pg2.time, pg2.device, pg2.temp, pg1.temp
-FROM pg2dim pg2, pg1dim pg1
-WHERE pg2.device = pg1.device;
-                             QUERY PLAN                             
---------------------------------------------------------------------
- Merge Join
-   Output: pg2."time", pg2.device, pg2.temp, pg1.temp
-   Merge Cond: (pg1.device = pg2.device)
-   ->  Sort
-         Output: pg1.temp, pg1.device
-         Sort Key: pg1.device
-         ->  Append
-               ->  Seq Scan on public.pg1dim_h1 pg1_1
-                     Output: pg1_1.temp, pg1_1.device
-               ->  Seq Scan on public.pg1dim_h2 pg1_2
-                     Output: pg1_2.temp, pg1_2.device
-   ->  Sort
-         Output: pg2."time", pg2.device, pg2.temp
-         Sort Key: pg2.device
-         ->  Append
-               ->  Seq Scan on public.pg2dim_h1_t1 pg2_1
-                     Output: pg2_1."time", pg2_1.device, pg2_1.temp
-               ->  Seq Scan on public.pg2dim_h1_t2 pg2_2
-                     Output: pg2_2."time", pg2_2.device, pg2_2.temp
-               ->  Seq Scan on public.pg2dim_h2_t1 pg2_3
-                     Output: pg2_3."time", pg2_3.device, pg2_3.temp
-               ->  Seq Scan on public.pg2dim_h2_t2 pg2_4
-                     Output: pg2_4."time", pg2_4.device, pg2_4.temp
-(23 rows)
-
-SET enable_partitionwise_join = 'on';
-:PREFIX
-SELECT h.time, h.device, h.temp, hm.info
-FROM hyper h, hyper_meta hm
-WHERE h.device = hm.device;
-                                                       QUERY PLAN                                                        
--------------------------------------------------------------------------------------------------------------------------
- Merge Join
-   Output: h_1."time", h_1.device, h_1.temp, hm_1.info
-   Merge Cond: (hm_1.device = h_1.device)
-   ->  Merge Append
-         Sort Key: hm_1.device
-         ->  Index Scan using _hyper_2_5_chunk_hyper_meta_device_time_idx on _timescaledb_internal._hyper_2_5_chunk hm_1
-               Output: hm_1.info, hm_1.device
-         ->  Index Scan using _hyper_2_6_chunk_hyper_meta_device_time_idx on _timescaledb_internal._hyper_2_6_chunk hm_2
-               Output: hm_2.info, hm_2.device
-   ->  Materialize
-         Output: h_1."time", h_1.device, h_1.temp
-         ->  Merge Append
-               Sort Key: h_1.device
-               ->  Index Scan using _hyper_1_1_chunk_hyper_device_time_idx on _timescaledb_internal._hyper_1_1_chunk h_1
-                     Output: h_1."time", h_1.device, h_1.temp
-               ->  Index Scan using _hyper_1_2_chunk_hyper_device_time_idx on _timescaledb_internal._hyper_1_2_chunk h_2
-                     Output: h_2."time", h_2.device, h_2.temp
-               ->  Index Scan using _hyper_1_3_chunk_hyper_device_time_idx on _timescaledb_internal._hyper_1_3_chunk h_3
-                     Output: h_3."time", h_3.device, h_3.temp
-               ->  Index Scan using _hyper_1_4_chunk_hyper_device_time_idx on _timescaledb_internal._hyper_1_4_chunk h_4
-                     Output: h_4."time", h_4.device, h_4.temp
-(21 rows)
-
-:PREFIX
-SELECT pg2.time, pg2.device, pg2.temp, pg1.temp
-FROM pg2dim pg2, pg1dim pg1
-WHERE pg2.device = pg1.device;
-                                QUERY PLAN                                
---------------------------------------------------------------------------
- Append
-   ->  Merge Join
-         Output: pg2_2."time", pg2_2.device, pg2_2.temp, pg1_1.temp
-         Merge Cond: (pg1_1.device = pg2_2.device)
-         ->  Sort
-               Output: pg1_1.temp, pg1_1.device
-               Sort Key: pg1_1.device
-               ->  Seq Scan on public.pg1dim_h1 pg1_1
-                     Output: pg1_1.temp, pg1_1.device
-         ->  Sort
-               Output: pg2_2."time", pg2_2.device, pg2_2.temp
-               Sort Key: pg2_2.device
-               ->  Append
-                     ->  Seq Scan on public.pg2dim_h1_t1 pg2_2
-                           Output: pg2_2."time", pg2_2.device, pg2_2.temp
-                     ->  Seq Scan on public.pg2dim_h1_t2 pg2_3
-                           Output: pg2_3."time", pg2_3.device, pg2_3.temp
-   ->  Merge Join
-         Output: pg2_5."time", pg2_5.device, pg2_5.temp, pg1_2.temp
-         Merge Cond: (pg1_2.device = pg2_5.device)
-         ->  Sort
-               Output: pg1_2.temp, pg1_2.device
-               Sort Key: pg1_2.device
-               ->  Seq Scan on public.pg1dim_h2 pg1_2
-                     Output: pg1_2.temp, pg1_2.device
-         ->  Sort
-               Output: pg2_5."time", pg2_5.device, pg2_5.temp
-               Sort Key: pg2_5.device
-               ->  Append
-                     ->  Seq Scan on public.pg2dim_h2_t1 pg2_5
-                           Output: pg2_5."time", pg2_5.device, pg2_5.temp
-                     ->  Seq Scan on public.pg2dim_h2_t2 pg2_6
-                           Output: pg2_6."time", pg2_6.device, pg2_6.temp
-(33 rows)
-
--- Test hypertable with time partitioning function
-CREATE OR REPLACE FUNCTION time_func(unixtime float8)
-    RETURNS TIMESTAMPTZ LANGUAGE PLPGSQL IMMUTABLE AS
-$BODY$
-DECLARE
-    retval TIMESTAMPTZ;
-BEGIN
-    retval := to_timestamp(unixtime);
-    RETURN retval;
-END
-$BODY$;
-CREATE TABLE hyper_timepart (time float8, device int, temp float);
-SELECT * FROM create_hypertable('hyper_timepart', 'time', 'device', 2, time_partitioning_func => 'time_func');
-NOTICE:  adding not-null constraint to column "time"
- hypertable_id | schema_name |   table_name   | created 
----------------+-------------+----------------+---------
-             3 | public      | hyper_timepart | t
-(1 row)
-
--- Planner won't pick push-down aggs on table with time function
--- unless a certain amount of data
-SELECT setseed(1);
- setseed 
----------
- 
-(1 row)
-
-INSERT INTO hyper_timepart
-SELECT x, ceil(random() * 8), random() * 20
-FROM generate_series(0,5000-1) AS x;
--- All partition keys covered (full partitionwise)
-SET enable_partitionwise_aggregate = 'off';
-:PREFIX
-SELECT time, device, avg(temp)
-FROM hyper_timepart
-GROUP BY 1, 2
-ORDER BY 1, 2
-LIMIT 10;
-                                                QUERY PLAN                                                 
------------------------------------------------------------------------------------------------------------
- Limit
-   Output: _hyper_3_7_chunk."time", _hyper_3_7_chunk.device, (avg(_hyper_3_7_chunk.temp))
-   ->  Sort
-         Output: _hyper_3_7_chunk."time", _hyper_3_7_chunk.device, (avg(_hyper_3_7_chunk.temp))
-         Sort Key: _hyper_3_7_chunk."time", _hyper_3_7_chunk.device
-         ->  HashAggregate
-               Output: _hyper_3_7_chunk."time", _hyper_3_7_chunk.device, avg(_hyper_3_7_chunk.temp)
-               Group Key: _hyper_3_7_chunk."time", _hyper_3_7_chunk.device
-               ->  Append
-                     ->  Seq Scan on _timescaledb_internal._hyper_3_7_chunk
-                           Output: _hyper_3_7_chunk."time", _hyper_3_7_chunk.device, _hyper_3_7_chunk.temp
-                     ->  Seq Scan on _timescaledb_internal._hyper_3_8_chunk
-                           Output: _hyper_3_8_chunk."time", _hyper_3_8_chunk.device, _hyper_3_8_chunk.temp
-(13 rows)
-
-:PREFIX
-SELECT time_func(time), device, avg(temp)
-FROM hyper_timepart
-GROUP BY 1, 2
-ORDER BY 1, 2
-LIMIT 10;
-                                                                     QUERY PLAN                                                                      
------------------------------------------------------------------------------------------------------------------------------------------------------
- Limit
-   Output: (time_func(_hyper_3_7_chunk."time")), _hyper_3_7_chunk.device, (avg(_hyper_3_7_chunk.temp))
-   ->  GroupAggregate
-         Output: (time_func(_hyper_3_7_chunk."time")), _hyper_3_7_chunk.device, avg(_hyper_3_7_chunk.temp)
-         Group Key: (time_func(_hyper_3_7_chunk."time")), _hyper_3_7_chunk.device
-         ->  Incremental Sort
-               Output: (time_func(_hyper_3_7_chunk."time")), _hyper_3_7_chunk.device, _hyper_3_7_chunk.temp
-               Sort Key: (time_func(_hyper_3_7_chunk."time")), _hyper_3_7_chunk.device
-               Presorted Key: (time_func(_hyper_3_7_chunk."time"))
-               ->  Result
-                     Output: (time_func(_hyper_3_7_chunk."time")), _hyper_3_7_chunk.device, _hyper_3_7_chunk.temp
-                     ->  Merge Append
-                           Sort Key: (time_func(_hyper_3_7_chunk."time"))
-                           ->  Index Scan Backward using _hyper_3_7_chunk_hyper_timepart_expr_idx on _timescaledb_internal._hyper_3_7_chunk
-                                 Output: _hyper_3_7_chunk."time", _hyper_3_7_chunk.device, _hyper_3_7_chunk.temp, time_func(_hyper_3_7_chunk."time")
-                           ->  Index Scan Backward using _hyper_3_8_chunk_hyper_timepart_expr_idx on _timescaledb_internal._hyper_3_8_chunk
-                                 Output: _hyper_3_8_chunk."time", _hyper_3_8_chunk.device, _hyper_3_8_chunk.temp, time_func(_hyper_3_8_chunk."time")
-(17 rows)
-
--- Grouping on original time column should be pushed-down
-SET enable_partitionwise_aggregate = 'on';
-:PREFIX
-SELECT time, device, avg(temp)
-FROM hyper_timepart
-GROUP BY 1, 2
-ORDER BY 1, 2
-LIMIT 10;
-                                                QUERY PLAN                                                 
------------------------------------------------------------------------------------------------------------
- Limit
-   Output: _hyper_3_7_chunk."time", _hyper_3_7_chunk.device, (avg(_hyper_3_7_chunk.temp))
-   ->  Sort
-         Output: _hyper_3_7_chunk."time", _hyper_3_7_chunk.device, (avg(_hyper_3_7_chunk.temp))
-         Sort Key: _hyper_3_7_chunk."time", _hyper_3_7_chunk.device
-         ->  Append
-               ->  HashAggregate
-                     Output: _hyper_3_7_chunk."time", _hyper_3_7_chunk.device, avg(_hyper_3_7_chunk.temp)
-                     Group Key: _hyper_3_7_chunk."time", _hyper_3_7_chunk.device
-                     ->  Seq Scan on _timescaledb_internal._hyper_3_7_chunk
-                           Output: _hyper_3_7_chunk."time", _hyper_3_7_chunk.device, _hyper_3_7_chunk.temp
-               ->  HashAggregate
-                     Output: _hyper_3_8_chunk."time", _hyper_3_8_chunk.device, avg(_hyper_3_8_chunk.temp)
-                     Group Key: _hyper_3_8_chunk."time", _hyper_3_8_chunk.device
-                     ->  Seq Scan on _timescaledb_internal._hyper_3_8_chunk
-                           Output: _hyper_3_8_chunk."time", _hyper_3_8_chunk.device, _hyper_3_8_chunk.temp
-(16 rows)
-
--- Applying the time partitioning function should also allow push-down
--- on open dimensions
-:PREFIX
-SELECT time_func(time), device, avg(temp)
-FROM hyper_timepart
-GROUP BY 1, 2
-ORDER BY 1, 2
-LIMIT 10;
-                                                              QUERY PLAN                                                              
---------------------------------------------------------------------------------------------------------------------------------------
- Limit
-   Output: (time_func(_hyper_3_7_chunk."time")), _hyper_3_7_chunk.device, (avg(_hyper_3_7_chunk.temp))
-   ->  Merge Append
-         Sort Key: (time_func(_hyper_3_7_chunk."time")), _hyper_3_7_chunk.device
-         ->  GroupAggregate
-               Output: (time_func(_hyper_3_7_chunk."time")), _hyper_3_7_chunk.device, avg(_hyper_3_7_chunk.temp)
-               Group Key: (time_func(_hyper_3_7_chunk."time")), _hyper_3_7_chunk.device
-               ->  Incremental Sort
-                     Output: (time_func(_hyper_3_7_chunk."time")), _hyper_3_7_chunk.device, _hyper_3_7_chunk.temp
-                     Sort Key: (time_func(_hyper_3_7_chunk."time")), _hyper_3_7_chunk.device
-                     Presorted Key: (time_func(_hyper_3_7_chunk."time"))
-                     ->  Index Scan Backward using _hyper_3_7_chunk_hyper_timepart_expr_idx on _timescaledb_internal._hyper_3_7_chunk
-                           Output: time_func(_hyper_3_7_chunk."time"), _hyper_3_7_chunk.device, _hyper_3_7_chunk.temp
-         ->  GroupAggregate
-               Output: (time_func(_hyper_3_8_chunk."time")), _hyper_3_8_chunk.device, avg(_hyper_3_8_chunk.temp)
-               Group Key: (time_func(_hyper_3_8_chunk."time")), _hyper_3_8_chunk.device
-               ->  Incremental Sort
-                     Output: (time_func(_hyper_3_8_chunk."time")), _hyper_3_8_chunk.device, _hyper_3_8_chunk.temp
-                     Sort Key: (time_func(_hyper_3_8_chunk."time")), _hyper_3_8_chunk.device
-                     Presorted Key: (time_func(_hyper_3_8_chunk."time"))
-                     ->  Index Scan Backward using _hyper_3_8_chunk_hyper_timepart_expr_idx on _timescaledb_internal._hyper_3_8_chunk
-                           Output: time_func(_hyper_3_8_chunk."time"), _hyper_3_8_chunk.device, _hyper_3_8_chunk.temp
-(22 rows)
-
--- Should also work to use partitioning function on closed dimensions
-:PREFIX
-SELECT time_func(time), _timescaledb_internal.get_partition_hash(device), avg(temp)
-FROM hyper_timepart
-GROUP BY 1, 2
-ORDER BY 1, 2
-LIMIT 10;
-                                                                           QUERY PLAN                                                                           
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
- Limit
-   Output: (time_func(_hyper_3_7_chunk."time")), (_timescaledb_internal.get_partition_hash(_hyper_3_7_chunk.device)), (avg(_hyper_3_7_chunk.temp))
-   ->  Merge Append
-         Sort Key: (time_func(_hyper_3_7_chunk."time")), (_timescaledb_internal.get_partition_hash(_hyper_3_7_chunk.device))
-         ->  GroupAggregate
-               Output: (time_func(_hyper_3_7_chunk."time")), (_timescaledb_internal.get_partition_hash(_hyper_3_7_chunk.device)), avg(_hyper_3_7_chunk.temp)
-               Group Key: (time_func(_hyper_3_7_chunk."time")), (_timescaledb_internal.get_partition_hash(_hyper_3_7_chunk.device))
-               ->  Incremental Sort
-                     Output: (time_func(_hyper_3_7_chunk."time")), (_timescaledb_internal.get_partition_hash(_hyper_3_7_chunk.device)), _hyper_3_7_chunk.temp
-                     Sort Key: (time_func(_hyper_3_7_chunk."time")), (_timescaledb_internal.get_partition_hash(_hyper_3_7_chunk.device))
-                     Presorted Key: (time_func(_hyper_3_7_chunk."time"))
-                     ->  Index Scan Backward using _hyper_3_7_chunk_hyper_timepart_expr_idx on _timescaledb_internal._hyper_3_7_chunk
-                           Output: time_func(_hyper_3_7_chunk."time"), _timescaledb_internal.get_partition_hash(_hyper_3_7_chunk.device), _hyper_3_7_chunk.temp
-         ->  GroupAggregate
-               Output: (time_func(_hyper_3_8_chunk."time")), (_timescaledb_internal.get_partition_hash(_hyper_3_8_chunk.device)), avg(_hyper_3_8_chunk.temp)
-               Group Key: (time_func(_hyper_3_8_chunk."time")), (_timescaledb_internal.get_partition_hash(_hyper_3_8_chunk.device))
-               ->  Incremental Sort
-                     Output: (time_func(_hyper_3_8_chunk."time")), (_timescaledb_internal.get_partition_hash(_hyper_3_8_chunk.device)), _hyper_3_8_chunk.temp
-                     Sort Key: (time_func(_hyper_3_8_chunk."time")), (_timescaledb_internal.get_partition_hash(_hyper_3_8_chunk.device))
-                     Presorted Key: (time_func(_hyper_3_8_chunk."time"))
-                     ->  Index Scan Backward using _hyper_3_8_chunk_hyper_timepart_expr_idx on _timescaledb_internal._hyper_3_8_chunk
-                           Output: time_func(_hyper_3_8_chunk."time"), _timescaledb_internal.get_partition_hash(_hyper_3_8_chunk.device), _hyper_3_8_chunk.temp
-(22 rows)
-
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/pg_join.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/pg_join.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/pg_join.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/pg_join.out	2023-11-25 05:27:44.129022707 +0000
@@ -1,201 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
--- this test suite is based on the postgres join tests
---
--- the tests have been adjusted to work with hypertables
--- statements that would generate an error have been commented out
--- because errors don't play nicely with psql output redirection
--- plan output has been disabled, because we are not interested in
--- the actual plans produced but in the correctness of the results
--- we need superuser because some of the tests modify statistics
-\c :TEST_DBNAME :ROLE_SUPERUSER
-\set TEST_BASE_NAME join
-SELECT format('include/%s_load.sql', :'TEST_BASE_NAME') as "TEST_LOAD_NAME",
-       format('include/%s_query.sql', :'TEST_BASE_NAME') as "TEST_QUERY_NAME",
-       format('%s/results/%s_results_optimized.out', :'TEST_OUTPUT_DIR', :'TEST_BASE_NAME') as "TEST_RESULTS_OPTIMIZED",
-       format('%s/results/%s_results_unoptimized.out', :'TEST_OUTPUT_DIR', :'TEST_BASE_NAME') as "TEST_RESULTS_UNOPTIMIZED"
-\gset
-SELECT format('\! diff -u --label "Unoptimized results" --label "Optimized results" %s %s', :'TEST_RESULTS_UNOPTIMIZED', :'TEST_RESULTS_OPTIMIZED') as "DIFF_CMD"
-\gset
-set client_min_messages to warning;
-\ir :TEST_LOAD_NAME
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
--- these table definitions have been adjusted from
--- table defintions of the postgres test suite
-CREATE TABLE INT2_TBL(f1 int2, ts timestamptz NOT NULL DEFAULT '2000-01-01');
-SELECT table_name FROM create_hypertable('int2_tbl','ts');
- table_name 
-------------
- int2_tbl
-(1 row)
-
-INSERT INTO INT2_TBL(f1) VALUES ('0   ');
-INSERT INTO INT2_TBL(f1) VALUES ('  1234 ');
-INSERT INTO INT2_TBL(f1) VALUES ('    -1234');
--- largest and smallest values
-INSERT INTO INT2_TBL(f1) VALUES ('32767');
-INSERT INTO INT2_TBL(f1) VALUES ('-32767');
-CREATE TABLE INT4_TBL(f1 int4, ts timestamptz NOT NULL DEFAULT '2000-01-01');
-SELECT table_name FROM create_hypertable('int4_tbl','ts');
- table_name 
-------------
- int4_tbl
-(1 row)
-
-INSERT INTO INT4_TBL(f1) VALUES ('   0  ');
-INSERT INTO INT4_TBL(f1) VALUES ('123456     ');
-INSERT INTO INT4_TBL(f1) VALUES ('    -123456');
--- largest and smallest values
-INSERT INTO INT4_TBL(f1) VALUES ('2147483647');
-INSERT INTO INT4_TBL(f1) VALUES ('-2147483647');
-CREATE TABLE INT8_TBL(q1 int8, q2 int8, ts timestamptz NOT NULL DEFAULT '2000-01-01');
-SELECT table_name FROM create_hypertable('int8_tbl','ts');
- table_name 
-------------
- int8_tbl
-(1 row)
-
-INSERT INTO INT8_TBL VALUES('  123   ','  456');
-INSERT INTO INT8_TBL VALUES('123   ','4567890123456789');
-INSERT INTO INT8_TBL VALUES('4567890123456789','123');
-INSERT INTO INT8_TBL VALUES(+4567890123456789,'4567890123456789');
-INSERT INTO INT8_TBL VALUES('+4567890123456789','-4567890123456789');
-CREATE TABLE FLOAT8_TBL(f1 float8, ts timestamptz NOT NULL DEFAULT '2000-01-01');
-SELECT table_name FROM create_hypertable('float8_tbl','ts');
- table_name 
-------------
- float8_tbl
-(1 row)
-
-INSERT INTO FLOAT8_TBL(f1) VALUES ('    0.0   ');
-INSERT INTO FLOAT8_TBL(f1) VALUES ('1004.30  ');
-INSERT INTO FLOAT8_TBL(f1) VALUES ('   -34.84');
-INSERT INTO FLOAT8_TBL(f1) VALUES ('1.2345678901234e+200');
-INSERT INTO FLOAT8_TBL(f1) VALUES ('1.2345678901234e-200');
-CREATE TABLE TEXT_TBL (f1 text, ts timestamptz NOT NULL DEFAULT '2000-01-01');
-SELECT table_name FROM create_hypertable('text_tbl','ts');
- table_name 
-------------
- text_tbl
-(1 row)
-
-INSERT INTO TEXT_TBL VALUES ('doh!');
-INSERT INTO TEXT_TBL VALUES ('hi de ho neighbor');
-CREATE TABLE a (aa TEXT);
-CREATE TABLE b (bb TEXT) INHERITS (a);
-CREATE TABLE c (cc TEXT) INHERITS (a);
-CREATE TABLE d (dd TEXT) INHERITS (b,c,a);
-INSERT INTO a(aa) VALUES('aaa');
-INSERT INTO a(aa) VALUES('aaaa');
-INSERT INTO a(aa) VALUES('aaaaa');
-INSERT INTO a(aa) VALUES('aaaaaa');
-INSERT INTO a(aa) VALUES('aaaaaaa');
-INSERT INTO a(aa) VALUES('aaaaaaaa');
-INSERT INTO b(aa) VALUES('bbb');
-INSERT INTO b(aa) VALUES('bbbb');
-INSERT INTO b(aa) VALUES('bbbbb');
-INSERT INTO b(aa) VALUES('bbbbbb');
-INSERT INTO b(aa) VALUES('bbbbbbb');
-INSERT INTO b(aa) VALUES('bbbbbbbb');
-INSERT INTO c(aa) VALUES('ccc');
-INSERT INTO c(aa) VALUES('cccc');
-INSERT INTO c(aa) VALUES('ccccc');
-INSERT INTO c(aa) VALUES('cccccc');
-INSERT INTO c(aa) VALUES('ccccccc');
-INSERT INTO c(aa) VALUES('cccccccc');
-INSERT INTO d(aa) VALUES('ddd');
-INSERT INTO d(aa) VALUES('dddd');
-INSERT INTO d(aa) VALUES('ddddd');
-INSERT INTO d(aa) VALUES('dddddd');
-INSERT INTO d(aa) VALUES('ddddddd');
-INSERT INTO d(aa) VALUES('dddddddd');
-CREATE TABLE onek (
-  unique1   int4,
-  unique2   int4,
-  two     int4,
-  four    int4,
-  ten     int4,
-  twenty    int4,
-  hundred   int4,
-  thousand  int4,
-  twothousand int4,
-  fivethous int4,
-  tenthous  int4,
-  odd     int4,
-  even    int4,
-  stringu1  name,
-  stringu2  name,
-  string4   name
-);
-SELECT table_name FROM create_hypertable('onek','unique2',chunk_time_interval:=1000);
- table_name 
-------------
- onek
-(1 row)
-
-\copy onek FROM 'data/onek.data'
-CREATE TABLE tenk1 (
-  unique1   int4,
-  unique2   int4,
-  two     int4,
-  four    int4,
-  ten     int4,
-  twenty    int4,
-  hundred   int4,
-  thousand  int4,
-  twothousand int4,
-  fivethous int4,
-  tenthous  int4,
-  odd     int4,
-  even    int4,
-  stringu1  name,
-  stringu2  name,
-  string4   name
-);
-SELECT table_name FROM create_hypertable('tenk1','unique2',chunk_time_interval:=1000);
- table_name 
-------------
- tenk1
-(1 row)
-
-\copy tenk1 FROM 'data/tenk.data'
-CREATE TABLE tenk2 (
-  unique1   int4,
-  unique2   int4,
-  two     int4,
-  four    int4,
-  ten     int4,
-  twenty    int4,
-  hundred   int4,
-  thousand  int4,
-  twothousand int4,
-  fivethous int4,
-  tenthous  int4,
-  odd     int4,
-  even    int4,
-  stringu1  name,
-  stringu2  name,
-  string4   name
-);
-SELECT table_name FROM create_hypertable('tenk2','unique2',chunk_time_interval:=1000);
- table_name 
-------------
- tenk2
-(1 row)
-
-INSERT INTO tenk2 SELECT * FROM tenk1;
-\set PREFIX ''
-\set ECHO errors
---- Unoptimized results
-+++ Optimized results
-@@ -1,6 +1,6 @@
-              setting              | value 
- ----------------------------------+-------
-- timescaledb.enable_optimizations | off
-+ timescaledb.enable_optimizations | on
- (1 row)
- 
-  table_name 
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/plain.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/plain.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/plain.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/plain.out	2023-11-25 05:27:44.129022707 +0000
@@ -1,49 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
--- Tests for plain PostgreSQL commands to ensure that they work while
--- the TimescaleDB extension is loaded. This is a mix of statements
--- added mostly as regression checks when bugs are discovered and
--- fixed.
-CREATE TABLE regular_table(time timestamp, temp float8, tag text, color integer);
--- Renaming indexes should work
-CREATE INDEX time_color_idx ON regular_table(time, color);
-ALTER INDEX time_color_idx RENAME TO time_color_idx2;
-ALTER TABLE regular_table ALTER COLUMN color TYPE bigint;
-SELECT * FROM test.show_columns('regular_table');
- Column |            Type             | NotNull 
---------+-----------------------------+---------
- time   | timestamp without time zone | f
- temp   | double precision            | f
- tag    | text                        | f
- color  | bigint                      | f
-(4 rows)
-
-SELECT * FROM test.show_indexes('regular_table');
-      Index      |   Columns    | Expr | Unique | Primary | Exclusion | Tablespace 
------------------+--------------+------+--------+---------+-----------+------------
- time_color_idx2 | {time,color} |      | f      | f       | f         | 
-(1 row)
-
--- Renaming types should work
-CREATE TYPE rainbow AS ENUM ('red', 'orange', 'yellow', 'green', 'blue', 'purple');
-ALTER TYPE rainbow RENAME TO colors;
-\dT+
-                                           List of data types
- Schema |  Name  | Internal name | Size | Elements |       Owner       | Access privileges | Description 
---------+--------+---------------+------+----------+-------------------+-------------------+-------------
- public | colors | colors        | 4    | red     +| default_perm_user |                   | 
-        |        |               |      | orange  +|                   |                   | 
-        |        |               |      | yellow  +|                   |                   | 
-        |        |               |      | green   +|                   |                   | 
-        |        |               |      | blue    +|                   |                   | 
-        |        |               |      | purple   |                   |                   | 
- public | rxid   | rxid          | 16   |          | super_user        |                   | 
-(2 rows)
-
-REINDEX TABLE regular_table;
-\c :TEST_DBNAME :ROLE_SUPERUSER
-REINDEX SCHEMA public;
--- Not only simple statements should work
-CREATE TABLE a (aa TEXT);
-CREATE TABLE z (b TEXT, PRIMARY KEY(aa, b)) inherits (a);
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/plan_expand_hypertable-15.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/plan_expand_hypertable-15.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/plan_expand_hypertable-15.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/plan_expand_hypertable-15.out	2023-11-25 05:27:44.129022707 +0000
@@ -1,3026 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-\set PREFIX 'EXPLAIN (costs off) '
-\ir include/plan_expand_hypertable_load.sql
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
---single time dimension
-CREATE TABLE hyper ("time_broken" bigint NOT NULL, "value" integer);
-ALTER TABLE hyper
-DROP COLUMN time_broken,
-ADD COLUMN time BIGINT;
-SELECT create_hypertable('hyper', 'time',  chunk_time_interval => 10);
-psql:include/plan_expand_hypertable_load.sql:12: NOTICE:  adding not-null constraint to column "time"
- create_hypertable  
---------------------
- (1,public,hyper,t)
-(1 row)
-
-INSERT INTO hyper SELECT g, g FROM generate_series(0,1000) g;
---insert a point with INT_MAX_64
-INSERT INTO hyper (time, value) SELECT 9223372036854775807::bigint, 0;
---time and space
-CREATE TABLE hyper_w_space ("time_broken" bigint NOT NULL, "device_id" text, "value" integer);
-ALTER TABLE hyper_w_space
-DROP COLUMN time_broken,
-ADD COLUMN time BIGINT;
-SELECT create_hypertable('hyper_w_space', 'time', 'device_id', 4, chunk_time_interval => 10);
-psql:include/plan_expand_hypertable_load.sql:26: NOTICE:  adding not-null constraint to column "time"
-     create_hypertable      
-----------------------------
- (2,public,hyper_w_space,t)
-(1 row)
-
-INSERT INTO hyper_w_space (time, device_id, value) SELECT g, 'dev' || g, g FROM generate_series(0,30) g;
-CREATE VIEW hyper_w_space_view AS (SELECT * FROM hyper_w_space);
---with timestamp and space
-CREATE TABLE tag (id serial PRIMARY KEY, name text);
-CREATE TABLE hyper_ts ("time_broken" timestamptz NOT NULL, "device_id" text, tag_id INT REFERENCES tag(id), "value" integer);
-ALTER TABLE hyper_ts
-DROP COLUMN time_broken,
-ADD COLUMN time TIMESTAMPTZ;
-SELECT create_hypertable('hyper_ts', 'time', 'device_id', 2, chunk_time_interval => '10 seconds'::interval);
-psql:include/plan_expand_hypertable_load.sql:41: NOTICE:  adding not-null constraint to column "time"
-   create_hypertable   
------------------------
- (3,public,hyper_ts,t)
-(1 row)
-
-INSERT INTO tag(name) SELECT 'tag'||g FROM generate_series(0,10) g;
-INSERT INTO hyper_ts (time, device_id, tag_id, value) SELECT to_timestamp(g), 'dev' || g, (random() /10)+1, g FROM generate_series(0,30) g;
---one in the future
-INSERT INTO hyper_ts (time, device_id, tag_id, value)  VALUES ('2100-01-01 02:03:04 PST', 'dev101', 1, 0);
---time partitioning function
-CREATE OR REPLACE FUNCTION unix_to_timestamp(unixtime float8)
-    RETURNS TIMESTAMPTZ LANGUAGE SQL IMMUTABLE PARALLEL SAFE STRICT AS
-$BODY$
-    SELECT to_timestamp(unixtime);
-$BODY$;
-CREATE TABLE hyper_timefunc ("time" float8 NOT NULL, "device_id" text, "value" integer);
-SELECT create_hypertable('hyper_timefunc', 'time', 'device_id', 4, chunk_time_interval => 10, time_partitioning_func => 'unix_to_timestamp');
-psql:include/plan_expand_hypertable_load.sql:57: WARNING:  unexpected interval: smaller than one second
-      create_hypertable      
------------------------------
- (4,public,hyper_timefunc,t)
-(1 row)
-
-INSERT INTO hyper_timefunc (time, device_id, value) SELECT g, 'dev' || g, g FROM generate_series(0,30) g;
-CREATE TABLE metrics_timestamp(time timestamp);
-SELECT create_hypertable('metrics_timestamp','time');
-psql:include/plan_expand_hypertable_load.sql:62: WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-psql:include/plan_expand_hypertable_load.sql:62: NOTICE:  adding not-null constraint to column "time"
-       create_hypertable        
---------------------------------
- (5,public,metrics_timestamp,t)
-(1 row)
-
-INSERT INTO metrics_timestamp SELECT generate_series('2000-01-01'::timestamp,'2000-02-01'::timestamp,'1d'::interval);
-CREATE TABLE metrics_timestamptz(time timestamptz, device_id int);
-SELECT create_hypertable('metrics_timestamptz','time');
-psql:include/plan_expand_hypertable_load.sql:66: NOTICE:  adding not-null constraint to column "time"
-        create_hypertable         
-----------------------------------
- (6,public,metrics_timestamptz,t)
-(1 row)
-
-INSERT INTO metrics_timestamptz SELECT generate_series('2000-01-01'::timestamptz,'2000-02-01'::timestamptz,'1d'::interval), 1;
-INSERT INTO metrics_timestamptz SELECT generate_series('2000-01-01'::timestamptz,'2000-02-01'::timestamptz,'1d'::interval), 2;
-INSERT INTO metrics_timestamptz SELECT generate_series('2000-01-01'::timestamptz,'2000-02-01'::timestamptz,'1d'::interval), 3;
---create a second table to test joins with
-CREATE TABLE metrics_timestamptz_2 (LIKE metrics_timestamptz);
-SELECT create_hypertable('metrics_timestamptz_2','time');
-         create_hypertable          
-------------------------------------
- (7,public,metrics_timestamptz_2,t)
-(1 row)
-
-INSERT INTO metrics_timestamptz_2
-SELECT * FROM metrics_timestamptz;
-INSERT INTO metrics_timestamptz_2 VALUES ('2000-12-01'::timestamptz, 3);
-CREATE TABLE metrics_date(time date);
-SELECT create_hypertable('metrics_date','time');
-psql:include/plan_expand_hypertable_load.sql:79: NOTICE:  adding not-null constraint to column "time"
-     create_hypertable     
----------------------------
- (8,public,metrics_date,t)
-(1 row)
-
-INSERT INTO metrics_date SELECT generate_series('2000-01-01'::date,'2000-02-01'::date,'1d'::interval);
-ANALYZE hyper;
-ANALYZE hyper_w_space;
-ANALYZE tag;
-ANALYZE hyper_ts;
-ANALYZE hyper_timefunc;
--- create normal table for JOIN tests
-CREATE TABLE regular_timestamptz(time timestamptz);
-INSERT INTO regular_timestamptz SELECT generate_series('2000-01-01'::timestamptz,'2000-02-01'::timestamptz,'1d'::interval);
-\ir include/plan_expand_hypertable_query.sql
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
---we want to see how our logic excludes chunks
---and not how much work constraint_exclusion does
-SET constraint_exclusion = 'off';
-\qecho test upper bounds
-test upper bounds
-:PREFIX SELECT * FROM hyper WHERE time < 10 ORDER BY value;
-             QUERY PLAN             
-------------------------------------
- Sort
-   Sort Key: _hyper_1_1_chunk.value
-   ->  Seq Scan on _hyper_1_1_chunk
-         Filter: ("time" < 10)
-(4 rows)
-
-:PREFIX SELECT * FROM hyper WHERE time < 11 ORDER BY value;
-                QUERY PLAN                
-------------------------------------------
- Sort
-   Sort Key: _hyper_1_1_chunk.value
-   ->  Append
-         ->  Seq Scan on _hyper_1_1_chunk
-               Filter: ("time" < 11)
-         ->  Seq Scan on _hyper_1_2_chunk
-               Filter: ("time" < 11)
-(7 rows)
-
-:PREFIX SELECT * FROM hyper WHERE time = 10 ORDER BY value;
-             QUERY PLAN             
-------------------------------------
- Sort
-   Sort Key: _hyper_1_2_chunk.value
-   ->  Seq Scan on _hyper_1_2_chunk
-         Filter: ("time" = 10)
-(4 rows)
-
-:PREFIX SELECT * FROM hyper WHERE 10 >= time ORDER BY value;
-                QUERY PLAN                
-------------------------------------------
- Sort
-   Sort Key: _hyper_1_1_chunk.value
-   ->  Append
-         ->  Seq Scan on _hyper_1_1_chunk
-               Filter: (10 >= "time")
-         ->  Seq Scan on _hyper_1_2_chunk
-               Filter: (10 >= "time")
-(7 rows)
-
-\qecho test lower bounds
-test lower bounds
-:PREFIX SELECT * FROM hyper WHERE time >= 10 and time < 20 ORDER BY value;
-                     QUERY PLAN                     
-----------------------------------------------------
- Sort
-   Sort Key: _hyper_1_2_chunk.value
-   ->  Seq Scan on _hyper_1_2_chunk
-         Filter: (("time" >= 10) AND ("time" < 20))
-(4 rows)
-
-:PREFIX SELECT * FROM hyper WHERE 10 < time and 20 >= time ORDER BY value;
-                        QUERY PLAN                        
-----------------------------------------------------------
- Sort
-   Sort Key: _hyper_1_2_chunk.value
-   ->  Append
-         ->  Seq Scan on _hyper_1_2_chunk
-               Filter: ((10 < "time") AND (20 >= "time"))
-         ->  Seq Scan on _hyper_1_3_chunk
-               Filter: ((10 < "time") AND (20 >= "time"))
-(7 rows)
-
-:PREFIX SELECT * FROM hyper WHERE time >= 9 and time < 20 ORDER BY value;
-                       QUERY PLAN                        
----------------------------------------------------------
- Sort
-   Sort Key: _hyper_1_1_chunk.value
-   ->  Append
-         ->  Seq Scan on _hyper_1_1_chunk
-               Filter: (("time" >= 9) AND ("time" < 20))
-         ->  Seq Scan on _hyper_1_2_chunk
-               Filter: (("time" >= 9) AND ("time" < 20))
-(7 rows)
-
-:PREFIX SELECT * FROM hyper WHERE time > 9 and time < 20 ORDER BY value;
-                    QUERY PLAN                    
---------------------------------------------------
- Sort
-   Sort Key: _hyper_1_2_chunk.value
-   ->  Seq Scan on _hyper_1_2_chunk
-         Filter: (("time" > 9) AND ("time" < 20))
-(4 rows)
-
-\qecho test empty result
-test empty result
-:PREFIX SELECT * FROM hyper WHERE time < 0;
-        QUERY PLAN        
---------------------------
- Result
-   One-Time Filter: false
-(2 rows)
-
-\qecho test expression evaluation
-test expression evaluation
-:PREFIX SELECT * FROM hyper WHERE time < (5*2)::smallint;
-             QUERY PLAN              
--------------------------------------
- Seq Scan on _hyper_1_1_chunk
-   Filter: ("time" < '10'::smallint)
-(2 rows)
-
-\qecho test logic at INT64_MAX
-test logic at INT64_MAX
-:PREFIX SELECT * FROM hyper WHERE time = 9223372036854775807::bigint ORDER BY value;
-                        QUERY PLAN                        
-----------------------------------------------------------
- Sort
-   Sort Key: _hyper_1_102_chunk.value
-   ->  Seq Scan on _hyper_1_102_chunk
-         Filter: ("time" = '9223372036854775807'::bigint)
-(4 rows)
-
-:PREFIX SELECT * FROM hyper WHERE time = 9223372036854775806::bigint ORDER BY value;
-                        QUERY PLAN                        
-----------------------------------------------------------
- Sort
-   Sort Key: _hyper_1_102_chunk.value
-   ->  Seq Scan on _hyper_1_102_chunk
-         Filter: ("time" = '9223372036854775806'::bigint)
-(4 rows)
-
-:PREFIX SELECT * FROM hyper WHERE time >= 9223372036854775807::bigint ORDER BY value;
-                        QUERY PLAN                         
------------------------------------------------------------
- Sort
-   Sort Key: _hyper_1_102_chunk.value
-   ->  Seq Scan on _hyper_1_102_chunk
-         Filter: ("time" >= '9223372036854775807'::bigint)
-(4 rows)
-
-:PREFIX SELECT * FROM hyper WHERE time > 9223372036854775807::bigint ORDER BY value;
-           QUERY PLAN           
---------------------------------
- Sort
-   Sort Key: value
-   ->  Result
-         One-Time Filter: false
-(4 rows)
-
-:PREFIX SELECT * FROM hyper WHERE time > 9223372036854775806::bigint ORDER BY value;
-                        QUERY PLAN                        
-----------------------------------------------------------
- Sort
-   Sort Key: _hyper_1_102_chunk.value
-   ->  Seq Scan on _hyper_1_102_chunk
-         Filter: ("time" > '9223372036854775806'::bigint)
-(4 rows)
-
-\qecho cte
-cte
-:PREFIX WITH cte AS(
-  SELECT * FROM hyper WHERE time < 10
-)
-SELECT * FROM cte ORDER BY value;
-             QUERY PLAN             
-------------------------------------
- Sort
-   Sort Key: _hyper_1_1_chunk.value
-   ->  Seq Scan on _hyper_1_1_chunk
-         Filter: ("time" < 10)
-(4 rows)
-
-\qecho subquery
-subquery
-:PREFIX SELECT 0 = ANY (SELECT value FROM hyper WHERE time < 10);
-              QUERY PLAN              
---------------------------------------
- Result
-   SubPlan 1
-     ->  Seq Scan on _hyper_1_1_chunk
-           Filter: ("time" < 10)
-(4 rows)
-
-\qecho no space constraint
-no space constraint
-:PREFIX SELECT * FROM hyper_w_space WHERE time < 10 ORDER BY value;
-                 QUERY PLAN                 
---------------------------------------------
- Sort
-   Sort Key: _hyper_2_103_chunk.value
-   ->  Append
-         ->  Seq Scan on _hyper_2_103_chunk
-               Filter: ("time" < 10)
-         ->  Seq Scan on _hyper_2_104_chunk
-               Filter: ("time" < 10)
-         ->  Seq Scan on _hyper_2_105_chunk
-               Filter: ("time" < 10)
-         ->  Seq Scan on _hyper_2_106_chunk
-               Filter: ("time" < 10)
-(11 rows)
-
-\qecho valid space constraint
-valid space constraint
-:PREFIX SELECT * FROM hyper_w_space WHERE time < 10 and device_id = 'dev5' ORDER BY value;
-                           QUERY PLAN                           
-----------------------------------------------------------------
- Sort
-   Sort Key: _hyper_2_106_chunk.value
-   ->  Seq Scan on _hyper_2_106_chunk
-         Filter: (("time" < 10) AND (device_id = 'dev5'::text))
-(4 rows)
-
-:PREFIX SELECT * FROM hyper_w_space WHERE time < 10 and 'dev5' = device_id ORDER BY value;
-                           QUERY PLAN                           
-----------------------------------------------------------------
- Sort
-   Sort Key: _hyper_2_106_chunk.value
-   ->  Seq Scan on _hyper_2_106_chunk
-         Filter: (("time" < 10) AND ('dev5'::text = device_id))
-(4 rows)
-
-:PREFIX SELECT * FROM hyper_w_space WHERE time < 10 and 'dev'||(2+3) = device_id ORDER BY value;
-                           QUERY PLAN                           
-----------------------------------------------------------------
- Sort
-   Sort Key: _hyper_2_106_chunk.value
-   ->  Seq Scan on _hyper_2_106_chunk
-         Filter: (("time" < 10) AND ('dev5'::text = device_id))
-(4 rows)
-
-\qecho only space constraint
-only space constraint
-:PREFIX SELECT * FROM hyper_w_space WHERE 'dev5' = device_id ORDER BY value;
-                    QUERY PLAN                    
---------------------------------------------------
- Sort
-   Sort Key: _hyper_2_106_chunk.value
-   ->  Append
-         ->  Seq Scan on _hyper_2_106_chunk
-               Filter: ('dev5'::text = device_id)
-         ->  Seq Scan on _hyper_2_109_chunk
-               Filter: ('dev5'::text = device_id)
-         ->  Seq Scan on _hyper_2_111_chunk
-               Filter: ('dev5'::text = device_id)
-(9 rows)
-
-\qecho unhandled space constraint
-unhandled space constraint
-:PREFIX SELECT * FROM hyper_w_space WHERE time < 10 and device_id > 'dev5' ORDER BY value;
-                              QUERY PLAN                              
-----------------------------------------------------------------------
- Sort
-   Sort Key: _hyper_2_103_chunk.value
-   ->  Append
-         ->  Seq Scan on _hyper_2_103_chunk
-               Filter: (("time" < 10) AND (device_id > 'dev5'::text))
-         ->  Seq Scan on _hyper_2_104_chunk
-               Filter: (("time" < 10) AND (device_id > 'dev5'::text))
-         ->  Seq Scan on _hyper_2_105_chunk
-               Filter: (("time" < 10) AND (device_id > 'dev5'::text))
-         ->  Seq Scan on _hyper_2_106_chunk
-               Filter: (("time" < 10) AND (device_id > 'dev5'::text))
-(11 rows)
-
-\qecho use of OR - does not filter chunks
-use of OR - does not filter chunks
-:PREFIX SELECT * FROM hyper_w_space WHERE time < 10 AND (device_id = 'dev5' or device_id = 'dev6') ORDER BY value;
-                                              QUERY PLAN                                              
-------------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: _hyper_2_103_chunk.value
-   ->  Append
-         ->  Seq Scan on _hyper_2_103_chunk
-               Filter: (("time" < 10) AND ((device_id = 'dev5'::text) OR (device_id = 'dev6'::text)))
-         ->  Seq Scan on _hyper_2_104_chunk
-               Filter: (("time" < 10) AND ((device_id = 'dev5'::text) OR (device_id = 'dev6'::text)))
-         ->  Seq Scan on _hyper_2_105_chunk
-               Filter: (("time" < 10) AND ((device_id = 'dev5'::text) OR (device_id = 'dev6'::text)))
-         ->  Seq Scan on _hyper_2_106_chunk
-               Filter: (("time" < 10) AND ((device_id = 'dev5'::text) OR (device_id = 'dev6'::text)))
-(11 rows)
-
-\qecho cte
-cte
-:PREFIX WITH cte AS(
-   SELECT * FROM hyper_w_space WHERE time < 10 and device_id = 'dev5'
-)
-SELECT * FROM cte ORDER BY value;
-                           QUERY PLAN                           
-----------------------------------------------------------------
- Sort
-   Sort Key: _hyper_2_106_chunk.value
-   ->  Seq Scan on _hyper_2_106_chunk
-         Filter: (("time" < 10) AND (device_id = 'dev5'::text))
-(4 rows)
-
-\qecho subquery
-subquery
-:PREFIX SELECT 0 = ANY (SELECT value FROM hyper_w_space WHERE time < 10 and device_id = 'dev5');
-                            QUERY PLAN                            
-------------------------------------------------------------------
- Result
-   SubPlan 1
-     ->  Seq Scan on _hyper_2_106_chunk
-           Filter: (("time" < 10) AND (device_id = 'dev5'::text))
-(4 rows)
-
-\qecho view
-view
-:PREFIX SELECT * FROM hyper_w_space_view WHERE time < 10 and device_id = 'dev5' ORDER BY value;
-                           QUERY PLAN                           
-----------------------------------------------------------------
- Sort
-   Sort Key: _hyper_2_106_chunk.value
-   ->  Seq Scan on _hyper_2_106_chunk
-         Filter: (("time" < 10) AND (device_id = 'dev5'::text))
-(4 rows)
-
-\qecho IN statement - simple
-IN statement - simple
-:PREFIX SELECT * FROM hyper_w_space WHERE time < 10 AND device_id IN ('dev5') ORDER BY value;
-                           QUERY PLAN                           
-----------------------------------------------------------------
- Sort
-   Sort Key: _hyper_2_106_chunk.value
-   ->  Seq Scan on _hyper_2_106_chunk
-         Filter: (("time" < 10) AND (device_id = 'dev5'::text))
-(4 rows)
-
-\qecho IN statement - two chunks
-IN statement - two chunks
-:PREFIX SELECT * FROM hyper_w_space WHERE time < 10 AND device_id IN ('dev5','dev6') ORDER BY value;
-                                     QUERY PLAN                                      
--------------------------------------------------------------------------------------
- Sort
-   Sort Key: _hyper_2_105_chunk.value
-   ->  Append
-         ->  Seq Scan on _hyper_2_105_chunk
-               Filter: (("time" < 10) AND (device_id = ANY ('{dev5,dev6}'::text[])))
-         ->  Seq Scan on _hyper_2_106_chunk
-               Filter: (("time" < 10) AND (device_id = ANY ('{dev5,dev6}'::text[])))
-(7 rows)
-
-\qecho IN statement - one chunk
-IN statement - one chunk
-:PREFIX SELECT * FROM hyper_w_space WHERE time < 10 AND device_id IN ('dev4','dev5') ORDER BY value;
-                                  QUERY PLAN                                   
--------------------------------------------------------------------------------
- Sort
-   Sort Key: _hyper_2_106_chunk.value
-   ->  Seq Scan on _hyper_2_106_chunk
-         Filter: (("time" < 10) AND (device_id = ANY ('{dev4,dev5}'::text[])))
-(4 rows)
-
-\qecho NOT IN - does not filter chunks
-NOT IN - does not filter chunks
-:PREFIX SELECT * FROM hyper_w_space WHERE time < 10 AND device_id NOT IN ('dev5','dev6') ORDER BY value;
-                                      QUERY PLAN                                      
---------------------------------------------------------------------------------------
- Sort
-   Sort Key: _hyper_2_103_chunk.value
-   ->  Append
-         ->  Seq Scan on _hyper_2_103_chunk
-               Filter: (("time" < 10) AND (device_id <> ALL ('{dev5,dev6}'::text[])))
-         ->  Seq Scan on _hyper_2_104_chunk
-               Filter: (("time" < 10) AND (device_id <> ALL ('{dev5,dev6}'::text[])))
-         ->  Seq Scan on _hyper_2_105_chunk
-               Filter: (("time" < 10) AND (device_id <> ALL ('{dev5,dev6}'::text[])))
-         ->  Seq Scan on _hyper_2_106_chunk
-               Filter: (("time" < 10) AND (device_id <> ALL ('{dev5,dev6}'::text[])))
-(11 rows)
-
-\qecho IN statement with subquery - does not filter chunks
-IN statement with subquery - does not filter chunks
-:PREFIX SELECT * FROM hyper_w_space WHERE time < 10 AND device_id IN (SELECT 'dev5'::text) ORDER BY value;
-                           QUERY PLAN                           
-----------------------------------------------------------------
- Sort
-   Sort Key: _hyper_2_106_chunk.value
-   ->  Seq Scan on _hyper_2_106_chunk
-         Filter: (("time" < 10) AND (device_id = 'dev5'::text))
-(4 rows)
-
-\qecho ANY
-ANY
-:PREFIX SELECT * FROM hyper_w_space WHERE time < 10 AND device_id = ANY(ARRAY['dev5','dev6']) ORDER BY value;
-                                     QUERY PLAN                                      
--------------------------------------------------------------------------------------
- Sort
-   Sort Key: _hyper_2_105_chunk.value
-   ->  Append
-         ->  Seq Scan on _hyper_2_105_chunk
-               Filter: (("time" < 10) AND (device_id = ANY ('{dev5,dev6}'::text[])))
-         ->  Seq Scan on _hyper_2_106_chunk
-               Filter: (("time" < 10) AND (device_id = ANY ('{dev5,dev6}'::text[])))
-(7 rows)
-
-\qecho ANY with intersection
-ANY with intersection
-:PREFIX SELECT * FROM hyper_w_space WHERE time < 10 AND device_id = ANY(ARRAY['dev5','dev6']) AND device_id = ANY(ARRAY['dev6','dev7']) ORDER BY value;
-                                                         QUERY PLAN                                                          
------------------------------------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: _hyper_2_105_chunk.value
-   ->  Seq Scan on _hyper_2_105_chunk
-         Filter: (("time" < 10) AND (device_id = ANY ('{dev5,dev6}'::text[])) AND (device_id = ANY ('{dev6,dev7}'::text[])))
-(4 rows)
-
-\qecho ANY without intersection shouldnt scan any chunks
-ANY without intersection shouldnt scan any chunks
-:PREFIX SELECT * FROM hyper_w_space WHERE time < 10 AND device_id = ANY(ARRAY['dev5','dev6']) AND device_id = ANY(ARRAY['dev8','dev9']) ORDER BY value;
-           QUERY PLAN           
---------------------------------
- Sort
-   Sort Key: value
-   ->  Result
-         One-Time Filter: false
-(4 rows)
-
-\qecho ANY/IN/ALL only works for equals operator
-ANY/IN/ALL only works for equals operator
-:PREFIX SELECT * FROM hyper_w_space WHERE device_id < ANY(ARRAY['dev5','dev6']) ORDER BY value;
-                           QUERY PLAN                            
------------------------------------------------------------------
- Sort
-   Sort Key: _hyper_2_103_chunk.value
-   ->  Append
-         ->  Seq Scan on _hyper_2_103_chunk
-               Filter: (device_id < ANY ('{dev5,dev6}'::text[]))
-         ->  Seq Scan on _hyper_2_104_chunk
-               Filter: (device_id < ANY ('{dev5,dev6}'::text[]))
-         ->  Seq Scan on _hyper_2_105_chunk
-               Filter: (device_id < ANY ('{dev5,dev6}'::text[]))
-         ->  Seq Scan on _hyper_2_106_chunk
-               Filter: (device_id < ANY ('{dev5,dev6}'::text[]))
-         ->  Seq Scan on _hyper_2_107_chunk
-               Filter: (device_id < ANY ('{dev5,dev6}'::text[]))
-         ->  Seq Scan on _hyper_2_108_chunk
-               Filter: (device_id < ANY ('{dev5,dev6}'::text[]))
-         ->  Seq Scan on _hyper_2_109_chunk
-               Filter: (device_id < ANY ('{dev5,dev6}'::text[]))
-         ->  Seq Scan on _hyper_2_110_chunk
-               Filter: (device_id < ANY ('{dev5,dev6}'::text[]))
-         ->  Seq Scan on _hyper_2_111_chunk
-               Filter: (device_id < ANY ('{dev5,dev6}'::text[]))
-         ->  Seq Scan on _hyper_2_112_chunk
-               Filter: (device_id < ANY ('{dev5,dev6}'::text[]))
-         ->  Seq Scan on _hyper_2_113_chunk
-               Filter: (device_id < ANY ('{dev5,dev6}'::text[]))
-         ->  Seq Scan on _hyper_2_114_chunk
-               Filter: (device_id < ANY ('{dev5,dev6}'::text[]))
-         ->  Seq Scan on _hyper_2_115_chunk
-               Filter: (device_id < ANY ('{dev5,dev6}'::text[]))
-(29 rows)
-
-\qecho ALL with equals and different values shouldnt scan any chunks
-ALL with equals and different values shouldnt scan any chunks
-:PREFIX SELECT * FROM hyper_w_space WHERE device_id = ALL(ARRAY['dev5','dev6']) ORDER BY value;
-           QUERY PLAN           
---------------------------------
- Sort
-   Sort Key: value
-   ->  Result
-         One-Time Filter: false
-(4 rows)
-
-\qecho Multi AND
-Multi AND
-:PREFIX SELECT * FROM hyper_w_space WHERE time < 10 AND time < 100 ORDER BY value;
-                        QUERY PLAN                        
-----------------------------------------------------------
- Sort
-   Sort Key: _hyper_2_103_chunk.value
-   ->  Append
-         ->  Seq Scan on _hyper_2_103_chunk
-               Filter: (("time" < 10) AND ("time" < 100))
-         ->  Seq Scan on _hyper_2_104_chunk
-               Filter: (("time" < 10) AND ("time" < 100))
-         ->  Seq Scan on _hyper_2_105_chunk
-               Filter: (("time" < 10) AND ("time" < 100))
-         ->  Seq Scan on _hyper_2_106_chunk
-               Filter: (("time" < 10) AND ("time" < 100))
-(11 rows)
-
-\qecho Time dimension doesnt filter chunks when using IN/ANY with multiple arguments
-Time dimension doesnt filter chunks when using IN/ANY with multiple arguments
-:PREFIX SELECT * FROM hyper_w_space WHERE time < ANY(ARRAY[1,2]) ORDER BY value;
-                        QUERY PLAN                         
------------------------------------------------------------
- Sort
-   Sort Key: _hyper_2_103_chunk.value
-   ->  Append
-         ->  Seq Scan on _hyper_2_103_chunk
-               Filter: ("time" < ANY ('{1,2}'::integer[]))
-         ->  Seq Scan on _hyper_2_104_chunk
-               Filter: ("time" < ANY ('{1,2}'::integer[]))
-         ->  Seq Scan on _hyper_2_105_chunk
-               Filter: ("time" < ANY ('{1,2}'::integer[]))
-         ->  Seq Scan on _hyper_2_106_chunk
-               Filter: ("time" < ANY ('{1,2}'::integer[]))
-         ->  Seq Scan on _hyper_2_107_chunk
-               Filter: ("time" < ANY ('{1,2}'::integer[]))
-         ->  Seq Scan on _hyper_2_108_chunk
-               Filter: ("time" < ANY ('{1,2}'::integer[]))
-         ->  Seq Scan on _hyper_2_109_chunk
-               Filter: ("time" < ANY ('{1,2}'::integer[]))
-         ->  Seq Scan on _hyper_2_110_chunk
-               Filter: ("time" < ANY ('{1,2}'::integer[]))
-         ->  Seq Scan on _hyper_2_111_chunk
-               Filter: ("time" < ANY ('{1,2}'::integer[]))
-         ->  Seq Scan on _hyper_2_112_chunk
-               Filter: ("time" < ANY ('{1,2}'::integer[]))
-         ->  Seq Scan on _hyper_2_113_chunk
-               Filter: ("time" < ANY ('{1,2}'::integer[]))
-         ->  Seq Scan on _hyper_2_114_chunk
-               Filter: ("time" < ANY ('{1,2}'::integer[]))
-         ->  Seq Scan on _hyper_2_115_chunk
-               Filter: ("time" < ANY ('{1,2}'::integer[]))
-(29 rows)
-
-\qecho Time dimension chunk filtering works for ANY with single argument
-Time dimension chunk filtering works for ANY with single argument
-:PREFIX SELECT * FROM hyper_w_space WHERE time < ANY(ARRAY[1]) ORDER BY value;
-                       QUERY PLAN                        
----------------------------------------------------------
- Sort
-   Sort Key: _hyper_2_103_chunk.value
-   ->  Append
-         ->  Seq Scan on _hyper_2_103_chunk
-               Filter: ("time" < ANY ('{1}'::integer[]))
-         ->  Seq Scan on _hyper_2_104_chunk
-               Filter: ("time" < ANY ('{1}'::integer[]))
-         ->  Seq Scan on _hyper_2_105_chunk
-               Filter: ("time" < ANY ('{1}'::integer[]))
-         ->  Seq Scan on _hyper_2_106_chunk
-               Filter: ("time" < ANY ('{1}'::integer[]))
-(11 rows)
-
-\qecho Time dimension chunk filtering works for ALL with single argument
-Time dimension chunk filtering works for ALL with single argument
-:PREFIX SELECT * FROM hyper_w_space WHERE time < ALL(ARRAY[1]) ORDER BY value;
-                       QUERY PLAN                        
----------------------------------------------------------
- Sort
-   Sort Key: _hyper_2_103_chunk.value
-   ->  Append
-         ->  Seq Scan on _hyper_2_103_chunk
-               Filter: ("time" < ALL ('{1}'::integer[]))
-         ->  Seq Scan on _hyper_2_104_chunk
-               Filter: ("time" < ALL ('{1}'::integer[]))
-         ->  Seq Scan on _hyper_2_105_chunk
-               Filter: ("time" < ALL ('{1}'::integer[]))
-         ->  Seq Scan on _hyper_2_106_chunk
-               Filter: ("time" < ALL ('{1}'::integer[]))
-(11 rows)
-
-\qecho Time dimension chunk filtering works for ALL with multiple arguments
-Time dimension chunk filtering works for ALL with multiple arguments
-:PREFIX SELECT * FROM hyper_w_space WHERE time < ALL(ARRAY[1,10,20,30]) ORDER BY value;
-                            QUERY PLAN                            
-------------------------------------------------------------------
- Sort
-   Sort Key: _hyper_2_103_chunk.value
-   ->  Append
-         ->  Seq Scan on _hyper_2_103_chunk
-               Filter: ("time" < ALL ('{1,10,20,30}'::integer[]))
-         ->  Seq Scan on _hyper_2_104_chunk
-               Filter: ("time" < ALL ('{1,10,20,30}'::integer[]))
-         ->  Seq Scan on _hyper_2_105_chunk
-               Filter: ("time" < ALL ('{1,10,20,30}'::integer[]))
-         ->  Seq Scan on _hyper_2_106_chunk
-               Filter: ("time" < ALL ('{1,10,20,30}'::integer[]))
-(11 rows)
-
-\qecho AND intersection using IN and EQUALS
-AND intersection using IN and EQUALS
-:PREFIX SELECT * FROM hyper_w_space WHERE device_id IN ('dev1','dev2') AND device_id = 'dev1' ORDER BY value;
-                                            QUERY PLAN                                            
---------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: _hyper_2_103_chunk.value
-   ->  Append
-         ->  Seq Scan on _hyper_2_103_chunk
-               Filter: ((device_id = ANY ('{dev1,dev2}'::text[])) AND (device_id = 'dev1'::text))
-         ->  Seq Scan on _hyper_2_110_chunk
-               Filter: ((device_id = ANY ('{dev1,dev2}'::text[])) AND (device_id = 'dev1'::text))
-         ->  Seq Scan on _hyper_2_114_chunk
-               Filter: ((device_id = ANY ('{dev1,dev2}'::text[])) AND (device_id = 'dev1'::text))
-(9 rows)
-
-\qecho AND with no intersection using IN and EQUALS
-AND with no intersection using IN and EQUALS
-:PREFIX SELECT * FROM hyper_w_space WHERE device_id IN ('dev1','dev2') AND device_id = 'dev3' ORDER BY value;
-           QUERY PLAN           
---------------------------------
- Sort
-   Sort Key: value
-   ->  Result
-         One-Time Filter: false
-(4 rows)
-
-\qecho timestamps
-timestamps
-\qecho these should work since they are immutable functions
-these should work since they are immutable functions
-:PREFIX SELECT * FROM hyper_ts WHERE time < 'Wed Dec 31 16:00:10 1969 PST'::timestamptz ORDER BY value;
-                                        QUERY PLAN                                         
--------------------------------------------------------------------------------------------
- Sort
-   Sort Key: _hyper_3_116_chunk.value
-   ->  Append
-         ->  Seq Scan on _hyper_3_116_chunk
-               Filter: ("time" < 'Wed Dec 31 16:00:10 1969 PST'::timestamp with time zone)
-         ->  Seq Scan on _hyper_3_117_chunk
-               Filter: ("time" < 'Wed Dec 31 16:00:10 1969 PST'::timestamp with time zone)
-(7 rows)
-
-:PREFIX SELECT * FROM hyper_ts WHERE time < to_timestamp(10) ORDER BY value;
-                                        QUERY PLAN                                         
--------------------------------------------------------------------------------------------
- Sort
-   Sort Key: _hyper_3_116_chunk.value
-   ->  Append
-         ->  Seq Scan on _hyper_3_116_chunk
-               Filter: ("time" < 'Wed Dec 31 16:00:10 1969 PST'::timestamp with time zone)
-         ->  Seq Scan on _hyper_3_117_chunk
-               Filter: ("time" < 'Wed Dec 31 16:00:10 1969 PST'::timestamp with time zone)
-(7 rows)
-
-:PREFIX SELECT * FROM hyper_ts WHERE time < 'Wed Dec 31 16:00:10 1969'::timestamp AT TIME ZONE 'PST' ORDER BY value;
-                                        QUERY PLAN                                         
--------------------------------------------------------------------------------------------
- Sort
-   Sort Key: _hyper_3_116_chunk.value
-   ->  Append
-         ->  Seq Scan on _hyper_3_116_chunk
-               Filter: ("time" < 'Wed Dec 31 16:00:10 1969 PST'::timestamp with time zone)
-         ->  Seq Scan on _hyper_3_117_chunk
-               Filter: ("time" < 'Wed Dec 31 16:00:10 1969 PST'::timestamp with time zone)
-(7 rows)
-
-:PREFIX SELECT * FROM hyper_ts WHERE time < to_timestamp(10) and device_id = 'dev1' ORDER BY value;
-                                                      QUERY PLAN                                                      
-----------------------------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: _hyper_3_116_chunk.value
-   ->  Seq Scan on _hyper_3_116_chunk
-         Filter: (("time" < 'Wed Dec 31 16:00:10 1969 PST'::timestamp with time zone) AND (device_id = 'dev1'::text))
-(4 rows)
-
-\qecho these should not work since uses stable functions;
-these should not work since uses stable functions;
-:PREFIX SELECT * FROM hyper_ts WHERE time < 'Wed Dec 31 16:00:10 1969'::timestamp ORDER BY value;
-                                        QUERY PLAN                                        
-------------------------------------------------------------------------------------------
- Sort
-   Sort Key: hyper_ts.value
-   ->  Custom Scan (ChunkAppend) on hyper_ts
-         Chunks excluded during startup: 6
-         ->  Seq Scan on _hyper_3_116_chunk
-               Filter: ("time" < 'Wed Dec 31 16:00:10 1969'::timestamp without time zone)
-         ->  Seq Scan on _hyper_3_117_chunk
-               Filter: ("time" < 'Wed Dec 31 16:00:10 1969'::timestamp without time zone)
-(8 rows)
-
-:PREFIX SELECT * FROM hyper_ts WHERE time < ('Wed Dec 31 16:00:10 1969'::timestamp::timestamptz) ORDER BY value;
-                                                      QUERY PLAN                                                      
-----------------------------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: hyper_ts.value
-   ->  Custom Scan (ChunkAppend) on hyper_ts
-         Chunks excluded during startup: 6
-         ->  Seq Scan on _hyper_3_116_chunk
-               Filter: ("time" < ('Wed Dec 31 16:00:10 1969'::timestamp without time zone)::timestamp with time zone)
-         ->  Seq Scan on _hyper_3_117_chunk
-               Filter: ("time" < ('Wed Dec 31 16:00:10 1969'::timestamp without time zone)::timestamp with time zone)
-(8 rows)
-
-:PREFIX SELECT * FROM hyper_ts WHERE NOW() < time ORDER BY value;
-                 QUERY PLAN                  
----------------------------------------------
- Sort
-   Sort Key: hyper_ts.value
-   ->  Custom Scan (ChunkAppend) on hyper_ts
-         Chunks excluded during startup: 7
-         ->  Seq Scan on _hyper_3_123_chunk
-               Filter: (now() < "time")
-(6 rows)
-
-\qecho joins
-joins
-:PREFIX SELECT * FROM hyper_ts WHERE tag_id IN (SELECT id FROM tag WHERE tag.id=1) and time < to_timestamp(10) and device_id = 'dev1' ORDER BY value;
-                                                                 QUERY PLAN                                                                  
----------------------------------------------------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: _hyper_3_116_chunk.value
-   ->  Nested Loop Semi Join
-         ->  Seq Scan on _hyper_3_116_chunk
-               Filter: (("time" < 'Wed Dec 31 16:00:10 1969 PST'::timestamp with time zone) AND (device_id = 'dev1'::text) AND (tag_id = 1))
-         ->  Seq Scan on tag
-               Filter: (id = 1)
-(7 rows)
-
-:PREFIX SELECT * FROM hyper_ts WHERE tag_id IN (SELECT id FROM tag WHERE tag.id=1) or (time < to_timestamp(10) and device_id = 'dev1') ORDER BY value;
-                                                                     QUERY PLAN                                                                     
-----------------------------------------------------------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: hyper_ts.value
-   ->  Custom Scan (ChunkAppend) on hyper_ts
-         ->  Seq Scan on _hyper_3_116_chunk
-               Filter: ((hashed SubPlan 1) OR (("time" < 'Wed Dec 31 16:00:10 1969 PST'::timestamp with time zone) AND (device_id = 'dev1'::text)))
-               SubPlan 1
-                 ->  Seq Scan on tag
-                       Filter: (id = 1)
-         ->  Seq Scan on _hyper_3_117_chunk
-               Filter: ((hashed SubPlan 1) OR (("time" < 'Wed Dec 31 16:00:10 1969 PST'::timestamp with time zone) AND (device_id = 'dev1'::text)))
-         ->  Seq Scan on _hyper_3_118_chunk
-               Filter: ((hashed SubPlan 1) OR (("time" < 'Wed Dec 31 16:00:10 1969 PST'::timestamp with time zone) AND (device_id = 'dev1'::text)))
-         ->  Seq Scan on _hyper_3_119_chunk
-               Filter: ((hashed SubPlan 1) OR (("time" < 'Wed Dec 31 16:00:10 1969 PST'::timestamp with time zone) AND (device_id = 'dev1'::text)))
-         ->  Seq Scan on _hyper_3_120_chunk
-               Filter: ((hashed SubPlan 1) OR (("time" < 'Wed Dec 31 16:00:10 1969 PST'::timestamp with time zone) AND (device_id = 'dev1'::text)))
-         ->  Seq Scan on _hyper_3_121_chunk
-               Filter: ((hashed SubPlan 1) OR (("time" < 'Wed Dec 31 16:00:10 1969 PST'::timestamp with time zone) AND (device_id = 'dev1'::text)))
-         ->  Seq Scan on _hyper_3_122_chunk
-               Filter: ((hashed SubPlan 1) OR (("time" < 'Wed Dec 31 16:00:10 1969 PST'::timestamp with time zone) AND (device_id = 'dev1'::text)))
-         ->  Seq Scan on _hyper_3_123_chunk
-               Filter: ((hashed SubPlan 1) OR (("time" < 'Wed Dec 31 16:00:10 1969 PST'::timestamp with time zone) AND (device_id = 'dev1'::text)))
-(22 rows)
-
-:PREFIX SELECT * FROM hyper_ts WHERE tag_id IN (SELECT id FROM tag WHERE tag.name='tag1') and time < to_timestamp(10) and device_id = 'dev1' ORDER BY value;
-                                                         QUERY PLAN                                                         
-----------------------------------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: _hyper_3_116_chunk.value
-   ->  Nested Loop
-         Join Filter: (_hyper_3_116_chunk.tag_id = tag.id)
-         ->  Seq Scan on _hyper_3_116_chunk
-               Filter: (("time" < 'Wed Dec 31 16:00:10 1969 PST'::timestamp with time zone) AND (device_id = 'dev1'::text))
-         ->  Seq Scan on tag
-               Filter: (name = 'tag1'::text)
-(8 rows)
-
-:PREFIX SELECT * FROM hyper_ts JOIN tag on (hyper_ts.tag_id = tag.id ) WHERE time < to_timestamp(10) and device_id = 'dev1' ORDER BY value;
-                                                            QUERY PLAN                                                            
-----------------------------------------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: _hyper_3_116_chunk.value
-   ->  Merge Join
-         Merge Cond: (tag.id = _hyper_3_116_chunk.tag_id)
-         ->  Index Scan using tag_pkey on tag
-         ->  Sort
-               Sort Key: _hyper_3_116_chunk.tag_id
-               ->  Seq Scan on _hyper_3_116_chunk
-                     Filter: (("time" < 'Wed Dec 31 16:00:10 1969 PST'::timestamp with time zone) AND (device_id = 'dev1'::text))
-(9 rows)
-
-:PREFIX SELECT * FROM hyper_ts JOIN tag on (hyper_ts.tag_id = tag.id ) WHERE tag.name = 'tag1' and time < to_timestamp(10) and device_id = 'dev1' ORDER BY value;
-                                                         QUERY PLAN                                                         
-----------------------------------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: _hyper_3_116_chunk.value
-   ->  Nested Loop
-         Join Filter: (_hyper_3_116_chunk.tag_id = tag.id)
-         ->  Seq Scan on _hyper_3_116_chunk
-               Filter: (("time" < 'Wed Dec 31 16:00:10 1969 PST'::timestamp with time zone) AND (device_id = 'dev1'::text))
-         ->  Seq Scan on tag
-               Filter: (name = 'tag1'::text)
-(8 rows)
-
-\qecho test constraint exclusion for constraints in ON clause of JOINs
-test constraint exclusion for constraints in ON clause of JOINs
-\qecho should exclude chunks on m1 and propagate qual to m2 because of INNER JOIN
-should exclude chunks on m1 and propagate qual to m2 because of INNER JOIN
-:PREFIX SELECT m1.time,m2.time FROM metrics_timestamptz m1 INNER JOIN metrics_timestamptz_2 m2 ON m1.time = m2.time AND m1.time < '2000-01-10' ORDER BY m1.time;
-                                                          QUERY PLAN                                                           
--------------------------------------------------------------------------------------------------------------------------------
- Merge Join
-   Merge Cond: (m1."time" = m2."time")
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-         Order: m1."time"
-         ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-               Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-         ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-               Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz_2 m2
-               Order: m2."time"
-               ->  Index Only Scan Backward using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk m2_1
-                     Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               ->  Index Only Scan Backward using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk m2_2
-                     Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-(15 rows)
-
-\qecho should exclude chunks on m2 and propagate qual to m1 because of INNER JOIN
-should exclude chunks on m2 and propagate qual to m1 because of INNER JOIN
-:PREFIX SELECT m1.time,m2.time FROM metrics_timestamptz m1 INNER JOIN metrics_timestamptz_2 m2 ON m1.time = m2.time AND m2.time < '2000-01-10' ORDER BY m1.time;
-                                                          QUERY PLAN                                                           
--------------------------------------------------------------------------------------------------------------------------------
- Merge Join
-   Merge Cond: (m1."time" = m2."time")
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-         Order: m1."time"
-         ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-               Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-         ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-               Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz_2 m2
-               Order: m2."time"
-               ->  Index Only Scan Backward using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk m2_1
-                     Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               ->  Index Only Scan Backward using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk m2_2
-                     Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-(15 rows)
-
-\qecho must not exclude on m1
-must not exclude on m1
-:PREFIX SELECT m1.time,m2.time FROM metrics_timestamptz m1 LEFT JOIN metrics_timestamptz_2 m2 ON m1.time = m2.time AND m1.time < '2000-01-10' ORDER BY m1.time;
-                                                          QUERY PLAN                                                           
--------------------------------------------------------------------------------------------------------------------------------
- Merge Left Join
-   Merge Cond: (m1."time" = m2."time")
-   Join Filter: (m1."time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-         Order: m1."time"
-         ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-         ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-         ->  Index Only Scan Backward using _hyper_6_162_chunk_metrics_timestamptz_time_idx on _hyper_6_162_chunk m1_3
-         ->  Index Only Scan Backward using _hyper_6_163_chunk_metrics_timestamptz_time_idx on _hyper_6_163_chunk m1_4
-         ->  Index Only Scan Backward using _hyper_6_164_chunk_metrics_timestamptz_time_idx on _hyper_6_164_chunk m1_5
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz_2 m2
-               Order: m2."time"
-               ->  Index Only Scan Backward using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk m2_1
-               ->  Index Only Scan Backward using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk m2_2
-               ->  Index Only Scan Backward using _hyper_7_167_chunk_metrics_timestamptz_2_time_idx on _hyper_7_167_chunk m2_3
-               ->  Index Only Scan Backward using _hyper_7_168_chunk_metrics_timestamptz_2_time_idx on _hyper_7_168_chunk m2_4
-               ->  Index Only Scan Backward using _hyper_7_169_chunk_metrics_timestamptz_2_time_idx on _hyper_7_169_chunk m2_5
-               ->  Index Only Scan Backward using _hyper_7_170_chunk_metrics_timestamptz_2_time_idx on _hyper_7_170_chunk m2_6
-(19 rows)
-
-\qecho should exclude chunks on m2
-should exclude chunks on m2
-:PREFIX SELECT m1.time,m2.time FROM metrics_timestamptz m1 LEFT JOIN metrics_timestamptz_2 m2 ON m1.time = m2.time AND m2.time < '2000-01-10' ORDER BY m1.time;
-                                                          QUERY PLAN                                                           
--------------------------------------------------------------------------------------------------------------------------------
- Merge Left Join
-   Merge Cond: (m1."time" = m2."time")
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-         Order: m1."time"
-         ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-         ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-         ->  Index Only Scan Backward using _hyper_6_162_chunk_metrics_timestamptz_time_idx on _hyper_6_162_chunk m1_3
-         ->  Index Only Scan Backward using _hyper_6_163_chunk_metrics_timestamptz_time_idx on _hyper_6_163_chunk m1_4
-         ->  Index Only Scan Backward using _hyper_6_164_chunk_metrics_timestamptz_time_idx on _hyper_6_164_chunk m1_5
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz_2 m2
-               Order: m2."time"
-               ->  Index Only Scan Backward using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk m2_1
-                     Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               ->  Index Only Scan Backward using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk m2_2
-                     Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               ->  Index Only Scan Backward using _hyper_7_167_chunk_metrics_timestamptz_2_time_idx on _hyper_7_167_chunk m2_3
-                     Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               ->  Index Only Scan Backward using _hyper_7_168_chunk_metrics_timestamptz_2_time_idx on _hyper_7_168_chunk m2_4
-                     Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               ->  Index Only Scan Backward using _hyper_7_169_chunk_metrics_timestamptz_2_time_idx on _hyper_7_169_chunk m2_5
-                     Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               ->  Index Only Scan Backward using _hyper_7_170_chunk_metrics_timestamptz_2_time_idx on _hyper_7_170_chunk m2_6
-                     Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-(24 rows)
-
-\qecho should exclude chunks on m1
-should exclude chunks on m1
-:PREFIX SELECT m1.time,m2.time FROM metrics_timestamptz m1 RIGHT JOIN metrics_timestamptz_2 m2 ON m1.time = m2.time AND m1.time < '2000-01-10' ORDER BY m1.time;
-                                                             QUERY PLAN                                                              
--------------------------------------------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: m1."time"
-   ->  Merge Right Join
-         Merge Cond: (m1."time" = m2."time")
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-               Order: m1."time"
-               ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-                     Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-                     Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               ->  Index Only Scan Backward using _hyper_6_162_chunk_metrics_timestamptz_time_idx on _hyper_6_162_chunk m1_3
-                     Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               ->  Index Only Scan Backward using _hyper_6_163_chunk_metrics_timestamptz_time_idx on _hyper_6_163_chunk m1_4
-                     Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               ->  Index Only Scan Backward using _hyper_6_164_chunk_metrics_timestamptz_time_idx on _hyper_6_164_chunk m1_5
-                     Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-         ->  Materialize
-               ->  Custom Scan (ChunkAppend) on metrics_timestamptz_2 m2
-                     Order: m2."time"
-                     ->  Index Only Scan Backward using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk m2_1
-                     ->  Index Only Scan Backward using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk m2_2
-                     ->  Index Only Scan Backward using _hyper_7_167_chunk_metrics_timestamptz_2_time_idx on _hyper_7_167_chunk m2_3
-                     ->  Index Only Scan Backward using _hyper_7_168_chunk_metrics_timestamptz_2_time_idx on _hyper_7_168_chunk m2_4
-                     ->  Index Only Scan Backward using _hyper_7_169_chunk_metrics_timestamptz_2_time_idx on _hyper_7_169_chunk m2_5
-                     ->  Index Only Scan Backward using _hyper_7_170_chunk_metrics_timestamptz_2_time_idx on _hyper_7_170_chunk m2_6
-(25 rows)
-
-\qecho must not exclude chunks on m2
-must not exclude chunks on m2
-:PREFIX SELECT m1.time,m2.time FROM metrics_timestamptz m1 RIGHT JOIN metrics_timestamptz_2 m2 ON m1.time = m2.time AND m2.time < '2000-01-10' ORDER BY m1.time;
-                                                            QUERY PLAN                                                             
------------------------------------------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: m1."time"
-   ->  Merge Left Join
-         Merge Cond: (m2."time" = m1."time")
-         Join Filter: (m2."time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz_2 m2
-               Order: m2."time"
-               ->  Index Only Scan Backward using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk m2_1
-               ->  Index Only Scan Backward using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk m2_2
-               ->  Index Only Scan Backward using _hyper_7_167_chunk_metrics_timestamptz_2_time_idx on _hyper_7_167_chunk m2_3
-               ->  Index Only Scan Backward using _hyper_7_168_chunk_metrics_timestamptz_2_time_idx on _hyper_7_168_chunk m2_4
-               ->  Index Only Scan Backward using _hyper_7_169_chunk_metrics_timestamptz_2_time_idx on _hyper_7_169_chunk m2_5
-               ->  Index Only Scan Backward using _hyper_7_170_chunk_metrics_timestamptz_2_time_idx on _hyper_7_170_chunk m2_6
-         ->  Materialize
-               ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-                     Order: m1."time"
-                     ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-                     ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-                     ->  Index Only Scan Backward using _hyper_6_162_chunk_metrics_timestamptz_time_idx on _hyper_6_162_chunk m1_3
-                     ->  Index Only Scan Backward using _hyper_6_163_chunk_metrics_timestamptz_time_idx on _hyper_6_163_chunk m1_4
-                     ->  Index Only Scan Backward using _hyper_6_164_chunk_metrics_timestamptz_time_idx on _hyper_6_164_chunk m1_5
-(21 rows)
-
-\qecho time_bucket exclusion
-time_bucket exclusion
-:PREFIX SELECT * FROM hyper WHERE time_bucket(10, time) < 10::bigint ORDER BY time;
-                                            QUERY PLAN                                            
---------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: _hyper_1_1_chunk."time"
-   ->  Seq Scan on _hyper_1_1_chunk
-         Filter: (("time" < '10'::bigint) AND (time_bucket('10'::bigint, "time") < '10'::bigint))
-(4 rows)
-
-:PREFIX SELECT * FROM hyper WHERE time_bucket(10, time) < 11::bigint ORDER BY time;
-                                               QUERY PLAN                                               
---------------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: _hyper_1_1_chunk."time"
-   ->  Append
-         ->  Seq Scan on _hyper_1_1_chunk
-               Filter: (("time" < '21'::bigint) AND (time_bucket('10'::bigint, "time") < '11'::bigint))
-         ->  Seq Scan on _hyper_1_2_chunk
-               Filter: (("time" < '21'::bigint) AND (time_bucket('10'::bigint, "time") < '11'::bigint))
-         ->  Seq Scan on _hyper_1_3_chunk
-               Filter: (("time" < '21'::bigint) AND (time_bucket('10'::bigint, "time") < '11'::bigint))
-(9 rows)
-
-:PREFIX SELECT * FROM hyper WHERE time_bucket(10, time) <= 10::bigint ORDER BY time;
-                                                QUERY PLAN                                                
-----------------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: _hyper_1_1_chunk."time"
-   ->  Append
-         ->  Seq Scan on _hyper_1_1_chunk
-               Filter: (("time" <= '20'::bigint) AND (time_bucket('10'::bigint, "time") <= '10'::bigint))
-         ->  Seq Scan on _hyper_1_2_chunk
-               Filter: (("time" <= '20'::bigint) AND (time_bucket('10'::bigint, "time") <= '10'::bigint))
-         ->  Seq Scan on _hyper_1_3_chunk
-               Filter: (("time" <= '20'::bigint) AND (time_bucket('10'::bigint, "time") <= '10'::bigint))
-(9 rows)
-
-:PREFIX SELECT * FROM hyper WHERE 10::bigint > time_bucket(10, time) ORDER BY time;
-                                            QUERY PLAN                                            
---------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: _hyper_1_1_chunk."time"
-   ->  Seq Scan on _hyper_1_1_chunk
-         Filter: (("time" < '10'::bigint) AND ('10'::bigint > time_bucket('10'::bigint, "time")))
-(4 rows)
-
-:PREFIX SELECT * FROM hyper WHERE 11::bigint > time_bucket(10, time) ORDER BY time;
-                                               QUERY PLAN                                               
---------------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: _hyper_1_1_chunk."time"
-   ->  Append
-         ->  Seq Scan on _hyper_1_1_chunk
-               Filter: (("time" < '21'::bigint) AND ('11'::bigint > time_bucket('10'::bigint, "time")))
-         ->  Seq Scan on _hyper_1_2_chunk
-               Filter: (("time" < '21'::bigint) AND ('11'::bigint > time_bucket('10'::bigint, "time")))
-         ->  Seq Scan on _hyper_1_3_chunk
-               Filter: (("time" < '21'::bigint) AND ('11'::bigint > time_bucket('10'::bigint, "time")))
-(9 rows)
-
-\qecho test overflow behaviour of time_bucket exclusion
-test overflow behaviour of time_bucket exclusion
-:PREFIX SELECT * FROM hyper WHERE time > 950 AND time_bucket(10, time) < '9223372036854775807'::bigint ORDER BY time;
-                                                   QUERY PLAN                                                   
-----------------------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: _hyper_1_96_chunk."time"
-   ->  Append
-         ->  Seq Scan on _hyper_1_96_chunk
-               Filter: (("time" > 950) AND (time_bucket('10'::bigint, "time") < '9223372036854775807'::bigint))
-         ->  Seq Scan on _hyper_1_97_chunk
-               Filter: (("time" > 950) AND (time_bucket('10'::bigint, "time") < '9223372036854775807'::bigint))
-         ->  Seq Scan on _hyper_1_98_chunk
-               Filter: (("time" > 950) AND (time_bucket('10'::bigint, "time") < '9223372036854775807'::bigint))
-         ->  Seq Scan on _hyper_1_99_chunk
-               Filter: (("time" > 950) AND (time_bucket('10'::bigint, "time") < '9223372036854775807'::bigint))
-         ->  Seq Scan on _hyper_1_100_chunk
-               Filter: (("time" > 950) AND (time_bucket('10'::bigint, "time") < '9223372036854775807'::bigint))
-         ->  Seq Scan on _hyper_1_101_chunk
-               Filter: (("time" > 950) AND (time_bucket('10'::bigint, "time") < '9223372036854775807'::bigint))
-         ->  Seq Scan on _hyper_1_102_chunk
-               Filter: (("time" > 950) AND (time_bucket('10'::bigint, "time") < '9223372036854775807'::bigint))
-(17 rows)
-
-\qecho test timestamp upper boundary
-test timestamp upper boundary
-\qecho there should be no transformation if we are out of the supported (TimescaleDB-specific) range
-there should be no transformation if we are out of the supported (TimescaleDB-specific) range
-:PREFIX SELECT * FROM metrics_timestamp WHERE time_bucket('1d',time) < '294276-01-01'::timestamp ORDER BY time;
-                                                       QUERY PLAN                                                       
-------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamp
-   Order: metrics_timestamp."time"
-   ->  Index Only Scan Backward using _hyper_5_155_chunk_metrics_timestamp_time_idx on _hyper_5_155_chunk
-         Filter: (time_bucket('@ 1 day'::interval, "time") < 'Sat Jan 01 00:00:00 294276'::timestamp without time zone)
-   ->  Index Only Scan Backward using _hyper_5_156_chunk_metrics_timestamp_time_idx on _hyper_5_156_chunk
-         Filter: (time_bucket('@ 1 day'::interval, "time") < 'Sat Jan 01 00:00:00 294276'::timestamp without time zone)
-   ->  Index Only Scan Backward using _hyper_5_157_chunk_metrics_timestamp_time_idx on _hyper_5_157_chunk
-         Filter: (time_bucket('@ 1 day'::interval, "time") < 'Sat Jan 01 00:00:00 294276'::timestamp without time zone)
-   ->  Index Only Scan Backward using _hyper_5_158_chunk_metrics_timestamp_time_idx on _hyper_5_158_chunk
-         Filter: (time_bucket('@ 1 day'::interval, "time") < 'Sat Jan 01 00:00:00 294276'::timestamp without time zone)
-   ->  Index Only Scan Backward using _hyper_5_159_chunk_metrics_timestamp_time_idx on _hyper_5_159_chunk
-         Filter: (time_bucket('@ 1 day'::interval, "time") < 'Sat Jan 01 00:00:00 294276'::timestamp without time zone)
-(12 rows)
-
-\qecho transformation would be out of range
-transformation would be out of range
-:PREFIX SELECT * FROM metrics_timestamp WHERE time_bucket('1000d',time) < '294276-01-01'::timestamp ORDER BY time;
-                                                         QUERY PLAN                                                         
-----------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamp
-   Order: metrics_timestamp."time"
-   ->  Index Only Scan Backward using _hyper_5_155_chunk_metrics_timestamp_time_idx on _hyper_5_155_chunk
-         Filter: (time_bucket('@ 1000 days'::interval, "time") < 'Sat Jan 01 00:00:00 294276'::timestamp without time zone)
-   ->  Index Only Scan Backward using _hyper_5_156_chunk_metrics_timestamp_time_idx on _hyper_5_156_chunk
-         Filter: (time_bucket('@ 1000 days'::interval, "time") < 'Sat Jan 01 00:00:00 294276'::timestamp without time zone)
-   ->  Index Only Scan Backward using _hyper_5_157_chunk_metrics_timestamp_time_idx on _hyper_5_157_chunk
-         Filter: (time_bucket('@ 1000 days'::interval, "time") < 'Sat Jan 01 00:00:00 294276'::timestamp without time zone)
-   ->  Index Only Scan Backward using _hyper_5_158_chunk_metrics_timestamp_time_idx on _hyper_5_158_chunk
-         Filter: (time_bucket('@ 1000 days'::interval, "time") < 'Sat Jan 01 00:00:00 294276'::timestamp without time zone)
-   ->  Index Only Scan Backward using _hyper_5_159_chunk_metrics_timestamp_time_idx on _hyper_5_159_chunk
-         Filter: (time_bucket('@ 1000 days'::interval, "time") < 'Sat Jan 01 00:00:00 294276'::timestamp without time zone)
-(12 rows)
-
-\qecho test timestamptz upper boundary
-test timestamptz upper boundary
-\qecho there should be no transformation if we are out of the supported (TimescaleDB-specific) range
-there should be no transformation if we are out of the supported (TimescaleDB-specific) range
-:PREFIX SELECT time FROM metrics_timestamptz WHERE time_bucket('1d',time) < '294276-01-01'::timestamptz ORDER BY time;
-                                                       QUERY PLAN                                                        
--------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamptz
-   Order: metrics_timestamptz."time"
-   ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk
-         Filter: (time_bucket('@ 1 day'::interval, "time") < 'Sat Jan 01 00:00:00 294276 PST'::timestamp with time zone)
-   ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk
-         Filter: (time_bucket('@ 1 day'::interval, "time") < 'Sat Jan 01 00:00:00 294276 PST'::timestamp with time zone)
-   ->  Index Only Scan Backward using _hyper_6_162_chunk_metrics_timestamptz_time_idx on _hyper_6_162_chunk
-         Filter: (time_bucket('@ 1 day'::interval, "time") < 'Sat Jan 01 00:00:00 294276 PST'::timestamp with time zone)
-   ->  Index Only Scan Backward using _hyper_6_163_chunk_metrics_timestamptz_time_idx on _hyper_6_163_chunk
-         Filter: (time_bucket('@ 1 day'::interval, "time") < 'Sat Jan 01 00:00:00 294276 PST'::timestamp with time zone)
-   ->  Index Only Scan Backward using _hyper_6_164_chunk_metrics_timestamptz_time_idx on _hyper_6_164_chunk
-         Filter: (time_bucket('@ 1 day'::interval, "time") < 'Sat Jan 01 00:00:00 294276 PST'::timestamp with time zone)
-(12 rows)
-
-\qecho transformation would be out of range
-transformation would be out of range
-:PREFIX SELECT time FROM metrics_timestamptz WHERE time_bucket('1000d',time) < '294276-01-01'::timestamptz ORDER BY time;
-                                                         QUERY PLAN                                                          
------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamptz
-   Order: metrics_timestamptz."time"
-   ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk
-         Filter: (time_bucket('@ 1000 days'::interval, "time") < 'Sat Jan 01 00:00:00 294276 PST'::timestamp with time zone)
-   ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk
-         Filter: (time_bucket('@ 1000 days'::interval, "time") < 'Sat Jan 01 00:00:00 294276 PST'::timestamp with time zone)
-   ->  Index Only Scan Backward using _hyper_6_162_chunk_metrics_timestamptz_time_idx on _hyper_6_162_chunk
-         Filter: (time_bucket('@ 1000 days'::interval, "time") < 'Sat Jan 01 00:00:00 294276 PST'::timestamp with time zone)
-   ->  Index Only Scan Backward using _hyper_6_163_chunk_metrics_timestamptz_time_idx on _hyper_6_163_chunk
-         Filter: (time_bucket('@ 1000 days'::interval, "time") < 'Sat Jan 01 00:00:00 294276 PST'::timestamp with time zone)
-   ->  Index Only Scan Backward using _hyper_6_164_chunk_metrics_timestamptz_time_idx on _hyper_6_164_chunk
-         Filter: (time_bucket('@ 1000 days'::interval, "time") < 'Sat Jan 01 00:00:00 294276 PST'::timestamp with time zone)
-(12 rows)
-
-:PREFIX SELECT * FROM hyper WHERE time_bucket(10, time) > 10 AND time_bucket(10, time) < 100 ORDER BY time;
-                                                                          QUERY PLAN                                                                           
----------------------------------------------------------------------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: _hyper_1_2_chunk."time"
-   ->  Append
-         ->  Seq Scan on _hyper_1_2_chunk
-               Filter: (("time" > 10) AND ("time" < '100'::bigint) AND (time_bucket('10'::bigint, "time") > 10) AND (time_bucket('10'::bigint, "time") < 100))
-         ->  Seq Scan on _hyper_1_3_chunk
-               Filter: (("time" > 10) AND ("time" < '100'::bigint) AND (time_bucket('10'::bigint, "time") > 10) AND (time_bucket('10'::bigint, "time") < 100))
-         ->  Seq Scan on _hyper_1_4_chunk
-               Filter: (("time" > 10) AND ("time" < '100'::bigint) AND (time_bucket('10'::bigint, "time") > 10) AND (time_bucket('10'::bigint, "time") < 100))
-         ->  Seq Scan on _hyper_1_5_chunk
-               Filter: (("time" > 10) AND ("time" < '100'::bigint) AND (time_bucket('10'::bigint, "time") > 10) AND (time_bucket('10'::bigint, "time") < 100))
-         ->  Seq Scan on _hyper_1_6_chunk
-               Filter: (("time" > 10) AND ("time" < '100'::bigint) AND (time_bucket('10'::bigint, "time") > 10) AND (time_bucket('10'::bigint, "time") < 100))
-         ->  Seq Scan on _hyper_1_7_chunk
-               Filter: (("time" > 10) AND ("time" < '100'::bigint) AND (time_bucket('10'::bigint, "time") > 10) AND (time_bucket('10'::bigint, "time") < 100))
-         ->  Seq Scan on _hyper_1_8_chunk
-               Filter: (("time" > 10) AND ("time" < '100'::bigint) AND (time_bucket('10'::bigint, "time") > 10) AND (time_bucket('10'::bigint, "time") < 100))
-         ->  Seq Scan on _hyper_1_9_chunk
-               Filter: (("time" > 10) AND ("time" < '100'::bigint) AND (time_bucket('10'::bigint, "time") > 10) AND (time_bucket('10'::bigint, "time") < 100))
-         ->  Seq Scan on _hyper_1_10_chunk
-               Filter: (("time" > 10) AND ("time" < '100'::bigint) AND (time_bucket('10'::bigint, "time") > 10) AND (time_bucket('10'::bigint, "time") < 100))
-(21 rows)
-
-:PREFIX SELECT * FROM hyper WHERE time_bucket(10, time) > 10 AND time_bucket(10, time) < 20 ORDER BY time;
-                                                                      QUERY PLAN                                                                       
--------------------------------------------------------------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: _hyper_1_2_chunk."time"
-   ->  Seq Scan on _hyper_1_2_chunk
-         Filter: (("time" > 10) AND ("time" < '20'::bigint) AND (time_bucket('10'::bigint, "time") > 10) AND (time_bucket('10'::bigint, "time") < 20))
-(4 rows)
-
-:PREFIX SELECT * FROM hyper WHERE time_bucket(1, time) > 11 AND time_bucket(1, time) < 19 ORDER BY time;
-                                                                     QUERY PLAN                                                                      
------------------------------------------------------------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: _hyper_1_2_chunk."time"
-   ->  Seq Scan on _hyper_1_2_chunk
-         Filter: (("time" > 11) AND ("time" < '19'::bigint) AND (time_bucket('1'::bigint, "time") > 11) AND (time_bucket('1'::bigint, "time") < 19))
-(4 rows)
-
-:PREFIX SELECT * FROM hyper WHERE 10 < time_bucket(10, time) AND 20 > time_bucket(10,time) ORDER BY time;
-                                                                      QUERY PLAN                                                                       
--------------------------------------------------------------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: _hyper_1_2_chunk."time"
-   ->  Seq Scan on _hyper_1_2_chunk
-         Filter: (("time" > 10) AND ("time" < '20'::bigint) AND (10 < time_bucket('10'::bigint, "time")) AND (20 > time_bucket('10'::bigint, "time")))
-(4 rows)
-
-\qecho time_bucket exclusion with date
-time_bucket exclusion with date
-:PREFIX SELECT * FROM metrics_date WHERE time_bucket('1d',time) < '2000-01-03' ORDER BY time;
-                                          QUERY PLAN                                           
------------------------------------------------------------------------------------------------
- Index Only Scan Backward using _hyper_8_171_chunk_metrics_date_time_idx on _hyper_8_171_chunk
-   Index Cond: ("time" < '01-03-2000'::date)
-   Filter: (time_bucket('@ 1 day'::interval, "time") < '01-03-2000'::date)
-(3 rows)
-
-:PREFIX SELECT * FROM metrics_date WHERE time_bucket('1d',time) >= '2000-01-03' AND time_bucket('1d',time) <= '2000-01-10' ORDER BY time;
-                                                                       QUERY PLAN                                                                        
----------------------------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_date
-   Order: metrics_date."time"
-   ->  Index Only Scan Backward using _hyper_8_171_chunk_metrics_date_time_idx on _hyper_8_171_chunk
-         Index Cond: (("time" >= '01-03-2000'::date) AND ("time" <= '01-11-2000'::date))
-         Filter: ((time_bucket('@ 1 day'::interval, "time") >= '01-03-2000'::date) AND (time_bucket('@ 1 day'::interval, "time") <= '01-10-2000'::date))
-   ->  Index Only Scan Backward using _hyper_8_172_chunk_metrics_date_time_idx on _hyper_8_172_chunk
-         Index Cond: (("time" >= '01-03-2000'::date) AND ("time" <= '01-11-2000'::date))
-         Filter: ((time_bucket('@ 1 day'::interval, "time") >= '01-03-2000'::date) AND (time_bucket('@ 1 day'::interval, "time") <= '01-10-2000'::date))
-(8 rows)
-
-\qecho time_bucket exclusion with timestamp
-time_bucket exclusion with timestamp
-:PREFIX SELECT * FROM metrics_timestamp WHERE time_bucket('1d',time) < '2000-01-03' ORDER BY time;
-                                                   QUERY PLAN                                                   
-----------------------------------------------------------------------------------------------------------------
- Index Only Scan Backward using _hyper_5_155_chunk_metrics_timestamp_time_idx on _hyper_5_155_chunk
-   Index Cond: ("time" < 'Mon Jan 03 00:00:00 2000'::timestamp without time zone)
-   Filter: (time_bucket('@ 1 day'::interval, "time") < 'Mon Jan 03 00:00:00 2000'::timestamp without time zone)
-(3 rows)
-
-:PREFIX SELECT * FROM metrics_timestamp WHERE time_bucket('1d',time) >= '2000-01-03' AND time_bucket('1d',time) <= '2000-01-10' ORDER BY time;
-                                                                                                            QUERY PLAN                                                                                                             
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamp
-   Order: metrics_timestamp."time"
-   ->  Index Only Scan Backward using _hyper_5_155_chunk_metrics_timestamp_time_idx on _hyper_5_155_chunk
-         Index Cond: (("time" >= 'Mon Jan 03 00:00:00 2000'::timestamp without time zone) AND ("time" <= 'Tue Jan 11 00:00:00 2000'::timestamp without time zone))
-         Filter: ((time_bucket('@ 1 day'::interval, "time") >= 'Mon Jan 03 00:00:00 2000'::timestamp without time zone) AND (time_bucket('@ 1 day'::interval, "time") <= 'Mon Jan 10 00:00:00 2000'::timestamp without time zone))
-   ->  Index Only Scan Backward using _hyper_5_156_chunk_metrics_timestamp_time_idx on _hyper_5_156_chunk
-         Index Cond: (("time" >= 'Mon Jan 03 00:00:00 2000'::timestamp without time zone) AND ("time" <= 'Tue Jan 11 00:00:00 2000'::timestamp without time zone))
-         Filter: ((time_bucket('@ 1 day'::interval, "time") >= 'Mon Jan 03 00:00:00 2000'::timestamp without time zone) AND (time_bucket('@ 1 day'::interval, "time") <= 'Mon Jan 10 00:00:00 2000'::timestamp without time zone))
-(8 rows)
-
-\qecho time_bucket exclusion with timestamptz
-time_bucket exclusion with timestamptz
-:PREFIX SELECT time FROM metrics_timestamptz WHERE time_bucket('6h',time) < '2000-01-03' ORDER BY time;
-                                                    QUERY PLAN                                                     
--------------------------------------------------------------------------------------------------------------------
- Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk
-   Index Cond: ("time" < 'Mon Jan 03 06:00:00 2000 PST'::timestamp with time zone)
-   Filter: (time_bucket('@ 6 hours'::interval, "time") < 'Mon Jan 03 00:00:00 2000 PST'::timestamp with time zone)
-(3 rows)
-
-:PREFIX SELECT time FROM metrics_timestamptz WHERE time_bucket('6h',time) >= '2000-01-03' AND time_bucket('6h',time) <= '2000-01-10' ORDER BY time;
-                                                                                                               QUERY PLAN                                                                                                                
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamptz
-   Order: metrics_timestamptz."time"
-   ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk
-         Index Cond: (("time" >= 'Mon Jan 03 00:00:00 2000 PST'::timestamp with time zone) AND ("time" <= 'Mon Jan 10 06:00:00 2000 PST'::timestamp with time zone))
-         Filter: ((time_bucket('@ 6 hours'::interval, "time") >= 'Mon Jan 03 00:00:00 2000 PST'::timestamp with time zone) AND (time_bucket('@ 6 hours'::interval, "time") <= 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-   ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk
-         Index Cond: (("time" >= 'Mon Jan 03 00:00:00 2000 PST'::timestamp with time zone) AND ("time" <= 'Mon Jan 10 06:00:00 2000 PST'::timestamp with time zone))
-         Filter: ((time_bucket('@ 6 hours'::interval, "time") >= 'Mon Jan 03 00:00:00 2000 PST'::timestamp with time zone) AND (time_bucket('@ 6 hours'::interval, "time") <= 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-(8 rows)
-
-\qecho time_bucket exclusion with timestamptz and day interval
-time_bucket exclusion with timestamptz and day interval
-:PREFIX SELECT time FROM metrics_timestamptz WHERE time_bucket('1d',time) < '2000-01-03' ORDER BY time;
-                                                   QUERY PLAN                                                    
------------------------------------------------------------------------------------------------------------------
- Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk
-   Index Cond: ("time" < 'Tue Jan 04 00:00:00 2000 PST'::timestamp with time zone)
-   Filter: (time_bucket('@ 1 day'::interval, "time") < 'Mon Jan 03 00:00:00 2000 PST'::timestamp with time zone)
-(3 rows)
-
-:PREFIX SELECT time FROM metrics_timestamptz WHERE time_bucket('1d',time) >= '2000-01-03' AND time_bucket('1d',time) <= '2000-01-10' ORDER BY time;
-                                                                                                             QUERY PLAN                                                                                                              
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamptz
-   Order: metrics_timestamptz."time"
-   ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk
-         Index Cond: (("time" >= 'Mon Jan 03 00:00:00 2000 PST'::timestamp with time zone) AND ("time" <= 'Tue Jan 11 00:00:00 2000 PST'::timestamp with time zone))
-         Filter: ((time_bucket('@ 1 day'::interval, "time") >= 'Mon Jan 03 00:00:00 2000 PST'::timestamp with time zone) AND (time_bucket('@ 1 day'::interval, "time") <= 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-   ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk
-         Index Cond: (("time" >= 'Mon Jan 03 00:00:00 2000 PST'::timestamp with time zone) AND ("time" <= 'Tue Jan 11 00:00:00 2000 PST'::timestamp with time zone))
-         Filter: ((time_bucket('@ 1 day'::interval, "time") >= 'Mon Jan 03 00:00:00 2000 PST'::timestamp with time zone) AND (time_bucket('@ 1 day'::interval, "time") <= 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-(8 rows)
-
-:PREFIX SELECT time FROM metrics_timestamptz WHERE time_bucket('1d',time) >= '2000-01-03' AND time_bucket('7d',time) <= '2000-01-10' ORDER BY time;
-                                                                                                              QUERY PLAN                                                                                                              
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on metrics_timestamptz
-   Order: metrics_timestamptz."time"
-   ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk
-         Index Cond: (("time" >= 'Mon Jan 03 00:00:00 2000 PST'::timestamp with time zone) AND ("time" <= 'Mon Jan 17 00:00:00 2000 PST'::timestamp with time zone))
-         Filter: ((time_bucket('@ 1 day'::interval, "time") >= 'Mon Jan 03 00:00:00 2000 PST'::timestamp with time zone) AND (time_bucket('@ 7 days'::interval, "time") <= 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-   ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk
-         Index Cond: (("time" >= 'Mon Jan 03 00:00:00 2000 PST'::timestamp with time zone) AND ("time" <= 'Mon Jan 17 00:00:00 2000 PST'::timestamp with time zone))
-         Filter: ((time_bucket('@ 1 day'::interval, "time") >= 'Mon Jan 03 00:00:00 2000 PST'::timestamp with time zone) AND (time_bucket('@ 7 days'::interval, "time") <= 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-   ->  Index Only Scan Backward using _hyper_6_162_chunk_metrics_timestamptz_time_idx on _hyper_6_162_chunk
-         Index Cond: (("time" >= 'Mon Jan 03 00:00:00 2000 PST'::timestamp with time zone) AND ("time" <= 'Mon Jan 17 00:00:00 2000 PST'::timestamp with time zone))
-         Filter: ((time_bucket('@ 1 day'::interval, "time") >= 'Mon Jan 03 00:00:00 2000 PST'::timestamp with time zone) AND (time_bucket('@ 7 days'::interval, "time") <= 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-(11 rows)
-
-\qecho no transformation
-no transformation
-:PREFIX SELECT * FROM hyper WHERE time_bucket(10 + floor(random())::int, time) > 10 AND time_bucket(10 + floor(random())::int, time) < 100 AND time < 150 ORDER BY time;
-                                                                                           QUERY PLAN                                                                                            
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: hyper."time"
-   ->  Custom Scan (ChunkAppend) on hyper
-         Chunks excluded during startup: 0
-         ->  Seq Scan on _hyper_1_1_chunk
-               Filter: (("time" < 150) AND (time_bucket(((10 + (floor(random()))::integer))::bigint, "time") > 10) AND (time_bucket(((10 + (floor(random()))::integer))::bigint, "time") < 100))
-         ->  Seq Scan on _hyper_1_2_chunk
-               Filter: (("time" < 150) AND (time_bucket(((10 + (floor(random()))::integer))::bigint, "time") > 10) AND (time_bucket(((10 + (floor(random()))::integer))::bigint, "time") < 100))
-         ->  Seq Scan on _hyper_1_3_chunk
-               Filter: (("time" < 150) AND (time_bucket(((10 + (floor(random()))::integer))::bigint, "time") > 10) AND (time_bucket(((10 + (floor(random()))::integer))::bigint, "time") < 100))
-         ->  Seq Scan on _hyper_1_4_chunk
-               Filter: (("time" < 150) AND (time_bucket(((10 + (floor(random()))::integer))::bigint, "time") > 10) AND (time_bucket(((10 + (floor(random()))::integer))::bigint, "time") < 100))
-         ->  Seq Scan on _hyper_1_5_chunk
-               Filter: (("time" < 150) AND (time_bucket(((10 + (floor(random()))::integer))::bigint, "time") > 10) AND (time_bucket(((10 + (floor(random()))::integer))::bigint, "time") < 100))
-         ->  Seq Scan on _hyper_1_6_chunk
-               Filter: (("time" < 150) AND (time_bucket(((10 + (floor(random()))::integer))::bigint, "time") > 10) AND (time_bucket(((10 + (floor(random()))::integer))::bigint, "time") < 100))
-         ->  Seq Scan on _hyper_1_7_chunk
-               Filter: (("time" < 150) AND (time_bucket(((10 + (floor(random()))::integer))::bigint, "time") > 10) AND (time_bucket(((10 + (floor(random()))::integer))::bigint, "time") < 100))
-         ->  Seq Scan on _hyper_1_8_chunk
-               Filter: (("time" < 150) AND (time_bucket(((10 + (floor(random()))::integer))::bigint, "time") > 10) AND (time_bucket(((10 + (floor(random()))::integer))::bigint, "time") < 100))
-         ->  Seq Scan on _hyper_1_9_chunk
-               Filter: (("time" < 150) AND (time_bucket(((10 + (floor(random()))::integer))::bigint, "time") > 10) AND (time_bucket(((10 + (floor(random()))::integer))::bigint, "time") < 100))
-         ->  Seq Scan on _hyper_1_10_chunk
-               Filter: (("time" < 150) AND (time_bucket(((10 + (floor(random()))::integer))::bigint, "time") > 10) AND (time_bucket(((10 + (floor(random()))::integer))::bigint, "time") < 100))
-         ->  Seq Scan on _hyper_1_11_chunk
-               Filter: (("time" < 150) AND (time_bucket(((10 + (floor(random()))::integer))::bigint, "time") > 10) AND (time_bucket(((10 + (floor(random()))::integer))::bigint, "time") < 100))
-         ->  Seq Scan on _hyper_1_12_chunk
-               Filter: (("time" < 150) AND (time_bucket(((10 + (floor(random()))::integer))::bigint, "time") > 10) AND (time_bucket(((10 + (floor(random()))::integer))::bigint, "time") < 100))
-         ->  Seq Scan on _hyper_1_13_chunk
-               Filter: (("time" < 150) AND (time_bucket(((10 + (floor(random()))::integer))::bigint, "time") > 10) AND (time_bucket(((10 + (floor(random()))::integer))::bigint, "time") < 100))
-         ->  Seq Scan on _hyper_1_14_chunk
-               Filter: (("time" < 150) AND (time_bucket(((10 + (floor(random()))::integer))::bigint, "time") > 10) AND (time_bucket(((10 + (floor(random()))::integer))::bigint, "time") < 100))
-         ->  Seq Scan on _hyper_1_15_chunk
-               Filter: (("time" < 150) AND (time_bucket(((10 + (floor(random()))::integer))::bigint, "time") > 10) AND (time_bucket(((10 + (floor(random()))::integer))::bigint, "time") < 100))
-(34 rows)
-
-\qecho exclude chunks based on time column with partitioning function. This
-exclude chunks based on time column with partitioning function. This
-\qecho transparently applies the time partitioning function on the time
-transparently applies the time partitioning function on the time
-\qecho value to be able to exclude chunks (similar to a closed dimension).
-value to be able to exclude chunks (similar to a closed dimension).
-:PREFIX SELECT * FROM hyper_timefunc WHERE time < 4 ORDER BY value;
-                       QUERY PLAN                       
---------------------------------------------------------
- Sort
-   Sort Key: _hyper_4_124_chunk.value
-   ->  Append
-         ->  Seq Scan on _hyper_4_124_chunk
-               Filter: ("time" < '4'::double precision)
-         ->  Seq Scan on _hyper_4_125_chunk
-               Filter: ("time" < '4'::double precision)
-         ->  Seq Scan on _hyper_4_126_chunk
-               Filter: ("time" < '4'::double precision)
-         ->  Seq Scan on _hyper_4_127_chunk
-               Filter: ("time" < '4'::double precision)
-(11 rows)
-
-\qecho excluding based on time expression is currently unoptimized
-excluding based on time expression is currently unoptimized
-:PREFIX SELECT * FROM hyper_timefunc WHERE unix_to_timestamp(time) < 'Wed Dec 31 16:00:04 1969 PST' ORDER BY value;
-                                               QUERY PLAN                                                
----------------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: _hyper_4_124_chunk.value
-   ->  Append
-         ->  Seq Scan on _hyper_4_124_chunk
-               Filter: (to_timestamp("time") < 'Wed Dec 31 16:00:04 1969 PST'::timestamp with time zone)
-         ->  Seq Scan on _hyper_4_125_chunk
-               Filter: (to_timestamp("time") < 'Wed Dec 31 16:00:04 1969 PST'::timestamp with time zone)
-         ->  Seq Scan on _hyper_4_126_chunk
-               Filter: (to_timestamp("time") < 'Wed Dec 31 16:00:04 1969 PST'::timestamp with time zone)
-         ->  Seq Scan on _hyper_4_127_chunk
-               Filter: (to_timestamp("time") < 'Wed Dec 31 16:00:04 1969 PST'::timestamp with time zone)
-         ->  Seq Scan on _hyper_4_128_chunk
-               Filter: (to_timestamp("time") < 'Wed Dec 31 16:00:04 1969 PST'::timestamp with time zone)
-         ->  Seq Scan on _hyper_4_129_chunk
-               Filter: (to_timestamp("time") < 'Wed Dec 31 16:00:04 1969 PST'::timestamp with time zone)
-         ->  Seq Scan on _hyper_4_130_chunk
-               Filter: (to_timestamp("time") < 'Wed Dec 31 16:00:04 1969 PST'::timestamp with time zone)
-         ->  Seq Scan on _hyper_4_131_chunk
-               Filter: (to_timestamp("time") < 'Wed Dec 31 16:00:04 1969 PST'::timestamp with time zone)
-         ->  Seq Scan on _hyper_4_132_chunk
-               Filter: (to_timestamp("time") < 'Wed Dec 31 16:00:04 1969 PST'::timestamp with time zone)
-         ->  Seq Scan on _hyper_4_133_chunk
-               Filter: (to_timestamp("time") < 'Wed Dec 31 16:00:04 1969 PST'::timestamp with time zone)
-         ->  Seq Scan on _hyper_4_134_chunk
-               Filter: (to_timestamp("time") < 'Wed Dec 31 16:00:04 1969 PST'::timestamp with time zone)
-         ->  Seq Scan on _hyper_4_135_chunk
-               Filter: (to_timestamp("time") < 'Wed Dec 31 16:00:04 1969 PST'::timestamp with time zone)
-         ->  Seq Scan on _hyper_4_136_chunk
-               Filter: (to_timestamp("time") < 'Wed Dec 31 16:00:04 1969 PST'::timestamp with time zone)
-         ->  Seq Scan on _hyper_4_137_chunk
-               Filter: (to_timestamp("time") < 'Wed Dec 31 16:00:04 1969 PST'::timestamp with time zone)
-         ->  Seq Scan on _hyper_4_138_chunk
-               Filter: (to_timestamp("time") < 'Wed Dec 31 16:00:04 1969 PST'::timestamp with time zone)
-         ->  Seq Scan on _hyper_4_139_chunk
-               Filter: (to_timestamp("time") < 'Wed Dec 31 16:00:04 1969 PST'::timestamp with time zone)
-         ->  Seq Scan on _hyper_4_140_chunk
-               Filter: (to_timestamp("time") < 'Wed Dec 31 16:00:04 1969 PST'::timestamp with time zone)
-         ->  Seq Scan on _hyper_4_141_chunk
-               Filter: (to_timestamp("time") < 'Wed Dec 31 16:00:04 1969 PST'::timestamp with time zone)
-         ->  Seq Scan on _hyper_4_142_chunk
-               Filter: (to_timestamp("time") < 'Wed Dec 31 16:00:04 1969 PST'::timestamp with time zone)
-         ->  Seq Scan on _hyper_4_143_chunk
-               Filter: (to_timestamp("time") < 'Wed Dec 31 16:00:04 1969 PST'::timestamp with time zone)
-         ->  Seq Scan on _hyper_4_144_chunk
-               Filter: (to_timestamp("time") < 'Wed Dec 31 16:00:04 1969 PST'::timestamp with time zone)
-         ->  Seq Scan on _hyper_4_145_chunk
-               Filter: (to_timestamp("time") < 'Wed Dec 31 16:00:04 1969 PST'::timestamp with time zone)
-         ->  Seq Scan on _hyper_4_146_chunk
-               Filter: (to_timestamp("time") < 'Wed Dec 31 16:00:04 1969 PST'::timestamp with time zone)
-         ->  Seq Scan on _hyper_4_147_chunk
-               Filter: (to_timestamp("time") < 'Wed Dec 31 16:00:04 1969 PST'::timestamp with time zone)
-         ->  Seq Scan on _hyper_4_148_chunk
-               Filter: (to_timestamp("time") < 'Wed Dec 31 16:00:04 1969 PST'::timestamp with time zone)
-         ->  Seq Scan on _hyper_4_149_chunk
-               Filter: (to_timestamp("time") < 'Wed Dec 31 16:00:04 1969 PST'::timestamp with time zone)
-         ->  Seq Scan on _hyper_4_150_chunk
-               Filter: (to_timestamp("time") < 'Wed Dec 31 16:00:04 1969 PST'::timestamp with time zone)
-         ->  Seq Scan on _hyper_4_151_chunk
-               Filter: (to_timestamp("time") < 'Wed Dec 31 16:00:04 1969 PST'::timestamp with time zone)
-         ->  Seq Scan on _hyper_4_152_chunk
-               Filter: (to_timestamp("time") < 'Wed Dec 31 16:00:04 1969 PST'::timestamp with time zone)
-         ->  Seq Scan on _hyper_4_153_chunk
-               Filter: (to_timestamp("time") < 'Wed Dec 31 16:00:04 1969 PST'::timestamp with time zone)
-         ->  Seq Scan on _hyper_4_154_chunk
-               Filter: (to_timestamp("time") < 'Wed Dec 31 16:00:04 1969 PST'::timestamp with time zone)
-(65 rows)
-
-\qecho test qual propagation for joins
-test qual propagation for joins
-RESET constraint_exclusion;
-\qecho nothing to propagate
-nothing to propagate
-:PREFIX SELECT m1.time FROM metrics_timestamptz m1, metrics_timestamptz_2 m2 WHERE m1.time = m2.time ORDER BY m1.time;
-                                                          QUERY PLAN                                                           
--------------------------------------------------------------------------------------------------------------------------------
- Merge Join
-   Merge Cond: (m1."time" = m2."time")
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-         Order: m1."time"
-         ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-         ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-         ->  Index Only Scan Backward using _hyper_6_162_chunk_metrics_timestamptz_time_idx on _hyper_6_162_chunk m1_3
-         ->  Index Only Scan Backward using _hyper_6_163_chunk_metrics_timestamptz_time_idx on _hyper_6_163_chunk m1_4
-         ->  Index Only Scan Backward using _hyper_6_164_chunk_metrics_timestamptz_time_idx on _hyper_6_164_chunk m1_5
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz_2 m2
-               Order: m2."time"
-               ->  Index Only Scan Backward using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk m2_1
-               ->  Index Only Scan Backward using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk m2_2
-               ->  Index Only Scan Backward using _hyper_7_167_chunk_metrics_timestamptz_2_time_idx on _hyper_7_167_chunk m2_3
-               ->  Index Only Scan Backward using _hyper_7_168_chunk_metrics_timestamptz_2_time_idx on _hyper_7_168_chunk m2_4
-               ->  Index Only Scan Backward using _hyper_7_169_chunk_metrics_timestamptz_2_time_idx on _hyper_7_169_chunk m2_5
-               ->  Index Only Scan Backward using _hyper_7_170_chunk_metrics_timestamptz_2_time_idx on _hyper_7_170_chunk m2_6
-(18 rows)
-
-:PREFIX SELECT m1.time FROM metrics_timestamptz m1 INNER JOIN metrics_timestamptz_2 m2 ON m1.time = m2.time ORDER BY m1.time;
-                                                          QUERY PLAN                                                           
--------------------------------------------------------------------------------------------------------------------------------
- Merge Join
-   Merge Cond: (m1."time" = m2."time")
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-         Order: m1."time"
-         ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-         ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-         ->  Index Only Scan Backward using _hyper_6_162_chunk_metrics_timestamptz_time_idx on _hyper_6_162_chunk m1_3
-         ->  Index Only Scan Backward using _hyper_6_163_chunk_metrics_timestamptz_time_idx on _hyper_6_163_chunk m1_4
-         ->  Index Only Scan Backward using _hyper_6_164_chunk_metrics_timestamptz_time_idx on _hyper_6_164_chunk m1_5
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz_2 m2
-               Order: m2."time"
-               ->  Index Only Scan Backward using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk m2_1
-               ->  Index Only Scan Backward using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk m2_2
-               ->  Index Only Scan Backward using _hyper_7_167_chunk_metrics_timestamptz_2_time_idx on _hyper_7_167_chunk m2_3
-               ->  Index Only Scan Backward using _hyper_7_168_chunk_metrics_timestamptz_2_time_idx on _hyper_7_168_chunk m2_4
-               ->  Index Only Scan Backward using _hyper_7_169_chunk_metrics_timestamptz_2_time_idx on _hyper_7_169_chunk m2_5
-               ->  Index Only Scan Backward using _hyper_7_170_chunk_metrics_timestamptz_2_time_idx on _hyper_7_170_chunk m2_6
-(18 rows)
-
-:PREFIX SELECT m1.time FROM metrics_timestamptz m1 LEFT JOIN metrics_timestamptz_2 m2 ON m1.time = m2.time ORDER BY m1.time;
-                                                          QUERY PLAN                                                           
--------------------------------------------------------------------------------------------------------------------------------
- Merge Left Join
-   Merge Cond: (m1."time" = m2."time")
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-         Order: m1."time"
-         ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-         ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-         ->  Index Only Scan Backward using _hyper_6_162_chunk_metrics_timestamptz_time_idx on _hyper_6_162_chunk m1_3
-         ->  Index Only Scan Backward using _hyper_6_163_chunk_metrics_timestamptz_time_idx on _hyper_6_163_chunk m1_4
-         ->  Index Only Scan Backward using _hyper_6_164_chunk_metrics_timestamptz_time_idx on _hyper_6_164_chunk m1_5
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz_2 m2
-               Order: m2."time"
-               ->  Index Only Scan Backward using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk m2_1
-               ->  Index Only Scan Backward using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk m2_2
-               ->  Index Only Scan Backward using _hyper_7_167_chunk_metrics_timestamptz_2_time_idx on _hyper_7_167_chunk m2_3
-               ->  Index Only Scan Backward using _hyper_7_168_chunk_metrics_timestamptz_2_time_idx on _hyper_7_168_chunk m2_4
-               ->  Index Only Scan Backward using _hyper_7_169_chunk_metrics_timestamptz_2_time_idx on _hyper_7_169_chunk m2_5
-               ->  Index Only Scan Backward using _hyper_7_170_chunk_metrics_timestamptz_2_time_idx on _hyper_7_170_chunk m2_6
-(18 rows)
-
-:PREFIX SELECT m1.time FROM metrics_timestamptz m1 RIGHT JOIN metrics_timestamptz_2 m2 ON m1.time = m2.time ORDER BY m1.time;
-                                                             QUERY PLAN                                                              
--------------------------------------------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: m1."time"
-   ->  Merge Right Join
-         Merge Cond: (m1."time" = m2."time")
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-               Order: m1."time"
-               ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-               ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-               ->  Index Only Scan Backward using _hyper_6_162_chunk_metrics_timestamptz_time_idx on _hyper_6_162_chunk m1_3
-               ->  Index Only Scan Backward using _hyper_6_163_chunk_metrics_timestamptz_time_idx on _hyper_6_163_chunk m1_4
-               ->  Index Only Scan Backward using _hyper_6_164_chunk_metrics_timestamptz_time_idx on _hyper_6_164_chunk m1_5
-         ->  Materialize
-               ->  Custom Scan (ChunkAppend) on metrics_timestamptz_2 m2
-                     Order: m2."time"
-                     ->  Index Only Scan Backward using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk m2_1
-                     ->  Index Only Scan Backward using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk m2_2
-                     ->  Index Only Scan Backward using _hyper_7_167_chunk_metrics_timestamptz_2_time_idx on _hyper_7_167_chunk m2_3
-                     ->  Index Only Scan Backward using _hyper_7_168_chunk_metrics_timestamptz_2_time_idx on _hyper_7_168_chunk m2_4
-                     ->  Index Only Scan Backward using _hyper_7_169_chunk_metrics_timestamptz_2_time_idx on _hyper_7_169_chunk m2_5
-                     ->  Index Only Scan Backward using _hyper_7_170_chunk_metrics_timestamptz_2_time_idx on _hyper_7_170_chunk m2_6
-(20 rows)
-
-\qecho OR constraints should not propagate
-OR constraints should not propagate
-:PREFIX SELECT m1.time FROM metrics_timestamptz m1 INNER JOIN metrics_timestamptz_2 m2 ON m1.time = m2.time WHERE m1.time < '2000-01-10' OR m1.time > '2001-01-01' ORDER BY m1.time;
-                                                                             QUERY PLAN                                                                             
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Merge Join
-   Merge Cond: (m1."time" = m2."time")
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-         Order: m1."time"
-         ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-               Filter: (("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone) OR ("time" > 'Mon Jan 01 00:00:00 2001 PST'::timestamp with time zone))
-         ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-               Filter: (("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone) OR ("time" > 'Mon Jan 01 00:00:00 2001 PST'::timestamp with time zone))
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz_2 m2
-               Order: m2."time"
-               ->  Index Only Scan Backward using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk m2_1
-               ->  Index Only Scan Backward using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk m2_2
-               ->  Index Only Scan Backward using _hyper_7_167_chunk_metrics_timestamptz_2_time_idx on _hyper_7_167_chunk m2_3
-               ->  Index Only Scan Backward using _hyper_7_168_chunk_metrics_timestamptz_2_time_idx on _hyper_7_168_chunk m2_4
-               ->  Index Only Scan Backward using _hyper_7_169_chunk_metrics_timestamptz_2_time_idx on _hyper_7_169_chunk m2_5
-               ->  Index Only Scan Backward using _hyper_7_170_chunk_metrics_timestamptz_2_time_idx on _hyper_7_170_chunk m2_6
-(17 rows)
-
-\qecho test single constraint
-test single constraint
-\qecho constraint should be on both scans
-constraint should be on both scans
-\qecho these will propagate even for LEFT/RIGHT JOIN because the constraints are not in the ON clause and therefore imply a NOT NULL condition on the JOIN column
-these will propagate even for LEFT/RIGHT JOIN because the constraints are not in the ON clause and therefore imply a NOT NULL condition on the JOIN column
-:PREFIX SELECT m1.time FROM metrics_timestamptz m1, metrics_timestamptz_2 m2 WHERE m1.time = m2.time AND m1.time < '2000-01-10' ORDER BY m1.time;
-                                                          QUERY PLAN                                                           
--------------------------------------------------------------------------------------------------------------------------------
- Merge Join
-   Merge Cond: (m1."time" = m2."time")
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-         Order: m1."time"
-         ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-               Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-         ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-               Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz_2 m2
-               Order: m2."time"
-               ->  Index Only Scan Backward using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk m2_1
-                     Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               ->  Index Only Scan Backward using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk m2_2
-                     Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-(15 rows)
-
-:PREFIX SELECT m1.time FROM metrics_timestamptz m1 INNER JOIN metrics_timestamptz_2 m2 ON m1.time = m2.time WHERE m1.time < '2000-01-10' ORDER BY m1.time;
-                                                          QUERY PLAN                                                           
--------------------------------------------------------------------------------------------------------------------------------
- Merge Join
-   Merge Cond: (m1."time" = m2."time")
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-         Order: m1."time"
-         ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-               Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-         ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-               Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz_2 m2
-               Order: m2."time"
-               ->  Index Only Scan Backward using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk m2_1
-                     Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               ->  Index Only Scan Backward using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk m2_2
-                     Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-(15 rows)
-
-:PREFIX SELECT m1.time FROM metrics_timestamptz m1 LEFT JOIN metrics_timestamptz_2 m2 ON m1.time = m2.time WHERE m1.time < '2000-01-10' ORDER BY m1.time;
-                                                          QUERY PLAN                                                           
--------------------------------------------------------------------------------------------------------------------------------
- Merge Left Join
-   Merge Cond: (m1."time" = m2."time")
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-         Order: m1."time"
-         ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-               Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-         ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-               Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz_2 m2
-               Order: m2."time"
-               ->  Index Only Scan Backward using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk m2_1
-               ->  Index Only Scan Backward using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk m2_2
-               ->  Index Only Scan Backward using _hyper_7_167_chunk_metrics_timestamptz_2_time_idx on _hyper_7_167_chunk m2_3
-               ->  Index Only Scan Backward using _hyper_7_168_chunk_metrics_timestamptz_2_time_idx on _hyper_7_168_chunk m2_4
-               ->  Index Only Scan Backward using _hyper_7_169_chunk_metrics_timestamptz_2_time_idx on _hyper_7_169_chunk m2_5
-               ->  Index Only Scan Backward using _hyper_7_170_chunk_metrics_timestamptz_2_time_idx on _hyper_7_170_chunk m2_6
-(17 rows)
-
-:PREFIX SELECT m1.time FROM metrics_timestamptz m1 RIGHT JOIN metrics_timestamptz_2 m2 ON m1.time = m2.time WHERE m1.time < '2000-01-10' ORDER BY m1.time;
-                                                          QUERY PLAN                                                           
--------------------------------------------------------------------------------------------------------------------------------
- Merge Join
-   Merge Cond: (m1."time" = m2."time")
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-         Order: m1."time"
-         ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-               Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-         ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-               Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz_2 m2
-               Order: m2."time"
-               ->  Index Only Scan Backward using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk m2_1
-                     Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               ->  Index Only Scan Backward using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk m2_2
-                     Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-(15 rows)
-
-\qecho test 2 constraints on single relation
-test 2 constraints on single relation
-\qecho these will propagate even for LEFT/RIGHT JOIN because the constraints are not in the ON clause and therefore imply a NOT NULL condition on the JOIN column
-these will propagate even for LEFT/RIGHT JOIN because the constraints are not in the ON clause and therefore imply a NOT NULL condition on the JOIN column
-:PREFIX SELECT m1.time FROM metrics_timestamptz m1, metrics_timestamptz_2 m2 WHERE m1.time = m2.time AND m1.time > '2000-01-01' AND m1.time < '2000-01-10' ORDER BY m1.time;
-                                                                                  QUERY PLAN                                                                                   
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Merge Join
-   Merge Cond: (m1."time" = m2."time")
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-         Order: m1."time"
-         ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-               Index Cond: (("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-         ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-               Index Cond: (("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz_2 m2
-               Order: m2."time"
-               ->  Index Only Scan Backward using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk m2_1
-                     Index Cond: (("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-               ->  Index Only Scan Backward using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk m2_2
-                     Index Cond: (("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-(15 rows)
-
-:PREFIX SELECT m1.time FROM metrics_timestamptz m1 INNER JOIN metrics_timestamptz_2 m2 ON m1.time = m2.time WHERE m1.time > '2000-01-01' AND m1.time < '2000-01-10' ORDER BY m1.time;
-                                                                                  QUERY PLAN                                                                                   
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Merge Join
-   Merge Cond: (m1."time" = m2."time")
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-         Order: m1."time"
-         ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-               Index Cond: (("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-         ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-               Index Cond: (("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz_2 m2
-               Order: m2."time"
-               ->  Index Only Scan Backward using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk m2_1
-                     Index Cond: (("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-               ->  Index Only Scan Backward using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk m2_2
-                     Index Cond: (("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-(15 rows)
-
-:PREFIX SELECT m1.time FROM metrics_timestamptz m1 LEFT JOIN metrics_timestamptz_2 m2 ON m1.time = m2.time WHERE m1.time > '2000-01-01' AND m1.time < '2000-01-10' ORDER BY m1.time;
-                                                                               QUERY PLAN                                                                                
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Nested Loop Left Join
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-         Order: m1."time"
-         ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-               Index Cond: (("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-         ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-               Index Cond: (("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-   ->  Append
-         ->  Index Only Scan using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk m2_1
-               Index Cond: ("time" = m1."time")
-         ->  Index Only Scan using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk m2_2
-               Index Cond: ("time" = m1."time")
-         ->  Index Only Scan using _hyper_7_167_chunk_metrics_timestamptz_2_time_idx on _hyper_7_167_chunk m2_3
-               Index Cond: ("time" = m1."time")
-         ->  Index Only Scan using _hyper_7_168_chunk_metrics_timestamptz_2_time_idx on _hyper_7_168_chunk m2_4
-               Index Cond: ("time" = m1."time")
-         ->  Index Only Scan using _hyper_7_169_chunk_metrics_timestamptz_2_time_idx on _hyper_7_169_chunk m2_5
-               Index Cond: ("time" = m1."time")
-         ->  Index Only Scan using _hyper_7_170_chunk_metrics_timestamptz_2_time_idx on _hyper_7_170_chunk m2_6
-               Index Cond: ("time" = m1."time")
-(20 rows)
-
-:PREFIX SELECT m1.time FROM metrics_timestamptz m1 RIGHT JOIN metrics_timestamptz_2 m2 ON m1.time = m2.time WHERE m1.time > '2000-01-01' AND m1.time < '2000-01-10' ORDER BY m1.time;
-                                                                                  QUERY PLAN                                                                                   
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Merge Join
-   Merge Cond: (m1."time" = m2."time")
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-         Order: m1."time"
-         ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-               Index Cond: (("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-         ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-               Index Cond: (("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz_2 m2
-               Order: m2."time"
-               ->  Index Only Scan Backward using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk m2_1
-                     Index Cond: (("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-               ->  Index Only Scan Backward using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk m2_2
-                     Index Cond: (("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-(15 rows)
-
-\qecho test 2 constraints with 1 constraint on each relation
-test 2 constraints with 1 constraint on each relation
-\qecho these will propagate even for LEFT/RIGHT JOIN because the constraints are not in the ON clause and therefore imply a NOT NULL condition on the JOIN column
-these will propagate even for LEFT/RIGHT JOIN because the constraints are not in the ON clause and therefore imply a NOT NULL condition on the JOIN column
-:PREFIX SELECT m1.time FROM metrics_timestamptz m1, metrics_timestamptz_2 m2 WHERE m1.time = m2.time AND m1.time > '2000-01-01' AND m2.time < '2000-01-10' ORDER BY m1.time;
-                                                                                  QUERY PLAN                                                                                   
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Merge Join
-   Merge Cond: (m1."time" = m2."time")
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-         Order: m1."time"
-         ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-               Index Cond: (("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-         ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-               Index Cond: (("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz_2 m2
-               Order: m2."time"
-               ->  Index Only Scan Backward using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk m2_1
-                     Index Cond: (("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone) AND ("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone))
-               ->  Index Only Scan Backward using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk m2_2
-                     Index Cond: (("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone) AND ("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone))
-(15 rows)
-
-:PREFIX SELECT m1.time FROM metrics_timestamptz m1 INNER JOIN metrics_timestamptz_2 m2 ON m1.time = m2.time WHERE m1.time > '2000-01-01' AND m2.time < '2000-01-10' ORDER BY m1.time;
-                                                                                  QUERY PLAN                                                                                   
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Merge Join
-   Merge Cond: (m1."time" = m2."time")
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-         Order: m1."time"
-         ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-               Index Cond: (("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-         ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-               Index Cond: (("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz_2 m2
-               Order: m2."time"
-               ->  Index Only Scan Backward using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk m2_1
-                     Index Cond: (("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone) AND ("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone))
-               ->  Index Only Scan Backward using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk m2_2
-                     Index Cond: (("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone) AND ("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone))
-(15 rows)
-
-:PREFIX SELECT m1.time FROM metrics_timestamptz m1 LEFT JOIN metrics_timestamptz_2 m2 ON m1.time = m2.time WHERE m1.time > '2000-01-01' AND m2.time < '2000-01-10' ORDER BY m1.time;
-                                                                                  QUERY PLAN                                                                                   
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Merge Join
-   Merge Cond: (m1."time" = m2."time")
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-         Order: m1."time"
-         ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-               Index Cond: (("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-         ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-               Index Cond: (("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz_2 m2
-               Order: m2."time"
-               ->  Index Only Scan Backward using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk m2_1
-                     Index Cond: (("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone) AND ("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone))
-               ->  Index Only Scan Backward using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk m2_2
-                     Index Cond: (("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone) AND ("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone))
-(15 rows)
-
-:PREFIX SELECT m1.time FROM metrics_timestamptz m1 RIGHT JOIN metrics_timestamptz_2 m2 ON m1.time = m2.time WHERE m1.time > '2000-01-01' AND m2.time < '2000-01-10' ORDER BY m1.time;
-                                                                                  QUERY PLAN                                                                                   
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Merge Join
-   Merge Cond: (m1."time" = m2."time")
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-         Order: m1."time"
-         ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-               Index Cond: (("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-         ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-               Index Cond: (("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz_2 m2
-               Order: m2."time"
-               ->  Index Only Scan Backward using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk m2_1
-                     Index Cond: (("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone) AND ("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone))
-               ->  Index Only Scan Backward using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk m2_2
-                     Index Cond: (("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone) AND ("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone))
-(15 rows)
-
-\qecho test constraints in ON clause of INNER JOIN
-test constraints in ON clause of INNER JOIN
-:PREFIX SELECT m1.time FROM metrics_timestamptz m1 INNER JOIN metrics_timestamptz_2 m2 ON m1.time = m2.time AND m2.time > '2000-01-01' AND m2.time < '2000-01-10' ORDER BY m1.time;
-                                                                                  QUERY PLAN                                                                                   
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Merge Join
-   Merge Cond: (m1."time" = m2."time")
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-         Order: m1."time"
-         ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-               Index Cond: (("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-         ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-               Index Cond: (("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz_2 m2
-               Order: m2."time"
-               ->  Index Only Scan Backward using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk m2_1
-                     Index Cond: (("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-               ->  Index Only Scan Backward using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk m2_2
-                     Index Cond: (("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-(15 rows)
-
-\qecho test constraints in ON clause of LEFT JOIN
-test constraints in ON clause of LEFT JOIN
-\qecho must not propagate
-must not propagate
-:PREFIX SELECT m1.time FROM metrics_timestamptz m1 LEFT JOIN metrics_timestamptz_2 m2 ON m1.time = m2.time AND m2.time > '2000-01-01' AND m2.time < '2000-01-10' ORDER BY m1.time;
-                                                                                  QUERY PLAN                                                                                   
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Merge Left Join
-   Merge Cond: (m1."time" = m2."time")
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-         Order: m1."time"
-         ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-         ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-         ->  Index Only Scan Backward using _hyper_6_162_chunk_metrics_timestamptz_time_idx on _hyper_6_162_chunk m1_3
-         ->  Index Only Scan Backward using _hyper_6_163_chunk_metrics_timestamptz_time_idx on _hyper_6_163_chunk m1_4
-         ->  Index Only Scan Backward using _hyper_6_164_chunk_metrics_timestamptz_time_idx on _hyper_6_164_chunk m1_5
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz_2 m2
-               Order: m2."time"
-               ->  Index Only Scan Backward using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk m2_1
-                     Index Cond: (("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-               ->  Index Only Scan Backward using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk m2_2
-                     Index Cond: (("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-(16 rows)
-
-\qecho test constraints in ON clause of RIGHT JOIN
-test constraints in ON clause of RIGHT JOIN
-\qecho must not propagate
-must not propagate
-:PREFIX SELECT m1.time FROM metrics_timestamptz m1 RIGHT JOIN metrics_timestamptz_2 m2 ON m1.time = m2.time AND m2.time > '2000-01-01' AND m2.time < '2000-01-10' ORDER BY m1.time;
-                                                                                     QUERY PLAN                                                                                     
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Gather Merge
-   Workers Planned: 2
-   ->  Sort
-         Sort Key: m1_1."time"
-         ->  Parallel Hash Left Join
-               Hash Cond: (m2_1."time" = m1_1."time")
-               Join Filter: ((m2_1."time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone) AND (m2_1."time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-               ->  Parallel Append
-                     ->  Parallel Seq Scan on _hyper_7_165_chunk m2_1
-                     ->  Parallel Seq Scan on _hyper_7_166_chunk m2_2
-                     ->  Parallel Seq Scan on _hyper_7_167_chunk m2_3
-                     ->  Parallel Seq Scan on _hyper_7_168_chunk m2_4
-                     ->  Parallel Seq Scan on _hyper_7_169_chunk m2_5
-                     ->  Parallel Seq Scan on _hyper_7_170_chunk m2_6
-               ->  Parallel Hash
-                     ->  Parallel Append
-                           ->  Parallel Seq Scan on _hyper_6_160_chunk m1_1
-                           ->  Parallel Seq Scan on _hyper_6_161_chunk m1_2
-                           ->  Parallel Seq Scan on _hyper_6_162_chunk m1_3
-                           ->  Parallel Seq Scan on _hyper_6_163_chunk m1_4
-                           ->  Parallel Seq Scan on _hyper_6_164_chunk m1_5
-(21 rows)
-
-\qecho test equality condition not in ON clause
-test equality condition not in ON clause
-:PREFIX SELECT m1.time FROM metrics_timestamptz m1 INNER JOIN metrics_timestamptz_2 m2 ON true WHERE m2.time = m1.time AND m2.time < '2000-01-10' ORDER BY m1.time;
-                                                          QUERY PLAN                                                           
--------------------------------------------------------------------------------------------------------------------------------
- Merge Join
-   Merge Cond: (m1."time" = m2."time")
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-         Order: m1."time"
-         ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-               Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-         ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-               Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz_2 m2
-               Order: m2."time"
-               ->  Index Only Scan Backward using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk m2_1
-                     Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               ->  Index Only Scan Backward using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk m2_2
-                     Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-(15 rows)
-
-\qecho test constraints not joined on
-test constraints not joined on
-\qecho device_id constraint must not propagate
-device_id constraint must not propagate
-:PREFIX SELECT m1.time FROM metrics_timestamptz m1 INNER JOIN metrics_timestamptz_2 m2 ON true WHERE m2.time = m1.time AND m2.time < '2000-01-10' AND m1.device_id = 1 ORDER BY m1.time;
-                                                        QUERY PLAN                                                        
---------------------------------------------------------------------------------------------------------------------------
- Nested Loop
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-         Order: m1."time"
-         ->  Index Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-               Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               Filter: (device_id = 1)
-         ->  Index Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-               Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               Filter: (device_id = 1)
-   ->  Append
-         ->  Index Only Scan using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk m2_1
-               Index Cond: (("time" = m1."time") AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-         ->  Index Only Scan using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk m2_2
-               Index Cond: (("time" = m1."time") AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-(14 rows)
-
-\qecho test multiple join conditions
-test multiple join conditions
-\qecho device_id constraint should propagate
-device_id constraint should propagate
-:PREFIX SELECT m1.time FROM metrics_timestamptz m1 INNER JOIN metrics_timestamptz_2 m2 ON true WHERE m2.time = m1.time AND m1.device_id = m2.device_id AND m2.time < '2000-01-10' AND m1.device_id = 1 ORDER BY m1.time;
-                                                        QUERY PLAN                                                        
---------------------------------------------------------------------------------------------------------------------------
- Nested Loop
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-         Order: m1."time"
-         ->  Index Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-               Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               Filter: (device_id = 1)
-         ->  Index Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-               Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-               Filter: (device_id = 1)
-   ->  Append
-         ->  Index Scan using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk m2_1
-               Index Cond: (("time" = m1."time") AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-               Filter: (device_id = 1)
-         ->  Index Scan using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk m2_2
-               Index Cond: (("time" = m1."time") AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-               Filter: (device_id = 1)
-(16 rows)
-
-\qecho test join with 3 tables
-test join with 3 tables
-:PREFIX SELECT m1.time FROM metrics_timestamptz m1 INNER JOIN metrics_timestamptz_2 m2 ON m1.time = m2.time INNER JOIN metrics_timestamptz m3 ON m2.time=m3.time WHERE m1.time > '2000-01-01' AND m1.time < '2000-01-10' ORDER BY m1.time;
-                                                                                     QUERY PLAN                                                                                      
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Nested Loop
-   ->  Merge Join
-         Merge Cond: (m1."time" = m2."time")
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-               Order: m1."time"
-               ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-                     Index Cond: (("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-               ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-                     Index Cond: (("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-         ->  Materialize
-               ->  Custom Scan (ChunkAppend) on metrics_timestamptz_2 m2
-                     Order: m2."time"
-                     ->  Index Only Scan Backward using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk m2_1
-                           Index Cond: (("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-                     ->  Index Only Scan Backward using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk m2_2
-                           Index Cond: (("time" > 'Sat Jan 01 00:00:00 2000 PST'::timestamp with time zone) AND ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone))
-   ->  Append
-         ->  Index Only Scan using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m3_1
-               Index Cond: ("time" = m1."time")
-         ->  Index Only Scan using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m3_2
-               Index Cond: ("time" = m1."time")
-         ->  Index Only Scan using _hyper_6_162_chunk_metrics_timestamptz_time_idx on _hyper_6_162_chunk m3_3
-               Index Cond: ("time" = m1."time")
-         ->  Index Only Scan using _hyper_6_163_chunk_metrics_timestamptz_time_idx on _hyper_6_163_chunk m3_4
-               Index Cond: ("time" = m1."time")
-         ->  Index Only Scan using _hyper_6_164_chunk_metrics_timestamptz_time_idx on _hyper_6_164_chunk m3_5
-               Index Cond: ("time" = m1."time")
-(27 rows)
-
-\qecho test non-Const constraints
-test non-Const constraints
-:PREFIX SELECT m1.time FROM metrics_timestamptz m1 INNER JOIN metrics_timestamptz_2 m2 ON m1.time = m2.time WHERE m1.time < '2000-01-10'::text::timestamptz ORDER BY m1.time;
-                                                          QUERY PLAN                                                           
--------------------------------------------------------------------------------------------------------------------------------
- Merge Join
-   Merge Cond: (m1."time" = m2."time")
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-         Order: m1."time"
-         Chunks excluded during startup: 3
-         ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-               Index Cond: ("time" < ('2000-01-10'::cstring)::timestamp with time zone)
-         ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-               Index Cond: ("time" < ('2000-01-10'::cstring)::timestamp with time zone)
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz_2 m2
-               Order: m2."time"
-               Chunks excluded during startup: 4
-               ->  Index Only Scan Backward using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk m2_1
-                     Index Cond: ("time" < ('2000-01-10'::cstring)::timestamp with time zone)
-               ->  Index Only Scan Backward using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk m2_2
-                     Index Cond: ("time" < ('2000-01-10'::cstring)::timestamp with time zone)
-(17 rows)
-
-\qecho test now()
-test now()
-:PREFIX SELECT m1.time FROM metrics_timestamptz m1 INNER JOIN metrics_timestamptz_2 m2 ON m1.time = m2.time WHERE m1.time < now() ORDER BY m1.time;
-                                                          QUERY PLAN                                                           
--------------------------------------------------------------------------------------------------------------------------------
- Merge Join
-   Merge Cond: (m1."time" = m2."time")
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-         Order: m1."time"
-         Chunks excluded during startup: 0
-         ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-               Index Cond: ("time" < now())
-         ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-               Index Cond: ("time" < now())
-         ->  Index Only Scan Backward using _hyper_6_162_chunk_metrics_timestamptz_time_idx on _hyper_6_162_chunk m1_3
-               Index Cond: ("time" < now())
-         ->  Index Only Scan Backward using _hyper_6_163_chunk_metrics_timestamptz_time_idx on _hyper_6_163_chunk m1_4
-               Index Cond: ("time" < now())
-         ->  Index Only Scan Backward using _hyper_6_164_chunk_metrics_timestamptz_time_idx on _hyper_6_164_chunk m1_5
-               Index Cond: ("time" < now())
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz_2 m2
-               Order: m2."time"
-               Chunks excluded during startup: 0
-               ->  Index Only Scan Backward using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk m2_1
-                     Index Cond: ("time" < now())
-               ->  Index Only Scan Backward using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk m2_2
-                     Index Cond: ("time" < now())
-               ->  Index Only Scan Backward using _hyper_7_167_chunk_metrics_timestamptz_2_time_idx on _hyper_7_167_chunk m2_3
-                     Index Cond: ("time" < now())
-               ->  Index Only Scan Backward using _hyper_7_168_chunk_metrics_timestamptz_2_time_idx on _hyper_7_168_chunk m2_4
-                     Index Cond: ("time" < now())
-               ->  Index Only Scan Backward using _hyper_7_169_chunk_metrics_timestamptz_2_time_idx on _hyper_7_169_chunk m2_5
-                     Index Cond: ("time" < now())
-               ->  Index Only Scan Backward using _hyper_7_170_chunk_metrics_timestamptz_2_time_idx on _hyper_7_170_chunk m2_6
-                     Index Cond: ("time" < now())
-(31 rows)
-
-\qecho test volatile function
-test volatile function
-\qecho should not propagate
-should not propagate
-:PREFIX SELECT m1.time FROM metrics_timestamptz m1 INNER JOIN metrics_timestamptz_2 m2 ON m1.time = m2.time WHERE m1.time < clock_timestamp() ORDER BY m1.time;
-                                                          QUERY PLAN                                                           
--------------------------------------------------------------------------------------------------------------------------------
- Merge Join
-   Merge Cond: (m1."time" = m2."time")
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-         Order: m1."time"
-         Chunks excluded during startup: 0
-         ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-               Filter: ("time" < clock_timestamp())
-         ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-               Filter: ("time" < clock_timestamp())
-         ->  Index Only Scan Backward using _hyper_6_162_chunk_metrics_timestamptz_time_idx on _hyper_6_162_chunk m1_3
-               Filter: ("time" < clock_timestamp())
-         ->  Index Only Scan Backward using _hyper_6_163_chunk_metrics_timestamptz_time_idx on _hyper_6_163_chunk m1_4
-               Filter: ("time" < clock_timestamp())
-         ->  Index Only Scan Backward using _hyper_6_164_chunk_metrics_timestamptz_time_idx on _hyper_6_164_chunk m1_5
-               Filter: ("time" < clock_timestamp())
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz_2 m2
-               Order: m2."time"
-               ->  Index Only Scan Backward using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk m2_1
-               ->  Index Only Scan Backward using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk m2_2
-               ->  Index Only Scan Backward using _hyper_7_167_chunk_metrics_timestamptz_2_time_idx on _hyper_7_167_chunk m2_3
-               ->  Index Only Scan Backward using _hyper_7_168_chunk_metrics_timestamptz_2_time_idx on _hyper_7_168_chunk m2_4
-               ->  Index Only Scan Backward using _hyper_7_169_chunk_metrics_timestamptz_2_time_idx on _hyper_7_169_chunk m2_5
-               ->  Index Only Scan Backward using _hyper_7_170_chunk_metrics_timestamptz_2_time_idx on _hyper_7_170_chunk m2_6
-(24 rows)
-
-:PREFIX SELECT m1.time FROM metrics_timestamptz m1 INNER JOIN metrics_timestamptz_2 m2 ON m1.time = m2.time WHERE m2.time < clock_timestamp() ORDER BY m1.time;
-                                                         QUERY PLAN                                                          
------------------------------------------------------------------------------------------------------------------------------
- Merge Join
-   Merge Cond: (m2."time" = m1."time")
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz_2 m2
-         Order: m2."time"
-         Chunks excluded during startup: 0
-         ->  Index Only Scan Backward using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk m2_1
-               Filter: ("time" < clock_timestamp())
-         ->  Index Only Scan Backward using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk m2_2
-               Filter: ("time" < clock_timestamp())
-         ->  Index Only Scan Backward using _hyper_7_167_chunk_metrics_timestamptz_2_time_idx on _hyper_7_167_chunk m2_3
-               Filter: ("time" < clock_timestamp())
-         ->  Index Only Scan Backward using _hyper_7_168_chunk_metrics_timestamptz_2_time_idx on _hyper_7_168_chunk m2_4
-               Filter: ("time" < clock_timestamp())
-         ->  Index Only Scan Backward using _hyper_7_169_chunk_metrics_timestamptz_2_time_idx on _hyper_7_169_chunk m2_5
-               Filter: ("time" < clock_timestamp())
-         ->  Index Only Scan Backward using _hyper_7_170_chunk_metrics_timestamptz_2_time_idx on _hyper_7_170_chunk m2_6
-               Filter: ("time" < clock_timestamp())
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-               Order: m1."time"
-               ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-               ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-               ->  Index Only Scan Backward using _hyper_6_162_chunk_metrics_timestamptz_time_idx on _hyper_6_162_chunk m1_3
-               ->  Index Only Scan Backward using _hyper_6_163_chunk_metrics_timestamptz_time_idx on _hyper_6_163_chunk m1_4
-               ->  Index Only Scan Backward using _hyper_6_164_chunk_metrics_timestamptz_time_idx on _hyper_6_164_chunk m1_5
-(25 rows)
-
-\qecho test JOINs with normal table
-test JOINs with normal table
-\qecho will not propagate because constraints are only added to hypertables
-will not propagate because constraints are only added to hypertables
-:PREFIX SELECT m1.time FROM metrics_timestamptz m1 INNER JOIN regular_timestamptz m2 ON m1.time = m2.time WHERE m1.time < '2000-01-10' ORDER BY m1.time;
-                                                      QUERY PLAN                                                       
------------------------------------------------------------------------------------------------------------------------
- Merge Join
-   Merge Cond: (m1."time" = m2."time")
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-         Order: m1."time"
-         ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-               Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-         ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-               Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-   ->  Sort
-         Sort Key: m2."time"
-         ->  Seq Scan on regular_timestamptz m2
-(11 rows)
-
-\qecho test JOINs with normal table
-test JOINs with normal table
-:PREFIX SELECT m1.time FROM metrics_timestamptz m1 INNER JOIN regular_timestamptz m2 ON m1.time = m2.time WHERE m2.time < '2000-01-10' ORDER BY m1.time;
-                                                      QUERY PLAN                                                       
------------------------------------------------------------------------------------------------------------------------
- Merge Join
-   Merge Cond: (m1."time" = m2."time")
-   ->  Custom Scan (ChunkAppend) on metrics_timestamptz m1
-         Order: m1."time"
-         ->  Index Only Scan Backward using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk m1_1
-               Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-         ->  Index Only Scan Backward using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk m1_2
-               Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-   ->  Sort
-         Sort Key: m2."time"
-         ->  Seq Scan on regular_timestamptz m2
-               Filter: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-(12 rows)
-
-\qecho test quals are not pushed into OUTER JOIN
-test quals are not pushed into OUTER JOIN
-CREATE TABLE outer_join_1 (id int, name text,time timestamptz NOT NULL DEFAULT '2000-01-01');
-CREATE TABLE outer_join_2 (id int, name text,time timestamptz NOT NULL DEFAULT '2000-01-01');
-SELECT (SELECT table_name FROM create_hypertable(tbl, 'time')) FROM (VALUES ('outer_join_1'),('outer_join_2')) v(tbl);
-  table_name  
---------------
- outer_join_1
- outer_join_2
-(2 rows)
-
-INSERT INTO outer_join_1 VALUES(1,'a'), (2,'b');
-INSERT INTO outer_join_2 VALUES(1,'a');
-:PREFIX SELECT one.id, two.name FROM outer_join_1 one LEFT OUTER JOIN outer_join_2 two ON one.id=two.id WHERE one.id=2;
-                   QUERY PLAN                    
--------------------------------------------------
- Nested Loop Left Join
-   Join Filter: (one.id = two.id)
-   ->  Seq Scan on _hyper_9_176_chunk one
-         Filter: (id = 2)
-   ->  Materialize
-         ->  Seq Scan on _hyper_10_177_chunk two
-               Filter: (id = 2)
-(7 rows)
-
-:PREFIX SELECT one.id, two.name FROM outer_join_2 two RIGHT OUTER JOIN outer_join_1 one ON one.id=two.id WHERE one.id=2;
-                   QUERY PLAN                    
--------------------------------------------------
- Nested Loop Left Join
-   Join Filter: (one.id = two.id)
-   ->  Seq Scan on _hyper_9_176_chunk one
-         Filter: (id = 2)
-   ->  Materialize
-         ->  Seq Scan on _hyper_10_177_chunk two
-               Filter: (id = 2)
-(7 rows)
-
-DROP TABLE outer_join_1;
-DROP TABLE outer_join_2;
--- test UNION between regular table and hypertable
-SELECT time FROM regular_timestamptz UNION SELECT time FROM metrics_timestamptz ORDER BY 1;
-             time             
-------------------------------
- Sat Jan 01 00:00:00 2000 PST
- Sun Jan 02 00:00:00 2000 PST
- Mon Jan 03 00:00:00 2000 PST
- Tue Jan 04 00:00:00 2000 PST
- Wed Jan 05 00:00:00 2000 PST
- Thu Jan 06 00:00:00 2000 PST
- Fri Jan 07 00:00:00 2000 PST
- Sat Jan 08 00:00:00 2000 PST
- Sun Jan 09 00:00:00 2000 PST
- Mon Jan 10 00:00:00 2000 PST
- Tue Jan 11 00:00:00 2000 PST
- Wed Jan 12 00:00:00 2000 PST
- Thu Jan 13 00:00:00 2000 PST
- Fri Jan 14 00:00:00 2000 PST
- Sat Jan 15 00:00:00 2000 PST
- Sun Jan 16 00:00:00 2000 PST
- Mon Jan 17 00:00:00 2000 PST
- Tue Jan 18 00:00:00 2000 PST
- Wed Jan 19 00:00:00 2000 PST
- Thu Jan 20 00:00:00 2000 PST
- Fri Jan 21 00:00:00 2000 PST
- Sat Jan 22 00:00:00 2000 PST
- Sun Jan 23 00:00:00 2000 PST
- Mon Jan 24 00:00:00 2000 PST
- Tue Jan 25 00:00:00 2000 PST
- Wed Jan 26 00:00:00 2000 PST
- Thu Jan 27 00:00:00 2000 PST
- Fri Jan 28 00:00:00 2000 PST
- Sat Jan 29 00:00:00 2000 PST
- Sun Jan 30 00:00:00 2000 PST
- Mon Jan 31 00:00:00 2000 PST
- Tue Feb 01 00:00:00 2000 PST
-(32 rows)
-
--- test UNION ALL between regular table and hypertable
-SELECT time FROM regular_timestamptz UNION ALL SELECT time FROM metrics_timestamptz ORDER BY 1;
-             time             
-------------------------------
- Sat Jan 01 00:00:00 2000 PST
- Sat Jan 01 00:00:00 2000 PST
- Sat Jan 01 00:00:00 2000 PST
- Sat Jan 01 00:00:00 2000 PST
- Sun Jan 02 00:00:00 2000 PST
- Sun Jan 02 00:00:00 2000 PST
- Sun Jan 02 00:00:00 2000 PST
- Sun Jan 02 00:00:00 2000 PST
- Mon Jan 03 00:00:00 2000 PST
- Mon Jan 03 00:00:00 2000 PST
- Mon Jan 03 00:00:00 2000 PST
- Mon Jan 03 00:00:00 2000 PST
- Tue Jan 04 00:00:00 2000 PST
- Tue Jan 04 00:00:00 2000 PST
- Tue Jan 04 00:00:00 2000 PST
- Tue Jan 04 00:00:00 2000 PST
- Wed Jan 05 00:00:00 2000 PST
- Wed Jan 05 00:00:00 2000 PST
- Wed Jan 05 00:00:00 2000 PST
- Wed Jan 05 00:00:00 2000 PST
- Thu Jan 06 00:00:00 2000 PST
- Thu Jan 06 00:00:00 2000 PST
- Thu Jan 06 00:00:00 2000 PST
- Thu Jan 06 00:00:00 2000 PST
- Fri Jan 07 00:00:00 2000 PST
- Fri Jan 07 00:00:00 2000 PST
- Fri Jan 07 00:00:00 2000 PST
- Fri Jan 07 00:00:00 2000 PST
- Sat Jan 08 00:00:00 2000 PST
- Sat Jan 08 00:00:00 2000 PST
- Sat Jan 08 00:00:00 2000 PST
- Sat Jan 08 00:00:00 2000 PST
- Sun Jan 09 00:00:00 2000 PST
- Sun Jan 09 00:00:00 2000 PST
- Sun Jan 09 00:00:00 2000 PST
- Sun Jan 09 00:00:00 2000 PST
- Mon Jan 10 00:00:00 2000 PST
- Mon Jan 10 00:00:00 2000 PST
- Mon Jan 10 00:00:00 2000 PST
- Mon Jan 10 00:00:00 2000 PST
- Tue Jan 11 00:00:00 2000 PST
- Tue Jan 11 00:00:00 2000 PST
- Tue Jan 11 00:00:00 2000 PST
- Tue Jan 11 00:00:00 2000 PST
- Wed Jan 12 00:00:00 2000 PST
- Wed Jan 12 00:00:00 2000 PST
- Wed Jan 12 00:00:00 2000 PST
- Wed Jan 12 00:00:00 2000 PST
- Thu Jan 13 00:00:00 2000 PST
- Thu Jan 13 00:00:00 2000 PST
- Thu Jan 13 00:00:00 2000 PST
- Thu Jan 13 00:00:00 2000 PST
- Fri Jan 14 00:00:00 2000 PST
- Fri Jan 14 00:00:00 2000 PST
- Fri Jan 14 00:00:00 2000 PST
- Fri Jan 14 00:00:00 2000 PST
- Sat Jan 15 00:00:00 2000 PST
- Sat Jan 15 00:00:00 2000 PST
- Sat Jan 15 00:00:00 2000 PST
- Sat Jan 15 00:00:00 2000 PST
- Sun Jan 16 00:00:00 2000 PST
- Sun Jan 16 00:00:00 2000 PST
- Sun Jan 16 00:00:00 2000 PST
- Sun Jan 16 00:00:00 2000 PST
- Mon Jan 17 00:00:00 2000 PST
- Mon Jan 17 00:00:00 2000 PST
- Mon Jan 17 00:00:00 2000 PST
- Mon Jan 17 00:00:00 2000 PST
- Tue Jan 18 00:00:00 2000 PST
- Tue Jan 18 00:00:00 2000 PST
- Tue Jan 18 00:00:00 2000 PST
- Tue Jan 18 00:00:00 2000 PST
- Wed Jan 19 00:00:00 2000 PST
- Wed Jan 19 00:00:00 2000 PST
- Wed Jan 19 00:00:00 2000 PST
- Wed Jan 19 00:00:00 2000 PST
- Thu Jan 20 00:00:00 2000 PST
- Thu Jan 20 00:00:00 2000 PST
- Thu Jan 20 00:00:00 2000 PST
- Thu Jan 20 00:00:00 2000 PST
- Fri Jan 21 00:00:00 2000 PST
- Fri Jan 21 00:00:00 2000 PST
- Fri Jan 21 00:00:00 2000 PST
- Fri Jan 21 00:00:00 2000 PST
- Sat Jan 22 00:00:00 2000 PST
- Sat Jan 22 00:00:00 2000 PST
- Sat Jan 22 00:00:00 2000 PST
- Sat Jan 22 00:00:00 2000 PST
- Sun Jan 23 00:00:00 2000 PST
- Sun Jan 23 00:00:00 2000 PST
- Sun Jan 23 00:00:00 2000 PST
- Sun Jan 23 00:00:00 2000 PST
- Mon Jan 24 00:00:00 2000 PST
- Mon Jan 24 00:00:00 2000 PST
- Mon Jan 24 00:00:00 2000 PST
- Mon Jan 24 00:00:00 2000 PST
- Tue Jan 25 00:00:00 2000 PST
- Tue Jan 25 00:00:00 2000 PST
- Tue Jan 25 00:00:00 2000 PST
- Tue Jan 25 00:00:00 2000 PST
- Wed Jan 26 00:00:00 2000 PST
- Wed Jan 26 00:00:00 2000 PST
- Wed Jan 26 00:00:00 2000 PST
- Wed Jan 26 00:00:00 2000 PST
- Thu Jan 27 00:00:00 2000 PST
- Thu Jan 27 00:00:00 2000 PST
- Thu Jan 27 00:00:00 2000 PST
- Thu Jan 27 00:00:00 2000 PST
- Fri Jan 28 00:00:00 2000 PST
- Fri Jan 28 00:00:00 2000 PST
- Fri Jan 28 00:00:00 2000 PST
- Fri Jan 28 00:00:00 2000 PST
- Sat Jan 29 00:00:00 2000 PST
- Sat Jan 29 00:00:00 2000 PST
- Sat Jan 29 00:00:00 2000 PST
- Sat Jan 29 00:00:00 2000 PST
- Sun Jan 30 00:00:00 2000 PST
- Sun Jan 30 00:00:00 2000 PST
- Sun Jan 30 00:00:00 2000 PST
- Sun Jan 30 00:00:00 2000 PST
- Mon Jan 31 00:00:00 2000 PST
- Mon Jan 31 00:00:00 2000 PST
- Mon Jan 31 00:00:00 2000 PST
- Mon Jan 31 00:00:00 2000 PST
- Tue Feb 01 00:00:00 2000 PST
- Tue Feb 01 00:00:00 2000 PST
- Tue Feb 01 00:00:00 2000 PST
- Tue Feb 01 00:00:00 2000 PST
-(128 rows)
-
--- test nested join qual propagation
-:PREFIX
-SELECT * FROM (
-SELECT o1_m1.time FROM metrics_timestamptz o1_m1 INNER JOIN metrics_timestamptz_2 o1_m2 ON true WHERE o1_m2.time = o1_m1.time AND o1_m1.device_id = o1_m2.device_id AND o1_m2.time < '2000-01-10' AND o1_m1.device_id = 1
-) o1 FULL OUTER JOIN (
-SELECT o2_m1.time FROM metrics_timestamptz o2_m1 FULL OUTER JOIN metrics_timestamptz_2 o2_m2 ON true WHERE o2_m2.time = o2_m1.time AND o2_m1.device_id = o2_m2.device_id AND o2_m2.time > '2000-01-20' AND o2_m1.device_id = 2
-) o2 ON o1.time = o2.time ORDER BY 1,2;
-                                                               QUERY PLAN                                                                
------------------------------------------------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: o1_m1_1."time", o2_m1_1."time"
-   ->  Merge Full Join
-         Merge Cond: (o2_m1_1."time" = o1_m1_1."time")
-         ->  Nested Loop
-               ->  Merge Append
-                     Sort Key: o2_m2_1."time"
-                     ->  Index Scan Backward using _hyper_7_168_chunk_metrics_timestamptz_2_time_idx on _hyper_7_168_chunk o2_m2_1
-                           Index Cond: ("time" > 'Thu Jan 20 00:00:00 2000 PST'::timestamp with time zone)
-                           Filter: (device_id = 2)
-                     ->  Index Scan Backward using _hyper_7_169_chunk_metrics_timestamptz_2_time_idx on _hyper_7_169_chunk o2_m2_2
-                           Index Cond: ("time" > 'Thu Jan 20 00:00:00 2000 PST'::timestamp with time zone)
-                           Filter: (device_id = 2)
-                     ->  Index Scan Backward using _hyper_7_170_chunk_metrics_timestamptz_2_time_idx on _hyper_7_170_chunk o2_m2_3
-                           Index Cond: ("time" > 'Thu Jan 20 00:00:00 2000 PST'::timestamp with time zone)
-                           Filter: (device_id = 2)
-               ->  Append
-                     ->  Index Scan using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk o2_m1_1
-                           Index Cond: ("time" = o2_m2_1."time")
-                           Filter: (device_id = 2)
-                     ->  Index Scan using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk o2_m1_2
-                           Index Cond: ("time" = o2_m2_1."time")
-                           Filter: (device_id = 2)
-                     ->  Index Scan using _hyper_6_162_chunk_metrics_timestamptz_time_idx on _hyper_6_162_chunk o2_m1_3
-                           Index Cond: ("time" = o2_m2_1."time")
-                           Filter: (device_id = 2)
-                     ->  Index Scan using _hyper_6_163_chunk_metrics_timestamptz_time_idx on _hyper_6_163_chunk o2_m1_4
-                           Index Cond: ("time" = o2_m2_1."time")
-                           Filter: (device_id = 2)
-                     ->  Index Scan using _hyper_6_164_chunk_metrics_timestamptz_time_idx on _hyper_6_164_chunk o2_m1_5
-                           Index Cond: ("time" = o2_m2_1."time")
-                           Filter: (device_id = 2)
-         ->  Materialize
-               ->  Nested Loop
-                     ->  Custom Scan (ChunkAppend) on metrics_timestamptz_2 o1_m2
-                           Order: o1_m2."time"
-                           ->  Index Scan Backward using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk o1_m2_1
-                                 Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-                                 Filter: (device_id = 1)
-                           ->  Index Scan Backward using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk o1_m2_2
-                                 Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-                                 Filter: (device_id = 1)
-                     ->  Append
-                           ->  Index Scan using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk o1_m1_1
-                                 Index Cond: ("time" = o1_m2."time")
-                                 Filter: (device_id = 1)
-                           ->  Index Scan using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk o1_m1_2
-                                 Index Cond: ("time" = o1_m2."time")
-                                 Filter: (device_id = 1)
-                           ->  Index Scan using _hyper_6_162_chunk_metrics_timestamptz_time_idx on _hyper_6_162_chunk o1_m1_3
-                                 Index Cond: ("time" = o1_m2."time")
-                                 Filter: (device_id = 1)
-                           ->  Index Scan using _hyper_6_163_chunk_metrics_timestamptz_time_idx on _hyper_6_163_chunk o1_m1_4
-                                 Index Cond: ("time" = o1_m2."time")
-                                 Filter: (device_id = 1)
-                           ->  Index Scan using _hyper_6_164_chunk_metrics_timestamptz_time_idx on _hyper_6_164_chunk o1_m1_5
-                                 Index Cond: ("time" = o1_m2."time")
-                                 Filter: (device_id = 1)
-(58 rows)
-
-:PREFIX
-SELECT * FROM (
-SELECT o1_m1.time FROM metrics_timestamptz o1_m1 INNER JOIN metrics_timestamptz_2 o1_m2 ON o1_m2.time = o1_m1.time AND o1_m1.device_id = o1_m2.device_id WHERE o1_m2.time < '2000-01-10' AND o1_m1.device_id = 1
-) o1 FULL OUTER JOIN (
-SELECT o2_m1.time FROM metrics_timestamptz o2_m1 FULL OUTER JOIN metrics_timestamptz_2 o2_m2 ON o2_m2.time = o2_m1.time AND o2_m1.device_id = o2_m2.device_id WHERE o2_m2.time > '2000-01-20' AND o2_m1.device_id = 2
-) o2 ON o1.time = o2.time ORDER BY 1,2;
-                                                               QUERY PLAN                                                                
------------------------------------------------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: o1_m1_1."time", o2_m1_1."time"
-   ->  Merge Full Join
-         Merge Cond: (o2_m1_1."time" = o1_m1_1."time")
-         ->  Nested Loop
-               ->  Merge Append
-                     Sort Key: o2_m2_1."time"
-                     ->  Index Scan Backward using _hyper_7_168_chunk_metrics_timestamptz_2_time_idx on _hyper_7_168_chunk o2_m2_1
-                           Index Cond: ("time" > 'Thu Jan 20 00:00:00 2000 PST'::timestamp with time zone)
-                           Filter: (device_id = 2)
-                     ->  Index Scan Backward using _hyper_7_169_chunk_metrics_timestamptz_2_time_idx on _hyper_7_169_chunk o2_m2_2
-                           Index Cond: ("time" > 'Thu Jan 20 00:00:00 2000 PST'::timestamp with time zone)
-                           Filter: (device_id = 2)
-                     ->  Index Scan Backward using _hyper_7_170_chunk_metrics_timestamptz_2_time_idx on _hyper_7_170_chunk o2_m2_3
-                           Index Cond: ("time" > 'Thu Jan 20 00:00:00 2000 PST'::timestamp with time zone)
-                           Filter: (device_id = 2)
-               ->  Append
-                     ->  Index Scan using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk o2_m1_1
-                           Index Cond: ("time" = o2_m2_1."time")
-                           Filter: (device_id = 2)
-                     ->  Index Scan using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk o2_m1_2
-                           Index Cond: ("time" = o2_m2_1."time")
-                           Filter: (device_id = 2)
-                     ->  Index Scan using _hyper_6_162_chunk_metrics_timestamptz_time_idx on _hyper_6_162_chunk o2_m1_3
-                           Index Cond: ("time" = o2_m2_1."time")
-                           Filter: (device_id = 2)
-                     ->  Index Scan using _hyper_6_163_chunk_metrics_timestamptz_time_idx on _hyper_6_163_chunk o2_m1_4
-                           Index Cond: ("time" = o2_m2_1."time")
-                           Filter: (device_id = 2)
-                     ->  Index Scan using _hyper_6_164_chunk_metrics_timestamptz_time_idx on _hyper_6_164_chunk o2_m1_5
-                           Index Cond: ("time" = o2_m2_1."time")
-                           Filter: (device_id = 2)
-         ->  Materialize
-               ->  Nested Loop
-                     ->  Custom Scan (ChunkAppend) on metrics_timestamptz_2 o1_m2
-                           Order: o1_m2."time"
-                           ->  Index Scan Backward using _hyper_7_165_chunk_metrics_timestamptz_2_time_idx on _hyper_7_165_chunk o1_m2_1
-                                 Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-                                 Filter: (device_id = 1)
-                           ->  Index Scan Backward using _hyper_7_166_chunk_metrics_timestamptz_2_time_idx on _hyper_7_166_chunk o1_m2_2
-                                 Index Cond: ("time" < 'Mon Jan 10 00:00:00 2000 PST'::timestamp with time zone)
-                                 Filter: (device_id = 1)
-                     ->  Append
-                           ->  Index Scan using _hyper_6_160_chunk_metrics_timestamptz_time_idx on _hyper_6_160_chunk o1_m1_1
-                                 Index Cond: ("time" = o1_m2."time")
-                                 Filter: (device_id = 1)
-                           ->  Index Scan using _hyper_6_161_chunk_metrics_timestamptz_time_idx on _hyper_6_161_chunk o1_m1_2
-                                 Index Cond: ("time" = o1_m2."time")
-                                 Filter: (device_id = 1)
-                           ->  Index Scan using _hyper_6_162_chunk_metrics_timestamptz_time_idx on _hyper_6_162_chunk o1_m1_3
-                                 Index Cond: ("time" = o1_m2."time")
-                                 Filter: (device_id = 1)
-                           ->  Index Scan using _hyper_6_163_chunk_metrics_timestamptz_time_idx on _hyper_6_163_chunk o1_m1_4
-                                 Index Cond: ("time" = o1_m2."time")
-                                 Filter: (device_id = 1)
-                           ->  Index Scan using _hyper_6_164_chunk_metrics_timestamptz_time_idx on _hyper_6_164_chunk o1_m1_5
-                                 Index Cond: ("time" = o1_m2."time")
-                                 Filter: (device_id = 1)
-(58 rows)
-
-\ir include/plan_expand_hypertable_chunks_in_query.sql
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
---we want to see how our logic excludes chunks
---and not how much work constraint_exclusion does
-SET constraint_exclusion = 'off';
-:PREFIX SELECT * FROM hyper ORDER BY value;
-                 QUERY PLAN                 
---------------------------------------------
- Sort
-   Sort Key: _hyper_1_1_chunk.value
-   ->  Append
-         ->  Seq Scan on _hyper_1_1_chunk
-         ->  Seq Scan on _hyper_1_2_chunk
-         ->  Seq Scan on _hyper_1_3_chunk
-         ->  Seq Scan on _hyper_1_4_chunk
-         ->  Seq Scan on _hyper_1_5_chunk
-         ->  Seq Scan on _hyper_1_6_chunk
-         ->  Seq Scan on _hyper_1_7_chunk
-         ->  Seq Scan on _hyper_1_8_chunk
-         ->  Seq Scan on _hyper_1_9_chunk
-         ->  Seq Scan on _hyper_1_10_chunk
-         ->  Seq Scan on _hyper_1_11_chunk
-         ->  Seq Scan on _hyper_1_12_chunk
-         ->  Seq Scan on _hyper_1_13_chunk
-         ->  Seq Scan on _hyper_1_14_chunk
-         ->  Seq Scan on _hyper_1_15_chunk
-         ->  Seq Scan on _hyper_1_16_chunk
-         ->  Seq Scan on _hyper_1_17_chunk
-         ->  Seq Scan on _hyper_1_18_chunk
-         ->  Seq Scan on _hyper_1_19_chunk
-         ->  Seq Scan on _hyper_1_20_chunk
-         ->  Seq Scan on _hyper_1_21_chunk
-         ->  Seq Scan on _hyper_1_22_chunk
-         ->  Seq Scan on _hyper_1_23_chunk
-         ->  Seq Scan on _hyper_1_24_chunk
-         ->  Seq Scan on _hyper_1_25_chunk
-         ->  Seq Scan on _hyper_1_26_chunk
-         ->  Seq Scan on _hyper_1_27_chunk
-         ->  Seq Scan on _hyper_1_28_chunk
-         ->  Seq Scan on _hyper_1_29_chunk
-         ->  Seq Scan on _hyper_1_30_chunk
-         ->  Seq Scan on _hyper_1_31_chunk
-         ->  Seq Scan on _hyper_1_32_chunk
-         ->  Seq Scan on _hyper_1_33_chunk
-         ->  Seq Scan on _hyper_1_34_chunk
-         ->  Seq Scan on _hyper_1_35_chunk
-         ->  Seq Scan on _hyper_1_36_chunk
-         ->  Seq Scan on _hyper_1_37_chunk
-         ->  Seq Scan on _hyper_1_38_chunk
-         ->  Seq Scan on _hyper_1_39_chunk
-         ->  Seq Scan on _hyper_1_40_chunk
-         ->  Seq Scan on _hyper_1_41_chunk
-         ->  Seq Scan on _hyper_1_42_chunk
-         ->  Seq Scan on _hyper_1_43_chunk
-         ->  Seq Scan on _hyper_1_44_chunk
-         ->  Seq Scan on _hyper_1_45_chunk
-         ->  Seq Scan on _hyper_1_46_chunk
-         ->  Seq Scan on _hyper_1_47_chunk
-         ->  Seq Scan on _hyper_1_48_chunk
-         ->  Seq Scan on _hyper_1_49_chunk
-         ->  Seq Scan on _hyper_1_50_chunk
-         ->  Seq Scan on _hyper_1_51_chunk
-         ->  Seq Scan on _hyper_1_52_chunk
-         ->  Seq Scan on _hyper_1_53_chunk
-         ->  Seq Scan on _hyper_1_54_chunk
-         ->  Seq Scan on _hyper_1_55_chunk
-         ->  Seq Scan on _hyper_1_56_chunk
-         ->  Seq Scan on _hyper_1_57_chunk
-         ->  Seq Scan on _hyper_1_58_chunk
-         ->  Seq Scan on _hyper_1_59_chunk
-         ->  Seq Scan on _hyper_1_60_chunk
-         ->  Seq Scan on _hyper_1_61_chunk
-         ->  Seq Scan on _hyper_1_62_chunk
-         ->  Seq Scan on _hyper_1_63_chunk
-         ->  Seq Scan on _hyper_1_64_chunk
-         ->  Seq Scan on _hyper_1_65_chunk
-         ->  Seq Scan on _hyper_1_66_chunk
-         ->  Seq Scan on _hyper_1_67_chunk
-         ->  Seq Scan on _hyper_1_68_chunk
-         ->  Seq Scan on _hyper_1_69_chunk
-         ->  Seq Scan on _hyper_1_70_chunk
-         ->  Seq Scan on _hyper_1_71_chunk
-         ->  Seq Scan on _hyper_1_72_chunk
-         ->  Seq Scan on _hyper_1_73_chunk
-         ->  Seq Scan on _hyper_1_74_chunk
-         ->  Seq Scan on _hyper_1_75_chunk
-         ->  Seq Scan on _hyper_1_76_chunk
-         ->  Seq Scan on _hyper_1_77_chunk
-         ->  Seq Scan on _hyper_1_78_chunk
-         ->  Seq Scan on _hyper_1_79_chunk
-         ->  Seq Scan on _hyper_1_80_chunk
-         ->  Seq Scan on _hyper_1_81_chunk
-         ->  Seq Scan on _hyper_1_82_chunk
-         ->  Seq Scan on _hyper_1_83_chunk
-         ->  Seq Scan on _hyper_1_84_chunk
-         ->  Seq Scan on _hyper_1_85_chunk
-         ->  Seq Scan on _hyper_1_86_chunk
-         ->  Seq Scan on _hyper_1_87_chunk
-         ->  Seq Scan on _hyper_1_88_chunk
-         ->  Seq Scan on _hyper_1_89_chunk
-         ->  Seq Scan on _hyper_1_90_chunk
-         ->  Seq Scan on _hyper_1_91_chunk
-         ->  Seq Scan on _hyper_1_92_chunk
-         ->  Seq Scan on _hyper_1_93_chunk
-         ->  Seq Scan on _hyper_1_94_chunk
-         ->  Seq Scan on _hyper_1_95_chunk
-         ->  Seq Scan on _hyper_1_96_chunk
-         ->  Seq Scan on _hyper_1_97_chunk
-         ->  Seq Scan on _hyper_1_98_chunk
-         ->  Seq Scan on _hyper_1_99_chunk
-         ->  Seq Scan on _hyper_1_100_chunk
-         ->  Seq Scan on _hyper_1_101_chunk
-         ->  Seq Scan on _hyper_1_102_chunk
-(105 rows)
-
--- explicit chunk exclusion
-:PREFIX SELECT * FROM hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1,2]) ORDER BY value;
-                QUERY PLAN                
-------------------------------------------
- Sort
-   Sort Key: _hyper_1_1_chunk.value
-   ->  Append
-         ->  Seq Scan on _hyper_1_1_chunk
-         ->  Seq Scan on _hyper_1_2_chunk
-(5 rows)
-
-:PREFIX SELECT * FROM (SELECT * FROM hyper h WHERE _timescaledb_internal.chunks_in(h, ARRAY[1,2,3])) T ORDER BY value;
-                  QUERY PLAN                  
-----------------------------------------------
- Sort
-   Sort Key: h_1.value
-   ->  Append
-         ->  Seq Scan on _hyper_1_1_chunk h_1
-         ->  Seq Scan on _hyper_1_2_chunk h_2
-         ->  Seq Scan on _hyper_1_3_chunk h_3
-(6 rows)
-
-:PREFIX SELECT * FROM hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1,2,3]) AND time < 10 ORDER BY value;
-                QUERY PLAN                
-------------------------------------------
- Sort
-   Sort Key: _hyper_1_1_chunk.value
-   ->  Append
-         ->  Seq Scan on _hyper_1_1_chunk
-               Filter: ("time" < 10)
-         ->  Seq Scan on _hyper_1_2_chunk
-               Filter: ("time" < 10)
-         ->  Seq Scan on _hyper_1_3_chunk
-               Filter: ("time" < 10)
-(9 rows)
-
-:PREFIX SELECT * FROM hyper_ts WHERE device_id = 'dev1' AND time < to_timestamp(10) AND _timescaledb_internal.chunks_in(hyper_ts, ARRAY[116]) ORDER BY value;
-                                                      QUERY PLAN                                                      
-----------------------------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: _hyper_3_116_chunk.value
-   ->  Seq Scan on _hyper_3_116_chunk
-         Filter: (("time" < 'Wed Dec 31 16:00:10 1969 PST'::timestamp with time zone) AND (device_id = 'dev1'::text))
-(4 rows)
-
-:PREFIX SELECT * FROM hyper_ts h JOIN tag on (h.tag_id = tag.id ) WHERE _timescaledb_internal.chunks_in(h, ARRAY[116]) AND time < to_timestamp(10) AND device_id = 'dev1' ORDER BY value;
-                                                            QUERY PLAN                                                            
-----------------------------------------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: h.value
-   ->  Merge Join
-         Merge Cond: (tag.id = h.tag_id)
-         ->  Index Scan using tag_pkey on tag
-         ->  Sort
-               Sort Key: h.tag_id
-               ->  Seq Scan on _hyper_3_116_chunk h
-                     Filter: (("time" < 'Wed Dec 31 16:00:10 1969 PST'::timestamp with time zone) AND (device_id = 'dev1'::text))
-(9 rows)
-
-:PREFIX SELECT * FROM hyper_w_space h1 JOIN hyper_ts h2 ON h1.device_id=h2.device_id WHERE _timescaledb_internal.chunks_in(h1, ARRAY[104,105]) AND _timescaledb_internal.chunks_in(h2, ARRAY[116,117]) ORDER BY h1.value;
-                         QUERY PLAN                          
--------------------------------------------------------------
- Sort
-   Sort Key: h1_1.value
-   ->  Hash Join
-         Hash Cond: (h2_1.device_id = h1_1.device_id)
-         ->  Append
-               ->  Seq Scan on _hyper_3_116_chunk h2_1
-               ->  Seq Scan on _hyper_3_117_chunk h2_2
-         ->  Hash
-               ->  Append
-                     ->  Seq Scan on _hyper_2_104_chunk h1_1
-                     ->  Seq Scan on _hyper_2_105_chunk h1_2
-(11 rows)
-
-:PREFIX SELECT * FROM hyper_w_space h1 JOIN hyper_ts h2 ON h1.device_id=h2.device_id AND _timescaledb_internal.chunks_in(h2, ARRAY[116,117]) WHERE _timescaledb_internal.chunks_in(h1, ARRAY[104,105]) ORDER BY h1.value;
-                         QUERY PLAN                          
--------------------------------------------------------------
- Sort
-   Sort Key: h1_1.value
-   ->  Hash Join
-         Hash Cond: (h2_1.device_id = h1_1.device_id)
-         ->  Append
-               ->  Seq Scan on _hyper_3_116_chunk h2_1
-               ->  Seq Scan on _hyper_3_117_chunk h2_2
-         ->  Hash
-               ->  Append
-                     ->  Seq Scan on _hyper_2_104_chunk h1_1
-                     ->  Seq Scan on _hyper_2_105_chunk h1_2
-(11 rows)
-
-:PREFIX SELECT * FROM hyper h1, hyper h2 WHERE _timescaledb_internal.chunks_in(h1, ARRAY[1,2]) AND _timescaledb_internal.chunks_in(h2, ARRAY[2,3]);
-                     QUERY PLAN                      
------------------------------------------------------
- Nested Loop
-   ->  Append
-         ->  Seq Scan on _hyper_1_1_chunk h1_1
-         ->  Seq Scan on _hyper_1_2_chunk h1_2
-   ->  Materialize
-         ->  Append
-               ->  Seq Scan on _hyper_1_2_chunk h2_1
-               ->  Seq Scan on _hyper_1_3_chunk h2_2
-(8 rows)
-
-SET enable_seqscan=false;
--- Should perform index-only scan. Since we pass whole row into the function it might block planner from using index-only scan.
--- But since we'll remove the function from the query tree before planner decision it shouldn't affect index-only decision.
-:PREFIX SELECT time FROM hyper WHERE time=0 AND _timescaledb_internal.chunks_in(hyper, ARRAY[1]);
-                                QUERY PLAN                                 
----------------------------------------------------------------------------
- Index Only Scan using _hyper_1_1_chunk_hyper_time_idx on _hyper_1_1_chunk
-   Index Cond: ("time" = 0)
-(2 rows)
-
-:PREFIX SELECT first(value, time) FROM hyper h WHERE _timescaledb_internal.chunks_in(h, ARRAY[1]);
-                                          QUERY PLAN                                           
------------------------------------------------------------------------------------------------
- Result
-   InitPlan 1 (returns $0)
-     ->  Limit
-           ->  Index Scan Backward using _hyper_1_1_chunk_hyper_time_idx on _hyper_1_1_chunk h
-                 Index Cond: ("time" IS NOT NULL)
-(5 rows)
-
-\set ON_ERROR_STOP 0
-SELECT * FROM hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1,2]) AND _timescaledb_internal.chunks_in(hyper, ARRAY[2,3]);
-psql:include/plan_expand_hypertable_chunks_in_query.sql:26: ERROR:  illegal invocation of chunks_in function
-SELECT * FROM hyper WHERE _timescaledb_internal.chunks_in(2, ARRAY[1]);
-psql:include/plan_expand_hypertable_chunks_in_query.sql:27: ERROR:  function _timescaledb_internal.chunks_in(integer, integer[]) does not exist at character 27
-SELECT * FROM hyper WHERE time < 10 OR _timescaledb_internal.chunks_in(hyper, ARRAY[1,2]);
-psql:include/plan_expand_hypertable_chunks_in_query.sql:28: ERROR:  illegal invocation of chunks_in function
-SELECT _timescaledb_internal.chunks_in(hyper, ARRAY[1,2]) FROM hyper;
-psql:include/plan_expand_hypertable_chunks_in_query.sql:29: ERROR:  illegal invocation of chunks_in function
--- non existing chunk id
-SELECT * FROM hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[123456789]);
-psql:include/plan_expand_hypertable_chunks_in_query.sql:31: ERROR:  chunk id 123456789 not found
--- chunk that belongs to another hypertable
-SELECT * FROM hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[104]);
-psql:include/plan_expand_hypertable_chunks_in_query.sql:33: ERROR:  chunk id 104 does not belong to hypertable "hyper"
--- passing wrong row ref
-SELECT * FROM hyper WHERE _timescaledb_internal.chunks_in(ROW(1,2), ARRAY[104]);
-psql:include/plan_expand_hypertable_chunks_in_query.sql:35: ERROR:  first parameter for chunks_in function needs to be record
--- passing func as chunk id
-SELECT * FROM hyper h WHERE _timescaledb_internal.chunks_in(h, array_append(ARRAY[1],current_setting('server_version_num')::int));
-psql:include/plan_expand_hypertable_chunks_in_query.sql:37: ERROR:  second argument to chunk_in should contain only integer consts
--- NULL chunk IDs not allowed in chunk array
-SELECT * FROM hyper h WHERE _timescaledb_internal.chunks_in(h, ARRAY[NULL::int]);
-psql:include/plan_expand_hypertable_chunks_in_query.sql:39: ERROR:  chunk id can't be NULL
-\set ON_ERROR_STOP 1
--- chunks_in is STRICT function and for NULL arguments a null result is returned
-SELECT * FROM hyper h WHERE _timescaledb_internal.chunks_in(h, NULL);
- value | time 
--------+------
-(0 rows)
-
-\set ECHO errors
-RESET timescaledb.enable_optimizations;
-CREATE TABLE t(time timestamptz NOT NULL);
-SELECT table_name FROM create_hypertable('t','time');
- table_name 
-------------
- t
-(1 row)
-
-INSERT INTO t VALUES ('2000-01-01'), ('2010-01-01'), ('2020-01-01');
-EXPLAIN (costs off) SELECT * FROM t t1 INNER JOIN t t2 ON t1.time = t2.time WHERE t1.time < timestamptz '2010-01-01';
-                                                 QUERY PLAN                                                  
--------------------------------------------------------------------------------------------------------------
- Merge Join
-   Merge Cond: (t1_1."time" = t2_1."time")
-   ->  Merge Append
-         Sort Key: t1_1."time"
-         ->  Index Only Scan Backward using _hyper_15_182_chunk_t_time_idx on _hyper_15_182_chunk t1_1
-               Index Cond: ("time" < 'Fri Jan 01 00:00:00 2010 PST'::timestamp with time zone)
-         ->  Index Only Scan Backward using _hyper_15_183_chunk_t_time_idx on _hyper_15_183_chunk t1_2
-               Index Cond: ("time" < 'Fri Jan 01 00:00:00 2010 PST'::timestamp with time zone)
-   ->  Materialize
-         ->  Merge Append
-               Sort Key: t2_1."time"
-               ->  Index Only Scan Backward using _hyper_15_182_chunk_t_time_idx on _hyper_15_182_chunk t2_1
-                     Index Cond: ("time" < 'Fri Jan 01 00:00:00 2010 PST'::timestamp with time zone)
-               ->  Index Only Scan Backward using _hyper_15_183_chunk_t_time_idx on _hyper_15_183_chunk t2_2
-                     Index Cond: ("time" < 'Fri Jan 01 00:00:00 2010 PST'::timestamp with time zone)
-(15 rows)
-
-SET timescaledb.enable_qual_propagation TO false;
-EXPLAIN (costs off) SELECT * FROM t t1 INNER JOIN t t2 ON t1.time = t2.time WHERE t1.time < timestamptz '2010-01-01';
-                                                 QUERY PLAN                                                  
--------------------------------------------------------------------------------------------------------------
- Merge Join
-   Merge Cond: (t1_1."time" = t2_1."time")
-   ->  Merge Append
-         Sort Key: t1_1."time"
-         ->  Index Only Scan Backward using _hyper_15_182_chunk_t_time_idx on _hyper_15_182_chunk t1_1
-               Index Cond: ("time" < 'Fri Jan 01 00:00:00 2010 PST'::timestamp with time zone)
-         ->  Index Only Scan Backward using _hyper_15_183_chunk_t_time_idx on _hyper_15_183_chunk t1_2
-               Index Cond: ("time" < 'Fri Jan 01 00:00:00 2010 PST'::timestamp with time zone)
-   ->  Materialize
-         ->  Merge Append
-               Sort Key: t2_1."time"
-               ->  Index Only Scan Backward using _hyper_15_182_chunk_t_time_idx on _hyper_15_182_chunk t2_1
-               ->  Index Only Scan Backward using _hyper_15_183_chunk_t_time_idx on _hyper_15_183_chunk t2_2
-               ->  Index Only Scan Backward using _hyper_15_184_chunk_t_time_idx on _hyper_15_184_chunk t2_3
-(14 rows)
-
-RESET timescaledb.enable_qual_propagation;
-CREATE TABLE test (a int, time timestamptz NOT NULL);
-SELECT table_name FROM create_hypertable('public.test', 'time');
- table_name 
-------------
- test
-(1 row)
-
-INSERT INTO test SELECT i, '2020-04-01'::date-10-i from generate_series(1,20) i;
-CREATE OR REPLACE FUNCTION test_f(_ts timestamptz)
-RETURNS SETOF test LANGUAGE SQL STABLE PARALLEL SAFE
-AS $f$
-   SELECT DISTINCT ON (a) * FROM test WHERE time >= _ts ORDER BY a, time DESC
-$f$;
-EXPLAIN (costs off) SELECT * FROM test_f(now());
-                   QUERY PLAN                    
--------------------------------------------------
- Unique
-   ->  Sort
-         Sort Key: test.a, test."time" DESC
-         ->  Custom Scan (ChunkAppend) on test
-               Chunks excluded during startup: 4
-(5 rows)
-
-EXPLAIN (costs off) SELECT * FROM test_f(now());
-                   QUERY PLAN                    
--------------------------------------------------
- Unique
-   ->  Sort
-         Sort Key: test.a, test."time" DESC
-         ->  Custom Scan (ChunkAppend) on test
-               Chunks excluded during startup: 4
-(5 rows)
-
-CREATE TABLE t1 (a int, b int NOT NULL);
-SELECT create_hypertable('t1', 'b', chunk_time_interval=>10);
- create_hypertable 
--------------------
- (17,public,t1,t)
-(1 row)
-
-CREATE TABLE t2 (a int, b int NOT NULL);
-SELECT create_hypertable('t2', 'b', chunk_time_interval=>10);
- create_hypertable 
--------------------
- (18,public,t2,t)
-(1 row)
-
-CREATE OR REPLACE FUNCTION f_t1(_a int, _b int)
- RETURNS SETOF t1
- LANGUAGE SQL
- STABLE PARALLEL SAFE
-AS $function$
-   SELECT DISTINCT ON (a) * FROM t1 WHERE a = _a and b = _b ORDER BY a, b DESC
-$function$
-;
-CREATE OR REPLACE FUNCTION f_t2(_a int, _b int) RETURNS SETOF t2 LANGUAGE sql STABLE PARALLEL SAFE
-AS $function$
-   SELECT DISTINCT ON (j.a) j.*
-   FROM
-      f_t1(_a, _b) sc,
-      t2 j
-   WHERE
-      j.b = _b AND
-      j.a = _a
-   ORDER BY j.a, j.b DESC
-$function$
-;
-CREATE OR REPLACE FUNCTION f_t1_2(_b int) RETURNS SETOF t1 LANGUAGE SQL STABLE PARALLEL SAFE
-AS $function$
-   SELECT DISTINCT ON (j.a) jt.* FROM t1 j, f_t1(j.a, _b) jt
-$function$;
-EXPLAIN (costs off) SELECT * FROM f_t1_2(10);
-                          QUERY PLAN                           
----------------------------------------------------------------
- Subquery Scan on f_t1_2
-   ->  Unique
-         ->  Sort
-               Sort Key: j.a
-               ->  Nested Loop
-                     ->  Seq Scan on t1 j
-                     ->  Unique
-                           ->  Index Scan using t1_b_idx on t1
-                                 Index Cond: (b = 10)
-                                 Filter: (a = j.a)
-(10 rows)
-
-EXPLAIN (costs off) SELECT * FROM f_t1_2(10) sc, f_t2(sc.a, 10);
-                          QUERY PLAN                           
----------------------------------------------------------------
- Nested Loop
-   ->  Unique
-         ->  Sort
-               Sort Key: j.a
-               ->  Nested Loop
-                     ->  Seq Scan on t1 j
-                     ->  Unique
-                           ->  Index Scan using t1_b_idx on t1
-                                 Index Cond: (b = 10)
-                                 Filter: (a = j.a)
-   ->  Unique
-         ->  Nested Loop
-               ->  Unique
-                     ->  Index Scan using t1_b_idx on t1 t1_1
-                           Index Cond: (b = 10)
-                           Filter: (a = t1.a)
-               ->  Index Scan using t2_b_idx on t2 j_1
-                     Index Cond: (b = 10)
-                     Filter: (a = t1.a)
-(19 rows)
-
---TEST END--
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/plan_hashagg-15.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/plan_hashagg-15.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/plan_hashagg-15.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/plan_hashagg-15.out	2023-11-25 05:27:44.141022672 +0000
@@ -1,330 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-\set PREFIX 'EXPLAIN (costs off) '
-\ir include/plan_hashagg_load.sql
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-CREATE TABLE metric (id SERIAL PRIMARY KEY, value INT);
-CREATE TABLE hyper(time TIMESTAMP NOT NULL, time_int BIGINT, time_broken DATE, metricid int, value double precision);
-CREATE TABLE regular(time TIMESTAMP NOT NULL, time_int BIGINT, time_date DATE, metricid int, value double precision);
-SELECT create_hypertable('hyper', 'time', chunk_time_interval => interval '20 day', create_default_indexes=>FALSE);
-psql:include/plan_hashagg_load.sql:9: WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
- create_hypertable  
---------------------
- (1,public,hyper,t)
-(1 row)
-
-ALTER TABLE hyper
-DROP COLUMN time_broken,
-ADD COLUMN time_date DATE;
-INSERT INTO metric(value) SELECT random()*100 FROM generate_series(0,10);
-INSERT INTO hyper SELECT t,  EXTRACT(EPOCH FROM t), (EXTRACT(EPOCH FROM t)::int % 10)+1, 1.0, t::date FROM generate_series('2001-01-01', '2001-01-10', INTERVAL '1 second') t;
-INSERT INTO regular(time, time_int, time_date, metricid, value)
-  SELECT t,  EXTRACT(EPOCH FROM t), t::date, (EXTRACT(EPOCH FROM t)::int % 10) + 1, 1.0 FROM generate_series('2001-01-01', '2001-01-02', INTERVAL '1 second') t;
---test some queries before analyze;
-EXPLAIN (costs off) SELECT time_bucket('1 minute', time) AS MetricMinuteTs, AVG(value) as avg
-FROM hyper
-WHERE time >= '2001-01-04T00:00:00' AND time <= '2001-01-05T01:00:00'
-GROUP BY MetricMinuteTs
-ORDER BY MetricMinuteTs DESC;
-                                                                                      QUERY PLAN                                                                                       
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Finalize GroupAggregate
-   Group Key: (time_bucket('@ 1 min'::interval, _hyper_1_1_chunk."time"))
-   ->  Gather Merge
-         Workers Planned: 2
-         ->  Partial GroupAggregate
-               Group Key: (time_bucket('@ 1 min'::interval, _hyper_1_1_chunk."time"))
-               ->  Sort
-                     Sort Key: (time_bucket('@ 1 min'::interval, _hyper_1_1_chunk."time")) DESC
-                     ->  Result
-                           ->  Parallel Seq Scan on _hyper_1_1_chunk
-                                 Filter: (("time" >= 'Thu Jan 04 00:00:00 2001'::timestamp without time zone) AND ("time" <= 'Fri Jan 05 01:00:00 2001'::timestamp without time zone))
-(11 rows)
-
-EXPLAIN (costs off) SELECT date_trunc('minute', time) AS MetricMinuteTs, AVG(value) as avg
-FROM hyper
-WHERE time >= '2001-01-04T00:00:00' AND time <= '2001-01-05T01:00:00'
-GROUP BY MetricMinuteTs
-ORDER BY MetricMinuteTs DESC;
-                                                                                      QUERY PLAN                                                                                       
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Finalize GroupAggregate
-   Group Key: (date_trunc('minute'::text, _hyper_1_1_chunk."time"))
-   ->  Gather Merge
-         Workers Planned: 2
-         ->  Partial GroupAggregate
-               Group Key: (date_trunc('minute'::text, _hyper_1_1_chunk."time"))
-               ->  Sort
-                     Sort Key: (date_trunc('minute'::text, _hyper_1_1_chunk."time")) DESC
-                     ->  Result
-                           ->  Parallel Seq Scan on _hyper_1_1_chunk
-                                 Filter: (("time" >= 'Thu Jan 04 00:00:00 2001'::timestamp without time zone) AND ("time" <= 'Fri Jan 05 01:00:00 2001'::timestamp without time zone))
-(11 rows)
-
--- Test partitioning function on an open (time) dimension
-CREATE OR REPLACE FUNCTION unix_to_timestamp(unixtime float8)
-    RETURNS TIMESTAMPTZ LANGUAGE SQL IMMUTABLE AS
-$BODY$
-    SELECT to_timestamp(unixtime);
-$BODY$;
-CREATE TABLE hyper_timefunc(time float8 NOT NULL, metricid int, VALUE double precision, time_date DATE);
-SELECT create_hypertable('hyper_timefunc', 'time', chunk_time_interval => interval '20 day', create_default_indexes=>FALSE, time_partitioning_func => 'unix_to_timestamp');
-      create_hypertable      
------------------------------
- (2,public,hyper_timefunc,t)
-(1 row)
-
-INSERT INTO hyper_timefunc SELECT time_int, metricid, VALUE, time_date FROM hyper;
-ANALYZE metric;
-ANALYZE hyper;
-ANALYZE regular;
-ANALYZE hyper_timefunc;
-\ir include/plan_hashagg_query.sql
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-:PREFIX SELECT time_bucket('1 minute', time) AS MetricMinuteTs, AVG(value) as avg
-FROM hyper
-WHERE time >= '2001-01-04T00:00:00' AND time <= '2001-01-05T01:00:00'
-GROUP BY MetricMinuteTs
-ORDER BY MetricMinuteTs DESC;
-                                                                                QUERY PLAN                                                                                 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: (time_bucket('@ 1 min'::interval, _hyper_1_1_chunk."time")) DESC
-   ->  HashAggregate
-         Group Key: time_bucket('@ 1 min'::interval, _hyper_1_1_chunk."time")
-         ->  Result
-               ->  Seq Scan on _hyper_1_1_chunk
-                     Filter: (("time" >= 'Thu Jan 04 00:00:00 2001'::timestamp without time zone) AND ("time" <= 'Fri Jan 05 01:00:00 2001'::timestamp without time zone))
-(7 rows)
-
-:PREFIX SELECT time_bucket('1 hour', time) AS MetricMinuteTs, metricid, AVG(value) as avg
-FROM hyper
-WHERE time >= '2001-01-04T00:00:00' AND time <= '2001-01-05T01:00:00'
-GROUP BY MetricMinuteTs, metricid
-ORDER BY MetricMinuteTs DESC, metricid;
-                                                                                QUERY PLAN                                                                                 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: (time_bucket('@ 1 hour'::interval, _hyper_1_1_chunk."time")) DESC, _hyper_1_1_chunk.metricid
-   ->  HashAggregate
-         Group Key: time_bucket('@ 1 hour'::interval, _hyper_1_1_chunk."time"), _hyper_1_1_chunk.metricid
-         ->  Result
-               ->  Seq Scan on _hyper_1_1_chunk
-                     Filter: (("time" >= 'Thu Jan 04 00:00:00 2001'::timestamp without time zone) AND ("time" <= 'Fri Jan 05 01:00:00 2001'::timestamp without time zone))
-(7 rows)
-
---should be too many groups will not hashaggregate
-:PREFIX SELECT time_bucket('1 second', time) AS MetricMinuteTs, metricid, AVG(value) as avg
-FROM hyper
-WHERE time >= '2001-01-04T00:00:00' AND time <= '2001-01-05T01:00:00'
-GROUP BY MetricMinuteTs, metricid
-ORDER BY MetricMinuteTs DESC, metricid;
-                                                                                      QUERY PLAN                                                                                       
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Finalize GroupAggregate
-   Group Key: (time_bucket('@ 1 sec'::interval, _hyper_1_1_chunk."time")), _hyper_1_1_chunk.metricid
-   ->  Gather Merge
-         Workers Planned: 2
-         ->  Partial GroupAggregate
-               Group Key: (time_bucket('@ 1 sec'::interval, _hyper_1_1_chunk."time")), _hyper_1_1_chunk.metricid
-               ->  Sort
-                     Sort Key: (time_bucket('@ 1 sec'::interval, _hyper_1_1_chunk."time")) DESC, _hyper_1_1_chunk.metricid
-                     ->  Result
-                           ->  Parallel Seq Scan on _hyper_1_1_chunk
-                                 Filter: (("time" >= 'Thu Jan 04 00:00:00 2001'::timestamp without time zone) AND ("time" <= 'Fri Jan 05 01:00:00 2001'::timestamp without time zone))
-(11 rows)
-
-:PREFIX SELECT time_bucket('1 minute', time, INTERVAL '30 seconds') AS MetricMinuteTs, AVG(value) as avg
-FROM hyper
-WHERE time >= '2001-01-04T00:00:00' AND time <= '2001-01-05T01:00:00'
-GROUP BY MetricMinuteTs
-ORDER BY MetricMinuteTs DESC;
-                                                                                QUERY PLAN                                                                                 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: (time_bucket('@ 1 min'::interval, _hyper_1_1_chunk."time", '@ 30 secs'::interval)) DESC
-   ->  HashAggregate
-         Group Key: time_bucket('@ 1 min'::interval, _hyper_1_1_chunk."time", '@ 30 secs'::interval)
-         ->  Result
-               ->  Seq Scan on _hyper_1_1_chunk
-                     Filter: (("time" >= 'Thu Jan 04 00:00:00 2001'::timestamp without time zone) AND ("time" <= 'Fri Jan 05 01:00:00 2001'::timestamp without time zone))
-(7 rows)
-
-:PREFIX SELECT time_bucket(60, time_int) AS MetricMinuteTs, AVG(value) as avg
-FROM hyper
-WHERE time >= '2001-01-04T00:00:00' AND time <= '2001-01-05T01:00:00'
-GROUP BY MetricMinuteTs
-ORDER BY MetricMinuteTs DESC;
-                                                                                QUERY PLAN                                                                                 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: (time_bucket('60'::bigint, _hyper_1_1_chunk.time_int)) DESC
-   ->  HashAggregate
-         Group Key: time_bucket('60'::bigint, _hyper_1_1_chunk.time_int)
-         ->  Result
-               ->  Seq Scan on _hyper_1_1_chunk
-                     Filter: (("time" >= 'Thu Jan 04 00:00:00 2001'::timestamp without time zone) AND ("time" <= 'Fri Jan 05 01:00:00 2001'::timestamp without time zone))
-(7 rows)
-
-:PREFIX SELECT time_bucket(60, time_int, 10) AS MetricMinuteTs, AVG(value) as avg
-FROM hyper
-WHERE time >= '2001-01-04T00:00:00' AND time <= '2001-01-05T01:00:00'
-GROUP BY MetricMinuteTs
-ORDER BY MetricMinuteTs DESC;
-                                                                                QUERY PLAN                                                                                 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: (time_bucket('60'::bigint, _hyper_1_1_chunk.time_int, '10'::bigint)) DESC
-   ->  HashAggregate
-         Group Key: time_bucket('60'::bigint, _hyper_1_1_chunk.time_int, '10'::bigint)
-         ->  Result
-               ->  Seq Scan on _hyper_1_1_chunk
-                     Filter: (("time" >= 'Thu Jan 04 00:00:00 2001'::timestamp without time zone) AND ("time" <= 'Fri Jan 05 01:00:00 2001'::timestamp without time zone))
-(7 rows)
-
-:PREFIX SELECT time_bucket('1 day', time_date) AS MetricMinuteTs, AVG(value) as avg
-FROM hyper
-WHERE time >= '2001-01-04T00:00:00' AND time <= '2001-01-05T01:00:00'
-GROUP BY MetricMinuteTs
-ORDER BY MetricMinuteTs DESC;
-                                                                                      QUERY PLAN                                                                                       
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Finalize GroupAggregate
-   Group Key: (time_bucket('@ 1 day'::interval, _hyper_1_1_chunk.time_date))
-   ->  Gather Merge
-         Workers Planned: 2
-         ->  Sort
-               Sort Key: (time_bucket('@ 1 day'::interval, _hyper_1_1_chunk.time_date)) DESC
-               ->  Partial HashAggregate
-                     Group Key: time_bucket('@ 1 day'::interval, _hyper_1_1_chunk.time_date)
-                     ->  Result
-                           ->  Parallel Seq Scan on _hyper_1_1_chunk
-                                 Filter: (("time" >= 'Thu Jan 04 00:00:00 2001'::timestamp without time zone) AND ("time" <= 'Fri Jan 05 01:00:00 2001'::timestamp without time zone))
-(11 rows)
-
-:PREFIX SELECT date_trunc('minute', time) AS MetricMinuteTs, AVG(value) as avg
-FROM hyper
-WHERE time >= '2001-01-04T00:00:00' AND time <= '2001-01-05T01:00:00'
-GROUP BY MetricMinuteTs
-ORDER BY MetricMinuteTs DESC;
-                                                                                QUERY PLAN                                                                                 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: (date_trunc('minute'::text, _hyper_1_1_chunk."time")) DESC
-   ->  HashAggregate
-         Group Key: date_trunc('minute'::text, _hyper_1_1_chunk."time")
-         ->  Result
-               ->  Seq Scan on _hyper_1_1_chunk
-                     Filter: (("time" >= 'Thu Jan 04 00:00:00 2001'::timestamp without time zone) AND ("time" <= 'Fri Jan 05 01:00:00 2001'::timestamp without time zone))
-(7 rows)
-
-\set ON_ERROR_STOP 0
---can't optimize invalid time unit
-:PREFIX SELECT date_trunc('invalid', time) AS MetricMinuteTs, AVG(value) as avg
-FROM hyper
-WHERE time >= '2001-01-04T00:00:00' AND time <= '2001-01-05T01:00:00'
-GROUP BY MetricMinuteTs
-ORDER BY MetricMinuteTs DESC;
-                                                                                      QUERY PLAN                                                                                       
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Finalize GroupAggregate
-   Group Key: (date_trunc('invalid'::text, _hyper_1_1_chunk."time"))
-   ->  Gather Merge
-         Workers Planned: 2
-         ->  Partial GroupAggregate
-               Group Key: (date_trunc('invalid'::text, _hyper_1_1_chunk."time"))
-               ->  Sort
-                     Sort Key: (date_trunc('invalid'::text, _hyper_1_1_chunk."time")) DESC
-                     ->  Result
-                           ->  Parallel Seq Scan on _hyper_1_1_chunk
-                                 Filter: (("time" >= 'Thu Jan 04 00:00:00 2001'::timestamp without time zone) AND ("time" <= 'Fri Jan 05 01:00:00 2001'::timestamp without time zone))
-(11 rows)
-
-\set ON_ERROR_STOP 1
-:PREFIX SELECT date_trunc('day', time_date) AS MetricMinuteTs, AVG(value) as avg
-FROM hyper
-WHERE time >= '2001-01-04T00:00:00' AND time <= '2001-01-05T01:00:00'
-GROUP BY MetricMinuteTs
-ORDER BY MetricMinuteTs DESC;
-                                                                                      QUERY PLAN                                                                                       
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Finalize GroupAggregate
-   Group Key: (date_trunc('day'::text, (_hyper_1_1_chunk.time_date)::timestamp with time zone))
-   ->  Gather Merge
-         Workers Planned: 2
-         ->  Sort
-               Sort Key: (date_trunc('day'::text, (_hyper_1_1_chunk.time_date)::timestamp with time zone)) DESC
-               ->  Partial HashAggregate
-                     Group Key: date_trunc('day'::text, (_hyper_1_1_chunk.time_date)::timestamp with time zone)
-                     ->  Result
-                           ->  Parallel Seq Scan on _hyper_1_1_chunk
-                                 Filter: (("time" >= 'Thu Jan 04 00:00:00 2001'::timestamp without time zone) AND ("time" <= 'Fri Jan 05 01:00:00 2001'::timestamp without time zone))
-(11 rows)
-
---joins
---with hypertable, optimize
-:PREFIX SELECT time_bucket(3600, time_int, 10) AS MetricMinuteTs, metric.value, AVG(hyper.value) as avg
-FROM hyper
-JOIN metric ON (hyper.metricid = metric.id)
-WHERE time >= '2001-01-04T00:00:00' AND time <= '2001-01-05T01:00:00'
-GROUP BY MetricMinuteTs, metric.id
-ORDER BY MetricMinuteTs DESC, metric.id;
-                                                                                QUERY PLAN                                                                                 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Sort
-   Sort Key: (time_bucket('3600'::bigint, _hyper_1_1_chunk.time_int, '10'::bigint)) DESC, metric.id
-   ->  HashAggregate
-         Group Key: time_bucket('3600'::bigint, _hyper_1_1_chunk.time_int, '10'::bigint), metric.id
-         ->  Hash Join
-               Hash Cond: (_hyper_1_1_chunk.metricid = metric.id)
-               ->  Seq Scan on _hyper_1_1_chunk
-                     Filter: (("time" >= 'Thu Jan 04 00:00:00 2001'::timestamp without time zone) AND ("time" <= 'Fri Jan 05 01:00:00 2001'::timestamp without time zone))
-               ->  Hash
-                     ->  Seq Scan on metric
-(10 rows)
-
---no hypertable involved, no optimization
-:PREFIX SELECT time_bucket(3600, time_int, 10) AS MetricMinuteTs, metric.value, AVG(regular.value) as avg
-FROM regular
-JOIN metric ON (regular.metricid = metric.id)
-WHERE time >= '2001-01-04T00:00:00' AND time <= '2001-01-05T01:00:00'
-GROUP BY MetricMinuteTs, metric.id
-ORDER BY MetricMinuteTs DESC, metric.id;
-                                                                                QUERY PLAN                                                                                 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- GroupAggregate
-   Group Key: (time_bucket('3600'::bigint, regular.time_int, '10'::bigint)), metric.id
-   ->  Sort
-         Sort Key: (time_bucket('3600'::bigint, regular.time_int, '10'::bigint)) DESC, metric.id
-         ->  Nested Loop
-               Join Filter: (regular.metricid = metric.id)
-               ->  Seq Scan on regular
-                     Filter: (("time" >= 'Thu Jan 04 00:00:00 2001'::timestamp without time zone) AND ("time" <= 'Fri Jan 05 01:00:00 2001'::timestamp without time zone))
-               ->  Seq Scan on metric
-(9 rows)
-
--- Try with time partitioning function. Currently not optimized for hash aggregates
-:PREFIX SELECT time_bucket('1 minute', unix_to_timestamp(time)) AS MetricMinuteTs, AVG(value) as avg
-FROM hyper_timefunc
-WHERE unix_to_timestamp(time) >= '2001-01-04T00:00:00' AND unix_to_timestamp(time) <= '2001-01-05T01:00:00'
-GROUP BY MetricMinuteTs
-ORDER BY MetricMinuteTs DESC;
-                                                                                               QUERY PLAN                                                                                                
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- GroupAggregate
-   Group Key: (time_bucket('@ 1 min'::interval, to_timestamp(_hyper_2_2_chunk."time")))
-   ->  Sort
-         Sort Key: (time_bucket('@ 1 min'::interval, to_timestamp(_hyper_2_2_chunk."time"))) DESC
-         ->  Result
-               ->  Seq Scan on _hyper_2_2_chunk
-                     Filter: ((to_timestamp("time") >= 'Thu Jan 04 00:00:00 2001 PST'::timestamp with time zone) AND (to_timestamp("time") <= 'Fri Jan 05 01:00:00 2001 PST'::timestamp with time zone))
-(7 rows)
-
-\set ECHO none
-psql:include/plan_hashagg_query.sql:60: ERROR:  unit "invalid" not recognized for type timestamp without time zone
-psql:include/plan_hashagg_query.sql:60: ERROR:  unit "invalid" not recognized for type timestamp without time zone
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/plan_hypertable_inline-15.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/plan_hypertable_inline-15.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/plan_hypertable_inline-15.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/plan_hypertable_inline-15.out	2023-11-25 05:27:44.129022707 +0000
@@ -1,149 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
--- test hypertable classification when query is in an inlineable function
-\set PREFIX 'EXPLAIN (costs off)'
-CREATE TABLE test (a int, b bigint NOT NULL);
-SELECT create_hypertable('public.test', 'b', chunk_time_interval=>10);
- create_hypertable 
--------------------
- (1,public,test,t)
-(1 row)
-
-INSERT INTO test SELECT i, i FROM generate_series(1, 20) i;
-CREATE OR REPLACE FUNCTION test_f(_ts bigint)
-RETURNS SETOF test LANGUAGE SQL STABLE
-as $f$
-   SELECT DISTINCT ON (a) * FROM test WHERE b >= _ts AND b <= _ts + 2
-$f$;
--- plans must be the same in both cases
--- specifically, the first plan should not contain the parent hypertable
--- as that is a sign the pruning was not done successfully
-:PREFIX SELECT * FROM test_f(5);
-                                  QUERY PLAN                                  
-------------------------------------------------------------------------------
- Unique
-   ->  Sort
-         Sort Key: _hyper_1_1_chunk.a
-         ->  Index Scan using _hyper_1_1_chunk_test_b_idx on _hyper_1_1_chunk
-               Index Cond: ((b >= '5'::bigint) AND (b <= '7'::bigint))
-(5 rows)
-
-:PREFIX SELECT DISTINCT ON (a) * FROM test WHERE b >= 5 AND b <= 5 + 2;
-                                  QUERY PLAN                                  
-------------------------------------------------------------------------------
- Unique
-   ->  Sort
-         Sort Key: _hyper_1_1_chunk.a
-         ->  Index Scan using _hyper_1_1_chunk_test_b_idx on _hyper_1_1_chunk
-               Index Cond: ((b >= 5) AND (b <= 7))
-(5 rows)
-
--- test with FOR UPDATE
-CREATE OR REPLACE FUNCTION test_f(_ts bigint)
-RETURNS SETOF test LANGUAGE SQL STABLE
-as $f$
-   SELECT * FROM test WHERE b >= _ts AND b <= _ts + 2 FOR UPDATE
-$f$;
--- pruning should not be done by TimescaleDb in this case
--- specifically, the parent hypertable must exist in the output plan
-:PREFIX SELECT * FROM test_f(5);
-                                        QUERY PLAN                                         
--------------------------------------------------------------------------------------------
- Subquery Scan on test_f
-   ->  LockRows
-         ->  Append
-               ->  Seq Scan on test test_1
-                     Filter: ((b >= '5'::bigint) AND (b <= '7'::bigint))
-               ->  Index Scan using _hyper_1_1_chunk_test_b_idx on _hyper_1_1_chunk test_2
-                     Index Cond: ((b >= '5'::bigint) AND (b <= '7'::bigint))
-(7 rows)
-
-:PREFIX SELECT * FROM test WHERE b >= 5 AND b <= 5 + 2 FOR UPDATE;
-                                     QUERY PLAN                                      
--------------------------------------------------------------------------------------
- LockRows
-   ->  Append
-         ->  Seq Scan on test test_1
-               Filter: ((b >= 5) AND (b <= 7))
-         ->  Index Scan using _hyper_1_1_chunk_test_b_idx on _hyper_1_1_chunk test_2
-               Index Cond: ((b >= 5) AND (b <= 7))
-(6 rows)
-
--- test with CTE
--- these cases are just to make sure we're everything is alright with
--- the way we identify hypertables to prune chunks - we abuse ctename
--- for this purpose. So double-check if we're not breaking plans
--- with CTEs here.
-CREATE OR REPLACE FUNCTION test_f(_ts bigint)
-RETURNS SETOF test LANGUAGE SQL STABLE
-as $f$
-   WITH ct AS MATERIALIZED (
-      SELECT DISTINCT ON (a) * FROM test WHERE b >= _ts AND b <= _ts + 2
-   )
-   SELECT * FROM ct
-$f$;
-:PREFIX SELECT * FROM test_f(5);
-                                      QUERY PLAN                                      
---------------------------------------------------------------------------------------
- CTE Scan on ct
-   CTE ct
-     ->  Unique
-           ->  Sort
-                 Sort Key: _hyper_1_1_chunk.a
-                 ->  Index Scan using _hyper_1_1_chunk_test_b_idx on _hyper_1_1_chunk
-                       Index Cond: ((b >= '5'::bigint) AND (b <= '7'::bigint))
-(7 rows)
-
-:PREFIX
-WITH ct AS MATERIALIZED (
-   SELECT DISTINCT ON (a) * FROM test WHERE b >= 5 AND b <= 5 + 2
-)
-SELECT * FROM ct;
-                                      QUERY PLAN                                      
---------------------------------------------------------------------------------------
- CTE Scan on ct
-   CTE ct
-     ->  Unique
-           ->  Sort
-                 Sort Key: _hyper_1_1_chunk.a
-                 ->  Index Scan using _hyper_1_1_chunk_test_b_idx on _hyper_1_1_chunk
-                       Index Cond: ((b >= 5) AND (b <= 7))
-(7 rows)
-
--- CTE within CTE
-:PREFIX
-WITH ct AS MATERIALIZED (
-   SELECT * FROM test_f(5)
-)
-SELECT * FROM ct;
-                                          QUERY PLAN                                          
-----------------------------------------------------------------------------------------------
- CTE Scan on ct
-   CTE ct
-     ->  CTE Scan on ct ct_1
-           CTE ct
-             ->  Unique
-                   ->  Sort
-                         Sort Key: _hyper_1_1_chunk.a
-                         ->  Index Scan using _hyper_1_1_chunk_test_b_idx on _hyper_1_1_chunk
-                               Index Cond: ((b >= '5'::bigint) AND (b <= '7'::bigint))
-(9 rows)
-
--- CTE within NO MATERIALIZED CTE
-:PREFIX
-WITH ct AS NOT MATERIALIZED (
-   SELECT * FROM test_f(5)
-)
-SELECT * FROM ct;
-                                      QUERY PLAN                                      
---------------------------------------------------------------------------------------
- CTE Scan on ct
-   CTE ct
-     ->  Unique
-           ->  Sort
-                 Sort Key: _hyper_1_1_chunk.a
-                 ->  Index Scan using _hyper_1_1_chunk_test_b_idx on _hyper_1_1_chunk
-                       Index Cond: ((b >= '5'::bigint) AND (b <= '7'::bigint))
-(7 rows)
-
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/plan_ordered_append.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/plan_ordered_append.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/plan_ordered_append.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/plan_ordered_append.out	2023-11-25 05:27:44.137022684 +0000
@@ -1,629 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
--- we run these with analyze to confirm that nodes that are not
--- needed to fulfill the limit are not executed
--- unfortunately this doesn't work on PostgreSQL 9.6 which lacks
--- the ability to turn off analyze timing summary so we run
--- them without ANALYZE on PostgreSQL 9.6, but since LATERAL plans
--- are different across versions we need version specific output
--- here anyway.
-\set TEST_BASE_NAME plan_ordered_append
-SELECT format('include/%s_load.sql', :'TEST_BASE_NAME') as "TEST_LOAD_NAME",
-       format('include/%s_query.sql', :'TEST_BASE_NAME') as "TEST_QUERY_NAME",
-       format('%s/results/%s_results_optimized.out', :'TEST_OUTPUT_DIR', :'TEST_BASE_NAME') as "TEST_RESULTS_OPTIMIZED",
-       format('%s/results/%s_results_unoptimized.out', :'TEST_OUTPUT_DIR', :'TEST_BASE_NAME') as "TEST_RESULTS_UNOPTIMIZED"
-\gset
-SELECT format('\! diff -u --label "Unoptimized result" --label "Optimized result" %s %s', :'TEST_RESULTS_UNOPTIMIZED', :'TEST_RESULTS_OPTIMIZED') as "DIFF_CMD"
-\gset
-\set PREFIX 'EXPLAIN (analyze, costs off, timing off, summary off)'
-\set PREFIX_NO_ANALYZE 'EXPLAIN (costs off)'
-\ir :TEST_LOAD_NAME
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
--- create a now() function for repeatable testing that always returns
--- the same timestamp. It needs to be marked STABLE
-CREATE OR REPLACE FUNCTION now_s()
-RETURNS timestamptz LANGUAGE PLPGSQL STABLE AS
-$BODY$
-BEGIN
-    RETURN '2000-01-08T0:00:00+0'::timestamptz;
-END;
-$BODY$;
-CREATE TABLE devices(device_id INT PRIMARY KEY, name TEXT);
-INSERT INTO devices VALUES
-(1,'Device 1'),
-(2,'Device 2'),
-(3,'Device 3');
--- create a second table where we create chunks in reverse order
-CREATE TABLE ordered_append_reverse(time timestamptz NOT NULL, device_id INT, value float);
-SELECT create_hypertable('ordered_append_reverse','time');
-          create_hypertable          
--------------------------------------
- (1,public,ordered_append_reverse,t)
-(1 row)
-
-INSERT INTO ordered_append_reverse SELECT generate_series('2000-01-18'::timestamptz,'2000-01-01'::timestamptz,'-1m'::interval), 1, 0.5;
--- table where dimension column is last column
-CREATE TABLE IF NOT EXISTS dimension_last(
-    id INT8 NOT NULL,
-    device_id INT NOT NULL,
-    name TEXT NOT NULL,
-    time timestamptz NOT NULL
-);
-SELECT create_hypertable('dimension_last', 'time', chunk_time_interval => interval '1day', if_not_exists => True);
-      create_hypertable      
------------------------------
- (2,public,dimension_last,t)
-(1 row)
-
--- table with only dimension column
-CREATE TABLE IF NOT EXISTS dimension_only(
-    time timestamptz NOT NULL
-);
-SELECT create_hypertable('dimension_only', 'time', chunk_time_interval => interval '1day', if_not_exists => True);
-      create_hypertable      
------------------------------
- (3,public,dimension_only,t)
-(1 row)
-
-INSERT INTO dimension_last SELECT 1,1,'Device 1',generate_series('2000-01-01 0:00:00+0'::timestamptz,'2000-01-04 23:59:00+0'::timestamptz,'1m'::interval);
-INSERT INTO dimension_only VALUES
-('2000-01-01'),
-('2000-01-03'),
-('2000-01-05'),
-('2000-01-07');
-ANALYZE devices;
-ANALYZE ordered_append_reverse;
-ANALYZE dimension_last;
-ANALYZE dimension_only;
--- create hypertable with indexes not on all chunks
-CREATE TABLE ht_missing_indexes(time timestamptz NOT NULL, device_id int, value float);
-SELECT create_hypertable('ht_missing_indexes','time');
-        create_hypertable        
----------------------------------
- (4,public,ht_missing_indexes,t)
-(1 row)
-
-INSERT INTO ht_missing_indexes SELECT generate_series('2000-01-01'::timestamptz,'2000-01-18'::timestamptz,'1m'::interval), 1, 0.5;
-INSERT INTO ht_missing_indexes SELECT generate_series('2000-01-01'::timestamptz,'2000-01-18'::timestamptz,'1m'::interval), 2, 1.5;
-INSERT INTO ht_missing_indexes SELECT generate_series('2000-01-01'::timestamptz,'2000-01-18'::timestamptz,'1m'::interval), 3, 2.5;
--- drop index from 2nd chunk of ht_missing_indexes
-SELECT format('%I.%I',i.schemaname,i.indexname) AS "INDEX_NAME"
-FROM _timescaledb_catalog.chunk c
-INNER JOIN _timescaledb_catalog.hypertable ht ON c.hypertable_id = ht.id
-INNER JOIN pg_indexes i ON i.schemaname = c.schema_name AND i.tablename=c.table_name
-WHERE ht.table_name = 'ht_missing_indexes'
-ORDER BY c.id LIMIT 1 OFFSET 1 \gset
-DROP INDEX :INDEX_NAME;
-ANALYZE ht_missing_indexes;
--- create hypertable with with dropped columns
-CREATE TABLE ht_dropped_columns(c1 int, c2 int, c3 int, c4 int, c5 int, time timestamptz NOT NULL, device_id int, value float);
-SELECT create_hypertable('ht_dropped_columns','time');
-        create_hypertable        
----------------------------------
- (5,public,ht_dropped_columns,t)
-(1 row)
-
-ALTER TABLE ht_dropped_columns DROP COLUMN c1;
-INSERT INTO ht_dropped_columns(time,device_id,value) SELECT generate_series('2000-01-01'::timestamptz,'2000-01-02'::timestamptz,'1m'::interval), 1, 0.5;
-ALTER TABLE ht_dropped_columns DROP COLUMN c2;
-INSERT INTO ht_dropped_columns(time,device_id,value) SELECT generate_series('2000-01-08'::timestamptz,'2000-01-09'::timestamptz,'1m'::interval), 1, 0.5;
-ALTER TABLE ht_dropped_columns DROP COLUMN c3;
-INSERT INTO ht_dropped_columns(time,device_id,value) SELECT generate_series('2000-01-15'::timestamptz,'2000-01-16'::timestamptz,'1m'::interval), 1, 0.5;
-ALTER TABLE ht_dropped_columns DROP COLUMN c4;
-INSERT INTO ht_dropped_columns(time,device_id,value) SELECT generate_series('2000-01-22'::timestamptz,'2000-01-23'::timestamptz,'1m'::interval), 1, 0.5;
-ALTER TABLE ht_dropped_columns DROP COLUMN c5;
-INSERT INTO ht_dropped_columns(time,device_id,value) SELECT generate_series('2000-01-29'::timestamptz,'2000-01-30'::timestamptz,'1m'::interval), 1, 0.5;
-ANALYZE ht_dropped_columns;
-CREATE TABLE space2(time timestamptz NOT NULL, device_id int NOT NULL, tag_id int NOT NULL, value float);
-SELECT create_hypertable('space2','time','device_id',number_partitions:=3);
-  create_hypertable  
----------------------
- (6,public,space2,t)
-(1 row)
-
-SELECT add_dimension('space2','tag_id',number_partitions:=3);
-       add_dimension        
-----------------------------
- (8,public,space2,tag_id,t)
-(1 row)
-
-INSERT INTO space2 SELECT generate_series('2000-01-10'::timestamptz,'2000-01-01'::timestamptz,'-1m'::interval), 1, 1, 1.5;
-INSERT INTO space2 SELECT generate_series('2000-01-10'::timestamptz,'2000-01-01'::timestamptz,'-1m'::interval), 2, 1, 2.5;
-INSERT INTO space2 SELECT generate_series('2000-01-10'::timestamptz,'2000-01-01'::timestamptz,'-1m'::interval), 3, 1, 3.5;
-INSERT INTO space2 SELECT generate_series('2000-01-10'::timestamptz,'2000-01-01'::timestamptz,'-1m'::interval), 1, 2, 1.5;
-INSERT INTO space2 SELECT generate_series('2000-01-10'::timestamptz,'2000-01-01'::timestamptz,'-1m'::interval), 2, 2, 2.5;
-INSERT INTO space2 SELECT generate_series('2000-01-10'::timestamptz,'2000-01-01'::timestamptz,'-1m'::interval), 3, 2, 3.5;
-INSERT INTO space2 SELECT generate_series('2000-01-10'::timestamptz,'2000-01-01'::timestamptz,'-1m'::interval), 1, 3, 1.5;
-INSERT INTO space2 SELECT generate_series('2000-01-10'::timestamptz,'2000-01-01'::timestamptz,'-1m'::interval), 2, 3, 2.5;
-INSERT INTO space2 SELECT generate_series('2000-01-10'::timestamptz,'2000-01-01'::timestamptz,'-1m'::interval), 3, 3, 3.5;
-ANALYZE space2;
-CREATE TABLE space3(time timestamptz NOT NULL, x int NOT NULL, y int NOT NULL, z int NOT NULL, value float);
-SELECT create_hypertable('space3','time','x',number_partitions:=2);
-  create_hypertable  
----------------------
- (7,public,space3,t)
-(1 row)
-
-SELECT add_dimension('space3','y',number_partitions:=2);
-     add_dimension      
-------------------------
- (11,public,space3,y,t)
-(1 row)
-
-SELECT add_dimension('space3','z',number_partitions:=2);
-     add_dimension      
-------------------------
- (12,public,space3,z,t)
-(1 row)
-
-INSERT INTO space3 SELECT generate_series('2000-01-10'::timestamptz,'2000-01-01'::timestamptz,'-1m'::interval), 1, 1, 1, 1.5;
-INSERT INTO space3 SELECT generate_series('2000-01-10'::timestamptz,'2000-01-01'::timestamptz,'-1m'::interval), 1, 1, 2, 1.5;
-INSERT INTO space3 SELECT generate_series('2000-01-10'::timestamptz,'2000-01-01'::timestamptz,'-1m'::interval), 1, 2, 1, 1.5;
-INSERT INTO space3 SELECT generate_series('2000-01-10'::timestamptz,'2000-01-01'::timestamptz,'-1m'::interval), 1, 2, 2, 1.5;
-INSERT INTO space3 SELECT generate_series('2000-01-10'::timestamptz,'2000-01-01'::timestamptz,'-1m'::interval), 2, 1, 1, 1.5;
-INSERT INTO space3 SELECT generate_series('2000-01-10'::timestamptz,'2000-01-01'::timestamptz,'-1m'::interval), 2, 1, 2, 1.5;
-INSERT INTO space3 SELECT generate_series('2000-01-10'::timestamptz,'2000-01-01'::timestamptz,'-1m'::interval), 2, 2, 1, 1.5;
-INSERT INTO space3 SELECT generate_series('2000-01-10'::timestamptz,'2000-01-01'::timestamptz,'-1m'::interval), 2, 2, 2, 1.5;
-ANALYZE space3;
-CREATE TABLE sortopt_test(time timestamptz NOT NULL, device TEXT);
-SELECT create_hypertable('sortopt_test','time',create_default_indexes:=false);
-     create_hypertable     
----------------------------
- (8,public,sortopt_test,t)
-(1 row)
-
--- since alpine does not support locales we cant test collations in our ci
--- CREATE COLLATION IF NOT EXISTS en_US(LOCALE='en_US.utf8');
--- CREATE INDEX time_device_utf8 ON sortopt_test(time, device COLLATE "en_US");
-CREATE INDEX time_device_nullsfirst ON sortopt_test(time, device NULLS FIRST);
-CREATE INDEX time_device_nullslast ON sortopt_test(time, device DESC NULLS LAST);
-INSERT INTO sortopt_test SELECT generate_series('2000-01-10'::timestamptz,'2000-01-01'::timestamptz,'-1m'::interval), 'Device 1';
-ANALYZE sortopt_test;
-\ir :TEST_QUERY_NAME
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
--- print chunks ordered by time to ensure ordering we want
-SELECT
-  ht.table_name AS hypertable,
-  c.table_name AS chunk,
-  ds.range_start
-FROM
-  _timescaledb_catalog.chunk c
-  INNER JOIN LATERAL(SELECT * FROM _timescaledb_catalog.chunk_constraint cc WHERE c.id = cc.chunk_id ORDER BY cc.dimension_slice_id LIMIT 1) cc ON true
-  INNER JOIN _timescaledb_catalog.dimension_slice ds ON ds.id=cc.dimension_slice_id
-  INNER JOIN _timescaledb_catalog.dimension d ON ds.dimension_id = d.id
-  INNER JOIN _timescaledb_catalog.hypertable ht ON d.hypertable_id = ht.id
-ORDER BY ht.table_name, range_start, chunk;
-       hypertable       |       chunk       |     range_start      
-------------------------+-------------------+----------------------
- dimension_last         | _hyper_2_4_chunk  |      946684800000000
- dimension_last         | _hyper_2_5_chunk  |      946771200000000
- dimension_last         | _hyper_2_6_chunk  |      946857600000000
- dimension_last         | _hyper_2_7_chunk  |      946944000000000
- dimension_only         | _hyper_3_8_chunk  |      946684800000000
- dimension_only         | _hyper_3_9_chunk  |      946857600000000
- dimension_only         | _hyper_3_10_chunk |      947030400000000
- dimension_only         | _hyper_3_11_chunk |      947203200000000
- ht_dropped_columns     | _hyper_5_15_chunk |      946512000000000
- ht_dropped_columns     | _hyper_5_16_chunk |      947116800000000
- ht_dropped_columns     | _hyper_5_17_chunk |      947721600000000
- ht_dropped_columns     | _hyper_5_18_chunk |      948326400000000
- ht_dropped_columns     | _hyper_5_19_chunk |      948931200000000
- ht_missing_indexes     | _hyper_4_12_chunk |      946512000000000
- ht_missing_indexes     | _hyper_4_13_chunk |      947116800000000
- ht_missing_indexes     | _hyper_4_14_chunk |      947721600000000
- ordered_append_reverse | _hyper_1_3_chunk  |      946512000000000
- ordered_append_reverse | _hyper_1_2_chunk  |      947116800000000
- ordered_append_reverse | _hyper_1_1_chunk  |      947721600000000
- sortopt_test           | _hyper_8_55_chunk |      946512000000000
- sortopt_test           | _hyper_8_54_chunk |      947116800000000
- space2                 | _hyper_6_21_chunk | -9223372036854775808
- space2                 | _hyper_6_23_chunk | -9223372036854775808
- space2                 | _hyper_6_25_chunk | -9223372036854775808
- space2                 | _hyper_6_27_chunk | -9223372036854775808
- space2                 | _hyper_6_33_chunk | -9223372036854775808
- space2                 | _hyper_6_29_chunk |      946512000000000
- space2                 | _hyper_6_31_chunk |      946512000000000
- space2                 | _hyper_6_35_chunk |      946512000000000
- space2                 | _hyper_6_37_chunk |      946512000000000
- space2                 | _hyper_6_20_chunk |      947116800000000
- space2                 | _hyper_6_22_chunk |      947116800000000
- space2                 | _hyper_6_24_chunk |      947116800000000
- space2                 | _hyper_6_26_chunk |      947116800000000
- space2                 | _hyper_6_28_chunk |      947116800000000
- space2                 | _hyper_6_30_chunk |      947116800000000
- space2                 | _hyper_6_32_chunk |      947116800000000
- space2                 | _hyper_6_34_chunk |      947116800000000
- space2                 | _hyper_6_36_chunk |      947116800000000
- space3                 | _hyper_7_39_chunk | -9223372036854775808
- space3                 | _hyper_7_41_chunk | -9223372036854775808
- space3                 | _hyper_7_43_chunk | -9223372036854775808
- space3                 | _hyper_7_45_chunk | -9223372036854775808
- space3                 | _hyper_7_47_chunk | -9223372036854775808
- space3                 | _hyper_7_49_chunk | -9223372036854775808
- space3                 | _hyper_7_51_chunk | -9223372036854775808
- space3                 | _hyper_7_53_chunk |      946512000000000
- space3                 | _hyper_7_38_chunk |      947116800000000
- space3                 | _hyper_7_40_chunk |      947116800000000
- space3                 | _hyper_7_42_chunk |      947116800000000
- space3                 | _hyper_7_44_chunk |      947116800000000
- space3                 | _hyper_7_46_chunk |      947116800000000
- space3                 | _hyper_7_48_chunk |      947116800000000
- space3                 | _hyper_7_50_chunk |      947116800000000
- space3                 | _hyper_7_52_chunk |      947116800000000
-(55 rows)
-
--- test ASC for reverse ordered chunks
-:PREFIX SELECT
-  time, device_id, value
-FROM ordered_append_reverse
-ORDER BY time ASC LIMIT 1;
-                                                             QUERY PLAN                                                             
-------------------------------------------------------------------------------------------------------------------------------------
- Limit (actual rows=1 loops=1)
-   ->  Custom Scan (ChunkAppend) on ordered_append_reverse (actual rows=1 loops=1)
-         Order: ordered_append_reverse."time"
-         ->  Index Scan Backward using _hyper_1_3_chunk_ordered_append_reverse_time_idx on _hyper_1_3_chunk (actual rows=1 loops=1)
-         ->  Index Scan Backward using _hyper_1_2_chunk_ordered_append_reverse_time_idx on _hyper_1_2_chunk (never executed)
-         ->  Index Scan Backward using _hyper_1_1_chunk_ordered_append_reverse_time_idx on _hyper_1_1_chunk (never executed)
-(6 rows)
-
--- test DESC for reverse ordered chunks
-:PREFIX SELECT
-  time, device_id, value
-FROM ordered_append_reverse
-ORDER BY time DESC LIMIT 1;
-                                                        QUERY PLAN                                                         
----------------------------------------------------------------------------------------------------------------------------
- Limit (actual rows=1 loops=1)
-   ->  Custom Scan (ChunkAppend) on ordered_append_reverse (actual rows=1 loops=1)
-         Order: ordered_append_reverse."time" DESC
-         ->  Index Scan using _hyper_1_1_chunk_ordered_append_reverse_time_idx on _hyper_1_1_chunk (actual rows=1 loops=1)
-         ->  Index Scan using _hyper_1_2_chunk_ordered_append_reverse_time_idx on _hyper_1_2_chunk (never executed)
-         ->  Index Scan using _hyper_1_3_chunk_ordered_append_reverse_time_idx on _hyper_1_3_chunk (never executed)
-(6 rows)
-
--- test query with ORDER BY time_bucket, device_id
--- must not use ordered append
-:PREFIX SELECT
-  time_bucket('1d',time), device_id, name
-FROM dimension_last
-ORDER BY time_bucket('1d',time), device_id LIMIT 1;
-                                                QUERY PLAN                                                 
------------------------------------------------------------------------------------------------------------
- Limit (actual rows=1 loops=1)
-   ->  Sort (actual rows=1 loops=1)
-         Sort Key: (time_bucket('@ 1 day'::interval, _hyper_2_4_chunk."time")), _hyper_2_4_chunk.device_id
-         Sort Method: top-N heapsort 
-         ->  Result (actual rows=5760 loops=1)
-               ->  Append (actual rows=5760 loops=1)
-                     ->  Seq Scan on _hyper_2_4_chunk (actual rows=1440 loops=1)
-                     ->  Seq Scan on _hyper_2_5_chunk (actual rows=1440 loops=1)
-                     ->  Seq Scan on _hyper_2_6_chunk (actual rows=1440 loops=1)
-                     ->  Seq Scan on _hyper_2_7_chunk (actual rows=1440 loops=1)
-(10 rows)
-
--- test query with ORDER BY date_trunc, device_id
--- must not use ordered append
-:PREFIX SELECT
-  date_trunc('day',time), device_id, name
-FROM dimension_last
-ORDER BY 1,2 LIMIT 1;
-                                            QUERY PLAN                                            
---------------------------------------------------------------------------------------------------
- Limit (actual rows=1 loops=1)
-   ->  Sort (actual rows=1 loops=1)
-         Sort Key: (date_trunc('day'::text, _hyper_2_4_chunk."time")), _hyper_2_4_chunk.device_id
-         Sort Method: top-N heapsort 
-         ->  Result (actual rows=5760 loops=1)
-               ->  Append (actual rows=5760 loops=1)
-                     ->  Seq Scan on _hyper_2_4_chunk (actual rows=1440 loops=1)
-                     ->  Seq Scan on _hyper_2_5_chunk (actual rows=1440 loops=1)
-                     ->  Seq Scan on _hyper_2_6_chunk (actual rows=1440 loops=1)
-                     ->  Seq Scan on _hyper_2_7_chunk (actual rows=1440 loops=1)
-(10 rows)
-
--- test with table with only dimension column
-:PREFIX SELECT * FROM dimension_only ORDER BY time DESC LIMIT 1;
-                                                        QUERY PLAN                                                        
---------------------------------------------------------------------------------------------------------------------------
- Limit (actual rows=1 loops=1)
-   ->  Custom Scan (ChunkAppend) on dimension_only (actual rows=1 loops=1)
-         Order: dimension_only."time" DESC
-         ->  Index Only Scan using _hyper_3_11_chunk_dimension_only_time_idx on _hyper_3_11_chunk (actual rows=1 loops=1)
-               Heap Fetches: 1
-         ->  Index Only Scan using _hyper_3_10_chunk_dimension_only_time_idx on _hyper_3_10_chunk (never executed)
-               Heap Fetches: 0
-         ->  Index Only Scan using _hyper_3_9_chunk_dimension_only_time_idx on _hyper_3_9_chunk (never executed)
-               Heap Fetches: 0
-         ->  Index Only Scan using _hyper_3_8_chunk_dimension_only_time_idx on _hyper_3_8_chunk (never executed)
-               Heap Fetches: 0
-(11 rows)
-
--- test LEFT JOIN against hypertable
-:PREFIX_NO_ANALYZE SELECT *
-FROM dimension_last
-LEFT JOIN dimension_only USING (time)
-ORDER BY dimension_last.time DESC
-LIMIT 2;
-                                           QUERY PLAN                                            
--------------------------------------------------------------------------------------------------
- Limit
-   ->  Nested Loop Left Join
-         Join Filter: (dimension_last."time" = _hyper_3_11_chunk."time")
-         ->  Custom Scan (ChunkAppend) on dimension_last
-               Order: dimension_last."time" DESC
-               ->  Index Scan using _hyper_2_7_chunk_dimension_last_time_idx on _hyper_2_7_chunk
-               ->  Index Scan using _hyper_2_6_chunk_dimension_last_time_idx on _hyper_2_6_chunk
-               ->  Index Scan using _hyper_2_5_chunk_dimension_last_time_idx on _hyper_2_5_chunk
-               ->  Index Scan using _hyper_2_4_chunk_dimension_last_time_idx on _hyper_2_4_chunk
-         ->  Materialize
-               ->  Append
-                     ->  Seq Scan on _hyper_3_11_chunk
-                     ->  Seq Scan on _hyper_3_10_chunk
-                     ->  Seq Scan on _hyper_3_9_chunk
-                     ->  Seq Scan on _hyper_3_8_chunk
-(15 rows)
-
--- test INNER JOIN against non-hypertable
-:PREFIX_NO_ANALYZE SELECT *
-FROM dimension_last
-INNER JOIN dimension_only USING (time)
-ORDER BY dimension_last.time DESC
-LIMIT 2;
-                                               QUERY PLAN                                               
---------------------------------------------------------------------------------------------------------
- Limit
-   ->  Nested Loop
-         ->  Custom Scan (ChunkAppend) on dimension_only
-               Order: dimension_only."time" DESC
-               ->  Index Only Scan using _hyper_3_11_chunk_dimension_only_time_idx on _hyper_3_11_chunk
-               ->  Index Only Scan using _hyper_3_10_chunk_dimension_only_time_idx on _hyper_3_10_chunk
-               ->  Index Only Scan using _hyper_3_9_chunk_dimension_only_time_idx on _hyper_3_9_chunk
-               ->  Index Only Scan using _hyper_3_8_chunk_dimension_only_time_idx on _hyper_3_8_chunk
-         ->  Append
-               ->  Index Scan using _hyper_2_7_chunk_dimension_last_time_idx on _hyper_2_7_chunk
-                     Index Cond: ("time" = dimension_only."time")
-               ->  Index Scan using _hyper_2_6_chunk_dimension_last_time_idx on _hyper_2_6_chunk
-                     Index Cond: ("time" = dimension_only."time")
-               ->  Index Scan using _hyper_2_5_chunk_dimension_last_time_idx on _hyper_2_5_chunk
-                     Index Cond: ("time" = dimension_only."time")
-               ->  Index Scan using _hyper_2_4_chunk_dimension_last_time_idx on _hyper_2_4_chunk
-                     Index Cond: ("time" = dimension_only."time")
-(17 rows)
-
--- test join against non-hypertable
-:PREFIX SELECT *
-FROM dimension_last
-INNER JOIN devices USING(device_id)
-ORDER BY dimension_last.time DESC
-LIMIT 2;
-                                                       QUERY PLAN                                                        
--------------------------------------------------------------------------------------------------------------------------
- Limit (actual rows=2 loops=1)
-   ->  Nested Loop (actual rows=2 loops=1)
-         Join Filter: (dimension_last.device_id = devices.device_id)
-         ->  Custom Scan (ChunkAppend) on dimension_last (actual rows=2 loops=1)
-               Order: dimension_last."time" DESC
-               ->  Index Scan using _hyper_2_7_chunk_dimension_last_time_idx on _hyper_2_7_chunk (actual rows=2 loops=1)
-               ->  Index Scan using _hyper_2_6_chunk_dimension_last_time_idx on _hyper_2_6_chunk (never executed)
-               ->  Index Scan using _hyper_2_5_chunk_dimension_last_time_idx on _hyper_2_5_chunk (never executed)
-               ->  Index Scan using _hyper_2_4_chunk_dimension_last_time_idx on _hyper_2_4_chunk (never executed)
-         ->  Materialize (actual rows=1 loops=2)
-               ->  Seq Scan on devices (actual rows=1 loops=1)
-(11 rows)
-
--- test hypertable with index missing on one chunk
-:PREFIX SELECT
-  time, device_id, value
-FROM ht_missing_indexes
-ORDER BY time ASC LIMIT 1;
-                                                            QUERY PLAN                                                            
-----------------------------------------------------------------------------------------------------------------------------------
- Limit (actual rows=1 loops=1)
-   ->  Custom Scan (ChunkAppend) on ht_missing_indexes (actual rows=1 loops=1)
-         Order: ht_missing_indexes."time"
-         ->  Index Scan Backward using _hyper_4_12_chunk_ht_missing_indexes_time_idx on _hyper_4_12_chunk (actual rows=1 loops=1)
-         ->  Sort (never executed)
-               Sort Key: _hyper_4_13_chunk."time"
-               ->  Seq Scan on _hyper_4_13_chunk (never executed)
-         ->  Index Scan Backward using _hyper_4_14_chunk_ht_missing_indexes_time_idx on _hyper_4_14_chunk (never executed)
-(8 rows)
-
--- test hypertable with index missing on one chunk
--- and no data
-:PREFIX SELECT
-  time, device_id, value
-FROM ht_missing_indexes
-WHERE device_id = 2
-ORDER BY time DESC LIMIT 1;
-                                                       QUERY PLAN                                                        
--------------------------------------------------------------------------------------------------------------------------
- Limit (actual rows=1 loops=1)
-   ->  Custom Scan (ChunkAppend) on ht_missing_indexes (actual rows=1 loops=1)
-         Order: ht_missing_indexes."time" DESC
-         ->  Index Scan using _hyper_4_14_chunk_ht_missing_indexes_time_idx on _hyper_4_14_chunk (actual rows=1 loops=1)
-               Filter: (device_id = 2)
-               Rows Removed by Filter: 1
-         ->  Sort (never executed)
-               Sort Key: _hyper_4_13_chunk."time" DESC
-               ->  Seq Scan on _hyper_4_13_chunk (never executed)
-                     Filter: (device_id = 2)
-         ->  Index Scan using _hyper_4_12_chunk_ht_missing_indexes_time_idx on _hyper_4_12_chunk (never executed)
-               Filter: (device_id = 2)
-(12 rows)
-
--- test hypertable with index missing on one chunk
--- and no data
-:PREFIX SELECT
-  time, device_id, value
-FROM ht_missing_indexes
-WHERE time > '2000-01-07'
-ORDER BY time LIMIT 10;
-                                                        QUERY PLAN                                                         
----------------------------------------------------------------------------------------------------------------------------
- Limit (actual rows=10 loops=1)
-   ->  Custom Scan (ChunkAppend) on ht_missing_indexes (actual rows=10 loops=1)
-         Order: ht_missing_indexes."time"
-         ->  Sort (actual rows=10 loops=1)
-               Sort Key: _hyper_4_13_chunk."time"
-               Sort Method: top-N heapsort 
-               ->  Seq Scan on _hyper_4_13_chunk (actual rows=24477 loops=1)
-                     Filter: ("time" > 'Fri Jan 07 00:00:00 2000 PST'::timestamp with time zone)
-                     Rows Removed by Filter: 5763
-         ->  Index Scan Backward using _hyper_4_14_chunk_ht_missing_indexes_time_idx on _hyper_4_14_chunk (never executed)
-               Index Cond: ("time" > 'Fri Jan 07 00:00:00 2000 PST'::timestamp with time zone)
-(11 rows)
-
--- test hypertable with dropped columns
-:PREFIX SELECT
-  time, device_id, value
-FROM ht_dropped_columns
-ORDER BY time ASC LIMIT 1;
-                                                            QUERY PLAN                                                            
-----------------------------------------------------------------------------------------------------------------------------------
- Limit (actual rows=1 loops=1)
-   ->  Custom Scan (ChunkAppend) on ht_dropped_columns (actual rows=1 loops=1)
-         Order: ht_dropped_columns."time"
-         ->  Index Scan Backward using _hyper_5_15_chunk_ht_dropped_columns_time_idx on _hyper_5_15_chunk (actual rows=1 loops=1)
-         ->  Index Scan Backward using _hyper_5_16_chunk_ht_dropped_columns_time_idx on _hyper_5_16_chunk (never executed)
-         ->  Index Scan Backward using _hyper_5_17_chunk_ht_dropped_columns_time_idx on _hyper_5_17_chunk (never executed)
-         ->  Index Scan Backward using _hyper_5_18_chunk_ht_dropped_columns_time_idx on _hyper_5_18_chunk (never executed)
-         ->  Index Scan Backward using _hyper_5_19_chunk_ht_dropped_columns_time_idx on _hyper_5_19_chunk (never executed)
-(8 rows)
-
--- test hypertable with dropped columns
-:PREFIX SELECT
-  time, device_id, value
-FROM ht_dropped_columns
-WHERE device_id = 1
-ORDER BY time DESC;
-                                                      QUERY PLAN                                                      
-----------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on ht_dropped_columns (actual rows=7205 loops=1)
-   Order: ht_dropped_columns."time" DESC
-   ->  Index Scan using _hyper_5_19_chunk_ht_dropped_columns_time_idx on _hyper_5_19_chunk (actual rows=1441 loops=1)
-         Filter: (device_id = 1)
-   ->  Index Scan using _hyper_5_18_chunk_ht_dropped_columns_time_idx on _hyper_5_18_chunk (actual rows=1441 loops=1)
-         Filter: (device_id = 1)
-   ->  Index Scan using _hyper_5_17_chunk_ht_dropped_columns_time_idx on _hyper_5_17_chunk (actual rows=1441 loops=1)
-         Filter: (device_id = 1)
-   ->  Index Scan using _hyper_5_16_chunk_ht_dropped_columns_time_idx on _hyper_5_16_chunk (actual rows=1441 loops=1)
-         Filter: (device_id = 1)
-   ->  Index Scan using _hyper_5_15_chunk_ht_dropped_columns_time_idx on _hyper_5_15_chunk (actual rows=1441 loops=1)
-         Filter: (device_id = 1)
-(12 rows)
-
--- test hypertable with 2 space dimensions
-:PREFIX SELECT
-  time, device_id, value
-FROM space2
-ORDER BY time DESC;
-                                                   QUERY PLAN                                                   
-----------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on space2 (actual rows=116649 loops=1)
-   Order: space2."time" DESC
-   ->  Merge Append (actual rows=56169 loops=1)
-         Sort Key: _hyper_6_36_chunk."time" DESC
-         ->  Index Scan using _hyper_6_36_chunk_space2_time_idx on _hyper_6_36_chunk (actual rows=6241 loops=1)
-         ->  Index Scan using _hyper_6_34_chunk_space2_time_idx on _hyper_6_34_chunk (actual rows=6241 loops=1)
-         ->  Index Scan using _hyper_6_32_chunk_space2_time_idx on _hyper_6_32_chunk (actual rows=6241 loops=1)
-         ->  Index Scan using _hyper_6_30_chunk_space2_time_idx on _hyper_6_30_chunk (actual rows=6241 loops=1)
-         ->  Index Scan using _hyper_6_28_chunk_space2_time_idx on _hyper_6_28_chunk (actual rows=6241 loops=1)
-         ->  Index Scan using _hyper_6_26_chunk_space2_time_idx on _hyper_6_26_chunk (actual rows=6241 loops=1)
-         ->  Index Scan using _hyper_6_24_chunk_space2_time_idx on _hyper_6_24_chunk (actual rows=6241 loops=1)
-         ->  Index Scan using _hyper_6_22_chunk_space2_time_idx on _hyper_6_22_chunk (actual rows=6241 loops=1)
-         ->  Index Scan using _hyper_6_20_chunk_space2_time_idx on _hyper_6_20_chunk (actual rows=6241 loops=1)
-   ->  Merge Append (actual rows=60480 loops=1)
-         Sort Key: _hyper_6_37_chunk."time" DESC
-         ->  Index Scan using _hyper_6_37_chunk_space2_time_idx on _hyper_6_37_chunk (actual rows=6720 loops=1)
-         ->  Index Scan using _hyper_6_35_chunk_space2_time_idx on _hyper_6_35_chunk (actual rows=6720 loops=1)
-         ->  Index Scan using _hyper_6_33_chunk_space2_time_idx on _hyper_6_33_chunk (actual rows=6720 loops=1)
-         ->  Index Scan using _hyper_6_31_chunk_space2_time_idx on _hyper_6_31_chunk (actual rows=6720 loops=1)
-         ->  Index Scan using _hyper_6_29_chunk_space2_time_idx on _hyper_6_29_chunk (actual rows=6720 loops=1)
-         ->  Index Scan using _hyper_6_27_chunk_space2_time_idx on _hyper_6_27_chunk (actual rows=6720 loops=1)
-         ->  Index Scan using _hyper_6_25_chunk_space2_time_idx on _hyper_6_25_chunk (actual rows=6720 loops=1)
-         ->  Index Scan using _hyper_6_23_chunk_space2_time_idx on _hyper_6_23_chunk (actual rows=6720 loops=1)
-         ->  Index Scan using _hyper_6_21_chunk_space2_time_idx on _hyper_6_21_chunk (actual rows=6720 loops=1)
-(24 rows)
-
--- test hypertable with 3 space dimensions
-:PREFIX SELECT
-  time
-FROM space3
-ORDER BY time DESC;
-                                                     QUERY PLAN                                                      
----------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on space3 (actual rows=103688 loops=1)
-   Order: space3."time" DESC
-   ->  Merge Append (actual rows=49928 loops=1)
-         Sort Key: _hyper_7_52_chunk."time" DESC
-         ->  Index Only Scan using _hyper_7_52_chunk_space3_time_idx on _hyper_7_52_chunk (actual rows=6241 loops=1)
-               Heap Fetches: 6241
-         ->  Index Only Scan using _hyper_7_50_chunk_space3_time_idx on _hyper_7_50_chunk (actual rows=6241 loops=1)
-               Heap Fetches: 6241
-         ->  Index Only Scan using _hyper_7_48_chunk_space3_time_idx on _hyper_7_48_chunk (actual rows=6241 loops=1)
-               Heap Fetches: 6241
-         ->  Index Only Scan using _hyper_7_46_chunk_space3_time_idx on _hyper_7_46_chunk (actual rows=6241 loops=1)
-               Heap Fetches: 6241
-         ->  Index Only Scan using _hyper_7_44_chunk_space3_time_idx on _hyper_7_44_chunk (actual rows=6241 loops=1)
-               Heap Fetches: 6241
-         ->  Index Only Scan using _hyper_7_42_chunk_space3_time_idx on _hyper_7_42_chunk (actual rows=6241 loops=1)
-               Heap Fetches: 6241
-         ->  Index Only Scan using _hyper_7_40_chunk_space3_time_idx on _hyper_7_40_chunk (actual rows=6241 loops=1)
-               Heap Fetches: 6241
-         ->  Index Only Scan using _hyper_7_38_chunk_space3_time_idx on _hyper_7_38_chunk (actual rows=6241 loops=1)
-               Heap Fetches: 6241
-   ->  Merge Append (actual rows=53760 loops=1)
-         Sort Key: _hyper_7_53_chunk."time" DESC
-         ->  Index Only Scan using _hyper_7_53_chunk_space3_time_idx on _hyper_7_53_chunk (actual rows=6720 loops=1)
-               Heap Fetches: 6720
-         ->  Index Only Scan using _hyper_7_51_chunk_space3_time_idx on _hyper_7_51_chunk (actual rows=6720 loops=1)
-               Heap Fetches: 6720
-         ->  Index Only Scan using _hyper_7_49_chunk_space3_time_idx on _hyper_7_49_chunk (actual rows=6720 loops=1)
-               Heap Fetches: 6720
-         ->  Index Only Scan using _hyper_7_47_chunk_space3_time_idx on _hyper_7_47_chunk (actual rows=6720 loops=1)
-               Heap Fetches: 6720
-         ->  Index Only Scan using _hyper_7_45_chunk_space3_time_idx on _hyper_7_45_chunk (actual rows=6720 loops=1)
-               Heap Fetches: 6720
-         ->  Index Only Scan using _hyper_7_43_chunk_space3_time_idx on _hyper_7_43_chunk (actual rows=6720 loops=1)
-               Heap Fetches: 6720
-         ->  Index Only Scan using _hyper_7_41_chunk_space3_time_idx on _hyper_7_41_chunk (actual rows=6720 loops=1)
-               Heap Fetches: 6720
-         ->  Index Only Scan using _hyper_7_39_chunk_space3_time_idx on _hyper_7_39_chunk (actual rows=6720 loops=1)
-               Heap Fetches: 6720
-(38 rows)
-
--- test COLLATION
--- cant be tested in our ci because alpine doesnt support locales
--- :PREFIX SELECT * FROM sortopt_test ORDER BY time, device COLLATE "en_US.utf8";
--- test NULLS FIRST
-:PREFIX SELECT * FROM sortopt_test ORDER BY time, device NULLS FIRST;
-                                                      QUERY PLAN                                                      
-----------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on sortopt_test (actual rows=12961 loops=1)
-   Order: sortopt_test."time", sortopt_test.device NULLS FIRST
-   ->  Index Only Scan using _hyper_8_55_chunk_time_device_nullsfirst on _hyper_8_55_chunk (actual rows=6720 loops=1)
-         Heap Fetches: 6720
-   ->  Index Only Scan using _hyper_8_54_chunk_time_device_nullsfirst on _hyper_8_54_chunk (actual rows=6241 loops=1)
-         Heap Fetches: 6241
-(6 rows)
-
--- test NULLS LAST
-:PREFIX SELECT * FROM sortopt_test ORDER BY time, device DESC NULLS LAST;
-                                                     QUERY PLAN                                                      
----------------------------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on sortopt_test (actual rows=12961 loops=1)
-   Order: sortopt_test."time", sortopt_test.device DESC NULLS LAST
-   ->  Index Only Scan using _hyper_8_55_chunk_time_device_nullslast on _hyper_8_55_chunk (actual rows=6720 loops=1)
-         Heap Fetches: 6720
-   ->  Index Only Scan using _hyper_8_54_chunk_time_device_nullslast on _hyper_8_54_chunk (actual rows=6241 loops=1)
-         Heap Fetches: 6241
-(6 rows)
-
---generate the results into two different files
-\set ECHO errors
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/query-15.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/query-15.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/query-15.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/query-15.out	2023-11-25 05:27:44.129022707 +0000
@@ -1,401 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-\set TEST_BASE_NAME query
-SELECT format('include/%s_load.sql', :'TEST_BASE_NAME') as "TEST_LOAD_NAME",
-       format('include/%s_query.sql', :'TEST_BASE_NAME') as "TEST_QUERY_NAME",
-       format('%s/results/%s_results_optimized.out', :'TEST_OUTPUT_DIR', :'TEST_BASE_NAME') as "TEST_RESULTS_OPTIMIZED",
-       format('%s/results/%s_results_unoptimized.out', :'TEST_OUTPUT_DIR', :'TEST_BASE_NAME') as "TEST_RESULTS_UNOPTIMIZED"
-\gset
-SELECT format('\! diff -u  --label "Unoptimized result" --label "Optimized result" %s %s', :'TEST_RESULTS_UNOPTIMIZED', :'TEST_RESULTS_OPTIMIZED') as "DIFF_CMD"
-\gset
-\set PREFIX 'EXPLAIN (costs OFF)'
-\ir :TEST_LOAD_NAME
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-CREATE TABLE PUBLIC.hyper_1 (
-  time TIMESTAMP NOT NULL,
-  series_0 DOUBLE PRECISION NULL,
-  series_1 DOUBLE PRECISION NULL,
-  series_2 DOUBLE PRECISION NULL
-);
-CREATE INDEX "time_plain" ON PUBLIC.hyper_1 (time DESC, series_0);
-SELECT * FROM create_hypertable('"public"."hyper_1"'::regclass, 'time'::name, number_partitions => 1, create_default_indexes=>false);
-psql:include/query_load.sql:13: WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
- hypertable_id | schema_name | table_name | created 
----------------+-------------+------------+---------
-             1 | public      | hyper_1    | t
-(1 row)
-
-INSERT INTO hyper_1 SELECT to_timestamp(ser), ser, ser+10000, sqrt(ser::numeric) FROM generate_series(0,10000) ser;
-INSERT INTO hyper_1 SELECT to_timestamp(ser), ser, ser+10000, sqrt(ser::numeric) FROM generate_series(10001,20000) ser;
-CREATE TABLE PUBLIC.hyper_1_tz (
-  time TIMESTAMPTZ NOT NULL,
-  series_0 DOUBLE PRECISION NULL,
-  series_1 DOUBLE PRECISION NULL,
-  series_2 DOUBLE PRECISION NULL
-);
-CREATE INDEX "time_plain_tz" ON PUBLIC.hyper_1_tz (time DESC, series_0);
-SELECT * FROM create_hypertable('"public"."hyper_1_tz"'::regclass, 'time'::name, number_partitions => 1, create_default_indexes=>false);
- hypertable_id | schema_name | table_name | created 
----------------+-------------+------------+---------
-             2 | public      | hyper_1_tz | t
-(1 row)
-
-INSERT INTO hyper_1_tz SELECT to_timestamp(ser), ser, ser+10000, sqrt(ser::numeric) FROM generate_series(0,10000) ser;
-INSERT INTO hyper_1_tz SELECT to_timestamp(ser), ser, ser+10000, sqrt(ser::numeric) FROM generate_series(10001,20000) ser;
-CREATE TABLE PUBLIC.hyper_1_int (
-  time int NOT NULL,
-  series_0 DOUBLE PRECISION NULL,
-  series_1 DOUBLE PRECISION NULL,
-  series_2 DOUBLE PRECISION NULL
-);
-CREATE INDEX "time_plain_int" ON PUBLIC.hyper_1_int (time DESC, series_0);
-SELECT * FROM create_hypertable('"public"."hyper_1_int"'::regclass, 'time'::name, number_partitions => 1, chunk_time_interval=>10000, create_default_indexes=>FALSE);
- hypertable_id | schema_name | table_name  | created 
----------------+-------------+-------------+---------
-             3 | public      | hyper_1_int | t
-(1 row)
-
-INSERT INTO hyper_1_int SELECT ser, ser, ser+10000, sqrt(ser::numeric) FROM generate_series(0,10000) ser;
-INSERT INTO hyper_1_int SELECT ser, ser, ser+10000, sqrt(ser::numeric) FROM generate_series(10001,20000) ser;
-CREATE TABLE PUBLIC.hyper_1_date (
-  time date NOT NULL,
-  series_0 DOUBLE PRECISION NULL,
-  series_1 DOUBLE PRECISION NULL,
-  series_2 DOUBLE PRECISION NULL
-);
-CREATE INDEX "time_plain_date" ON PUBLIC.hyper_1_date (time DESC, series_0);
-SELECT * FROM create_hypertable('"public"."hyper_1_date"'::regclass, 'time'::name, number_partitions => 1, chunk_time_interval=>86400000000, create_default_indexes=>FALSE);
- hypertable_id | schema_name |  table_name  | created 
----------------+-------------+--------------+---------
-             4 | public      | hyper_1_date | t
-(1 row)
-
-INSERT INTO hyper_1_date SELECT to_timestamp(ser)::date, ser, ser+10000, sqrt(ser::numeric) FROM generate_series(0,10000) ser;
-INSERT INTO hyper_1_date SELECT to_timestamp(ser)::date, ser, ser+10000, sqrt(ser::numeric) FROM generate_series(10001,20000) ser;
---below needed to create enough unique dates to trigger an index scan
-INSERT INTO hyper_1_date SELECT to_timestamp(ser*100)::date, ser, ser+10000, sqrt(ser::numeric) FROM generate_series(10001,20000) ser;
-CREATE TABLE PUBLIC.plain_table (
-  time TIMESTAMPTZ NOT NULL,
-  series_0 DOUBLE PRECISION NULL,
-  series_1 DOUBLE PRECISION NULL,
-  series_2 DOUBLE PRECISION NULL
-);
-CREATE INDEX "time_plain_plain_table" ON PUBLIC.plain_table (time DESC, series_0);
-INSERT INTO plain_table SELECT to_timestamp(ser), ser, ser+10000, sqrt(ser::numeric) FROM generate_series(0,10000) ser;
-INSERT INTO plain_table SELECT to_timestamp(ser), ser, ser+10000, sqrt(ser::numeric) FROM generate_series(10001,20000) ser;
--- Table with a time partitioning function
-CREATE TABLE PUBLIC.hyper_timefunc (
-  time float8 NOT NULL,
-  series_0 DOUBLE PRECISION NULL,
-  series_1 DOUBLE PRECISION NULL,
-  series_2 DOUBLE PRECISION NULL
-);
-CREATE OR REPLACE FUNCTION unix_to_timestamp(unixtime float8)
-    RETURNS TIMESTAMPTZ LANGUAGE SQL IMMUTABLE AS
-$BODY$
-    SELECT to_timestamp(unixtime);
-$BODY$;
-CREATE INDEX "time_plain_timefunc" ON PUBLIC.hyper_timefunc (to_timestamp(time) DESC, series_0);
-SELECT * FROM create_hypertable('"public"."hyper_timefunc"'::regclass, 'time'::name, number_partitions => 1, create_default_indexes=>false, time_partitioning_func => 'unix_to_timestamp');
- hypertable_id | schema_name |   table_name   | created 
----------------+-------------+----------------+---------
-             5 | public      | hyper_timefunc | t
-(1 row)
-
-INSERT INTO hyper_timefunc SELECT ser, ser, ser+10000, sqrt(ser::numeric) FROM generate_series(0,10000) ser;
-INSERT INTO hyper_timefunc SELECT ser, ser, ser+10000, sqrt(ser::numeric) FROM generate_series(10001,20000) ser;
-ANALYZE plain_table;
-ANALYZE hyper_timefunc;
-ANALYZE hyper_1;
-ANALYZE hyper_1_tz;
-ANALYZE hyper_1_int;
-ANALYZE hyper_1_date;
-\ir :TEST_QUERY_NAME
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-SHOW timescaledb.enable_optimizations;
- timescaledb.enable_optimizations 
-----------------------------------
- on
-(1 row)
-
---non-aggregates use MergeAppend in both optimized and non-optimized
-:PREFIX SELECT * FROM hyper_1 ORDER BY "time" DESC limit 2;
-                               QUERY PLAN                               
-------------------------------------------------------------------------
- Limit
-   ->  Index Scan using _hyper_1_1_chunk_time_plain on _hyper_1_1_chunk
-(2 rows)
-
-:PREFIX SELECT * FROM hyper_timefunc ORDER BY unix_to_timestamp("time") DESC limit 2;
-                                    QUERY PLAN                                     
------------------------------------------------------------------------------------
- Limit
-   ->  Index Scan using _hyper_5_19_chunk_time_plain_timefunc on _hyper_5_19_chunk
-(2 rows)
-
---Aggregates use MergeAppend only in optimized
-:PREFIX SELECT date_trunc('minute', time) t, avg(series_0), min(series_1), avg(series_2) FROM hyper_1 GROUP BY t ORDER BY t DESC limit 2;
-                                     QUERY PLAN                                     
-------------------------------------------------------------------------------------
- Limit
-   ->  GroupAggregate
-         Group Key: (date_trunc('minute'::text, _hyper_1_1_chunk."time"))
-         ->  Result
-               ->  Index Scan using _hyper_1_1_chunk_time_plain on _hyper_1_1_chunk
-(5 rows)
-
-:PREFIX SELECT date_trunc('minute', time) t, avg(series_0), min(series_1), avg(series_2) FROM hyper_1_date GROUP BY t ORDER BY t DESC limit 2;
-                                                      QUERY PLAN                                                      
-----------------------------------------------------------------------------------------------------------------------
- Limit
-   ->  GroupAggregate
-         Group Key: (date_trunc('minute'::text, (_hyper_4_6_chunk."time")::timestamp with time zone))
-         ->  Result
-               ->  Merge Append
-                     Sort Key: (date_trunc('minute'::text, (_hyper_4_6_chunk."time")::timestamp with time zone)) DESC
-                     ->  Index Scan using _hyper_4_6_chunk_time_plain_date on _hyper_4_6_chunk
-                     ->  Index Scan using _hyper_4_7_chunk_time_plain_date on _hyper_4_7_chunk
-                     ->  Index Scan using _hyper_4_8_chunk_time_plain_date on _hyper_4_8_chunk
-                     ->  Index Scan using _hyper_4_9_chunk_time_plain_date on _hyper_4_9_chunk
-                     ->  Index Scan using _hyper_4_10_chunk_time_plain_date on _hyper_4_10_chunk
-                     ->  Index Scan using _hyper_4_11_chunk_time_plain_date on _hyper_4_11_chunk
-                     ->  Index Scan using _hyper_4_12_chunk_time_plain_date on _hyper_4_12_chunk
-                     ->  Index Scan using _hyper_4_13_chunk_time_plain_date on _hyper_4_13_chunk
-                     ->  Index Scan using _hyper_4_14_chunk_time_plain_date on _hyper_4_14_chunk
-                     ->  Index Scan using _hyper_4_15_chunk_time_plain_date on _hyper_4_15_chunk
-                     ->  Index Scan using _hyper_4_16_chunk_time_plain_date on _hyper_4_16_chunk
-                     ->  Index Scan using _hyper_4_17_chunk_time_plain_date on _hyper_4_17_chunk
-                     ->  Index Scan using _hyper_4_18_chunk_time_plain_date on _hyper_4_18_chunk
-(19 rows)
-
---the minute and second results should be diff
-:PREFIX SELECT date_trunc('minute', time) t, avg(series_0), min(series_1), avg(series_2) FROM hyper_1 GROUP BY t ORDER BY t DESC limit 2;
-                                     QUERY PLAN                                     
-------------------------------------------------------------------------------------
- Limit
-   ->  GroupAggregate
-         Group Key: (date_trunc('minute'::text, _hyper_1_1_chunk."time"))
-         ->  Result
-               ->  Index Scan using _hyper_1_1_chunk_time_plain on _hyper_1_1_chunk
-(5 rows)
-
-:PREFIX SELECT date_trunc('second', time) t, avg(series_0), min(series_1), avg(series_2) FROM hyper_1 GROUP BY t ORDER BY t DESC limit 2;
-                                     QUERY PLAN                                     
-------------------------------------------------------------------------------------
- Limit
-   ->  GroupAggregate
-         Group Key: (date_trunc('second'::text, _hyper_1_1_chunk."time"))
-         ->  Result
-               ->  Index Scan using _hyper_1_1_chunk_time_plain on _hyper_1_1_chunk
-(5 rows)
-
---test that when index on time used by constraint, still works correctly
-:PREFIX
-SELECT date_trunc('minute', time) t, avg(series_0), min(series_1), avg(series_2)
-FROM hyper_1
-WHERE time < to_timestamp(900)
-GROUP BY t
-ORDER BY t DESC
-LIMIT 2;
-                                                QUERY PLAN                                                 
------------------------------------------------------------------------------------------------------------
- Limit
-   ->  GroupAggregate
-         Group Key: (date_trunc('minute'::text, hyper_1."time"))
-         ->  Result
-               ->  Custom Scan (ChunkAppend) on hyper_1
-                     Order: date_trunc('minute'::text, hyper_1."time") DESC
-                     Chunks excluded during startup: 0
-                     ->  Index Scan using _hyper_1_1_chunk_time_plain on _hyper_1_1_chunk
-                           Index Cond: ("time" < 'Wed Dec 31 16:15:00 1969 PST'::timestamp with time zone)
-(9 rows)
-
---test on table with time partitioning function. Currently not
---optimized to use index for ordering since the index is an expression
---on time (e.g., timefunc(time)), and we currently don't handle that
---case.
-:PREFIX
-SELECT date_trunc('minute', to_timestamp(time)) t, avg(series_0), min(series_1), avg(series_2)
-FROM hyper_timefunc
-WHERE to_timestamp(time) < to_timestamp(900)
-GROUP BY t
-ORDER BY t DESC
-LIMIT 2;
-                                                       QUERY PLAN                                                        
--------------------------------------------------------------------------------------------------------------------------
- Limit
-   ->  Sort
-         Sort Key: (date_trunc('minute'::text, to_timestamp(_hyper_5_19_chunk."time"))) DESC
-         ->  HashAggregate
-               Group Key: date_trunc('minute'::text, to_timestamp(_hyper_5_19_chunk."time"))
-               ->  Result
-                     ->  Index Scan using _hyper_5_19_chunk_time_plain_timefunc on _hyper_5_19_chunk
-                           Index Cond: (to_timestamp("time") < 'Wed Dec 31 16:15:00 1969 PST'::timestamp with time zone)
-(8 rows)
-
-BEGIN;
-  --test that still works with an expression index on data_trunc.
-  DROP INDEX "time_plain";
-  CREATE INDEX "time_trunc" ON PUBLIC.hyper_1 (date_trunc('minute', time));
-  ANALYZE hyper_1;
-  :PREFIX SELECT date_trunc('minute', time) t, avg(series_0), min(series_1), avg(series_2) FROM hyper_1 GROUP BY t ORDER BY t DESC limit 2;
-                                         QUERY PLAN                                          
----------------------------------------------------------------------------------------------
- Limit
-   ->  GroupAggregate
-         Group Key: (date_trunc('minute'::text, _hyper_1_1_chunk."time"))
-         ->  Result
-               ->  Index Scan Backward using _hyper_1_1_chunk_time_trunc on _hyper_1_1_chunk
-(5 rows)
-
-  --test that works with both indexes
-  CREATE INDEX "time_plain" ON PUBLIC.hyper_1 (time DESC, series_0);
-  ANALYZE hyper_1;
-  :PREFIX SELECT date_trunc('minute', time) t, avg(series_0), min(series_1), avg(series_2) FROM hyper_1 GROUP BY t ORDER BY t DESC limit 2;
-                                         QUERY PLAN                                          
----------------------------------------------------------------------------------------------
- Limit
-   ->  GroupAggregate
-         Group Key: (date_trunc('minute'::text, _hyper_1_1_chunk."time"))
-         ->  Result
-               ->  Index Scan Backward using _hyper_1_1_chunk_time_trunc on _hyper_1_1_chunk
-(5 rows)
-
-  :PREFIX SELECT time_bucket('1 minute', time) t, avg(series_0), min(series_1), trunc(avg(series_2)::numeric, 5)
-  FROM hyper_1 GROUP BY t ORDER BY t DESC limit 2;
-                                     QUERY PLAN                                     
-------------------------------------------------------------------------------------
- Limit
-   ->  GroupAggregate
-         Group Key: (time_bucket('@ 1 min'::interval, _hyper_1_1_chunk."time"))
-         ->  Result
-               ->  Index Scan using _hyper_1_1_chunk_time_plain on _hyper_1_1_chunk
-(5 rows)
-
-  :PREFIX SELECT time_bucket('1 minute', time, INTERVAL '30 seconds') t, avg(series_0), min(series_1), trunc(avg(series_2)::numeric,5)
-  FROM hyper_1 GROUP BY t ORDER BY t DESC limit 2;
-                                              QUERY PLAN                                               
--------------------------------------------------------------------------------------------------------
- Limit
-   ->  GroupAggregate
-         Group Key: (time_bucket('@ 1 min'::interval, _hyper_1_1_chunk."time", '@ 30 secs'::interval))
-         ->  Result
-               ->  Index Scan using _hyper_1_1_chunk_time_plain on _hyper_1_1_chunk
-(5 rows)
-
-  :PREFIX SELECT time_bucket('1 minute', time - INTERVAL '30 seconds') t, avg(series_0), min(series_1), trunc(avg(series_2)::numeric,5)
-  FROM hyper_1 GROUP BY t ORDER BY t DESC limit 2;
-                                                QUERY PLAN                                                
-----------------------------------------------------------------------------------------------------------
- Limit
-   ->  GroupAggregate
-         Group Key: (time_bucket('@ 1 min'::interval, (_hyper_1_1_chunk."time" - '@ 30 secs'::interval)))
-         ->  Result
-               ->  Index Scan using _hyper_1_1_chunk_time_plain on _hyper_1_1_chunk
-(5 rows)
-
-  :PREFIX SELECT time_bucket('1 minute', time - INTERVAL '30 seconds') + INTERVAL '30 seconds' t, avg(series_0), min(series_1), trunc(avg(series_2)::numeric,5)
-  FROM hyper_1 GROUP BY t ORDER BY t DESC limit 2;
-                                                             QUERY PLAN                                                             
-------------------------------------------------------------------------------------------------------------------------------------
- Limit
-   ->  GroupAggregate
-         Group Key: ((time_bucket('@ 1 min'::interval, (_hyper_1_1_chunk."time" - '@ 30 secs'::interval)) + '@ 30 secs'::interval))
-         ->  Result
-               ->  Index Scan using _hyper_1_1_chunk_time_plain on _hyper_1_1_chunk
-(5 rows)
-
-  :PREFIX SELECT time_bucket('1 minute', time) t, avg(series_0), min(series_1), avg(series_2)
-  FROM hyper_1_tz GROUP BY t ORDER BY t DESC limit 2;
-                                      QUERY PLAN                                       
----------------------------------------------------------------------------------------
- Limit
-   ->  GroupAggregate
-         Group Key: (time_bucket('@ 1 min'::interval, _hyper_2_2_chunk."time"))
-         ->  Result
-               ->  Index Scan using _hyper_2_2_chunk_time_plain_tz on _hyper_2_2_chunk
-(5 rows)
-
-  :PREFIX SELECT time_bucket('1 minute', time::timestamp) t, avg(series_0), min(series_1), avg(series_2)
-  FROM hyper_1_tz GROUP BY t ORDER BY t DESC limit 2;
-                                                  QUERY PLAN                                                   
----------------------------------------------------------------------------------------------------------------
- Limit
-   ->  GroupAggregate
-         Group Key: (time_bucket('@ 1 min'::interval, (_hyper_2_2_chunk."time")::timestamp without time zone))
-         ->  Result
-               ->  Index Scan using _hyper_2_2_chunk_time_plain_tz on _hyper_2_2_chunk
-(5 rows)
-
-  :PREFIX SELECT time_bucket(10, time) t, avg(series_0), min(series_1), avg(series_2)
-  FROM hyper_1_int GROUP BY t ORDER BY t DESC limit 2;
-                                          QUERY PLAN                                          
-----------------------------------------------------------------------------------------------
- Limit
-   ->  GroupAggregate
-         Group Key: (time_bucket(10, hyper_1_int."time"))
-         ->  Result
-               ->  Custom Scan (ChunkAppend) on hyper_1_int
-                     Order: time_bucket(10, hyper_1_int."time") DESC
-                     ->  Index Scan using _hyper_3_5_chunk_time_plain_int on _hyper_3_5_chunk
-                     ->  Index Scan using _hyper_3_4_chunk_time_plain_int on _hyper_3_4_chunk
-                     ->  Index Scan using _hyper_3_3_chunk_time_plain_int on _hyper_3_3_chunk
-(9 rows)
-
-  :PREFIX SELECT time_bucket(10, time, 2) t, avg(series_0), min(series_1), avg(series_2)
-  FROM hyper_1_int GROUP BY t ORDER BY t DESC limit 2;
-                                          QUERY PLAN                                          
-----------------------------------------------------------------------------------------------
- Limit
-   ->  GroupAggregate
-         Group Key: (time_bucket(10, hyper_1_int."time", 2))
-         ->  Result
-               ->  Custom Scan (ChunkAppend) on hyper_1_int
-                     Order: time_bucket(10, hyper_1_int."time", 2) DESC
-                     ->  Index Scan using _hyper_3_5_chunk_time_plain_int on _hyper_3_5_chunk
-                     ->  Index Scan using _hyper_3_4_chunk_time_plain_int on _hyper_3_4_chunk
-                     ->  Index Scan using _hyper_3_3_chunk_time_plain_int on _hyper_3_3_chunk
-(9 rows)
-
-ROLLBACK;
--- sort order optimization should not be applied to non-hypertables
-:PREFIX
-SELECT date_trunc('minute', time) t, avg(series_0), min(series_1), avg(series_2)
-FROM plain_table
-WHERE time < to_timestamp(900)
-GROUP BY t
-ORDER BY t DESC
-LIMIT 2;
-                                             QUERY PLAN                                              
------------------------------------------------------------------------------------------------------
- Limit
-   ->  Sort
-         Sort Key: (date_trunc('minute'::text, "time")) DESC
-         ->  HashAggregate
-               Group Key: date_trunc('minute'::text, "time")
-               ->  Index Scan using time_plain_plain_table on plain_table
-                     Index Cond: ("time" < 'Wed Dec 31 16:15:00 1969 PST'::timestamp with time zone)
-(7 rows)
-
---generate the results into two different files
-\set ECHO errors
---- Unoptimized result
-+++ Optimized result
-@@ -1,6 +1,6 @@
-  timescaledb.enable_optimizations 
- ----------------------------------
-- off
-+ on
- (1 row)
- 
-            time           | series_0 | series_1 |     series_2     
- ?column? 
-----------
- Done
-(1 row)
-
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/relocate_extension.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/relocate_extension.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/relocate_extension.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/relocate_extension.out	2023-11-25 05:27:44.149022649 +0000
@@ -1,184 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
--- Set this variable to avoid using a hard-coded path each time query
--- results are compared
-\set QUERY_RESULT_TEST_EQUAL_RELPATH 'include/query_result_test_equal.sql'
-\c postgres :ROLE_SUPERUSER
-DROP DATABASE :TEST_DBNAME;
-CREATE DATABASE :TEST_DBNAME;
-\c :TEST_DBNAME
-CREATE SCHEMA "testSchema0";
-SET client_min_messages=error;
-CREATE EXTENSION IF NOT EXISTS timescaledb SCHEMA "testSchema0";
-RESET client_min_messages;
-CREATE TABLE test_ts(time timestamp, temp float8, device text);
-CREATE TABLE test_tz(time timestamptz, temp float8, device text);
-CREATE TABLE test_dt(time date, temp float8, device text);
-SELECT "testSchema0".create_hypertable('test_ts', 'time', 'device', 2);
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-  create_hypertable   
-----------------------
- (1,public,test_ts,t)
-(1 row)
-
-SELECT "testSchema0".create_hypertable('test_tz', 'time', 'device', 2);
-NOTICE:  adding not-null constraint to column "time"
-  create_hypertable   
-----------------------
- (2,public,test_tz,t)
-(1 row)
-
-SELECT "testSchema0".create_hypertable('test_dt', 'time', 'device', 2);
-NOTICE:  adding not-null constraint to column "time"
-  create_hypertable   
-----------------------
- (3,public,test_dt,t)
-(1 row)
-
-SELECT * FROM _timescaledb_catalog.hypertable;
- id | schema_name | table_name | associated_schema_name | associated_table_prefix | num_dimensions | chunk_sizing_func_schema |  chunk_sizing_func_name  | chunk_target_size | compression_state | compressed_hypertable_id | replication_factor 
-----+-------------+------------+------------------------+-------------------------+----------------+--------------------------+--------------------------+-------------------+-------------------+--------------------------+--------------------
-  1 | public      | test_ts    | _timescaledb_internal  | _hyper_1                |              2 | _timescaledb_internal    | calculate_chunk_interval |                 0 |                 0 |                          |                   
-  2 | public      | test_tz    | _timescaledb_internal  | _hyper_2                |              2 | _timescaledb_internal    | calculate_chunk_interval |                 0 |                 0 |                          |                   
-  3 | public      | test_dt    | _timescaledb_internal  | _hyper_3                |              2 | _timescaledb_internal    | calculate_chunk_interval |                 0 |                 0 |                          |                   
-(3 rows)
-
-INSERT INTO test_ts VALUES('Mon Mar 20 09:17:00.936242 2017', 23.4, 'dev1');
-INSERT INTO test_ts VALUES('Mon Mar 20 09:27:00.936242 2017', 22, 'dev2');
-INSERT INTO test_ts VALUES('Mon Mar 20 09:28:00.936242 2017', 21.2, 'dev1');
-INSERT INTO test_ts VALUES('Mon Mar 20 09:37:00.936242 2017', 30, 'dev3');
-SELECT * FROM test_ts ORDER BY time;
-              time               | temp | device 
----------------------------------+------+--------
- Mon Mar 20 09:17:00.936242 2017 | 23.4 | dev1
- Mon Mar 20 09:27:00.936242 2017 |   22 | dev2
- Mon Mar 20 09:28:00.936242 2017 | 21.2 | dev1
- Mon Mar 20 09:37:00.936242 2017 |   30 | dev3
-(4 rows)
-
-INSERT INTO test_tz VALUES('Mon Mar 20 09:17:00.936242 2017', 23.4, 'dev1');
-INSERT INTO test_tz VALUES('Mon Mar 20 09:27:00.936242 2017', 22, 'dev2');
-INSERT INTO test_tz VALUES('Mon Mar 20 09:28:00.936242 2017', 21.2, 'dev1');
-INSERT INTO test_tz VALUES('Mon Mar 20 09:37:00.936242 2017', 30, 'dev3');
-SELECT * FROM test_tz ORDER BY time;
-                time                 | temp | device 
--------------------------------------+------+--------
- Mon Mar 20 09:17:00.936242 2017 PDT | 23.4 | dev1
- Mon Mar 20 09:27:00.936242 2017 PDT |   22 | dev2
- Mon Mar 20 09:28:00.936242 2017 PDT | 21.2 | dev1
- Mon Mar 20 09:37:00.936242 2017 PDT |   30 | dev3
-(4 rows)
-
-INSERT INTO test_dt VALUES('Mon Mar 20 09:17:00.936242 2017', 23.4, 'dev1');
-INSERT INTO test_dt VALUES('Mon Mar 21 09:27:00.936242 2017', 22, 'dev2');
-INSERT INTO test_dt VALUES('Mon Mar 22 09:28:00.936242 2017', 21.2, 'dev1');
-INSERT INTO test_dt VALUES('Mon Mar 23 09:37:00.936242 2017', 30, 'dev3');
-SELECT * FROM test_dt ORDER BY time;
-    time    | temp | device 
-------------+------+--------
- 03-20-2017 | 23.4 | dev1
- 03-21-2017 |   22 | dev2
- 03-22-2017 | 21.2 | dev1
- 03-23-2017 |   30 | dev3
-(4 rows)
-
--- testing time_bucket START
-SELECT AVG(temp) AS avg_tmp, "testSchema0".time_bucket('5 minutes', time, INTERVAL '1 minutes') AS ten_min FROM test_ts GROUP BY ten_min ORDER BY avg_tmp;
- avg_tmp |         ten_min          
----------+--------------------------
-    21.6 | Mon Mar 20 09:26:00 2017
-    23.4 | Mon Mar 20 09:16:00 2017
-      30 | Mon Mar 20 09:36:00 2017
-(3 rows)
-
-SELECT AVG(temp) AS avg_tmp, "testSchema0".time_bucket('5 minutes', time, INTERVAL '1 minutes') AS ten_min FROM test_tz GROUP BY ten_min ORDER BY avg_tmp;
- avg_tmp |           ten_min            
----------+------------------------------
-    21.6 | Mon Mar 20 09:26:00 2017 PDT
-    23.4 | Mon Mar 20 09:16:00 2017 PDT
-      30 | Mon Mar 20 09:36:00 2017 PDT
-(3 rows)
-
-SELECT AVG(temp) AS avg_tmp, "testSchema0".time_bucket('1 day', time, INTERVAL '-0.5 day') AS ten_min FROM test_dt GROUP BY ten_min ORDER BY avg_tmp;
- avg_tmp |  ten_min   
----------+------------
-    21.2 | 03-21-2017
-      22 | 03-20-2017
-    23.4 | 03-19-2017
-      30 | 03-22-2017
-(4 rows)
-
--- testing time_bucket END
--- testing drop_chunks START
--- show_chunks and drop_chunks output should be the same
-\set QUERY1 'SELECT "testSchema0".show_chunks(older_than => \'2017-03-01\'::timestamp, relation => \'test_ts\')::REGCLASS::TEXT'
-\set QUERY2 'SELECT "testSchema0".drop_chunks(\'test_ts\', \'2017-03-01\'::timestamp)::TEXT'
-\set ECHO errors
- Different Rows | Total Rows from Query 1 | Total Rows from Query 2 
-----------------+-------------------------+-------------------------
-              0 |                       0 |                       0
-(1 row)
-
-SELECT * FROM test_ts ORDER BY time;
-              time               | temp | device 
----------------------------------+------+--------
- Mon Mar 20 09:17:00.936242 2017 | 23.4 | dev1
- Mon Mar 20 09:27:00.936242 2017 |   22 | dev2
- Mon Mar 20 09:28:00.936242 2017 | 21.2 | dev1
- Mon Mar 20 09:37:00.936242 2017 |   30 | dev3
-(4 rows)
-
-\set QUERY1 'SELECT "testSchema0".show_chunks(older_than => interval \'1 minutes\', relation => \'test_tz\')::REGCLASS::TEXT'
-\set QUERY2 'SELECT "testSchema0".drop_chunks(\'test_tz\', interval \'1 minutes\')::TEXT'
-\set ECHO errors
- Different Rows | Total Rows from Query 1 | Total Rows from Query 2 
-----------------+-------------------------+-------------------------
-              0 |                       2 |                       2
-(1 row)
-
-SELECT * FROM test_tz ORDER BY time;
- time | temp | device 
-------+------+--------
-(0 rows)
-
-\set QUERY1 'SELECT "testSchema0".show_chunks(older_than => interval \'1 minutes\', relation => \'test_dt\')::REGCLASS::TEXT'
-\set QUERY2 'SELECT "testSchema0".drop_chunks(\'test_dt\', interval \'1 minutes\')::TEXT'
-\set ECHO errors
- Different Rows | Total Rows from Query 1 | Total Rows from Query 2 
-----------------+-------------------------+-------------------------
-              0 |                       3 |                       3
-(1 row)
-
-SELECT * FROM test_dt ORDER BY time;
- time | temp | device 
-------+------+--------
-(0 rows)
-
--- testing drop_chunks END
--- testing hypertable_detailed_size START
-SELECT * FROM "testSchema0".hypertable_detailed_size('test_ts');
- table_bytes | index_bytes | toast_bytes | total_bytes | node_name 
--------------+-------------+-------------+-------------+-----------
-       16384 |       81920 |       24576 |      122880 | 
-(1 row)
-
--- testing hypertable_detailed_size END
-SELECT * FROM "testSchema0".hypertable_index_size('test_ts_time_idx');
- hypertable_index_size 
------------------------
-                 40960
-(1 row)
-
-SELECT * FROM "testSchema0".hypertable_index_size('test_ts_device_time_idx');
- hypertable_index_size 
------------------------
-                 40960
-(1 row)
-
-CREATE SCHEMA "testSchema";
-\set ON_ERROR_STOP 0
-ALTER EXTENSION timescaledb SET SCHEMA "testSchema";
-ERROR:  extension "timescaledb" does not support SET SCHEMA
-\set ON_ERROR_STOP 1
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/reloptions.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/reloptions.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/reloptions.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/reloptions.out	2023-11-25 05:27:44.129022707 +0000
@@ -1,50 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-CREATE TABLE reloptions_test(time integer, temp float8, color integer)
-WITH (fillfactor=75, autovacuum_vacuum_threshold=100);
-SELECT create_hypertable('reloptions_test', 'time', chunk_time_interval => 3);
-NOTICE:  adding not-null constraint to column "time"
-      create_hypertable       
-------------------------------
- (1,public,reloptions_test,t)
-(1 row)
-
-INSERT INTO reloptions_test VALUES (4, 24.3, 1), (9, 13.3, 2);
--- Show that reloptions are inherited by chunks
-SELECT relname, reloptions FROM pg_class
-WHERE relname ~ '^_hyper.*' AND relkind = 'r';
-     relname      |                   reloptions                    
-------------------+-------------------------------------------------
- _hyper_1_1_chunk | {fillfactor=75,autovacuum_vacuum_threshold=100}
- _hyper_1_2_chunk | {fillfactor=75,autovacuum_vacuum_threshold=100}
-(2 rows)
-
--- Alter reloptions
-ALTER TABLE reloptions_test SET (fillfactor=80, parallel_workers=8);
-\set ON_ERROR_STOP 0
-ALTER TABLE reloptions_test SET (fillfactor=80), SET (parallel_workers=8);
-ERROR:  ALTER TABLE <hypertable> SET does not support multiple clauses
-\set ON_ERROR_STOP 1
-SELECT relname, reloptions FROM pg_class
-WHERE relname ~ '^_hyper.*' AND relkind = 'r';
-     relname      |                             reloptions                             
-------------------+--------------------------------------------------------------------
- _hyper_1_1_chunk | {autovacuum_vacuum_threshold=100,fillfactor=80,parallel_workers=8}
- _hyper_1_2_chunk | {autovacuum_vacuum_threshold=100,fillfactor=80,parallel_workers=8}
-(2 rows)
-
-ALTER TABLE reloptions_test RESET (fillfactor);
-SELECT relname, reloptions FROM pg_class
-WHERE relname ~ '^_hyper.*' AND relkind = 'r';
-     relname      |                      reloptions                      
-------------------+------------------------------------------------------
- _hyper_1_1_chunk | {autovacuum_vacuum_threshold=100,parallel_workers=8}
- _hyper_1_2_chunk | {autovacuum_vacuum_threshold=100,parallel_workers=8}
-(2 rows)
-
--- Test reloptions on a regular table
-CREATE TABLE reloptions_test2(time integer, temp float8, color integer);
-ALTER TABLE reloptions_test2 SET (fillfactor=80, parallel_workers=8);
-ALTER TABLE reloptions_test2 SET (fillfactor=80), SET (parallel_workers=8);
-DROP TABLE reloptions_test2;
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/rowsecurity-15.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/rowsecurity-15.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/rowsecurity-15.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/rowsecurity-15.out	2023-11-25 05:27:44.129022707 +0000
@@ -1,5037 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
---
--- Test of Row-level security feature
---
--- Clean up in case a prior regression run failed
-\c :TEST_DBNAME :ROLE_SUPERUSER
-\set ON_ERROR_STOP 0
-\set VERBOSITY default
-SET timescaledb.enable_constraint_exclusion TO off;
--- Suppress NOTICE messages when users/groups don't exist
-SET client_min_messages TO 'warning';
-DROP USER IF EXISTS regress_rls_alice;
-DROP USER IF EXISTS regress_rls_bob;
-DROP USER IF EXISTS regress_rls_carol;
-DROP USER IF EXISTS regress_rls_dave;
-DROP USER IF EXISTS regress_rls_exempt_user;
-DROP ROLE IF EXISTS regress_rls_group1;
-DROP ROLE IF EXISTS regress_rls_group2;
-DROP SCHEMA IF EXISTS regress_rls_schema CASCADE;
-RESET client_min_messages;
--- initial setup
-CREATE USER regress_rls_alice NOLOGIN;
-CREATE USER regress_rls_bob NOLOGIN;
-CREATE USER regress_rls_carol NOLOGIN;
-CREATE USER regress_rls_dave NOLOGIN;
-CREATE USER regress_rls_exempt_user BYPASSRLS NOLOGIN;
-CREATE ROLE regress_rls_group1 NOLOGIN;
-CREATE ROLE regress_rls_group2 NOLOGIN;
-GRANT regress_rls_group1 TO regress_rls_bob;
-GRANT regress_rls_group2 TO regress_rls_carol;
-CREATE SCHEMA regress_rls_schema;
-GRANT ALL ON SCHEMA regress_rls_schema to public;
-SET search_path = regress_rls_schema;
--- setup of malicious function
-CREATE OR REPLACE FUNCTION f_leak(text) RETURNS bool
-    COST 0.0000001 LANGUAGE plpgsql
-    AS 'BEGIN RAISE NOTICE ''f_leak => %'', $1; RETURN true; END';
-GRANT EXECUTE ON FUNCTION f_leak(text) TO public;
--- BASIC Row-Level Security Scenario
-SET SESSION AUTHORIZATION regress_rls_alice;
-CREATE TABLE uaccount (
-    pguser      name primary key,
-    seclv       int
-);
-GRANT SELECT ON uaccount TO public;
-INSERT INTO uaccount VALUES
-    ('regress_rls_alice', 99),
-    ('regress_rls_bob', 1),
-    ('regress_rls_carol', 2),
-    ('regress_rls_dave', 3);
-CREATE TABLE category (
-    cid        int primary key,
-    cname      text
-);
-GRANT ALL ON category TO public;
-INSERT INTO category VALUES
-    (11, 'novel'),
-    (22, 'science fiction'),
-    (33, 'technology'),
-    (44, 'manga');
-CREATE TABLE document (
-    did         int primary key,
-    cid         int references category(cid),
-    dlevel      int not null,
-    dauthor     name,
-    dtitle      text
-);
-GRANT ALL ON document TO public;
-SELECT public.create_hypertable('document', 'did', chunk_time_interval=>2);
-         create_hypertable         
------------------------------------
- (1,regress_rls_schema,document,t)
-(1 row)
-
-INSERT INTO document VALUES
-    ( 1, 11, 1, 'regress_rls_bob', 'my first novel'),
-    ( 2, 11, 2, 'regress_rls_bob', 'my second novel'),
-    ( 3, 22, 2, 'regress_rls_bob', 'my science fiction'),
-    ( 4, 44, 1, 'regress_rls_bob', 'my first manga'),
-    ( 5, 44, 2, 'regress_rls_bob', 'my second manga'),
-    ( 6, 22, 1, 'regress_rls_carol', 'great science fiction'),
-    ( 7, 33, 2, 'regress_rls_carol', 'great technology book'),
-    ( 8, 44, 1, 'regress_rls_carol', 'great manga'),
-    ( 9, 22, 1, 'regress_rls_dave', 'awesome science fiction'),
-    (10, 33, 2, 'regress_rls_dave', 'awesome technology book');
-ALTER TABLE document ENABLE ROW LEVEL SECURITY;
--- user's security level must be higher than or equal to document's
-CREATE POLICY p1 ON document AS PERMISSIVE
-    USING (dlevel <= (SELECT seclv FROM uaccount WHERE pguser = current_user));
--- try to create a policy of bogus type
-CREATE POLICY p1 ON document AS UGLY
-    USING (dlevel <= (SELECT seclv FROM uaccount WHERE pguser = current_user));
-ERROR:  unrecognized row security option "ugly"
-LINE 1: CREATE POLICY p1 ON document AS UGLY
-                                        ^
-HINT:  Only PERMISSIVE or RESTRICTIVE policies are supported currently.
--- but Dave isn't allowed to anything at cid 50 or above
--- this is to make sure that we sort the policies by name first
--- when applying WITH CHECK, a later INSERT by Dave should fail due
--- to p1r first
-CREATE POLICY p2r ON document AS RESTRICTIVE TO regress_rls_dave
-    USING (cid <> 44 AND cid < 50);
--- and Dave isn't allowed to see manga documents
-CREATE POLICY p1r ON document AS RESTRICTIVE TO regress_rls_dave
-    USING (cid <> 44);
-\dp
-                                                                  Access privileges
-       Schema       |   Name   | Type  |              Access privileges              | Column privileges |                  Policies                  
---------------------+----------+-------+---------------------------------------------+-------------------+--------------------------------------------
- regress_rls_schema | category | table | regress_rls_alice=arwdDxt/regress_rls_alice+|                   | 
-                    |          |       | =arwdDxt/regress_rls_alice                  |                   | 
- regress_rls_schema | document | table | regress_rls_alice=arwdDxt/regress_rls_alice+|                   | p1:                                       +
-                    |          |       | =arwdDxt/regress_rls_alice                  |                   |   (u): (dlevel <= ( SELECT uaccount.seclv +
-                    |          |       |                                             |                   |    FROM uaccount                          +
-                    |          |       |                                             |                   |   WHERE (uaccount.pguser = CURRENT_USER)))+
-                    |          |       |                                             |                   | p2r (RESTRICTIVE):                        +
-                    |          |       |                                             |                   |   (u): ((cid <> 44) AND (cid < 50))       +
-                    |          |       |                                             |                   |   to: regress_rls_dave                    +
-                    |          |       |                                             |                   | p1r (RESTRICTIVE):                        +
-                    |          |       |                                             |                   |   (u): (cid <> 44)                        +
-                    |          |       |                                             |                   |   to: regress_rls_dave
- regress_rls_schema | uaccount | table | regress_rls_alice=arwdDxt/regress_rls_alice+|                   | 
-                    |          |       | =r/regress_rls_alice                        |                   | 
-(3 rows)
-
-\d document
-        Table "regress_rls_schema.document"
- Column  |  Type   | Collation | Nullable | Default 
----------+---------+-----------+----------+---------
- did     | integer |           | not null | 
- cid     | integer |           |          | 
- dlevel  | integer |           | not null | 
- dauthor | name    |           |          | 
- dtitle  | text    |           |          | 
-Indexes:
-    "document_pkey" PRIMARY KEY, btree (did)
-Foreign-key constraints:
-    "document_cid_fkey" FOREIGN KEY (cid) REFERENCES category(cid)
-Policies:
-    POLICY "p1"
-      USING ((dlevel <= ( SELECT uaccount.seclv
-   FROM uaccount
-  WHERE (uaccount.pguser = CURRENT_USER))))
-    POLICY "p1r" AS RESTRICTIVE
-      TO regress_rls_dave
-      USING ((cid <> 44))
-    POLICY "p2r" AS RESTRICTIVE
-      TO regress_rls_dave
-      USING (((cid <> 44) AND (cid < 50)))
-Triggers:
-    ts_insert_blocker BEFORE INSERT ON document FOR EACH ROW EXECUTE FUNCTION _timescaledb_internal.insert_blocker()
-Number of child tables: 6 (Use \d+ to list them.)
-
-SELECT * FROM pg_policies WHERE schemaname = 'regress_rls_schema' AND tablename = 'document' ORDER BY policyname;
-     schemaname     | tablename | policyname | permissive  |       roles        | cmd |                    qual                    | with_check 
---------------------+-----------+------------+-------------+--------------------+-----+--------------------------------------------+------------
- regress_rls_schema | document  | p1         | PERMISSIVE  | {public}           | ALL | (dlevel <= ( SELECT uaccount.seclv        +| 
-                    |           |            |             |                    |     |    FROM uaccount                          +| 
-                    |           |            |             |                    |     |   WHERE (uaccount.pguser = CURRENT_USER))) | 
- regress_rls_schema | document  | p1r        | RESTRICTIVE | {regress_rls_dave} | ALL | (cid <> 44)                                | 
- regress_rls_schema | document  | p2r        | RESTRICTIVE | {regress_rls_dave} | ALL | ((cid <> 44) AND (cid < 50))               | 
-(3 rows)
-
--- viewpoint from regress_rls_bob
-SET SESSION AUTHORIZATION regress_rls_bob;
-SET row_security TO ON;
-SELECT * FROM document WHERE f_leak(dtitle) ORDER BY did;
-NOTICE:  f_leak => my first novel
-NOTICE:  f_leak => my first manga
-NOTICE:  f_leak => great science fiction
-NOTICE:  f_leak => great manga
-NOTICE:  f_leak => awesome science fiction
- did | cid | dlevel |      dauthor      |         dtitle          
------+-----+--------+-------------------+-------------------------
-   1 |  11 |      1 | regress_rls_bob   | my first novel
-   4 |  44 |      1 | regress_rls_bob   | my first manga
-   6 |  22 |      1 | regress_rls_carol | great science fiction
-   8 |  44 |      1 | regress_rls_carol | great manga
-   9 |  22 |      1 | regress_rls_dave  | awesome science fiction
-(5 rows)
-
-SELECT * FROM document NATURAL JOIN category WHERE f_leak(dtitle) ORDER BY did;
-NOTICE:  f_leak => my first novel
-NOTICE:  f_leak => my first manga
-NOTICE:  f_leak => great science fiction
-NOTICE:  f_leak => great manga
-NOTICE:  f_leak => awesome science fiction
- cid | did | dlevel |      dauthor      |         dtitle          |      cname      
------+-----+--------+-------------------+-------------------------+-----------------
-  11 |   1 |      1 | regress_rls_bob   | my first novel          | novel
-  44 |   4 |      1 | regress_rls_bob   | my first manga          | manga
-  22 |   6 |      1 | regress_rls_carol | great science fiction   | science fiction
-  44 |   8 |      1 | regress_rls_carol | great manga             | manga
-  22 |   9 |      1 | regress_rls_dave  | awesome science fiction | science fiction
-(5 rows)
-
--- try a sampled version
-SELECT * FROM document TABLESAMPLE BERNOULLI(50) REPEATABLE(0)
-  WHERE f_leak(dtitle) ORDER BY did;
- did | cid | dlevel | dauthor | dtitle 
------+-----+--------+---------+--------
-(0 rows)
-
--- viewpoint from regress_rls_carol
-SET SESSION AUTHORIZATION regress_rls_carol;
-SELECT * FROM document WHERE f_leak(dtitle) ORDER BY did;
-NOTICE:  f_leak => my first novel
-NOTICE:  f_leak => my second novel
-NOTICE:  f_leak => my science fiction
-NOTICE:  f_leak => my first manga
-NOTICE:  f_leak => my second manga
-NOTICE:  f_leak => great science fiction
-NOTICE:  f_leak => great technology book
-NOTICE:  f_leak => great manga
-NOTICE:  f_leak => awesome science fiction
-NOTICE:  f_leak => awesome technology book
- did | cid | dlevel |      dauthor      |         dtitle          
------+-----+--------+-------------------+-------------------------
-   1 |  11 |      1 | regress_rls_bob   | my first novel
-   2 |  11 |      2 | regress_rls_bob   | my second novel
-   3 |  22 |      2 | regress_rls_bob   | my science fiction
-   4 |  44 |      1 | regress_rls_bob   | my first manga
-   5 |  44 |      2 | regress_rls_bob   | my second manga
-   6 |  22 |      1 | regress_rls_carol | great science fiction
-   7 |  33 |      2 | regress_rls_carol | great technology book
-   8 |  44 |      1 | regress_rls_carol | great manga
-   9 |  22 |      1 | regress_rls_dave  | awesome science fiction
-  10 |  33 |      2 | regress_rls_dave  | awesome technology book
-(10 rows)
-
-SELECT * FROM document NATURAL JOIN category WHERE f_leak(dtitle) ORDER BY did;
-NOTICE:  f_leak => my first novel
-NOTICE:  f_leak => my second novel
-NOTICE:  f_leak => my science fiction
-NOTICE:  f_leak => my first manga
-NOTICE:  f_leak => my second manga
-NOTICE:  f_leak => great science fiction
-NOTICE:  f_leak => great technology book
-NOTICE:  f_leak => great manga
-NOTICE:  f_leak => awesome science fiction
-NOTICE:  f_leak => awesome technology book
- cid | did | dlevel |      dauthor      |         dtitle          |      cname      
------+-----+--------+-------------------+-------------------------+-----------------
-  11 |   1 |      1 | regress_rls_bob   | my first novel          | novel
-  11 |   2 |      2 | regress_rls_bob   | my second novel         | novel
-  22 |   3 |      2 | regress_rls_bob   | my science fiction      | science fiction
-  44 |   4 |      1 | regress_rls_bob   | my first manga          | manga
-  44 |   5 |      2 | regress_rls_bob   | my second manga         | manga
-  22 |   6 |      1 | regress_rls_carol | great science fiction   | science fiction
-  33 |   7 |      2 | regress_rls_carol | great technology book   | technology
-  44 |   8 |      1 | regress_rls_carol | great manga             | manga
-  22 |   9 |      1 | regress_rls_dave  | awesome science fiction | science fiction
-  33 |  10 |      2 | regress_rls_dave  | awesome technology book | technology
-(10 rows)
-
--- try a sampled version
-SELECT * FROM document TABLESAMPLE BERNOULLI(50) REPEATABLE(0)
-  WHERE f_leak(dtitle) ORDER BY did;
- did | cid | dlevel | dauthor | dtitle 
------+-----+--------+---------+--------
-(0 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM document WHERE f_leak(dtitle);
-                     QUERY PLAN                      
------------------------------------------------------
- Custom Scan (ChunkAppend) on document
-   Chunks excluded during startup: 0
-   InitPlan 1 (returns $0)
-     ->  Index Scan using uaccount_pkey on uaccount
-           Index Cond: (pguser = CURRENT_USER)
-   ->  Seq Scan on document document_1
-         Filter: ((dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_1_1_chunk document_2
-         Filter: ((dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_1_2_chunk document_3
-         Filter: ((dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_1_3_chunk document_4
-         Filter: ((dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_1_4_chunk document_5
-         Filter: ((dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_1_5_chunk document_6
-         Filter: ((dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_1_6_chunk document_7
-         Filter: ((dlevel <= $0) AND f_leak(dtitle))
-(19 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM document NATURAL JOIN category WHERE f_leak(dtitle);
-                        QUERY PLAN                         
------------------------------------------------------------
- Hash Join
-   Hash Cond: (document.cid = category.cid)
-   InitPlan 1 (returns $0)
-     ->  Index Scan using uaccount_pkey on uaccount
-           Index Cond: (pguser = CURRENT_USER)
-   ->  Custom Scan (ChunkAppend) on document
-         Chunks excluded during startup: 0
-         ->  Seq Scan on document document_1
-               Filter: ((dlevel <= $0) AND f_leak(dtitle))
-         ->  Seq Scan on _hyper_1_1_chunk document_2
-               Filter: ((dlevel <= $0) AND f_leak(dtitle))
-         ->  Seq Scan on _hyper_1_2_chunk document_3
-               Filter: ((dlevel <= $0) AND f_leak(dtitle))
-         ->  Seq Scan on _hyper_1_3_chunk document_4
-               Filter: ((dlevel <= $0) AND f_leak(dtitle))
-         ->  Seq Scan on _hyper_1_4_chunk document_5
-               Filter: ((dlevel <= $0) AND f_leak(dtitle))
-         ->  Seq Scan on _hyper_1_5_chunk document_6
-               Filter: ((dlevel <= $0) AND f_leak(dtitle))
-         ->  Seq Scan on _hyper_1_6_chunk document_7
-               Filter: ((dlevel <= $0) AND f_leak(dtitle))
-   ->  Hash
-         ->  Seq Scan on category
-(23 rows)
-
--- viewpoint from regress_rls_dave
-SET SESSION AUTHORIZATION regress_rls_dave;
-SELECT * FROM document WHERE f_leak(dtitle) ORDER BY did;
-NOTICE:  f_leak => my first novel
-NOTICE:  f_leak => my second novel
-NOTICE:  f_leak => my science fiction
-NOTICE:  f_leak => great science fiction
-NOTICE:  f_leak => great technology book
-NOTICE:  f_leak => awesome science fiction
-NOTICE:  f_leak => awesome technology book
- did | cid | dlevel |      dauthor      |         dtitle          
------+-----+--------+-------------------+-------------------------
-   1 |  11 |      1 | regress_rls_bob   | my first novel
-   2 |  11 |      2 | regress_rls_bob   | my second novel
-   3 |  22 |      2 | regress_rls_bob   | my science fiction
-   6 |  22 |      1 | regress_rls_carol | great science fiction
-   7 |  33 |      2 | regress_rls_carol | great technology book
-   9 |  22 |      1 | regress_rls_dave  | awesome science fiction
-  10 |  33 |      2 | regress_rls_dave  | awesome technology book
-(7 rows)
-
-SELECT * FROM document NATURAL JOIN category WHERE f_leak(dtitle) ORDER BY did;
-NOTICE:  f_leak => my first novel
-NOTICE:  f_leak => my second novel
-NOTICE:  f_leak => my science fiction
-NOTICE:  f_leak => great science fiction
-NOTICE:  f_leak => great technology book
-NOTICE:  f_leak => awesome science fiction
-NOTICE:  f_leak => awesome technology book
- cid | did | dlevel |      dauthor      |         dtitle          |      cname      
------+-----+--------+-------------------+-------------------------+-----------------
-  11 |   1 |      1 | regress_rls_bob   | my first novel          | novel
-  11 |   2 |      2 | regress_rls_bob   | my second novel         | novel
-  22 |   3 |      2 | regress_rls_bob   | my science fiction      | science fiction
-  22 |   6 |      1 | regress_rls_carol | great science fiction   | science fiction
-  33 |   7 |      2 | regress_rls_carol | great technology book   | technology
-  22 |   9 |      1 | regress_rls_dave  | awesome science fiction | science fiction
-  33 |  10 |      2 | regress_rls_dave  | awesome technology book | technology
-(7 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM document WHERE f_leak(dtitle);
-                                             QUERY PLAN                                             
-----------------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on document
-   Chunks excluded during startup: 0
-   InitPlan 1 (returns $0)
-     ->  Index Scan using uaccount_pkey on uaccount
-           Index Cond: (pguser = CURRENT_USER)
-   ->  Seq Scan on document document_1
-         Filter: ((cid <> 44) AND (cid <> 44) AND (cid < 50) AND (dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_1_1_chunk document_2
-         Filter: ((cid <> 44) AND (cid <> 44) AND (cid < 50) AND (dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_1_2_chunk document_3
-         Filter: ((cid <> 44) AND (cid <> 44) AND (cid < 50) AND (dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_1_3_chunk document_4
-         Filter: ((cid <> 44) AND (cid <> 44) AND (cid < 50) AND (dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_1_4_chunk document_5
-         Filter: ((cid <> 44) AND (cid <> 44) AND (cid < 50) AND (dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_1_5_chunk document_6
-         Filter: ((cid <> 44) AND (cid <> 44) AND (cid < 50) AND (dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_1_6_chunk document_7
-         Filter: ((cid <> 44) AND (cid <> 44) AND (cid < 50) AND (dlevel <= $0) AND f_leak(dtitle))
-(19 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM document NATURAL JOIN category WHERE f_leak(dtitle);
-                                                   QUERY PLAN                                                   
-----------------------------------------------------------------------------------------------------------------
- Hash Join
-   Hash Cond: (category.cid = document.cid)
-   InitPlan 1 (returns $0)
-     ->  Index Scan using uaccount_pkey on uaccount
-           Index Cond: (pguser = CURRENT_USER)
-   ->  Seq Scan on category
-   ->  Hash
-         ->  Custom Scan (ChunkAppend) on document
-               Chunks excluded during startup: 0
-               ->  Seq Scan on document document_1
-                     Filter: ((cid <> 44) AND (cid <> 44) AND (cid < 50) AND (dlevel <= $0) AND f_leak(dtitle))
-               ->  Seq Scan on _hyper_1_1_chunk document_2
-                     Filter: ((cid <> 44) AND (cid <> 44) AND (cid < 50) AND (dlevel <= $0) AND f_leak(dtitle))
-               ->  Seq Scan on _hyper_1_2_chunk document_3
-                     Filter: ((cid <> 44) AND (cid <> 44) AND (cid < 50) AND (dlevel <= $0) AND f_leak(dtitle))
-               ->  Seq Scan on _hyper_1_3_chunk document_4
-                     Filter: ((cid <> 44) AND (cid <> 44) AND (cid < 50) AND (dlevel <= $0) AND f_leak(dtitle))
-               ->  Seq Scan on _hyper_1_4_chunk document_5
-                     Filter: ((cid <> 44) AND (cid <> 44) AND (cid < 50) AND (dlevel <= $0) AND f_leak(dtitle))
-               ->  Seq Scan on _hyper_1_5_chunk document_6
-                     Filter: ((cid <> 44) AND (cid <> 44) AND (cid < 50) AND (dlevel <= $0) AND f_leak(dtitle))
-               ->  Seq Scan on _hyper_1_6_chunk document_7
-                     Filter: ((cid <> 44) AND (cid <> 44) AND (cid < 50) AND (dlevel <= $0) AND f_leak(dtitle))
-(23 rows)
-
--- 44 would technically fail for both p2r and p1r, but we should get an error
--- back from p1r for this because it sorts first
-INSERT INTO document VALUES (100, 44, 1, 'regress_rls_dave', 'testing sorting of policies'); -- fail
-ERROR:  new row violates row-level security policy "p1r" for table "document"
--- Just to see a p2r error
-INSERT INTO document VALUES (100, 55, 1, 'regress_rls_dave', 'testing sorting of policies'); -- fail
-ERROR:  new row violates row-level security policy "p2r" for table "document"
--- only owner can change policies
-ALTER POLICY p1 ON document USING (true);    --fail
-ERROR:  must be owner of table document
-DROP POLICY p1 ON document;                  --fail
-ERROR:  must be owner of relation document
-SET SESSION AUTHORIZATION regress_rls_alice;
-ALTER POLICY p1 ON document USING (dauthor = current_user);
--- viewpoint from regress_rls_bob again
-SET SESSION AUTHORIZATION regress_rls_bob;
-SELECT * FROM document WHERE f_leak(dtitle) ORDER BY did;
-NOTICE:  f_leak => my first novel
-NOTICE:  f_leak => my second novel
-NOTICE:  f_leak => my science fiction
-NOTICE:  f_leak => my first manga
-NOTICE:  f_leak => my second manga
- did | cid | dlevel |     dauthor     |       dtitle       
------+-----+--------+-----------------+--------------------
-   1 |  11 |      1 | regress_rls_bob | my first novel
-   2 |  11 |      2 | regress_rls_bob | my second novel
-   3 |  22 |      2 | regress_rls_bob | my science fiction
-   4 |  44 |      1 | regress_rls_bob | my first manga
-   5 |  44 |      2 | regress_rls_bob | my second manga
-(5 rows)
-
-SELECT * FROM document NATURAL JOIN category WHERE f_leak(dtitle) ORDER by did;
-NOTICE:  f_leak => my first novel
-NOTICE:  f_leak => my second novel
-NOTICE:  f_leak => my science fiction
-NOTICE:  f_leak => my first manga
-NOTICE:  f_leak => my second manga
- cid | did | dlevel |     dauthor     |       dtitle       |      cname      
------+-----+--------+-----------------+--------------------+-----------------
-  11 |   1 |      1 | regress_rls_bob | my first novel     | novel
-  11 |   2 |      2 | regress_rls_bob | my second novel    | novel
-  22 |   3 |      2 | regress_rls_bob | my science fiction | science fiction
-  44 |   4 |      1 | regress_rls_bob | my first manga     | manga
-  44 |   5 |      2 | regress_rls_bob | my second manga    | manga
-(5 rows)
-
--- viewpoint from rls_regres_carol again
-SET SESSION AUTHORIZATION regress_rls_carol;
-SELECT * FROM document WHERE f_leak(dtitle) ORDER BY did;
-NOTICE:  f_leak => great science fiction
-NOTICE:  f_leak => great technology book
-NOTICE:  f_leak => great manga
- did | cid | dlevel |      dauthor      |        dtitle         
------+-----+--------+-------------------+-----------------------
-   6 |  22 |      1 | regress_rls_carol | great science fiction
-   7 |  33 |      2 | regress_rls_carol | great technology book
-   8 |  44 |      1 | regress_rls_carol | great manga
-(3 rows)
-
-SELECT * FROM document NATURAL JOIN category WHERE f_leak(dtitle) ORDER by did;
-NOTICE:  f_leak => great science fiction
-NOTICE:  f_leak => great technology book
-NOTICE:  f_leak => great manga
- cid | did | dlevel |      dauthor      |        dtitle         |      cname      
------+-----+--------+-------------------+-----------------------+-----------------
-  22 |   6 |      1 | regress_rls_carol | great science fiction | science fiction
-  33 |   7 |      2 | regress_rls_carol | great technology book | technology
-  44 |   8 |      1 | regress_rls_carol | great manga           | manga
-(3 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM document WHERE f_leak(dtitle);
-                          QUERY PLAN                           
----------------------------------------------------------------
- Custom Scan (ChunkAppend) on document
-   Chunks excluded during startup: 0
-   ->  Seq Scan on document document_1
-         Filter: ((dauthor = CURRENT_USER) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_1_1_chunk document_2
-         Filter: ((dauthor = CURRENT_USER) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_1_2_chunk document_3
-         Filter: ((dauthor = CURRENT_USER) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_1_3_chunk document_4
-         Filter: ((dauthor = CURRENT_USER) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_1_4_chunk document_5
-         Filter: ((dauthor = CURRENT_USER) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_1_5_chunk document_6
-         Filter: ((dauthor = CURRENT_USER) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_1_6_chunk document_7
-         Filter: ((dauthor = CURRENT_USER) AND f_leak(dtitle))
-(16 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM document NATURAL JOIN category WHERE f_leak(dtitle);
-                             QUERY PLAN                              
----------------------------------------------------------------------
- Nested Loop
-   ->  Custom Scan (ChunkAppend) on document
-         Chunks excluded during startup: 0
-         ->  Seq Scan on document document_1
-               Filter: ((dauthor = CURRENT_USER) AND f_leak(dtitle))
-         ->  Seq Scan on _hyper_1_1_chunk document_2
-               Filter: ((dauthor = CURRENT_USER) AND f_leak(dtitle))
-         ->  Seq Scan on _hyper_1_2_chunk document_3
-               Filter: ((dauthor = CURRENT_USER) AND f_leak(dtitle))
-         ->  Seq Scan on _hyper_1_3_chunk document_4
-               Filter: ((dauthor = CURRENT_USER) AND f_leak(dtitle))
-         ->  Seq Scan on _hyper_1_4_chunk document_5
-               Filter: ((dauthor = CURRENT_USER) AND f_leak(dtitle))
-         ->  Seq Scan on _hyper_1_5_chunk document_6
-               Filter: ((dauthor = CURRENT_USER) AND f_leak(dtitle))
-         ->  Seq Scan on _hyper_1_6_chunk document_7
-               Filter: ((dauthor = CURRENT_USER) AND f_leak(dtitle))
-   ->  Index Scan using category_pkey on category
-         Index Cond: (cid = document.cid)
-(19 rows)
-
--- interaction of FK/PK constraints
-SET SESSION AUTHORIZATION regress_rls_alice;
-CREATE POLICY p2 ON category
-    USING (CASE WHEN current_user = 'regress_rls_bob' THEN cid IN (11, 33)
-           WHEN current_user = 'regress_rls_carol' THEN cid IN (22, 44)
-           ELSE false END);
-ALTER TABLE category ENABLE ROW LEVEL SECURITY;
--- cannot delete PK referenced by invisible FK
-SET SESSION AUTHORIZATION regress_rls_bob;
-SELECT * FROM document d FULL OUTER JOIN category c on d.cid = c.cid ORDER BY d.did, c.cid;
- did | cid | dlevel |     dauthor     |       dtitle       | cid |   cname    
------+-----+--------+-----------------+--------------------+-----+------------
-   1 |  11 |      1 | regress_rls_bob | my first novel     |  11 | novel
-   2 |  11 |      2 | regress_rls_bob | my second novel    |  11 | novel
-   3 |  22 |      2 | regress_rls_bob | my science fiction |     | 
-   4 |  44 |      1 | regress_rls_bob | my first manga     |     | 
-   5 |  44 |      2 | regress_rls_bob | my second manga    |     | 
-     |     |        |                 |                    |  33 | technology
-(6 rows)
-
-DELETE FROM category WHERE cid = 33;    -- fails with FK violation
-ERROR:  update or delete on table "category" violates foreign key constraint "4_7_document_cid_fkey" on table "_hyper_1_4_chunk"
-DETAIL:  Key is still referenced from table "_hyper_1_4_chunk".
--- can insert FK referencing invisible PK
-SET SESSION AUTHORIZATION regress_rls_carol;
-SELECT * FROM document d FULL OUTER JOIN category c on d.cid = c.cid ORDER BY d.did, c.cid;
- did | cid | dlevel |      dauthor      |        dtitle         | cid |      cname      
------+-----+--------+-------------------+-----------------------+-----+-----------------
-   6 |  22 |      1 | regress_rls_carol | great science fiction |  22 | science fiction
-   7 |  33 |      2 | regress_rls_carol | great technology book |     | 
-   8 |  44 |      1 | regress_rls_carol | great manga           |  44 | manga
-(3 rows)
-
-INSERT INTO document VALUES (11, 33, 1, current_user, 'hoge');
--- UNIQUE or PRIMARY KEY constraint violation DOES reveal presence of row
-SET SESSION AUTHORIZATION regress_rls_bob;
-INSERT INTO document VALUES (8, 44, 1, 'regress_rls_bob', 'my third manga'); -- Must fail with unique violation, revealing presence of did we can't see
-ERROR:  duplicate key value violates unique constraint "5_10_document_pkey"
-DETAIL:  Key (did)=(8) already exists.
-SELECT * FROM document WHERE did = 8; -- and confirm we can't see it
- did | cid | dlevel | dauthor | dtitle 
------+-----+--------+---------+--------
-(0 rows)
-
--- RLS policies are checked before constraints
-INSERT INTO document VALUES (8, 44, 1, 'regress_rls_carol', 'my third manga'); -- Should fail with RLS check violation, not duplicate key violation
-ERROR:  new row violates row-level security policy for table "document"
-UPDATE document SET did = 8, dauthor = 'regress_rls_carol' WHERE did = 5; -- Should fail with RLS check violation, not duplicate key violation
-ERROR:  new row violates row-level security policy for table "document"
--- database superuser does bypass RLS policy when enabled
-RESET SESSION AUTHORIZATION;
-SET row_security TO ON;
-SELECT * FROM document;
- did | cid | dlevel |      dauthor      |         dtitle          
------+-----+--------+-------------------+-------------------------
-   1 |  11 |      1 | regress_rls_bob   | my first novel
-   2 |  11 |      2 | regress_rls_bob   | my second novel
-   3 |  22 |      2 | regress_rls_bob   | my science fiction
-   4 |  44 |      1 | regress_rls_bob   | my first manga
-   5 |  44 |      2 | regress_rls_bob   | my second manga
-   6 |  22 |      1 | regress_rls_carol | great science fiction
-   7 |  33 |      2 | regress_rls_carol | great technology book
-   8 |  44 |      1 | regress_rls_carol | great manga
-   9 |  22 |      1 | regress_rls_dave  | awesome science fiction
-  10 |  33 |      2 | regress_rls_dave  | awesome technology book
-  11 |  33 |      1 | regress_rls_carol | hoge
-(11 rows)
-
-SELECT * FROM category;
- cid |      cname      
------+-----------------
-  11 | novel
-  22 | science fiction
-  33 | technology
-  44 | manga
-(4 rows)
-
--- database superuser does bypass RLS policy when disabled
-RESET SESSION AUTHORIZATION;
-SET row_security TO OFF;
-SELECT * FROM document;
- did | cid | dlevel |      dauthor      |         dtitle          
------+-----+--------+-------------------+-------------------------
-   1 |  11 |      1 | regress_rls_bob   | my first novel
-   2 |  11 |      2 | regress_rls_bob   | my second novel
-   3 |  22 |      2 | regress_rls_bob   | my science fiction
-   4 |  44 |      1 | regress_rls_bob   | my first manga
-   5 |  44 |      2 | regress_rls_bob   | my second manga
-   6 |  22 |      1 | regress_rls_carol | great science fiction
-   7 |  33 |      2 | regress_rls_carol | great technology book
-   8 |  44 |      1 | regress_rls_carol | great manga
-   9 |  22 |      1 | regress_rls_dave  | awesome science fiction
-  10 |  33 |      2 | regress_rls_dave  | awesome technology book
-  11 |  33 |      1 | regress_rls_carol | hoge
-(11 rows)
-
-SELECT * FROM category;
- cid |      cname      
------+-----------------
-  11 | novel
-  22 | science fiction
-  33 | technology
-  44 | manga
-(4 rows)
-
--- database non-superuser with bypass privilege can bypass RLS policy when disabled
-SET SESSION AUTHORIZATION regress_rls_exempt_user;
-SET row_security TO OFF;
-SELECT * FROM document;
- did | cid | dlevel |      dauthor      |         dtitle          
------+-----+--------+-------------------+-------------------------
-   1 |  11 |      1 | regress_rls_bob   | my first novel
-   2 |  11 |      2 | regress_rls_bob   | my second novel
-   3 |  22 |      2 | regress_rls_bob   | my science fiction
-   4 |  44 |      1 | regress_rls_bob   | my first manga
-   5 |  44 |      2 | regress_rls_bob   | my second manga
-   6 |  22 |      1 | regress_rls_carol | great science fiction
-   7 |  33 |      2 | regress_rls_carol | great technology book
-   8 |  44 |      1 | regress_rls_carol | great manga
-   9 |  22 |      1 | regress_rls_dave  | awesome science fiction
-  10 |  33 |      2 | regress_rls_dave  | awesome technology book
-  11 |  33 |      1 | regress_rls_carol | hoge
-(11 rows)
-
-SELECT * FROM category;
- cid |      cname      
------+-----------------
-  11 | novel
-  22 | science fiction
-  33 | technology
-  44 | manga
-(4 rows)
-
--- RLS policy does not apply to table owner when RLS enabled.
-SET SESSION AUTHORIZATION regress_rls_alice;
-SET row_security TO ON;
-SELECT * FROM document;
- did | cid | dlevel |      dauthor      |         dtitle          
------+-----+--------+-------------------+-------------------------
-   1 |  11 |      1 | regress_rls_bob   | my first novel
-   2 |  11 |      2 | regress_rls_bob   | my second novel
-   3 |  22 |      2 | regress_rls_bob   | my science fiction
-   4 |  44 |      1 | regress_rls_bob   | my first manga
-   5 |  44 |      2 | regress_rls_bob   | my second manga
-   6 |  22 |      1 | regress_rls_carol | great science fiction
-   7 |  33 |      2 | regress_rls_carol | great technology book
-   8 |  44 |      1 | regress_rls_carol | great manga
-   9 |  22 |      1 | regress_rls_dave  | awesome science fiction
-  10 |  33 |      2 | regress_rls_dave  | awesome technology book
-  11 |  33 |      1 | regress_rls_carol | hoge
-(11 rows)
-
-SELECT * FROM category;
- cid |      cname      
------+-----------------
-  11 | novel
-  22 | science fiction
-  33 | technology
-  44 | manga
-(4 rows)
-
--- RLS policy does not apply to table owner when RLS disabled.
-SET SESSION AUTHORIZATION regress_rls_alice;
-SET row_security TO OFF;
-SELECT * FROM document;
- did | cid | dlevel |      dauthor      |         dtitle          
------+-----+--------+-------------------+-------------------------
-   1 |  11 |      1 | regress_rls_bob   | my first novel
-   2 |  11 |      2 | regress_rls_bob   | my second novel
-   3 |  22 |      2 | regress_rls_bob   | my science fiction
-   4 |  44 |      1 | regress_rls_bob   | my first manga
-   5 |  44 |      2 | regress_rls_bob   | my second manga
-   6 |  22 |      1 | regress_rls_carol | great science fiction
-   7 |  33 |      2 | regress_rls_carol | great technology book
-   8 |  44 |      1 | regress_rls_carol | great manga
-   9 |  22 |      1 | regress_rls_dave  | awesome science fiction
-  10 |  33 |      2 | regress_rls_dave  | awesome technology book
-  11 |  33 |      1 | regress_rls_carol | hoge
-(11 rows)
-
-SELECT * FROM category;
- cid |      cname      
------+-----------------
-  11 | novel
-  22 | science fiction
-  33 | technology
-  44 | manga
-(4 rows)
-
---
--- Table inheritance and RLS policy
---
-SET SESSION AUTHORIZATION regress_rls_alice;
-SET row_security TO ON;
-CREATE TABLE t1 (a int, junk1 text, b text);
-ALTER TABLE t1 DROP COLUMN junk1;    -- just a disturbing factor
-GRANT ALL ON t1 TO public;
-COPY t1 FROM stdin;
-CREATE TABLE t2 (c float) INHERITS (t1);
-GRANT ALL ON t2 TO public;
-COPY t2 FROM stdin;
-CREATE TABLE t3 (c text, b text, a int);
-ALTER TABLE t3 INHERIT t1;
-GRANT ALL ON t3 TO public;
-COPY t3(a,b,c) FROM stdin;
-CREATE POLICY p1 ON t1 FOR ALL TO PUBLIC USING (a % 2 = 0); -- be even number
-CREATE POLICY p2 ON t2 FOR ALL TO PUBLIC USING (a % 2 = 1); -- be odd number
-ALTER TABLE t1 ENABLE ROW LEVEL SECURITY;
-ALTER TABLE t2 ENABLE ROW LEVEL SECURITY;
-SET SESSION AUTHORIZATION regress_rls_bob;
-SELECT * FROM t1;
- a |  b  
----+-----
- 2 | bbb
- 4 | dad
- 2 | bcd
- 4 | def
- 2 | yyy
-(5 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM t1;
-          QUERY PLAN           
--------------------------------
- Append
-   ->  Seq Scan on t1 t1_1
-         Filter: ((a % 2) = 0)
-   ->  Seq Scan on t2 t1_2
-         Filter: ((a % 2) = 0)
-   ->  Seq Scan on t3 t1_3
-         Filter: ((a % 2) = 0)
-(7 rows)
-
-SELECT * FROM t1 WHERE f_leak(b);
-NOTICE:  f_leak => bbb
-NOTICE:  f_leak => dad
-NOTICE:  f_leak => bcd
-NOTICE:  f_leak => def
-NOTICE:  f_leak => yyy
- a |  b  
----+-----
- 2 | bbb
- 4 | dad
- 2 | bcd
- 4 | def
- 2 | yyy
-(5 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM t1 WHERE f_leak(b);
-                  QUERY PLAN                   
------------------------------------------------
- Append
-   ->  Seq Scan on t1 t1_1
-         Filter: (((a % 2) = 0) AND f_leak(b))
-   ->  Seq Scan on t2 t1_2
-         Filter: (((a % 2) = 0) AND f_leak(b))
-   ->  Seq Scan on t3 t1_3
-         Filter: (((a % 2) = 0) AND f_leak(b))
-(7 rows)
-
--- reference to system column
-SELECT ctid, * FROM t1;
- ctid  | a |  b  
--------+---+-----
- (0,2) | 2 | bbb
- (0,4) | 4 | dad
- (0,2) | 2 | bcd
- (0,4) | 4 | def
- (0,2) | 2 | yyy
-(5 rows)
-
-EXPLAIN (COSTS OFF) SELECT *, t1 FROM t1;
-          QUERY PLAN           
--------------------------------
- Append
-   ->  Seq Scan on t1 t1_1
-         Filter: ((a % 2) = 0)
-   ->  Seq Scan on t2 t1_2
-         Filter: ((a % 2) = 0)
-   ->  Seq Scan on t3 t1_3
-         Filter: ((a % 2) = 0)
-(7 rows)
-
--- reference to whole-row reference
-SELECT *, t1 FROM t1;
- a |  b  |   t1    
----+-----+---------
- 2 | bbb | (2,bbb)
- 4 | dad | (4,dad)
- 2 | bcd | (2,bcd)
- 4 | def | (4,def)
- 2 | yyy | (2,yyy)
-(5 rows)
-
-EXPLAIN (COSTS OFF) SELECT *, t1 FROM t1;
-          QUERY PLAN           
--------------------------------
- Append
-   ->  Seq Scan on t1 t1_1
-         Filter: ((a % 2) = 0)
-   ->  Seq Scan on t2 t1_2
-         Filter: ((a % 2) = 0)
-   ->  Seq Scan on t3 t1_3
-         Filter: ((a % 2) = 0)
-(7 rows)
-
--- for share/update lock
-SELECT * FROM t1 FOR SHARE;
- a |  b  
----+-----
- 2 | bbb
- 4 | dad
- 2 | bcd
- 4 | def
- 2 | yyy
-(5 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM t1 FOR SHARE;
-             QUERY PLAN              
--------------------------------------
- LockRows
-   ->  Append
-         ->  Seq Scan on t1 t1_1
-               Filter: ((a % 2) = 0)
-         ->  Seq Scan on t2 t1_2
-               Filter: ((a % 2) = 0)
-         ->  Seq Scan on t3 t1_3
-               Filter: ((a % 2) = 0)
-(8 rows)
-
-SELECT * FROM t1 WHERE f_leak(b) FOR SHARE;
-NOTICE:  f_leak => bbb
-NOTICE:  f_leak => dad
-NOTICE:  f_leak => bcd
-NOTICE:  f_leak => def
-NOTICE:  f_leak => yyy
- a |  b  
----+-----
- 2 | bbb
- 4 | dad
- 2 | bcd
- 4 | def
- 2 | yyy
-(5 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM t1 WHERE f_leak(b) FOR SHARE;
-                     QUERY PLAN                      
------------------------------------------------------
- LockRows
-   ->  Append
-         ->  Seq Scan on t1 t1_1
-               Filter: (((a % 2) = 0) AND f_leak(b))
-         ->  Seq Scan on t2 t1_2
-               Filter: (((a % 2) = 0) AND f_leak(b))
-         ->  Seq Scan on t3 t1_3
-               Filter: (((a % 2) = 0) AND f_leak(b))
-(8 rows)
-
--- union all query
-SELECT a, b, ctid FROM t2 UNION ALL SELECT a, b, ctid FROM t3;
- a |  b  | ctid  
----+-----+-------
- 1 | abc | (0,1)
- 3 | cde | (0,3)
- 1 | xxx | (0,1)
- 2 | yyy | (0,2)
- 3 | zzz | (0,3)
-(5 rows)
-
-EXPLAIN (COSTS OFF) SELECT a, b, ctid FROM t2 UNION ALL SELECT a, b, ctid FROM t3;
-          QUERY PLAN           
--------------------------------
- Append
-   ->  Seq Scan on t2
-         Filter: ((a % 2) = 1)
-   ->  Seq Scan on t3
-(4 rows)
-
--- superuser is allowed to bypass RLS checks
-RESET SESSION AUTHORIZATION;
-SET row_security TO OFF;
-SELECT * FROM t1 WHERE f_leak(b);
-NOTICE:  f_leak => aba
-NOTICE:  f_leak => bbb
-NOTICE:  f_leak => ccc
-NOTICE:  f_leak => dad
-NOTICE:  f_leak => abc
-NOTICE:  f_leak => bcd
-NOTICE:  f_leak => cde
-NOTICE:  f_leak => def
-NOTICE:  f_leak => xxx
-NOTICE:  f_leak => yyy
-NOTICE:  f_leak => zzz
- a |  b  
----+-----
- 1 | aba
- 2 | bbb
- 3 | ccc
- 4 | dad
- 1 | abc
- 2 | bcd
- 3 | cde
- 4 | def
- 1 | xxx
- 2 | yyy
- 3 | zzz
-(11 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM t1 WHERE f_leak(b);
-        QUERY PLAN         
----------------------------
- Append
-   ->  Seq Scan on t1 t1_1
-         Filter: f_leak(b)
-   ->  Seq Scan on t2 t1_2
-         Filter: f_leak(b)
-   ->  Seq Scan on t3 t1_3
-         Filter: f_leak(b)
-(7 rows)
-
--- non-superuser with bypass privilege can bypass RLS policy when disabled
-SET SESSION AUTHORIZATION regress_rls_exempt_user;
-SET row_security TO OFF;
-SELECT * FROM t1 WHERE f_leak(b);
-NOTICE:  f_leak => aba
-NOTICE:  f_leak => bbb
-NOTICE:  f_leak => ccc
-NOTICE:  f_leak => dad
-NOTICE:  f_leak => abc
-NOTICE:  f_leak => bcd
-NOTICE:  f_leak => cde
-NOTICE:  f_leak => def
-NOTICE:  f_leak => xxx
-NOTICE:  f_leak => yyy
-NOTICE:  f_leak => zzz
- a |  b  
----+-----
- 1 | aba
- 2 | bbb
- 3 | ccc
- 4 | dad
- 1 | abc
- 2 | bcd
- 3 | cde
- 4 | def
- 1 | xxx
- 2 | yyy
- 3 | zzz
-(11 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM t1 WHERE f_leak(b);
-        QUERY PLAN         
----------------------------
- Append
-   ->  Seq Scan on t1 t1_1
-         Filter: f_leak(b)
-   ->  Seq Scan on t2 t1_2
-         Filter: f_leak(b)
-   ->  Seq Scan on t3 t1_3
-         Filter: f_leak(b)
-(7 rows)
-
---
--- Hyper Tables
---
-SET SESSION AUTHORIZATION regress_rls_alice;
-CREATE TABLE hyper_document (
-    did         int,
-    cid         int,
-    dlevel      int not null,
-    dauthor     name,
-    dtitle      text
-);
-GRANT ALL ON hyper_document TO public;
-SELECT public.create_hypertable('hyper_document', 'did', chunk_time_interval=>2);
-NOTICE:  adding not-null constraint to column "did"
-DETAIL:  Time dimensions cannot have NULL values.
-            create_hypertable            
------------------------------------------
- (2,regress_rls_schema,hyper_document,t)
-(1 row)
-
-INSERT INTO hyper_document VALUES
-    ( 1, 11, 1, 'regress_rls_bob', 'my first novel'),
-    ( 2, 11, 2, 'regress_rls_bob', 'my second novel'),
-    ( 3, 99, 2, 'regress_rls_bob', 'my science textbook'),
-    ( 4, 55, 1, 'regress_rls_bob', 'my first satire'),
-    ( 5, 99, 2, 'regress_rls_bob', 'my history book'),
-    ( 6, 11, 1, 'regress_rls_carol', 'great science fiction'),
-    ( 7, 99, 2, 'regress_rls_carol', 'great technology book'),
-    ( 8, 55, 2, 'regress_rls_carol', 'great satire'),
-    ( 9, 11, 1, 'regress_rls_dave', 'awesome science fiction'),
-    (10, 99, 2, 'regress_rls_dave', 'awesome technology book');
-ALTER TABLE hyper_document ENABLE ROW LEVEL SECURITY;
-GRANT ALL ON _timescaledb_internal._hyper_2_9_chunk TO public;
--- Create policy on parent
--- user's security level must be higher than or equal to document's
-CREATE POLICY pp1 ON hyper_document AS PERMISSIVE
-    USING (dlevel <= (SELECT seclv FROM uaccount WHERE pguser = current_user));
--- Dave is only allowed to see cid < 55
-CREATE POLICY pp1r ON hyper_document AS RESTRICTIVE TO regress_rls_dave
-    USING (cid < 55);
-\d+ hyper_document
-                         Table "regress_rls_schema.hyper_document"
- Column  |  Type   | Collation | Nullable | Default | Storage  | Stats target | Description 
----------+---------+-----------+----------+---------+----------+--------------+-------------
- did     | integer |           | not null |         | plain    |              | 
- cid     | integer |           |          |         | plain    |              | 
- dlevel  | integer |           | not null |         | plain    |              | 
- dauthor | name    |           |          |         | plain    |              | 
- dtitle  | text    |           |          |         | extended |              | 
-Indexes:
-    "hyper_document_did_idx" btree (did DESC)
-Policies:
-    POLICY "pp1"
-      USING ((dlevel <= ( SELECT uaccount.seclv
-   FROM uaccount
-  WHERE (uaccount.pguser = CURRENT_USER))))
-    POLICY "pp1r" AS RESTRICTIVE
-      TO regress_rls_dave
-      USING ((cid < 55))
-Triggers:
-    ts_insert_blocker BEFORE INSERT ON hyper_document FOR EACH ROW EXECUTE FUNCTION _timescaledb_internal.insert_blocker()
-Child tables: _timescaledb_internal._hyper_2_10_chunk,
-              _timescaledb_internal._hyper_2_11_chunk,
-              _timescaledb_internal._hyper_2_12_chunk,
-              _timescaledb_internal._hyper_2_13_chunk,
-              _timescaledb_internal._hyper_2_14_chunk,
-              _timescaledb_internal._hyper_2_9_chunk
-
-SELECT * FROM pg_policies WHERE schemaname = 'regress_rls_schema' AND tablename like '%hyper_document%' ORDER BY policyname;
-     schemaname     |   tablename    | policyname | permissive  |       roles        | cmd |                    qual                    | with_check 
---------------------+----------------+------------+-------------+--------------------+-----+--------------------------------------------+------------
- regress_rls_schema | hyper_document | pp1        | PERMISSIVE  | {public}           | ALL | (dlevel <= ( SELECT uaccount.seclv        +| 
-                    |                |            |             |                    |     |    FROM uaccount                          +| 
-                    |                |            |             |                    |     |   WHERE (uaccount.pguser = CURRENT_USER))) | 
- regress_rls_schema | hyper_document | pp1r       | RESTRICTIVE | {regress_rls_dave} | ALL | (cid < 55)                                 | 
-(2 rows)
-
--- viewpoint from regress_rls_bob
-SET SESSION AUTHORIZATION regress_rls_bob;
-SET row_security TO ON;
-SELECT * FROM hyper_document WHERE f_leak(dtitle) ORDER BY did;
-NOTICE:  f_leak => my first novel
-NOTICE:  f_leak => my first satire
-NOTICE:  f_leak => great science fiction
-NOTICE:  f_leak => awesome science fiction
- did | cid | dlevel |      dauthor      |         dtitle          
------+-----+--------+-------------------+-------------------------
-   1 |  11 |      1 | regress_rls_bob   | my first novel
-   4 |  55 |      1 | regress_rls_bob   | my first satire
-   6 |  11 |      1 | regress_rls_carol | great science fiction
-   9 |  11 |      1 | regress_rls_dave  | awesome science fiction
-(4 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM hyper_document WHERE f_leak(dtitle);
-                      QUERY PLAN                      
-------------------------------------------------------
- Custom Scan (ChunkAppend) on hyper_document
-   Chunks excluded during startup: 0
-   InitPlan 1 (returns $0)
-     ->  Index Scan using uaccount_pkey on uaccount
-           Index Cond: (pguser = CURRENT_USER)
-   ->  Seq Scan on hyper_document hyper_document_1
-         Filter: ((dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_9_chunk hyper_document_2
-         Filter: ((dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_10_chunk hyper_document_3
-         Filter: ((dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_11_chunk hyper_document_4
-         Filter: ((dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_12_chunk hyper_document_5
-         Filter: ((dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_13_chunk hyper_document_6
-         Filter: ((dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_14_chunk hyper_document_7
-         Filter: ((dlevel <= $0) AND f_leak(dtitle))
-(19 rows)
-
--- viewpoint from regress_rls_carol
-SET SESSION AUTHORIZATION regress_rls_carol;
-SELECT * FROM hyper_document WHERE f_leak(dtitle) ORDER BY did;
-NOTICE:  f_leak => my first novel
-NOTICE:  f_leak => my second novel
-NOTICE:  f_leak => my science textbook
-NOTICE:  f_leak => my first satire
-NOTICE:  f_leak => my history book
-NOTICE:  f_leak => great science fiction
-NOTICE:  f_leak => great technology book
-NOTICE:  f_leak => great satire
-NOTICE:  f_leak => awesome science fiction
-NOTICE:  f_leak => awesome technology book
- did | cid | dlevel |      dauthor      |         dtitle          
------+-----+--------+-------------------+-------------------------
-   1 |  11 |      1 | regress_rls_bob   | my first novel
-   2 |  11 |      2 | regress_rls_bob   | my second novel
-   3 |  99 |      2 | regress_rls_bob   | my science textbook
-   4 |  55 |      1 | regress_rls_bob   | my first satire
-   5 |  99 |      2 | regress_rls_bob   | my history book
-   6 |  11 |      1 | regress_rls_carol | great science fiction
-   7 |  99 |      2 | regress_rls_carol | great technology book
-   8 |  55 |      2 | regress_rls_carol | great satire
-   9 |  11 |      1 | regress_rls_dave  | awesome science fiction
-  10 |  99 |      2 | regress_rls_dave  | awesome technology book
-(10 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM hyper_document WHERE f_leak(dtitle);
-                      QUERY PLAN                      
-------------------------------------------------------
- Custom Scan (ChunkAppend) on hyper_document
-   Chunks excluded during startup: 0
-   InitPlan 1 (returns $0)
-     ->  Index Scan using uaccount_pkey on uaccount
-           Index Cond: (pguser = CURRENT_USER)
-   ->  Seq Scan on hyper_document hyper_document_1
-         Filter: ((dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_9_chunk hyper_document_2
-         Filter: ((dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_10_chunk hyper_document_3
-         Filter: ((dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_11_chunk hyper_document_4
-         Filter: ((dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_12_chunk hyper_document_5
-         Filter: ((dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_13_chunk hyper_document_6
-         Filter: ((dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_14_chunk hyper_document_7
-         Filter: ((dlevel <= $0) AND f_leak(dtitle))
-(19 rows)
-
--- viewpoint from regress_rls_dave
-SET SESSION AUTHORIZATION regress_rls_dave;
-SELECT * FROM hyper_document WHERE f_leak(dtitle) ORDER BY did;
-NOTICE:  f_leak => my first novel
-NOTICE:  f_leak => my second novel
-NOTICE:  f_leak => great science fiction
-NOTICE:  f_leak => awesome science fiction
- did | cid | dlevel |      dauthor      |         dtitle          
------+-----+--------+-------------------+-------------------------
-   1 |  11 |      1 | regress_rls_bob   | my first novel
-   2 |  11 |      2 | regress_rls_bob   | my second novel
-   6 |  11 |      1 | regress_rls_carol | great science fiction
-   9 |  11 |      1 | regress_rls_dave  | awesome science fiction
-(4 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM hyper_document WHERE f_leak(dtitle);
-                             QUERY PLAN                             
---------------------------------------------------------------------
- Custom Scan (ChunkAppend) on hyper_document
-   Chunks excluded during startup: 0
-   InitPlan 1 (returns $0)
-     ->  Index Scan using uaccount_pkey on uaccount
-           Index Cond: (pguser = CURRENT_USER)
-   ->  Seq Scan on hyper_document hyper_document_1
-         Filter: ((cid < 55) AND (dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_9_chunk hyper_document_2
-         Filter: ((cid < 55) AND (dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_10_chunk hyper_document_3
-         Filter: ((cid < 55) AND (dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_11_chunk hyper_document_4
-         Filter: ((cid < 55) AND (dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_12_chunk hyper_document_5
-         Filter: ((cid < 55) AND (dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_13_chunk hyper_document_6
-         Filter: ((cid < 55) AND (dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_14_chunk hyper_document_7
-         Filter: ((cid < 55) AND (dlevel <= $0) AND f_leak(dtitle))
-(19 rows)
-
--- pp1 ERROR
-INSERT INTO hyper_document VALUES (1, 11, 5, 'regress_rls_dave', 'testing pp1'); -- fail
-ERROR:  new row violates row-level security policy for table "hyper_document"
--- pp1r ERROR
-INSERT INTO hyper_document VALUES (1, 99, 1, 'regress_rls_dave', 'testing pp1r'); -- fail
-ERROR:  new row violates row-level security policy "pp1r" for table "hyper_document"
--- Show that RLS policy does not apply for direct inserts to children
--- This should fail with RLS POLICY pp1r violation.
-INSERT INTO hyper_document VALUES (1, 55, 1, 'regress_rls_dave', 'testing RLS with hypertables'); -- fail
-ERROR:  new row violates row-level security policy "pp1r" for table "hyper_document"
--- But this should succeed.
-INSERT INTO _timescaledb_internal._hyper_2_9_chunk VALUES (1, 55, 1, 'regress_rls_dave', 'testing RLS with hypertables'); -- success
--- We still cannot see the row using the parent
-SELECT * FROM hyper_document WHERE f_leak(dtitle) ORDER BY did, cid;
-NOTICE:  f_leak => my first novel
-NOTICE:  f_leak => my second novel
-NOTICE:  f_leak => great science fiction
-NOTICE:  f_leak => awesome science fiction
- did | cid | dlevel |      dauthor      |         dtitle          
------+-----+--------+-------------------+-------------------------
-   1 |  11 |      1 | regress_rls_bob   | my first novel
-   2 |  11 |      2 | regress_rls_bob   | my second novel
-   6 |  11 |      1 | regress_rls_carol | great science fiction
-   9 |  11 |      1 | regress_rls_dave  | awesome science fiction
-(4 rows)
-
--- But we can if we look directly
-SELECT * FROM _timescaledb_internal._hyper_2_9_chunk WHERE f_leak(dtitle) ORDER BY did, cid;
-NOTICE:  f_leak => my first novel
-NOTICE:  f_leak => testing RLS with hypertables
- did | cid | dlevel |     dauthor      |            dtitle            
------+-----+--------+------------------+------------------------------
-   1 |  11 |      1 | regress_rls_bob  | my first novel
-   1 |  55 |      1 | regress_rls_dave | testing RLS with hypertables
-(2 rows)
-
--- Turn on RLS and create policy on child to show RLS is checked before constraints
-SET SESSION AUTHORIZATION regress_rls_alice;
-ALTER TABLE _timescaledb_internal._hyper_2_9_chunk ENABLE ROW LEVEL SECURITY;
-CREATE POLICY pp3 ON _timescaledb_internal._hyper_2_9_chunk AS RESTRICTIVE
-    USING (cid < 55);
--- This should fail with RLS violation now.
-SET SESSION AUTHORIZATION regress_rls_dave;
-INSERT INTO _timescaledb_internal._hyper_2_9_chunk VALUES (1, 55, 1, 'regress_rls_dave', 'testing RLS with hypertables - round 2'); -- fail
-ERROR:  new row violates row-level security policy for table "_hyper_2_9_chunk"
--- And now we cannot see directly into the partition either, due to RLS
-SELECT * FROM _timescaledb_internal._hyper_2_9_chunk WHERE f_leak(dtitle) ORDER BY did, cid;
- did | cid | dlevel | dauthor | dtitle 
------+-----+--------+---------+--------
-(0 rows)
-
--- The parent looks same as before
--- viewpoint from regress_rls_dave
-SELECT * FROM hyper_document WHERE f_leak(dtitle) ORDER BY did, cid;
-NOTICE:  f_leak => my first novel
-NOTICE:  f_leak => my second novel
-NOTICE:  f_leak => great science fiction
-NOTICE:  f_leak => awesome science fiction
- did | cid | dlevel |      dauthor      |         dtitle          
------+-----+--------+-------------------+-------------------------
-   1 |  11 |      1 | regress_rls_bob   | my first novel
-   2 |  11 |      2 | regress_rls_bob   | my second novel
-   6 |  11 |      1 | regress_rls_carol | great science fiction
-   9 |  11 |      1 | regress_rls_dave  | awesome science fiction
-(4 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM hyper_document WHERE f_leak(dtitle);
-                             QUERY PLAN                             
---------------------------------------------------------------------
- Custom Scan (ChunkAppend) on hyper_document
-   Chunks excluded during startup: 0
-   InitPlan 1 (returns $0)
-     ->  Index Scan using uaccount_pkey on uaccount
-           Index Cond: (pguser = CURRENT_USER)
-   ->  Seq Scan on hyper_document hyper_document_1
-         Filter: ((cid < 55) AND (dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_9_chunk hyper_document_2
-         Filter: ((cid < 55) AND (dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_10_chunk hyper_document_3
-         Filter: ((cid < 55) AND (dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_11_chunk hyper_document_4
-         Filter: ((cid < 55) AND (dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_12_chunk hyper_document_5
-         Filter: ((cid < 55) AND (dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_13_chunk hyper_document_6
-         Filter: ((cid < 55) AND (dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_14_chunk hyper_document_7
-         Filter: ((cid < 55) AND (dlevel <= $0) AND f_leak(dtitle))
-(19 rows)
-
--- viewpoint from regress_rls_carol
-SET SESSION AUTHORIZATION regress_rls_carol;
-SELECT * FROM hyper_document WHERE f_leak(dtitle) ORDER BY did, cid;
-NOTICE:  f_leak => my first novel
-NOTICE:  f_leak => testing RLS with hypertables
-NOTICE:  f_leak => my second novel
-NOTICE:  f_leak => my science textbook
-NOTICE:  f_leak => my first satire
-NOTICE:  f_leak => my history book
-NOTICE:  f_leak => great science fiction
-NOTICE:  f_leak => great technology book
-NOTICE:  f_leak => great satire
-NOTICE:  f_leak => awesome science fiction
-NOTICE:  f_leak => awesome technology book
- did | cid | dlevel |      dauthor      |            dtitle            
------+-----+--------+-------------------+------------------------------
-   1 |  11 |      1 | regress_rls_bob   | my first novel
-   1 |  55 |      1 | regress_rls_dave  | testing RLS with hypertables
-   2 |  11 |      2 | regress_rls_bob   | my second novel
-   3 |  99 |      2 | regress_rls_bob   | my science textbook
-   4 |  55 |      1 | regress_rls_bob   | my first satire
-   5 |  99 |      2 | regress_rls_bob   | my history book
-   6 |  11 |      1 | regress_rls_carol | great science fiction
-   7 |  99 |      2 | regress_rls_carol | great technology book
-   8 |  55 |      2 | regress_rls_carol | great satire
-   9 |  11 |      1 | regress_rls_dave  | awesome science fiction
-  10 |  99 |      2 | regress_rls_dave  | awesome technology book
-(11 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM hyper_document WHERE f_leak(dtitle);
-                      QUERY PLAN                      
-------------------------------------------------------
- Custom Scan (ChunkAppend) on hyper_document
-   Chunks excluded during startup: 0
-   InitPlan 1 (returns $0)
-     ->  Index Scan using uaccount_pkey on uaccount
-           Index Cond: (pguser = CURRENT_USER)
-   ->  Seq Scan on hyper_document hyper_document_1
-         Filter: ((dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_9_chunk hyper_document_2
-         Filter: ((dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_10_chunk hyper_document_3
-         Filter: ((dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_11_chunk hyper_document_4
-         Filter: ((dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_12_chunk hyper_document_5
-         Filter: ((dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_13_chunk hyper_document_6
-         Filter: ((dlevel <= $0) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_14_chunk hyper_document_7
-         Filter: ((dlevel <= $0) AND f_leak(dtitle))
-(19 rows)
-
--- only owner can change policies
-ALTER POLICY pp1 ON hyper_document USING (true);    --fail
-ERROR:  must be owner of table hyper_document
-DROP POLICY pp1 ON hyper_document;                  --fail
-ERROR:  must be owner of relation hyper_document
-SET SESSION AUTHORIZATION regress_rls_alice;
-ALTER POLICY pp1 ON hyper_document USING (dauthor = current_user);
--- viewpoint from regress_rls_bob again
-SET SESSION AUTHORIZATION regress_rls_bob;
-SELECT * FROM hyper_document WHERE f_leak(dtitle) ORDER BY did, cid;
-NOTICE:  f_leak => my first novel
-NOTICE:  f_leak => my second novel
-NOTICE:  f_leak => my science textbook
-NOTICE:  f_leak => my first satire
-NOTICE:  f_leak => my history book
- did | cid | dlevel |     dauthor     |       dtitle        
------+-----+--------+-----------------+---------------------
-   1 |  11 |      1 | regress_rls_bob | my first novel
-   2 |  11 |      2 | regress_rls_bob | my second novel
-   3 |  99 |      2 | regress_rls_bob | my science textbook
-   4 |  55 |      1 | regress_rls_bob | my first satire
-   5 |  99 |      2 | regress_rls_bob | my history book
-(5 rows)
-
--- viewpoint from rls_regres_carol again
-SET SESSION AUTHORIZATION regress_rls_carol;
-SELECT * FROM hyper_document WHERE f_leak(dtitle) ORDER BY did, cid;
-NOTICE:  f_leak => great science fiction
-NOTICE:  f_leak => great technology book
-NOTICE:  f_leak => great satire
- did | cid | dlevel |      dauthor      |        dtitle         
------+-----+--------+-------------------+-----------------------
-   6 |  11 |      1 | regress_rls_carol | great science fiction
-   7 |  99 |      2 | regress_rls_carol | great technology book
-   8 |  55 |      2 | regress_rls_carol | great satire
-(3 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM hyper_document WHERE f_leak(dtitle);
-                          QUERY PLAN                           
----------------------------------------------------------------
- Custom Scan (ChunkAppend) on hyper_document
-   Chunks excluded during startup: 0
-   ->  Seq Scan on hyper_document hyper_document_1
-         Filter: ((dauthor = CURRENT_USER) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_9_chunk hyper_document_2
-         Filter: ((dauthor = CURRENT_USER) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_10_chunk hyper_document_3
-         Filter: ((dauthor = CURRENT_USER) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_11_chunk hyper_document_4
-         Filter: ((dauthor = CURRENT_USER) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_12_chunk hyper_document_5
-         Filter: ((dauthor = CURRENT_USER) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_13_chunk hyper_document_6
-         Filter: ((dauthor = CURRENT_USER) AND f_leak(dtitle))
-   ->  Seq Scan on _hyper_2_14_chunk hyper_document_7
-         Filter: ((dauthor = CURRENT_USER) AND f_leak(dtitle))
-(16 rows)
-
--- database superuser does bypass RLS policy when enabled
-RESET SESSION AUTHORIZATION;
-SET row_security TO ON;
-SELECT * FROM hyper_document ORDER BY did, cid;
- did | cid | dlevel |      dauthor      |            dtitle            
------+-----+--------+-------------------+------------------------------
-   1 |  11 |      1 | regress_rls_bob   | my first novel
-   1 |  55 |      1 | regress_rls_dave  | testing RLS with hypertables
-   2 |  11 |      2 | regress_rls_bob   | my second novel
-   3 |  99 |      2 | regress_rls_bob   | my science textbook
-   4 |  55 |      1 | regress_rls_bob   | my first satire
-   5 |  99 |      2 | regress_rls_bob   | my history book
-   6 |  11 |      1 | regress_rls_carol | great science fiction
-   7 |  99 |      2 | regress_rls_carol | great technology book
-   8 |  55 |      2 | regress_rls_carol | great satire
-   9 |  11 |      1 | regress_rls_dave  | awesome science fiction
-  10 |  99 |      2 | regress_rls_dave  | awesome technology book
-(11 rows)
-
-SELECT * FROM _timescaledb_internal._hyper_2_9_chunk ORDER BY did, cid;
- did | cid | dlevel |     dauthor      |            dtitle            
------+-----+--------+------------------+------------------------------
-   1 |  11 |      1 | regress_rls_bob  | my first novel
-   1 |  55 |      1 | regress_rls_dave | testing RLS with hypertables
-(2 rows)
-
--- database non-superuser with bypass privilege can bypass RLS policy when disabled
-SET SESSION AUTHORIZATION regress_rls_exempt_user;
-SET row_security TO OFF;
-SELECT * FROM hyper_document ORDER BY did, cid;
- did | cid | dlevel |      dauthor      |            dtitle            
------+-----+--------+-------------------+------------------------------
-   1 |  11 |      1 | regress_rls_bob   | my first novel
-   1 |  55 |      1 | regress_rls_dave  | testing RLS with hypertables
-   2 |  11 |      2 | regress_rls_bob   | my second novel
-   3 |  99 |      2 | regress_rls_bob   | my science textbook
-   4 |  55 |      1 | regress_rls_bob   | my first satire
-   5 |  99 |      2 | regress_rls_bob   | my history book
-   6 |  11 |      1 | regress_rls_carol | great science fiction
-   7 |  99 |      2 | regress_rls_carol | great technology book
-   8 |  55 |      2 | regress_rls_carol | great satire
-   9 |  11 |      1 | regress_rls_dave  | awesome science fiction
-  10 |  99 |      2 | regress_rls_dave  | awesome technology book
-(11 rows)
-
-SELECT * FROM _timescaledb_internal._hyper_2_9_chunk ORDER BY did, cid;
- did | cid | dlevel |     dauthor      |            dtitle            
------+-----+--------+------------------+------------------------------
-   1 |  11 |      1 | regress_rls_bob  | my first novel
-   1 |  55 |      1 | regress_rls_dave | testing RLS with hypertables
-(2 rows)
-
--- RLS policy does not apply to table owner when RLS enabled.
-SET SESSION AUTHORIZATION regress_rls_alice;
-SET row_security TO ON;
-SELECT * FROM hyper_document ORDER BY did, cid;
- did | cid | dlevel |      dauthor      |            dtitle            
------+-----+--------+-------------------+------------------------------
-   1 |  11 |      1 | regress_rls_bob   | my first novel
-   1 |  55 |      1 | regress_rls_dave  | testing RLS with hypertables
-   2 |  11 |      2 | regress_rls_bob   | my second novel
-   3 |  99 |      2 | regress_rls_bob   | my science textbook
-   4 |  55 |      1 | regress_rls_bob   | my first satire
-   5 |  99 |      2 | regress_rls_bob   | my history book
-   6 |  11 |      1 | regress_rls_carol | great science fiction
-   7 |  99 |      2 | regress_rls_carol | great technology book
-   8 |  55 |      2 | regress_rls_carol | great satire
-   9 |  11 |      1 | regress_rls_dave  | awesome science fiction
-  10 |  99 |      2 | regress_rls_dave  | awesome technology book
-(11 rows)
-
-SELECT * FROM _timescaledb_internal._hyper_2_9_chunk ORDER BY did, cid;
- did | cid | dlevel |     dauthor      |            dtitle            
------+-----+--------+------------------+------------------------------
-   1 |  11 |      1 | regress_rls_bob  | my first novel
-   1 |  55 |      1 | regress_rls_dave | testing RLS with hypertables
-(2 rows)
-
--- When RLS disabled, other users get ERROR.
-SET SESSION AUTHORIZATION regress_rls_dave;
-SET row_security TO OFF;
-SELECT * FROM hyper_document ORDER BY did, cid;
-ERROR:  query would be affected by row-level security policy for table "hyper_document"
-SELECT * FROM _timescaledb_internal._hyper_2_9_chunk ORDER BY did, cid;
-ERROR:  query would be affected by row-level security policy for table "_hyper_2_9_chunk"
--- Check behavior with a policy that uses a SubPlan not an InitPlan.
-SET SESSION AUTHORIZATION regress_rls_alice;
-SET row_security TO ON;
-CREATE POLICY pp3 ON hyper_document AS RESTRICTIVE
-    USING ((SELECT dlevel <= seclv FROM uaccount WHERE pguser = current_user));
-SET SESSION AUTHORIZATION regress_rls_carol;
-INSERT INTO hyper_document VALUES (100, 11, 5, 'regress_rls_carol', 'testing pp3'); -- fail
-ERROR:  new row violates row-level security policy "pp3" for table "hyper_document"
------ Dependencies -----
-SET SESSION AUTHORIZATION regress_rls_alice;
-SET row_security TO ON;
-CREATE TABLE dependee (x integer, y integer);
-SELECT public.create_hypertable('dependee', 'x', chunk_time_interval=>2);
-NOTICE:  adding not-null constraint to column "x"
-DETAIL:  Time dimensions cannot have NULL values.
-         create_hypertable         
------------------------------------
- (3,regress_rls_schema,dependee,t)
-(1 row)
-
-CREATE TABLE dependent (x integer, y integer);
-SELECT public.create_hypertable('dependent', 'x', chunk_time_interval=>2);
-NOTICE:  adding not-null constraint to column "x"
-DETAIL:  Time dimensions cannot have NULL values.
-         create_hypertable          
-------------------------------------
- (4,regress_rls_schema,dependent,t)
-(1 row)
-
-CREATE POLICY d1 ON dependent FOR ALL
-    TO PUBLIC
-    USING (x = (SELECT d.x FROM dependee d WHERE d.y = y));
-DROP TABLE dependee; -- Should fail without CASCADE due to dependency on row security qual?
-ERROR:  cannot drop table dependee because other objects depend on it
-DETAIL:  policy d1 on table dependent depends on table dependee
-HINT:  Use DROP ... CASCADE to drop the dependent objects too.
-DROP TABLE dependee CASCADE;
-NOTICE:  drop cascades to policy d1 on table dependent
-EXPLAIN (COSTS OFF) SELECT * FROM dependent; -- After drop, should be unqualified
-      QUERY PLAN       
------------------------
- Seq Scan on dependent
-(1 row)
-
------   RECURSION    ----
---
--- Simple recursion
---
-SET SESSION AUTHORIZATION regress_rls_alice;
-CREATE TABLE rec1 (x integer, y integer);
-SELECT public.create_hypertable('rec1', 'x', chunk_time_interval=>2);
-NOTICE:  adding not-null constraint to column "x"
-DETAIL:  Time dimensions cannot have NULL values.
-       create_hypertable       
--------------------------------
- (5,regress_rls_schema,rec1,t)
-(1 row)
-
-CREATE POLICY r1 ON rec1 USING (x = (SELECT r.x FROM rec1 r WHERE y = r.y));
-ALTER TABLE rec1 ENABLE ROW LEVEL SECURITY;
-SET SESSION AUTHORIZATION regress_rls_bob;
-SELECT * FROM rec1; -- fail, direct recursion
-ERROR:  infinite recursion detected in policy for relation "rec1"
---
--- Mutual recursion
---
-SET SESSION AUTHORIZATION regress_rls_alice;
-CREATE TABLE rec2 (a integer, b integer);
-SELECT public.create_hypertable('rec2', 'x', chunk_time_interval=>2);
-ERROR:  column "x" does not exist
-ALTER POLICY r1 ON rec1 USING (x = (SELECT a FROM rec2 WHERE b = y));
-CREATE POLICY r2 ON rec2 USING (a = (SELECT x FROM rec1 WHERE y = b));
-ALTER TABLE rec2 ENABLE ROW LEVEL SECURITY;
-SET SESSION AUTHORIZATION regress_rls_bob;
-SELECT * FROM rec1;    -- fail, mutual recursion
-ERROR:  infinite recursion detected in policy for relation "rec1"
---
--- Mutual recursion via views
---
-SET SESSION AUTHORIZATION regress_rls_bob;
-CREATE VIEW rec1v AS SELECT * FROM rec1;
-CREATE VIEW rec2v AS SELECT * FROM rec2;
-SET SESSION AUTHORIZATION regress_rls_alice;
-ALTER POLICY r1 ON rec1 USING (x = (SELECT a FROM rec2v WHERE b = y));
-ALTER POLICY r2 ON rec2 USING (a = (SELECT x FROM rec1v WHERE y = b));
-SET SESSION AUTHORIZATION regress_rls_bob;
-SELECT * FROM rec1;    -- fail, mutual recursion via views
-ERROR:  infinite recursion detected in policy for relation "rec1"
---
--- Mutual recursion via .s.b views
---
-SET SESSION AUTHORIZATION regress_rls_bob;
-\set VERBOSITY terse \\ -- suppress cascade details
-DROP VIEW rec1v, rec2v CASCADE;
-NOTICE:  drop cascades to 2 other objects
-\set VERBOSITY default
-CREATE VIEW rec1v WITH (security_barrier) AS SELECT * FROM rec1;
-CREATE VIEW rec2v WITH (security_barrier) AS SELECT * FROM rec2;
-SET SESSION AUTHORIZATION regress_rls_alice;
-CREATE POLICY r1 ON rec1 USING (x = (SELECT a FROM rec2v WHERE b = y));
-CREATE POLICY r2 ON rec2 USING (a = (SELECT x FROM rec1v WHERE y = b));
-SET SESSION AUTHORIZATION regress_rls_bob;
-SELECT * FROM rec1;    -- fail, mutual recursion via s.b. views
-ERROR:  infinite recursion detected in policy for relation "rec1"
---
--- recursive RLS and VIEWs in policy
---
-SET SESSION AUTHORIZATION regress_rls_alice;
-CREATE TABLE s1 (a int, b text);
-SELECT public.create_hypertable('s1', 'a', chunk_time_interval=>2);
-NOTICE:  adding not-null constraint to column "a"
-DETAIL:  Time dimensions cannot have NULL values.
-      create_hypertable      
------------------------------
- (6,regress_rls_schema,s1,t)
-(1 row)
-
-INSERT INTO s1 (SELECT x, md5(x::text) FROM generate_series(-10,10) x);
-CREATE TABLE s2 (x int, y text);
-SELECT public.create_hypertable('s2', 'x', chunk_time_interval=>2);
-NOTICE:  adding not-null constraint to column "x"
-DETAIL:  Time dimensions cannot have NULL values.
-      create_hypertable      
------------------------------
- (7,regress_rls_schema,s2,t)
-(1 row)
-
-INSERT INTO s2 (SELECT x, md5(x::text) FROM generate_series(-6,6) x);
-GRANT SELECT ON s1, s2 TO regress_rls_bob;
-CREATE POLICY p1 ON s1 USING (a in (select x from s2 where y like '%2f%'));
-CREATE POLICY p2 ON s2 USING (x in (select a from s1 where b like '%22%'));
-CREATE POLICY p3 ON s1 FOR INSERT WITH CHECK (a = (SELECT a FROM s1));
-ALTER TABLE s1 ENABLE ROW LEVEL SECURITY;
-ALTER TABLE s2 ENABLE ROW LEVEL SECURITY;
-SET SESSION AUTHORIZATION regress_rls_bob;
-CREATE VIEW v2 AS SELECT * FROM s2 WHERE y like '%af%';
-SELECT * FROM s1 WHERE f_leak(b); -- fail (infinite recursion)
-ERROR:  infinite recursion detected in policy for relation "s1"
-INSERT INTO s1 VALUES (1, 'foo'); -- fail (infinite recursion)
-ERROR:  infinite recursion detected in policy for relation "s1"
-SET SESSION AUTHORIZATION regress_rls_alice;
-DROP POLICY p3 on s1;
-ALTER POLICY p2 ON s2 USING (x % 2 = 0);
-SET SESSION AUTHORIZATION regress_rls_bob;
-SELECT * FROM s1 WHERE f_leak(b);	-- OK
-NOTICE:  f_leak => c81e728d9d4c2f636f067f89cc14862c
-NOTICE:  f_leak => a87ff679a2f3e71d9181a67b7542122c
- a |                b                 
----+----------------------------------
- 2 | c81e728d9d4c2f636f067f89cc14862c
- 4 | a87ff679a2f3e71d9181a67b7542122c
-(2 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM only s1 WHERE f_leak(b);
-                           QUERY PLAN                            
------------------------------------------------------------------
- Seq Scan on s1
-   Filter: ((hashed SubPlan 1) AND f_leak(b))
-   SubPlan 1
-     ->  Append
-           ->  Seq Scan on s2 s2_1
-                 Filter: (((x % 2) = 0) AND (y ~~ '%2f%'::text))
-           ->  Seq Scan on _hyper_7_27_chunk s2_2
-                 Filter: (((x % 2) = 0) AND (y ~~ '%2f%'::text))
-           ->  Seq Scan on _hyper_7_28_chunk s2_3
-                 Filter: (((x % 2) = 0) AND (y ~~ '%2f%'::text))
-           ->  Seq Scan on _hyper_7_29_chunk s2_4
-                 Filter: (((x % 2) = 0) AND (y ~~ '%2f%'::text))
-           ->  Seq Scan on _hyper_7_30_chunk s2_5
-                 Filter: (((x % 2) = 0) AND (y ~~ '%2f%'::text))
-           ->  Seq Scan on _hyper_7_31_chunk s2_6
-                 Filter: (((x % 2) = 0) AND (y ~~ '%2f%'::text))
-           ->  Seq Scan on _hyper_7_32_chunk s2_7
-                 Filter: (((x % 2) = 0) AND (y ~~ '%2f%'::text))
-           ->  Seq Scan on _hyper_7_33_chunk s2_8
-                 Filter: (((x % 2) = 0) AND (y ~~ '%2f%'::text))
-(20 rows)
-
-SET SESSION AUTHORIZATION regress_rls_alice;
-ALTER POLICY p1 ON s1 USING (a in (select x from v2)); -- using VIEW in RLS policy
-SET SESSION AUTHORIZATION regress_rls_bob;
-SELECT * FROM s1 WHERE f_leak(b);	-- OK
-NOTICE:  f_leak => 0267aaf632e87a63288a08331f22c7c3
-NOTICE:  f_leak => 1679091c5a880faf6fb5e6087eb1b2dc
- a  |                b                 
-----+----------------------------------
- -4 | 0267aaf632e87a63288a08331f22c7c3
-  6 | 1679091c5a880faf6fb5e6087eb1b2dc
-(2 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM s1 WHERE f_leak(b);
-                              QUERY PLAN                               
------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on s1
-   Chunks excluded during startup: 0
-   ->  Seq Scan on s1 s1_1
-         Filter: ((hashed SubPlan 1) AND f_leak(b))
-         SubPlan 1
-           ->  Append
-                 ->  Seq Scan on s2 s2_1
-                       Filter: (((x % 2) = 0) AND (y ~~ '%af%'::text))
-                 ->  Seq Scan on _hyper_7_27_chunk s2_2
-                       Filter: (((x % 2) = 0) AND (y ~~ '%af%'::text))
-                 ->  Seq Scan on _hyper_7_28_chunk s2_3
-                       Filter: (((x % 2) = 0) AND (y ~~ '%af%'::text))
-                 ->  Seq Scan on _hyper_7_29_chunk s2_4
-                       Filter: (((x % 2) = 0) AND (y ~~ '%af%'::text))
-                 ->  Seq Scan on _hyper_7_30_chunk s2_5
-                       Filter: (((x % 2) = 0) AND (y ~~ '%af%'::text))
-                 ->  Seq Scan on _hyper_7_31_chunk s2_6
-                       Filter: (((x % 2) = 0) AND (y ~~ '%af%'::text))
-                 ->  Seq Scan on _hyper_7_32_chunk s2_7
-                       Filter: (((x % 2) = 0) AND (y ~~ '%af%'::text))
-                 ->  Seq Scan on _hyper_7_33_chunk s2_8
-                       Filter: (((x % 2) = 0) AND (y ~~ '%af%'::text))
-   ->  Seq Scan on _hyper_6_16_chunk s1_2
-         Filter: ((hashed SubPlan 1) AND f_leak(b))
-   ->  Seq Scan on _hyper_6_17_chunk s1_3
-         Filter: ((hashed SubPlan 1) AND f_leak(b))
-   ->  Seq Scan on _hyper_6_18_chunk s1_4
-         Filter: ((hashed SubPlan 1) AND f_leak(b))
-   ->  Seq Scan on _hyper_6_19_chunk s1_5
-         Filter: ((hashed SubPlan 1) AND f_leak(b))
-   ->  Seq Scan on _hyper_6_20_chunk s1_6
-         Filter: ((hashed SubPlan 1) AND f_leak(b))
-   ->  Seq Scan on _hyper_6_21_chunk s1_7
-         Filter: ((hashed SubPlan 1) AND f_leak(b))
-   ->  Seq Scan on _hyper_6_22_chunk s1_8
-         Filter: ((hashed SubPlan 1) AND f_leak(b))
-   ->  Seq Scan on _hyper_6_23_chunk s1_9
-         Filter: ((hashed SubPlan 1) AND f_leak(b))
-   ->  Seq Scan on _hyper_6_24_chunk s1_10
-         Filter: ((hashed SubPlan 1) AND f_leak(b))
-   ->  Seq Scan on _hyper_6_25_chunk s1_11
-         Filter: ((hashed SubPlan 1) AND f_leak(b))
-   ->  Seq Scan on _hyper_6_26_chunk s1_12
-         Filter: ((hashed SubPlan 1) AND f_leak(b))
-(44 rows)
-
-SELECT (SELECT x FROM s1 LIMIT 1) xx, * FROM s2 WHERE y like '%28%';
- xx | x  |                y                 
-----+----+----------------------------------
- -6 | -6 | 596a3d04481816330f07e4f97510c28f
- -4 | -4 | 0267aaf632e87a63288a08331f22c7c3
-  2 |  2 | c81e728d9d4c2f636f067f89cc14862c
-(3 rows)
-
-EXPLAIN (COSTS OFF) SELECT (SELECT x FROM s1 LIMIT 1) xx, * FROM s2 WHERE y like '%28%';
-                                        QUERY PLAN                                         
--------------------------------------------------------------------------------------------
- Result
-   ->  Append
-         ->  Seq Scan on s2 s2_1
-               Filter: (((x % 2) = 0) AND (y ~~ '%28%'::text))
-         ->  Seq Scan on _hyper_7_27_chunk s2_2
-               Filter: (((x % 2) = 0) AND (y ~~ '%28%'::text))
-         ->  Seq Scan on _hyper_7_28_chunk s2_3
-               Filter: (((x % 2) = 0) AND (y ~~ '%28%'::text))
-         ->  Seq Scan on _hyper_7_29_chunk s2_4
-               Filter: (((x % 2) = 0) AND (y ~~ '%28%'::text))
-         ->  Seq Scan on _hyper_7_30_chunk s2_5
-               Filter: (((x % 2) = 0) AND (y ~~ '%28%'::text))
-         ->  Seq Scan on _hyper_7_31_chunk s2_6
-               Filter: (((x % 2) = 0) AND (y ~~ '%28%'::text))
-         ->  Seq Scan on _hyper_7_32_chunk s2_7
-               Filter: (((x % 2) = 0) AND (y ~~ '%28%'::text))
-         ->  Seq Scan on _hyper_7_33_chunk s2_8
-               Filter: (((x % 2) = 0) AND (y ~~ '%28%'::text))
-   SubPlan 2
-     ->  Limit
-           ->  Result
-                 ->  Custom Scan (ChunkAppend) on s1
-                       ->  Seq Scan on s1 s1_1
-                             Filter: (hashed SubPlan 1)
-                             SubPlan 1
-                               ->  Append
-                                     ->  Seq Scan on s2 s2_10
-                                           Filter: (((x % 2) = 0) AND (y ~~ '%af%'::text))
-                                     ->  Seq Scan on _hyper_7_27_chunk s2_11
-                                           Filter: (((x % 2) = 0) AND (y ~~ '%af%'::text))
-                                     ->  Seq Scan on _hyper_7_28_chunk s2_12
-                                           Filter: (((x % 2) = 0) AND (y ~~ '%af%'::text))
-                                     ->  Seq Scan on _hyper_7_29_chunk s2_13
-                                           Filter: (((x % 2) = 0) AND (y ~~ '%af%'::text))
-                                     ->  Seq Scan on _hyper_7_30_chunk s2_14
-                                           Filter: (((x % 2) = 0) AND (y ~~ '%af%'::text))
-                                     ->  Seq Scan on _hyper_7_31_chunk s2_15
-                                           Filter: (((x % 2) = 0) AND (y ~~ '%af%'::text))
-                                     ->  Seq Scan on _hyper_7_32_chunk s2_16
-                                           Filter: (((x % 2) = 0) AND (y ~~ '%af%'::text))
-                                     ->  Seq Scan on _hyper_7_33_chunk s2_17
-                                           Filter: (((x % 2) = 0) AND (y ~~ '%af%'::text))
-                       ->  Seq Scan on _hyper_6_16_chunk s1_2
-                             Filter: (hashed SubPlan 1)
-                       ->  Seq Scan on _hyper_6_17_chunk s1_3
-                             Filter: (hashed SubPlan 1)
-                       ->  Seq Scan on _hyper_6_18_chunk s1_4
-                             Filter: (hashed SubPlan 1)
-                       ->  Seq Scan on _hyper_6_19_chunk s1_5
-                             Filter: (hashed SubPlan 1)
-                       ->  Seq Scan on _hyper_6_20_chunk s1_6
-                             Filter: (hashed SubPlan 1)
-                       ->  Seq Scan on _hyper_6_21_chunk s1_7
-                             Filter: (hashed SubPlan 1)
-                       ->  Seq Scan on _hyper_6_22_chunk s1_8
-                             Filter: (hashed SubPlan 1)
-                       ->  Seq Scan on _hyper_6_23_chunk s1_9
-                             Filter: (hashed SubPlan 1)
-                       ->  Seq Scan on _hyper_6_24_chunk s1_10
-                             Filter: (hashed SubPlan 1)
-                       ->  Seq Scan on _hyper_6_25_chunk s1_11
-                             Filter: (hashed SubPlan 1)
-                       ->  Seq Scan on _hyper_6_26_chunk s1_12
-                             Filter: (hashed SubPlan 1)
-(64 rows)
-
-SET SESSION AUTHORIZATION regress_rls_alice;
-ALTER POLICY p2 ON s2 USING (x in (select a from s1 where b like '%d2%'));
-SET SESSION AUTHORIZATION regress_rls_bob;
-SELECT * FROM s1 WHERE f_leak(b);	-- fail (infinite recursion via view)
-ERROR:  infinite recursion detected in policy for relation "s1"
--- prepared statement with regress_rls_alice privilege
-PREPARE p1(int) AS SELECT * FROM t1 WHERE a <= $1;
-EXECUTE p1(2);
- a |  b  
----+-----
- 2 | bbb
- 2 | bcd
- 2 | yyy
-(3 rows)
-
-EXPLAIN (COSTS OFF) EXECUTE p1(2);
-                  QUERY PLAN                  
-----------------------------------------------
- Append
-   ->  Seq Scan on t1 t1_1
-         Filter: ((a <= 2) AND ((a % 2) = 0))
-   ->  Seq Scan on t2 t1_2
-         Filter: ((a <= 2) AND ((a % 2) = 0))
-   ->  Seq Scan on t3 t1_3
-         Filter: ((a <= 2) AND ((a % 2) = 0))
-(7 rows)
-
--- superuser is allowed to bypass RLS checks
-RESET SESSION AUTHORIZATION;
-SET row_security TO OFF;
-SELECT * FROM t1 WHERE f_leak(b);
-NOTICE:  f_leak => aba
-NOTICE:  f_leak => bbb
-NOTICE:  f_leak => ccc
-NOTICE:  f_leak => dad
-NOTICE:  f_leak => abc
-NOTICE:  f_leak => bcd
-NOTICE:  f_leak => cde
-NOTICE:  f_leak => def
-NOTICE:  f_leak => xxx
-NOTICE:  f_leak => yyy
-NOTICE:  f_leak => zzz
- a |  b  
----+-----
- 1 | aba
- 2 | bbb
- 3 | ccc
- 4 | dad
- 1 | abc
- 2 | bcd
- 3 | cde
- 4 | def
- 1 | xxx
- 2 | yyy
- 3 | zzz
-(11 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM t1 WHERE f_leak(b);
-        QUERY PLAN         
----------------------------
- Append
-   ->  Seq Scan on t1 t1_1
-         Filter: f_leak(b)
-   ->  Seq Scan on t2 t1_2
-         Filter: f_leak(b)
-   ->  Seq Scan on t3 t1_3
-         Filter: f_leak(b)
-(7 rows)
-
--- plan cache should be invalidated
-EXECUTE p1(2);
- a |  b  
----+-----
- 1 | aba
- 2 | bbb
- 1 | abc
- 2 | bcd
- 1 | xxx
- 2 | yyy
-(6 rows)
-
-EXPLAIN (COSTS OFF) EXECUTE p1(2);
-        QUERY PLAN         
----------------------------
- Append
-   ->  Seq Scan on t1 t1_1
-         Filter: (a <= 2)
-   ->  Seq Scan on t2 t1_2
-         Filter: (a <= 2)
-   ->  Seq Scan on t3 t1_3
-         Filter: (a <= 2)
-(7 rows)
-
-PREPARE p2(int) AS SELECT * FROM t1 WHERE a = $1;
-EXECUTE p2(2);
- a |  b  
----+-----
- 2 | bbb
- 2 | bcd
- 2 | yyy
-(3 rows)
-
-EXPLAIN (COSTS OFF) EXECUTE p2(2);
-        QUERY PLAN         
----------------------------
- Append
-   ->  Seq Scan on t1 t1_1
-         Filter: (a = 2)
-   ->  Seq Scan on t2 t1_2
-         Filter: (a = 2)
-   ->  Seq Scan on t3 t1_3
-         Filter: (a = 2)
-(7 rows)
-
--- also, case when privilege switch from superuser
-SET SESSION AUTHORIZATION regress_rls_bob;
-SET row_security TO ON;
-EXECUTE p2(2);
- a |  b  
----+-----
- 2 | bbb
- 2 | bcd
- 2 | yyy
-(3 rows)
-
-EXPLAIN (COSTS OFF) EXECUTE p2(2);
-                 QUERY PLAN                  
----------------------------------------------
- Append
-   ->  Seq Scan on t1 t1_1
-         Filter: ((a = 2) AND ((a % 2) = 0))
-   ->  Seq Scan on t2 t1_2
-         Filter: ((a = 2) AND ((a % 2) = 0))
-   ->  Seq Scan on t3 t1_3
-         Filter: ((a = 2) AND ((a % 2) = 0))
-(7 rows)
-
---
--- UPDATE / DELETE and Row-level security
---
-SET SESSION AUTHORIZATION regress_rls_bob;
-EXPLAIN (COSTS OFF) UPDATE t1 SET b = b || b WHERE f_leak(b);
-                        QUERY PLAN                         
------------------------------------------------------------
- Update on t1
-   Update on t1 t1_1
-   Update on t2 t1_2
-   Update on t3 t1_3
-   ->  Result
-         ->  Append
-               ->  Seq Scan on t1 t1_1
-                     Filter: (((a % 2) = 0) AND f_leak(b))
-               ->  Seq Scan on t2 t1_2
-                     Filter: (((a % 2) = 0) AND f_leak(b))
-               ->  Seq Scan on t3 t1_3
-                     Filter: (((a % 2) = 0) AND f_leak(b))
-(12 rows)
-
-UPDATE t1 SET b = b || b WHERE f_leak(b);
-NOTICE:  f_leak => bbb
-NOTICE:  f_leak => dad
-NOTICE:  f_leak => bcd
-NOTICE:  f_leak => def
-NOTICE:  f_leak => yyy
-EXPLAIN (COSTS OFF) UPDATE only t1 SET b = b || '_updt' WHERE f_leak(b);
-                  QUERY PLAN                   
------------------------------------------------
- Update on t1
-   ->  Seq Scan on t1
-         Filter: (((a % 2) = 0) AND f_leak(b))
-(3 rows)
-
-UPDATE only t1 SET b = b || '_updt' WHERE f_leak(b);
-NOTICE:  f_leak => bbbbbb
-NOTICE:  f_leak => daddad
--- returning clause with system column
-UPDATE only t1 SET b = b WHERE f_leak(b) RETURNING ctid, *, t1;
-NOTICE:  f_leak => bbbbbb_updt
-NOTICE:  f_leak => daddad_updt
-  ctid  | a |      b      |       t1        
---------+---+-------------+-----------------
- (0,9)  | 2 | bbbbbb_updt | (2,bbbbbb_updt)
- (0,10) | 4 | daddad_updt | (4,daddad_updt)
-(2 rows)
-
-UPDATE t1 SET b = b WHERE f_leak(b) RETURNING *;
-NOTICE:  f_leak => bbbbbb_updt
-NOTICE:  f_leak => daddad_updt
-NOTICE:  f_leak => bcdbcd
-NOTICE:  f_leak => defdef
-NOTICE:  f_leak => yyyyyy
- a |      b      
----+-------------
- 2 | bbbbbb_updt
- 4 | daddad_updt
- 2 | bcdbcd
- 4 | defdef
- 2 | yyyyyy
-(5 rows)
-
-UPDATE t1 SET b = b WHERE f_leak(b) RETURNING ctid, *, t1;
-NOTICE:  f_leak => bbbbbb_updt
-NOTICE:  f_leak => daddad_updt
-NOTICE:  f_leak => bcdbcd
-NOTICE:  f_leak => defdef
-NOTICE:  f_leak => yyyyyy
-  ctid  | a |      b      |       t1        
---------+---+-------------+-----------------
- (0,13) | 2 | bbbbbb_updt | (2,bbbbbb_updt)
- (0,14) | 4 | daddad_updt | (4,daddad_updt)
- (0,9)  | 2 | bcdbcd      | (2,bcdbcd)
- (0,10) | 4 | defdef      | (4,defdef)
- (0,6)  | 2 | yyyyyy      | (2,yyyyyy)
-(5 rows)
-
--- updates with from clause
-EXPLAIN (COSTS OFF) UPDATE t2 SET b=t2.b FROM t3
-WHERE t2.a = 3 and t3.a = 2 AND f_leak(t2.b) AND f_leak(t3.b);
-                           QUERY PLAN                            
------------------------------------------------------------------
- Update on t2
-   ->  Nested Loop
-         ->  Seq Scan on t2
-               Filter: ((a = 3) AND ((a % 2) = 1) AND f_leak(b))
-         ->  Seq Scan on t3
-               Filter: ((a = 2) AND f_leak(b))
-(6 rows)
-
-UPDATE t2 SET b=t2.b FROM t3
-WHERE t2.a = 3 and t3.a = 2 AND f_leak(t2.b) AND f_leak(t3.b);
-NOTICE:  f_leak => cde
-NOTICE:  f_leak => yyyyyy
-EXPLAIN (COSTS OFF) UPDATE t1 SET b=t1.b FROM t2
-WHERE t1.a = 3 and t2.a = 3 AND f_leak(t1.b) AND f_leak(t2.b);
-                              QUERY PLAN                               
------------------------------------------------------------------------
- Update on t1
-   Update on t1 t1_1
-   Update on t2 t1_2
-   Update on t3 t1_3
-   ->  Nested Loop
-         ->  Seq Scan on t2
-               Filter: ((a = 3) AND ((a % 2) = 1) AND f_leak(b))
-         ->  Append
-               ->  Seq Scan on t1 t1_1
-                     Filter: ((a = 3) AND ((a % 2) = 0) AND f_leak(b))
-               ->  Seq Scan on t2 t1_2
-                     Filter: ((a = 3) AND ((a % 2) = 0) AND f_leak(b))
-               ->  Seq Scan on t3 t1_3
-                     Filter: ((a = 3) AND ((a % 2) = 0) AND f_leak(b))
-(14 rows)
-
-UPDATE t1 SET b=t1.b FROM t2
-WHERE t1.a = 3 and t2.a = 3 AND f_leak(t1.b) AND f_leak(t2.b);
-NOTICE:  f_leak => cde
-EXPLAIN (COSTS OFF) UPDATE t2 SET b=t2.b FROM t1
-WHERE t1.a = 3 and t2.a = 3 AND f_leak(t1.b) AND f_leak(t2.b);
-                              QUERY PLAN                               
------------------------------------------------------------------------
- Update on t2
-   ->  Nested Loop
-         ->  Seq Scan on t2
-               Filter: ((a = 3) AND ((a % 2) = 1) AND f_leak(b))
-         ->  Append
-               ->  Seq Scan on t1 t1_1
-                     Filter: ((a = 3) AND ((a % 2) = 0) AND f_leak(b))
-               ->  Seq Scan on t2 t1_2
-                     Filter: ((a = 3) AND ((a % 2) = 0) AND f_leak(b))
-               ->  Seq Scan on t3 t1_3
-                     Filter: ((a = 3) AND ((a % 2) = 0) AND f_leak(b))
-(11 rows)
-
-UPDATE t2 SET b=t2.b FROM t1
-WHERE t1.a = 3 and t2.a = 3 AND f_leak(t1.b) AND f_leak(t2.b);
-NOTICE:  f_leak => cde
--- updates with from clause self join
-EXPLAIN (COSTS OFF) UPDATE t2 t2_1 SET b = t2_2.b FROM t2 t2_2
-WHERE t2_1.a = 3 AND t2_2.a = t2_1.a AND t2_2.b = t2_1.b
-AND f_leak(t2_1.b) AND f_leak(t2_2.b) RETURNING *, t2_1, t2_2;
-                           QUERY PLAN                            
------------------------------------------------------------------
- Update on t2 t2_1
-   ->  Nested Loop
-         Join Filter: (t2_1.b = t2_2.b)
-         ->  Seq Scan on t2 t2_1
-               Filter: ((a = 3) AND ((a % 2) = 1) AND f_leak(b))
-         ->  Seq Scan on t2 t2_2
-               Filter: ((a = 3) AND ((a % 2) = 1) AND f_leak(b))
-(7 rows)
-
-UPDATE t2 t2_1 SET b = t2_2.b FROM t2 t2_2
-WHERE t2_1.a = 3 AND t2_2.a = t2_1.a AND t2_2.b = t2_1.b
-AND f_leak(t2_1.b) AND f_leak(t2_2.b) RETURNING *, t2_1, t2_2;
-NOTICE:  f_leak => cde
-NOTICE:  f_leak => cde
- a |  b  |  c  | a |  b  |  c  |    t2_1     |    t2_2     
----+-----+-----+---+-----+-----+-------------+-------------
- 3 | cde | 3.3 | 3 | cde | 3.3 | (3,cde,3.3) | (3,cde,3.3)
-(1 row)
-
-EXPLAIN (COSTS OFF) UPDATE t1 t1_1 SET b = t1_2.b FROM t1 t1_2
-WHERE t1_1.a = 4 AND t1_2.a = t1_1.a AND t1_2.b = t1_1.b
-AND f_leak(t1_1.b) AND f_leak(t1_2.b) RETURNING *, t1_1, t1_2;
-                                 QUERY PLAN                                  
------------------------------------------------------------------------------
- Update on t1 t1_1
-   Update on t1 t1_1_1
-   Update on t2 t1_1_2
-   Update on t3 t1_1_3
-   ->  Nested Loop
-         Join Filter: (t1_1.b = t1_2.b)
-         ->  Append
-               ->  Seq Scan on t1 t1_1_1
-                     Filter: ((a = 4) AND ((a % 2) = 0) AND f_leak(b))
-               ->  Seq Scan on t2 t1_1_2
-                     Filter: ((a = 4) AND ((a % 2) = 0) AND f_leak(b))
-               ->  Seq Scan on t3 t1_1_3
-                     Filter: ((a = 4) AND ((a % 2) = 0) AND f_leak(b))
-         ->  Materialize
-               ->  Append
-                     ->  Seq Scan on t1 t1_2_1
-                           Filter: ((a = 4) AND ((a % 2) = 0) AND f_leak(b))
-                     ->  Seq Scan on t2 t1_2_2
-                           Filter: ((a = 4) AND ((a % 2) = 0) AND f_leak(b))
-                     ->  Seq Scan on t3 t1_2_3
-                           Filter: ((a = 4) AND ((a % 2) = 0) AND f_leak(b))
-(21 rows)
-
-UPDATE t1 t1_1 SET b = t1_2.b FROM t1 t1_2
-WHERE t1_1.a = 4 AND t1_2.a = t1_1.a AND t1_2.b = t1_1.b
-AND f_leak(t1_1.b) AND f_leak(t1_2.b) RETURNING *, t1_1, t1_2;
-NOTICE:  f_leak => daddad_updt
-NOTICE:  f_leak => daddad_updt
-NOTICE:  f_leak => defdef
-NOTICE:  f_leak => defdef
- a |      b      | a |      b      |      t1_1       |      t1_2       
----+-------------+---+-------------+-----------------+-----------------
- 4 | daddad_updt | 4 | daddad_updt | (4,daddad_updt) | (4,daddad_updt)
- 4 | defdef      | 4 | defdef      | (4,defdef)      | (4,defdef)
-(2 rows)
-
-RESET SESSION AUTHORIZATION;
-SET row_security TO OFF;
-SELECT * FROM t1 ORDER BY a,b;
- a |      b      
----+-------------
- 1 | aba
- 1 | abc
- 1 | xxx
- 2 | bbbbbb_updt
- 2 | bcdbcd
- 2 | yyyyyy
- 3 | ccc
- 3 | cde
- 3 | zzz
- 4 | daddad_updt
- 4 | defdef
-(11 rows)
-
-SET SESSION AUTHORIZATION regress_rls_bob;
-SET row_security TO ON;
-EXPLAIN (COSTS OFF) DELETE FROM only t1 WHERE f_leak(b);
-                  QUERY PLAN                   
------------------------------------------------
- Delete on t1
-   ->  Seq Scan on t1
-         Filter: (((a % 2) = 0) AND f_leak(b))
-(3 rows)
-
-EXPLAIN (COSTS OFF) DELETE FROM t1 WHERE f_leak(b);
-                     QUERY PLAN                      
------------------------------------------------------
- Delete on t1
-   Delete on t1 t1_1
-   Delete on t2 t1_2
-   Delete on t3 t1_3
-   ->  Append
-         ->  Seq Scan on t1 t1_1
-               Filter: (((a % 2) = 0) AND f_leak(b))
-         ->  Seq Scan on t2 t1_2
-               Filter: (((a % 2) = 0) AND f_leak(b))
-         ->  Seq Scan on t3 t1_3
-               Filter: (((a % 2) = 0) AND f_leak(b))
-(11 rows)
-
-DELETE FROM only t1 WHERE f_leak(b) RETURNING ctid, *, t1;
-NOTICE:  f_leak => bbbbbb_updt
-NOTICE:  f_leak => daddad_updt
-  ctid  | a |      b      |       t1        
---------+---+-------------+-----------------
- (0,13) | 2 | bbbbbb_updt | (2,bbbbbb_updt)
- (0,15) | 4 | daddad_updt | (4,daddad_updt)
-(2 rows)
-
-DELETE FROM t1 WHERE f_leak(b) RETURNING ctid, *, t1;
-NOTICE:  f_leak => bcdbcd
-NOTICE:  f_leak => defdef
-NOTICE:  f_leak => yyyyyy
-  ctid  | a |   b    |     t1     
---------+---+--------+------------
- (0,9)  | 2 | bcdbcd | (2,bcdbcd)
- (0,13) | 4 | defdef | (4,defdef)
- (0,6)  | 2 | yyyyyy | (2,yyyyyy)
-(3 rows)
-
---
--- S.b. view on top of Row-level security
---
-SET SESSION AUTHORIZATION regress_rls_alice;
-CREATE TABLE b1 (a int, b text);
-SELECT public.create_hypertable('b1', 'a', chunk_time_interval=>2);
-NOTICE:  adding not-null constraint to column "a"
-DETAIL:  Time dimensions cannot have NULL values.
-      create_hypertable      
------------------------------
- (8,regress_rls_schema,b1,t)
-(1 row)
-
-INSERT INTO b1 (SELECT x, md5(x::text) FROM generate_series(-10,10) x);
-CREATE POLICY p1 ON b1 USING (a % 2 = 0);
-ALTER TABLE b1 ENABLE ROW LEVEL SECURITY;
-GRANT ALL ON b1 TO regress_rls_bob;
-SET SESSION AUTHORIZATION regress_rls_bob;
-CREATE VIEW bv1 WITH (security_barrier) AS SELECT * FROM b1 WHERE a > 0 WITH CHECK OPTION;
-GRANT ALL ON bv1 TO regress_rls_carol;
-SET SESSION AUTHORIZATION regress_rls_carol;
-EXPLAIN (COSTS OFF) SELECT * FROM bv1 WHERE f_leak(b);
-                                    QUERY PLAN                                     
------------------------------------------------------------------------------------
- Subquery Scan on bv1
-   Filter: f_leak(bv1.b)
-   ->  Append
-         ->  Seq Scan on b1 b1_1
-               Filter: ((a > 0) AND ((a % 2) = 0))
-         ->  Index Scan using _hyper_8_39_chunk_b1_a_idx on _hyper_8_39_chunk b1_2
-               Index Cond: (a > 0)
-               Filter: ((a % 2) = 0)
-         ->  Index Scan using _hyper_8_40_chunk_b1_a_idx on _hyper_8_40_chunk b1_3
-               Index Cond: (a > 0)
-               Filter: ((a % 2) = 0)
-         ->  Index Scan using _hyper_8_41_chunk_b1_a_idx on _hyper_8_41_chunk b1_4
-               Index Cond: (a > 0)
-               Filter: ((a % 2) = 0)
-         ->  Index Scan using _hyper_8_42_chunk_b1_a_idx on _hyper_8_42_chunk b1_5
-               Index Cond: (a > 0)
-               Filter: ((a % 2) = 0)
-         ->  Index Scan using _hyper_8_43_chunk_b1_a_idx on _hyper_8_43_chunk b1_6
-               Index Cond: (a > 0)
-               Filter: ((a % 2) = 0)
-         ->  Index Scan using _hyper_8_44_chunk_b1_a_idx on _hyper_8_44_chunk b1_7
-               Index Cond: (a > 0)
-               Filter: ((a % 2) = 0)
-(23 rows)
-
-SELECT * FROM bv1 WHERE f_leak(b);
-NOTICE:  f_leak => c81e728d9d4c2f636f067f89cc14862c
-NOTICE:  f_leak => a87ff679a2f3e71d9181a67b7542122c
-NOTICE:  f_leak => 1679091c5a880faf6fb5e6087eb1b2dc
-NOTICE:  f_leak => c9f0f895fb98ab9159f51fd0297e236d
-NOTICE:  f_leak => d3d9446802a44259755d38e6d163e820
- a  |                b                 
-----+----------------------------------
-  2 | c81e728d9d4c2f636f067f89cc14862c
-  4 | a87ff679a2f3e71d9181a67b7542122c
-  6 | 1679091c5a880faf6fb5e6087eb1b2dc
-  8 | c9f0f895fb98ab9159f51fd0297e236d
- 10 | d3d9446802a44259755d38e6d163e820
-(5 rows)
-
-INSERT INTO bv1 VALUES (-1, 'xxx'); -- should fail view WCO
-ERROR:  new row violates row-level security policy for table "b1"
-INSERT INTO bv1 VALUES (11, 'xxx'); -- should fail RLS check
-ERROR:  new row violates row-level security policy for table "b1"
-INSERT INTO bv1 VALUES (12, 'xxx'); -- ok
-EXPLAIN (COSTS OFF) UPDATE bv1 SET b = 'yyy' WHERE a = 4 AND f_leak(b);
-                                          QUERY PLAN                                           
------------------------------------------------------------------------------------------------
- Custom Scan (HypertableModify)
-   ->  Update on b1
-         Update on _hyper_8_41_chunk b1_1
-         ->  Result
-               ->  Custom Scan (ChunkAppend) on b1
-                     Chunks excluded during startup: 0
-                     ->  Index Scan using _hyper_8_41_chunk_b1_a_idx on _hyper_8_41_chunk b1_1
-                           Index Cond: ((a > 0) AND (a = 4))
-                           Filter: (((a % 2) = 0) AND f_leak(b))
-(9 rows)
-
-UPDATE bv1 SET b = 'yyy' WHERE a = 4 AND f_leak(b);
-NOTICE:  f_leak => a87ff679a2f3e71d9181a67b7542122c
-EXPLAIN (COSTS OFF) DELETE FROM bv1 WHERE a = 6 AND f_leak(b);
-                                       QUERY PLAN                                        
------------------------------------------------------------------------------------------
- Custom Scan (HypertableModify)
-   ->  Delete on b1
-         Delete on _hyper_8_42_chunk b1_1
-         ->  Custom Scan (ChunkAppend) on b1
-               Chunks excluded during startup: 0
-               ->  Index Scan using _hyper_8_42_chunk_b1_a_idx on _hyper_8_42_chunk b1_1
-                     Index Cond: ((a > 0) AND (a = 6))
-                     Filter: (((a % 2) = 0) AND f_leak(b))
-(8 rows)
-
-DELETE FROM bv1 WHERE a = 6 AND f_leak(b);
-NOTICE:  f_leak => 1679091c5a880faf6fb5e6087eb1b2dc
-SET SESSION AUTHORIZATION regress_rls_alice;
-SELECT * FROM b1;
-  a  |                b                 
------+----------------------------------
- -10 | 1b0fd9efa5279c4203b7c70233f86dbf
-  -9 | 252e691406782824eec43d7eadc3d256
-  -8 | a8d2ec85eaf98407310b72eb73dda247
-  -7 | 74687a12d3915d3c4d83f1af7b3683d5
-  -6 | 596a3d04481816330f07e4f97510c28f
-  -5 | 47c1b025fa18ea96c33fbb6718688c0f
-  -4 | 0267aaf632e87a63288a08331f22c7c3
-  -3 | b3149ecea4628efd23d2f86e5a723472
-  -2 | 5d7b9adcbe1c629ec722529dd12e5129
-  -1 | 6bb61e3b7bce0931da574d19d1d82c88
-   0 | cfcd208495d565ef66e7dff9f98764da
-   1 | c4ca4238a0b923820dcc509a6f75849b
-   2 | c81e728d9d4c2f636f067f89cc14862c
-   3 | eccbc87e4b5ce2fe28308fd9f2a7baf3
-   5 | e4da3b7fbbce2345d7772b0674a318d5
-   4 | yyy
-   7 | 8f14e45fceea167a5a36dedd4bea2543
-   8 | c9f0f895fb98ab9159f51fd0297e236d
-   9 | 45c48cce2e2d7fbdea1afc51c7c6ad26
-  10 | d3d9446802a44259755d38e6d163e820
-  12 | xxx
-(21 rows)
-
---
--- INSERT ... ON CONFLICT DO UPDATE and Row-level security
---
-SET SESSION AUTHORIZATION regress_rls_alice;
-DROP POLICY p1 ON document;
-DROP POLICY p1r ON document;
-CREATE POLICY p1 ON document FOR SELECT USING (true);
-CREATE POLICY p2 ON document FOR INSERT WITH CHECK (dauthor = current_user);
-CREATE POLICY p3 ON document FOR UPDATE
-  USING (cid = (SELECT cid from category WHERE cname = 'novel'))
-  WITH CHECK (dauthor = current_user);
-SET SESSION AUTHORIZATION regress_rls_bob;
--- Exists...
-SELECT * FROM document WHERE did = 2;
- did | cid | dlevel |     dauthor     |     dtitle      
------+-----+--------+-----------------+-----------------
-   2 |  11 |      2 | regress_rls_bob | my second novel
-(1 row)
-
--- ...so violates actual WITH CHECK OPTION within UPDATE (not INSERT, since
--- alternative UPDATE path happens to be taken):
-INSERT INTO document VALUES (2, (SELECT cid from category WHERE cname = 'novel'), 1, 'regress_rls_carol', 'my first novel')
-    ON CONFLICT (did) DO UPDATE SET dtitle = EXCLUDED.dtitle, dauthor = EXCLUDED.dauthor;
-ERROR:  new row violates row-level security policy for table "document"
--- Violates USING qual for UPDATE policy p3.
---
--- UPDATE path is taken, but UPDATE fails purely because *existing* row to be
--- updated is not a "novel"/cid 11 (row is not leaked, even though we have
--- SELECT privileges sufficient to see the row in this instance):
-INSERT INTO document VALUES (33, 22, 1, 'regress_rls_bob', 'okay science fiction'); -- preparation for next statement
-INSERT INTO document VALUES (33, (SELECT cid from category WHERE cname = 'novel'), 1, 'regress_rls_bob', 'Some novel, replaces sci-fi') -- takes UPDATE path
-    ON CONFLICT (did) DO UPDATE SET dtitle = EXCLUDED.dtitle;
-ERROR:  new row violates row-level security policy (USING expression) for table "document"
--- Fine (we UPDATE, since INSERT WCOs and UPDATE security barrier quals + WCOs
--- not violated):
-INSERT INTO document VALUES (2, (SELECT cid from category WHERE cname = 'novel'), 1, 'regress_rls_bob', 'my first novel')
-    ON CONFLICT (did) DO UPDATE SET dtitle = EXCLUDED.dtitle RETURNING *;
- did | cid | dlevel |     dauthor     |     dtitle     
------+-----+--------+-----------------+----------------
-   2 |  11 |      2 | regress_rls_bob | my first novel
-(1 row)
-
--- Fine (we INSERT, so "cid = 33" ("technology") isn't evaluated):
-INSERT INTO document VALUES (78, (SELECT cid from category WHERE cname = 'novel'), 1, 'regress_rls_bob', 'some technology novel')
-    ON CONFLICT (did) DO UPDATE SET dtitle = EXCLUDED.dtitle, cid = 33 RETURNING *;
- did | cid | dlevel |     dauthor     |        dtitle         
------+-----+--------+-----------------+-----------------------
-  78 |  11 |      1 | regress_rls_bob | some technology novel
-(1 row)
-
--- Fine (same query, but we UPDATE, so "cid = 33", ("technology") is not the
--- case in respect of *existing* tuple):
-INSERT INTO document VALUES (78, (SELECT cid from category WHERE cname = 'novel'), 1, 'regress_rls_bob', 'some technology novel')
-    ON CONFLICT (did) DO UPDATE SET dtitle = EXCLUDED.dtitle, cid = 33 RETURNING *;
- did | cid | dlevel |     dauthor     |        dtitle         
------+-----+--------+-----------------+-----------------------
-  78 |  33 |      1 | regress_rls_bob | some technology novel
-(1 row)
-
--- Same query a third time, but now fails due to existing tuple finally not
--- passing quals:
-INSERT INTO document VALUES (78, (SELECT cid from category WHERE cname = 'novel'), 1, 'regress_rls_bob', 'some technology novel')
-    ON CONFLICT (did) DO UPDATE SET dtitle = EXCLUDED.dtitle, cid = 33 RETURNING *;
-ERROR:  new row violates row-level security policy (USING expression) for table "document"
--- Don't fail just because INSERT doesn't satisfy WITH CHECK option that
--- originated as a barrier/USING() qual from the UPDATE.  Note that the UPDATE
--- path *isn't* taken, and so UPDATE-related policy does not apply:
-INSERT INTO document VALUES (79, (SELECT cid from category WHERE cname = 'technology'), 1, 'regress_rls_bob', 'technology book, can only insert')
-    ON CONFLICT (did) DO UPDATE SET dtitle = EXCLUDED.dtitle RETURNING *;
- did | cid | dlevel |     dauthor     |              dtitle              
------+-----+--------+-----------------+----------------------------------
-  79 |  33 |      1 | regress_rls_bob | technology book, can only insert
-(1 row)
-
--- But this time, the same statement fails, because the UPDATE path is taken,
--- and updating the row just inserted falls afoul of security barrier qual
--- (enforced as WCO) -- what we might have updated target tuple to is
--- irrelevant, in fact.
-INSERT INTO document VALUES (79, (SELECT cid from category WHERE cname = 'technology'), 1, 'regress_rls_bob', 'technology book, can only insert')
-    ON CONFLICT (did) DO UPDATE SET dtitle = EXCLUDED.dtitle RETURNING *;
-ERROR:  new row violates row-level security policy (USING expression) for table "document"
--- Test default USING qual enforced as WCO
-SET SESSION AUTHORIZATION regress_rls_alice;
-DROP POLICY p1 ON document;
-DROP POLICY p2 ON document;
-DROP POLICY p3 ON document;
-CREATE POLICY p3_with_default ON document FOR UPDATE
-  USING (cid = (SELECT cid from category WHERE cname = 'novel'));
-SET SESSION AUTHORIZATION regress_rls_bob;
--- Just because WCO-style enforcement of USING quals occurs with
--- existing/target tuple does not mean that the implementation can be allowed
--- to fail to also enforce this qual against the final tuple appended to
--- relation (since in the absence of an explicit WCO, this is also interpreted
--- as an UPDATE/ALL WCO in general).
---
--- UPDATE path is taken here (fails due to existing tuple).  Note that this is
--- not reported as a "USING expression", because it's an RLS UPDATE check that originated as
--- a USING qual for the purposes of RLS in general, as opposed to an explicit
--- USING qual that is ordinarily a security barrier.  We leave it up to the
--- UPDATE to make this fail:
-INSERT INTO document VALUES (79, (SELECT cid from category WHERE cname = 'technology'), 1, 'regress_rls_bob', 'technology book, can only insert')
-    ON CONFLICT (did) DO UPDATE SET dtitle = EXCLUDED.dtitle RETURNING *;
-ERROR:  new row violates row-level security policy for table "document"
--- UPDATE path is taken here.  Existing tuple passes, since it's cid
--- corresponds to "novel", but default USING qual is enforced against
--- post-UPDATE tuple too (as always when updating with a policy that lacks an
--- explicit WCO), and so this fails:
-INSERT INTO document VALUES (2, (SELECT cid from category WHERE cname = 'technology'), 1, 'regress_rls_bob', 'my first novel')
-    ON CONFLICT (did) DO UPDATE SET cid = EXCLUDED.cid, dtitle = EXCLUDED.dtitle RETURNING *;
-ERROR:  new row violates row-level security policy for table "document"
-SET SESSION AUTHORIZATION regress_rls_alice;
-DROP POLICY p3_with_default ON document;
---
--- Test ALL policies with ON CONFLICT DO UPDATE (much the same as existing UPDATE
--- tests)
---
-CREATE POLICY p3_with_all ON document FOR ALL
-  USING (cid = (SELECT cid from category WHERE cname = 'novel'))
-  WITH CHECK (dauthor = current_user);
-SET SESSION AUTHORIZATION regress_rls_bob;
--- Fails, since ALL WCO is enforced in insert path:
-INSERT INTO document VALUES (80, (SELECT cid from category WHERE cname = 'novel'), 1, 'regress_rls_carol', 'my first novel')
-    ON CONFLICT (did) DO UPDATE SET dtitle = EXCLUDED.dtitle, cid = 33;
-ERROR:  new row violates row-level security policy for table "document"
--- Fails, since ALL policy USING qual is enforced (existing, target tuple is in
--- violation, since it has the "manga" cid):
-INSERT INTO document VALUES (4, (SELECT cid from category WHERE cname = 'novel'), 1, 'regress_rls_bob', 'my first novel')
-    ON CONFLICT (did) DO UPDATE SET dtitle = EXCLUDED.dtitle;
-ERROR:  new row violates row-level security policy (USING expression) for table "document"
--- Fails, since ALL WCO are enforced:
-INSERT INTO document VALUES (1, (SELECT cid from category WHERE cname = 'novel'), 1, 'regress_rls_bob', 'my first novel')
-    ON CONFLICT (did) DO UPDATE SET dauthor = 'regress_rls_carol';
-ERROR:  new row violates row-level security policy for table "document"
---
--- ROLE/GROUP
---
-SET SESSION AUTHORIZATION regress_rls_alice;
-CREATE TABLE z1 (a int, b text);
-SELECT public.create_hypertable('z1', 'a', chunk_time_interval=>2);
-NOTICE:  adding not-null constraint to column "a"
-DETAIL:  Time dimensions cannot have NULL values.
-      create_hypertable      
------------------------------
- (9,regress_rls_schema,z1,t)
-(1 row)
-
-CREATE TABLE z2 (a int, b text);
-SELECT public.create_hypertable('z2', 'a', chunk_time_interval=>2);
-NOTICE:  adding not-null constraint to column "a"
-DETAIL:  Time dimensions cannot have NULL values.
-      create_hypertable       
-------------------------------
- (10,regress_rls_schema,z2,t)
-(1 row)
-
-GRANT SELECT ON z1,z2 TO regress_rls_group1, regress_rls_group2,
-    regress_rls_bob, regress_rls_carol;
-INSERT INTO z1 VALUES
-    (1, 'aba'),
-    (2, 'bbb'),
-    (3, 'ccc'),
-    (4, 'dad');
-CREATE POLICY p1 ON z1 TO regress_rls_group1 USING (a % 2 = 0);
-CREATE POLICY p2 ON z1 TO regress_rls_group2 USING (a % 2 = 1);
-ALTER TABLE z1 ENABLE ROW LEVEL SECURITY;
-SET SESSION AUTHORIZATION regress_rls_bob;
-SELECT * FROM z1 WHERE f_leak(b);
-NOTICE:  f_leak => bbb
-NOTICE:  f_leak => dad
- a |  b  
----+-----
- 2 | bbb
- 4 | dad
-(2 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM z1 WHERE f_leak(b);
-                  QUERY PLAN                   
------------------------------------------------
- Custom Scan (ChunkAppend) on z1
-   Chunks excluded during startup: 0
-   ->  Seq Scan on z1 z1_1
-         Filter: (((a % 2) = 0) AND f_leak(b))
-   ->  Seq Scan on _hyper_9_49_chunk z1_2
-         Filter: (((a % 2) = 0) AND f_leak(b))
-   ->  Seq Scan on _hyper_9_50_chunk z1_3
-         Filter: (((a % 2) = 0) AND f_leak(b))
-   ->  Seq Scan on _hyper_9_51_chunk z1_4
-         Filter: (((a % 2) = 0) AND f_leak(b))
-(10 rows)
-
-PREPARE plancache_test AS SELECT * FROM z1 WHERE f_leak(b);
-EXPLAIN (COSTS OFF) EXECUTE plancache_test;
-                  QUERY PLAN                   
------------------------------------------------
- Custom Scan (ChunkAppend) on z1
-   Chunks excluded during startup: 0
-   ->  Seq Scan on z1 z1_1
-         Filter: (((a % 2) = 0) AND f_leak(b))
-   ->  Seq Scan on _hyper_9_49_chunk z1_2
-         Filter: (((a % 2) = 0) AND f_leak(b))
-   ->  Seq Scan on _hyper_9_50_chunk z1_3
-         Filter: (((a % 2) = 0) AND f_leak(b))
-   ->  Seq Scan on _hyper_9_51_chunk z1_4
-         Filter: (((a % 2) = 0) AND f_leak(b))
-(10 rows)
-
-PREPARE plancache_test2 AS WITH q AS MATERIALIZED (SELECT * FROM z1 WHERE f_leak(b)) SELECT * FROM q,z2;
-EXPLAIN (COSTS OFF) EXECUTE plancache_test2;
-                      QUERY PLAN                       
--------------------------------------------------------
- Nested Loop
-   CTE q
-     ->  Custom Scan (ChunkAppend) on z1
-           Chunks excluded during startup: 0
-           ->  Seq Scan on z1 z1_1
-                 Filter: (((a % 2) = 0) AND f_leak(b))
-           ->  Seq Scan on _hyper_9_49_chunk z1_2
-                 Filter: (((a % 2) = 0) AND f_leak(b))
-           ->  Seq Scan on _hyper_9_50_chunk z1_3
-                 Filter: (((a % 2) = 0) AND f_leak(b))
-           ->  Seq Scan on _hyper_9_51_chunk z1_4
-                 Filter: (((a % 2) = 0) AND f_leak(b))
-   ->  CTE Scan on q
-   ->  Materialize
-         ->  Seq Scan on z2
-(15 rows)
-
-PREPARE plancache_test4 AS WITH q AS (SELECT * FROM z1 WHERE f_leak(b)) SELECT * FROM q,z2;
-EXPLAIN (COSTS OFF) EXECUTE plancache_test4;
-                      QUERY PLAN                       
--------------------------------------------------------
- Nested Loop
-   CTE q
-     ->  Custom Scan (ChunkAppend) on z1
-           Chunks excluded during startup: 0
-           ->  Seq Scan on z1 z1_1
-                 Filter: (((a % 2) = 0) AND f_leak(b))
-           ->  Seq Scan on _hyper_9_49_chunk z1_2
-                 Filter: (((a % 2) = 0) AND f_leak(b))
-           ->  Seq Scan on _hyper_9_50_chunk z1_3
-                 Filter: (((a % 2) = 0) AND f_leak(b))
-           ->  Seq Scan on _hyper_9_51_chunk z1_4
-                 Filter: (((a % 2) = 0) AND f_leak(b))
-   ->  CTE Scan on q
-   ->  Materialize
-         ->  Seq Scan on z2
-(15 rows)
-
-PREPARE plancache_test6 AS WITH q AS NOT MATERIALIZED (SELECT * FROM z1 WHERE f_leak(b)) SELECT * FROM q,z2;
-EXPLAIN (COSTS OFF) EXECUTE plancache_test6;
-                      QUERY PLAN                       
--------------------------------------------------------
- Nested Loop
-   CTE q
-     ->  Custom Scan (ChunkAppend) on z1
-           Chunks excluded during startup: 0
-           ->  Seq Scan on z1 z1_1
-                 Filter: (((a % 2) = 0) AND f_leak(b))
-           ->  Seq Scan on _hyper_9_49_chunk z1_2
-                 Filter: (((a % 2) = 0) AND f_leak(b))
-           ->  Seq Scan on _hyper_9_50_chunk z1_3
-                 Filter: (((a % 2) = 0) AND f_leak(b))
-           ->  Seq Scan on _hyper_9_51_chunk z1_4
-                 Filter: (((a % 2) = 0) AND f_leak(b))
-   ->  CTE Scan on q
-   ->  Materialize
-         ->  Seq Scan on z2
-(15 rows)
-
-PREPARE plancache_test3 AS WITH q AS MATERIALIZED (SELECT * FROM z2) SELECT * FROM q,z1 WHERE f_leak(z1.b);
-EXPLAIN (COSTS OFF) EXECUTE plancache_test3;
-                        QUERY PLAN                         
------------------------------------------------------------
- Nested Loop
-   CTE q
-     ->  Seq Scan on z2
-   ->  CTE Scan on q
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on z1
-               Chunks excluded during startup: 0
-               ->  Seq Scan on z1 z1_1
-                     Filter: (((a % 2) = 0) AND f_leak(b))
-               ->  Seq Scan on _hyper_9_49_chunk z1_2
-                     Filter: (((a % 2) = 0) AND f_leak(b))
-               ->  Seq Scan on _hyper_9_50_chunk z1_3
-                     Filter: (((a % 2) = 0) AND f_leak(b))
-               ->  Seq Scan on _hyper_9_51_chunk z1_4
-                     Filter: (((a % 2) = 0) AND f_leak(b))
-(15 rows)
-
-PREPARE plancache_test5 AS WITH q AS (SELECT * FROM z2) SELECT * FROM q,z1 WHERE f_leak(z1.b);
-EXPLAIN (COSTS OFF) EXECUTE plancache_test5;
-                        QUERY PLAN                         
------------------------------------------------------------
- Nested Loop
-   ->  Seq Scan on z2
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on z1
-               Chunks excluded during startup: 0
-               ->  Seq Scan on z1 z1_1
-                     Filter: (((a % 2) = 0) AND f_leak(b))
-               ->  Seq Scan on _hyper_9_49_chunk z1_2
-                     Filter: (((a % 2) = 0) AND f_leak(b))
-               ->  Seq Scan on _hyper_9_50_chunk z1_3
-                     Filter: (((a % 2) = 0) AND f_leak(b))
-               ->  Seq Scan on _hyper_9_51_chunk z1_4
-                     Filter: (((a % 2) = 0) AND f_leak(b))
-(13 rows)
-
-PREPARE plancache_test7 AS WITH q AS NOT MATERIALIZED (SELECT * FROM z2) SELECT * FROM q,z1 WHERE f_leak(z1.b);
-EXPLAIN (COSTS OFF) EXECUTE plancache_test7;
-                        QUERY PLAN                         
------------------------------------------------------------
- Nested Loop
-   ->  Seq Scan on z2
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on z1
-               Chunks excluded during startup: 0
-               ->  Seq Scan on z1 z1_1
-                     Filter: (((a % 2) = 0) AND f_leak(b))
-               ->  Seq Scan on _hyper_9_49_chunk z1_2
-                     Filter: (((a % 2) = 0) AND f_leak(b))
-               ->  Seq Scan on _hyper_9_50_chunk z1_3
-                     Filter: (((a % 2) = 0) AND f_leak(b))
-               ->  Seq Scan on _hyper_9_51_chunk z1_4
-                     Filter: (((a % 2) = 0) AND f_leak(b))
-(13 rows)
-
-SET ROLE regress_rls_group1;
-SELECT * FROM z1 WHERE f_leak(b);
-NOTICE:  f_leak => bbb
-NOTICE:  f_leak => dad
- a |  b  
----+-----
- 2 | bbb
- 4 | dad
-(2 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM z1 WHERE f_leak(b);
-                  QUERY PLAN                   
------------------------------------------------
- Custom Scan (ChunkAppend) on z1
-   Chunks excluded during startup: 0
-   ->  Seq Scan on z1 z1_1
-         Filter: (((a % 2) = 0) AND f_leak(b))
-   ->  Seq Scan on _hyper_9_49_chunk z1_2
-         Filter: (((a % 2) = 0) AND f_leak(b))
-   ->  Seq Scan on _hyper_9_50_chunk z1_3
-         Filter: (((a % 2) = 0) AND f_leak(b))
-   ->  Seq Scan on _hyper_9_51_chunk z1_4
-         Filter: (((a % 2) = 0) AND f_leak(b))
-(10 rows)
-
-EXPLAIN (COSTS OFF) EXECUTE plancache_test;
-                  QUERY PLAN                   
------------------------------------------------
- Custom Scan (ChunkAppend) on z1
-   Chunks excluded during startup: 0
-   ->  Seq Scan on z1 z1_1
-         Filter: (((a % 2) = 0) AND f_leak(b))
-   ->  Seq Scan on _hyper_9_49_chunk z1_2
-         Filter: (((a % 2) = 0) AND f_leak(b))
-   ->  Seq Scan on _hyper_9_50_chunk z1_3
-         Filter: (((a % 2) = 0) AND f_leak(b))
-   ->  Seq Scan on _hyper_9_51_chunk z1_4
-         Filter: (((a % 2) = 0) AND f_leak(b))
-(10 rows)
-
-EXPLAIN (COSTS OFF) EXECUTE plancache_test2;
-                      QUERY PLAN                       
--------------------------------------------------------
- Nested Loop
-   CTE q
-     ->  Custom Scan (ChunkAppend) on z1
-           Chunks excluded during startup: 0
-           ->  Seq Scan on z1 z1_1
-                 Filter: (((a % 2) = 0) AND f_leak(b))
-           ->  Seq Scan on _hyper_9_49_chunk z1_2
-                 Filter: (((a % 2) = 0) AND f_leak(b))
-           ->  Seq Scan on _hyper_9_50_chunk z1_3
-                 Filter: (((a % 2) = 0) AND f_leak(b))
-           ->  Seq Scan on _hyper_9_51_chunk z1_4
-                 Filter: (((a % 2) = 0) AND f_leak(b))
-   ->  CTE Scan on q
-   ->  Materialize
-         ->  Seq Scan on z2
-(15 rows)
-
-EXPLAIN (COSTS OFF) EXECUTE plancache_test4;
-                      QUERY PLAN                       
--------------------------------------------------------
- Nested Loop
-   CTE q
-     ->  Custom Scan (ChunkAppend) on z1
-           Chunks excluded during startup: 0
-           ->  Seq Scan on z1 z1_1
-                 Filter: (((a % 2) = 0) AND f_leak(b))
-           ->  Seq Scan on _hyper_9_49_chunk z1_2
-                 Filter: (((a % 2) = 0) AND f_leak(b))
-           ->  Seq Scan on _hyper_9_50_chunk z1_3
-                 Filter: (((a % 2) = 0) AND f_leak(b))
-           ->  Seq Scan on _hyper_9_51_chunk z1_4
-                 Filter: (((a % 2) = 0) AND f_leak(b))
-   ->  CTE Scan on q
-   ->  Materialize
-         ->  Seq Scan on z2
-(15 rows)
-
-EXPLAIN (COSTS OFF) EXECUTE plancache_test3;
-                        QUERY PLAN                         
------------------------------------------------------------
- Nested Loop
-   CTE q
-     ->  Seq Scan on z2
-   ->  CTE Scan on q
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on z1
-               Chunks excluded during startup: 0
-               ->  Seq Scan on z1 z1_1
-                     Filter: (((a % 2) = 0) AND f_leak(b))
-               ->  Seq Scan on _hyper_9_49_chunk z1_2
-                     Filter: (((a % 2) = 0) AND f_leak(b))
-               ->  Seq Scan on _hyper_9_50_chunk z1_3
-                     Filter: (((a % 2) = 0) AND f_leak(b))
-               ->  Seq Scan on _hyper_9_51_chunk z1_4
-                     Filter: (((a % 2) = 0) AND f_leak(b))
-(15 rows)
-
-EXPLAIN (COSTS OFF) EXECUTE plancache_test5;
-                        QUERY PLAN                         
------------------------------------------------------------
- Nested Loop
-   ->  Seq Scan on z2
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on z1
-               Chunks excluded during startup: 0
-               ->  Seq Scan on z1 z1_1
-                     Filter: (((a % 2) = 0) AND f_leak(b))
-               ->  Seq Scan on _hyper_9_49_chunk z1_2
-                     Filter: (((a % 2) = 0) AND f_leak(b))
-               ->  Seq Scan on _hyper_9_50_chunk z1_3
-                     Filter: (((a % 2) = 0) AND f_leak(b))
-               ->  Seq Scan on _hyper_9_51_chunk z1_4
-                     Filter: (((a % 2) = 0) AND f_leak(b))
-(13 rows)
-
-SET SESSION AUTHORIZATION regress_rls_carol;
-SELECT * FROM z1 WHERE f_leak(b);
-NOTICE:  f_leak => aba
-NOTICE:  f_leak => ccc
- a |  b  
----+-----
- 1 | aba
- 3 | ccc
-(2 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM z1 WHERE f_leak(b);
-                  QUERY PLAN                   
------------------------------------------------
- Custom Scan (ChunkAppend) on z1
-   Chunks excluded during startup: 0
-   ->  Seq Scan on z1 z1_1
-         Filter: (((a % 2) = 1) AND f_leak(b))
-   ->  Seq Scan on _hyper_9_49_chunk z1_2
-         Filter: (((a % 2) = 1) AND f_leak(b))
-   ->  Seq Scan on _hyper_9_50_chunk z1_3
-         Filter: (((a % 2) = 1) AND f_leak(b))
-   ->  Seq Scan on _hyper_9_51_chunk z1_4
-         Filter: (((a % 2) = 1) AND f_leak(b))
-(10 rows)
-
-EXPLAIN (COSTS OFF) EXECUTE plancache_test;
-                  QUERY PLAN                   
------------------------------------------------
- Custom Scan (ChunkAppend) on z1
-   Chunks excluded during startup: 0
-   ->  Seq Scan on z1 z1_1
-         Filter: (((a % 2) = 1) AND f_leak(b))
-   ->  Seq Scan on _hyper_9_49_chunk z1_2
-         Filter: (((a % 2) = 1) AND f_leak(b))
-   ->  Seq Scan on _hyper_9_50_chunk z1_3
-         Filter: (((a % 2) = 1) AND f_leak(b))
-   ->  Seq Scan on _hyper_9_51_chunk z1_4
-         Filter: (((a % 2) = 1) AND f_leak(b))
-(10 rows)
-
-EXPLAIN (COSTS OFF) EXECUTE plancache_test2;
-                      QUERY PLAN                       
--------------------------------------------------------
- Nested Loop
-   CTE q
-     ->  Custom Scan (ChunkAppend) on z1
-           Chunks excluded during startup: 0
-           ->  Seq Scan on z1 z1_1
-                 Filter: (((a % 2) = 1) AND f_leak(b))
-           ->  Seq Scan on _hyper_9_49_chunk z1_2
-                 Filter: (((a % 2) = 1) AND f_leak(b))
-           ->  Seq Scan on _hyper_9_50_chunk z1_3
-                 Filter: (((a % 2) = 1) AND f_leak(b))
-           ->  Seq Scan on _hyper_9_51_chunk z1_4
-                 Filter: (((a % 2) = 1) AND f_leak(b))
-   ->  CTE Scan on q
-   ->  Materialize
-         ->  Seq Scan on z2
-(15 rows)
-
-EXPLAIN (COSTS OFF) EXECUTE plancache_test4;
-                      QUERY PLAN                       
--------------------------------------------------------
- Nested Loop
-   CTE q
-     ->  Custom Scan (ChunkAppend) on z1
-           Chunks excluded during startup: 0
-           ->  Seq Scan on z1 z1_1
-                 Filter: (((a % 2) = 1) AND f_leak(b))
-           ->  Seq Scan on _hyper_9_49_chunk z1_2
-                 Filter: (((a % 2) = 1) AND f_leak(b))
-           ->  Seq Scan on _hyper_9_50_chunk z1_3
-                 Filter: (((a % 2) = 1) AND f_leak(b))
-           ->  Seq Scan on _hyper_9_51_chunk z1_4
-                 Filter: (((a % 2) = 1) AND f_leak(b))
-   ->  CTE Scan on q
-   ->  Materialize
-         ->  Seq Scan on z2
-(15 rows)
-
-EXPLAIN (COSTS OFF) EXECUTE plancache_test3;
-                        QUERY PLAN                         
------------------------------------------------------------
- Nested Loop
-   CTE q
-     ->  Seq Scan on z2
-   ->  CTE Scan on q
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on z1
-               Chunks excluded during startup: 0
-               ->  Seq Scan on z1 z1_1
-                     Filter: (((a % 2) = 1) AND f_leak(b))
-               ->  Seq Scan on _hyper_9_49_chunk z1_2
-                     Filter: (((a % 2) = 1) AND f_leak(b))
-               ->  Seq Scan on _hyper_9_50_chunk z1_3
-                     Filter: (((a % 2) = 1) AND f_leak(b))
-               ->  Seq Scan on _hyper_9_51_chunk z1_4
-                     Filter: (((a % 2) = 1) AND f_leak(b))
-(15 rows)
-
-EXPLAIN (COSTS OFF) EXECUTE plancache_test5;
-                        QUERY PLAN                         
------------------------------------------------------------
- Nested Loop
-   ->  Seq Scan on z2
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on z1
-               Chunks excluded during startup: 0
-               ->  Seq Scan on z1 z1_1
-                     Filter: (((a % 2) = 1) AND f_leak(b))
-               ->  Seq Scan on _hyper_9_49_chunk z1_2
-                     Filter: (((a % 2) = 1) AND f_leak(b))
-               ->  Seq Scan on _hyper_9_50_chunk z1_3
-                     Filter: (((a % 2) = 1) AND f_leak(b))
-               ->  Seq Scan on _hyper_9_51_chunk z1_4
-                     Filter: (((a % 2) = 1) AND f_leak(b))
-(13 rows)
-
-SET ROLE regress_rls_group2;
-SELECT * FROM z1 WHERE f_leak(b);
-NOTICE:  f_leak => aba
-NOTICE:  f_leak => ccc
- a |  b  
----+-----
- 1 | aba
- 3 | ccc
-(2 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM z1 WHERE f_leak(b);
-                  QUERY PLAN                   
------------------------------------------------
- Custom Scan (ChunkAppend) on z1
-   Chunks excluded during startup: 0
-   ->  Seq Scan on z1 z1_1
-         Filter: (((a % 2) = 1) AND f_leak(b))
-   ->  Seq Scan on _hyper_9_49_chunk z1_2
-         Filter: (((a % 2) = 1) AND f_leak(b))
-   ->  Seq Scan on _hyper_9_50_chunk z1_3
-         Filter: (((a % 2) = 1) AND f_leak(b))
-   ->  Seq Scan on _hyper_9_51_chunk z1_4
-         Filter: (((a % 2) = 1) AND f_leak(b))
-(10 rows)
-
-EXPLAIN (COSTS OFF) EXECUTE plancache_test;
-                  QUERY PLAN                   
------------------------------------------------
- Custom Scan (ChunkAppend) on z1
-   Chunks excluded during startup: 0
-   ->  Seq Scan on z1 z1_1
-         Filter: (((a % 2) = 1) AND f_leak(b))
-   ->  Seq Scan on _hyper_9_49_chunk z1_2
-         Filter: (((a % 2) = 1) AND f_leak(b))
-   ->  Seq Scan on _hyper_9_50_chunk z1_3
-         Filter: (((a % 2) = 1) AND f_leak(b))
-   ->  Seq Scan on _hyper_9_51_chunk z1_4
-         Filter: (((a % 2) = 1) AND f_leak(b))
-(10 rows)
-
-EXPLAIN (COSTS OFF) EXECUTE plancache_test2;
-                      QUERY PLAN                       
--------------------------------------------------------
- Nested Loop
-   CTE q
-     ->  Custom Scan (ChunkAppend) on z1
-           Chunks excluded during startup: 0
-           ->  Seq Scan on z1 z1_1
-                 Filter: (((a % 2) = 1) AND f_leak(b))
-           ->  Seq Scan on _hyper_9_49_chunk z1_2
-                 Filter: (((a % 2) = 1) AND f_leak(b))
-           ->  Seq Scan on _hyper_9_50_chunk z1_3
-                 Filter: (((a % 2) = 1) AND f_leak(b))
-           ->  Seq Scan on _hyper_9_51_chunk z1_4
-                 Filter: (((a % 2) = 1) AND f_leak(b))
-   ->  CTE Scan on q
-   ->  Materialize
-         ->  Seq Scan on z2
-(15 rows)
-
-EXPLAIN (COSTS OFF) EXECUTE plancache_test4;
-                      QUERY PLAN                       
--------------------------------------------------------
- Nested Loop
-   CTE q
-     ->  Custom Scan (ChunkAppend) on z1
-           Chunks excluded during startup: 0
-           ->  Seq Scan on z1 z1_1
-                 Filter: (((a % 2) = 1) AND f_leak(b))
-           ->  Seq Scan on _hyper_9_49_chunk z1_2
-                 Filter: (((a % 2) = 1) AND f_leak(b))
-           ->  Seq Scan on _hyper_9_50_chunk z1_3
-                 Filter: (((a % 2) = 1) AND f_leak(b))
-           ->  Seq Scan on _hyper_9_51_chunk z1_4
-                 Filter: (((a % 2) = 1) AND f_leak(b))
-   ->  CTE Scan on q
-   ->  Materialize
-         ->  Seq Scan on z2
-(15 rows)
-
-EXPLAIN (COSTS OFF) EXECUTE plancache_test3;
-                        QUERY PLAN                         
------------------------------------------------------------
- Nested Loop
-   CTE q
-     ->  Seq Scan on z2
-   ->  CTE Scan on q
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on z1
-               Chunks excluded during startup: 0
-               ->  Seq Scan on z1 z1_1
-                     Filter: (((a % 2) = 1) AND f_leak(b))
-               ->  Seq Scan on _hyper_9_49_chunk z1_2
-                     Filter: (((a % 2) = 1) AND f_leak(b))
-               ->  Seq Scan on _hyper_9_50_chunk z1_3
-                     Filter: (((a % 2) = 1) AND f_leak(b))
-               ->  Seq Scan on _hyper_9_51_chunk z1_4
-                     Filter: (((a % 2) = 1) AND f_leak(b))
-(15 rows)
-
-EXPLAIN (COSTS OFF) EXECUTE plancache_test5;
-                        QUERY PLAN                         
------------------------------------------------------------
- Nested Loop
-   ->  Seq Scan on z2
-   ->  Materialize
-         ->  Custom Scan (ChunkAppend) on z1
-               Chunks excluded during startup: 0
-               ->  Seq Scan on z1 z1_1
-                     Filter: (((a % 2) = 1) AND f_leak(b))
-               ->  Seq Scan on _hyper_9_49_chunk z1_2
-                     Filter: (((a % 2) = 1) AND f_leak(b))
-               ->  Seq Scan on _hyper_9_50_chunk z1_3
-                     Filter: (((a % 2) = 1) AND f_leak(b))
-               ->  Seq Scan on _hyper_9_51_chunk z1_4
-                     Filter: (((a % 2) = 1) AND f_leak(b))
-(13 rows)
-
---
--- Views should follow policy for view owner.
---
--- View and Table owner are the same.
-SET SESSION AUTHORIZATION regress_rls_alice;
-CREATE VIEW rls_view AS SELECT * FROM z1 WHERE f_leak(b);
-GRANT SELECT ON rls_view TO regress_rls_bob;
--- Query as role that is not owner of view or table.  Should return all records.
-SET SESSION AUTHORIZATION regress_rls_bob;
-SELECT * FROM rls_view;
-NOTICE:  f_leak => aba
-NOTICE:  f_leak => bbb
-NOTICE:  f_leak => ccc
-NOTICE:  f_leak => dad
- a |  b  
----+-----
- 1 | aba
- 2 | bbb
- 3 | ccc
- 4 | dad
-(4 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM rls_view;
-                QUERY PLAN                
-------------------------------------------
- Custom Scan (ChunkAppend) on z1
-   Chunks excluded during startup: 0
-   ->  Seq Scan on z1 z1_1
-         Filter: f_leak(b)
-   ->  Seq Scan on _hyper_9_49_chunk z1_2
-         Filter: f_leak(b)
-   ->  Seq Scan on _hyper_9_50_chunk z1_3
-         Filter: f_leak(b)
-   ->  Seq Scan on _hyper_9_51_chunk z1_4
-         Filter: f_leak(b)
-(10 rows)
-
--- Query as view/table owner.  Should return all records.
-SET SESSION AUTHORIZATION regress_rls_alice;
-SELECT * FROM rls_view;
-NOTICE:  f_leak => aba
-NOTICE:  f_leak => bbb
-NOTICE:  f_leak => ccc
-NOTICE:  f_leak => dad
- a |  b  
----+-----
- 1 | aba
- 2 | bbb
- 3 | ccc
- 4 | dad
-(4 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM rls_view;
-                QUERY PLAN                
-------------------------------------------
- Custom Scan (ChunkAppend) on z1
-   Chunks excluded during startup: 0
-   ->  Seq Scan on z1 z1_1
-         Filter: f_leak(b)
-   ->  Seq Scan on _hyper_9_49_chunk z1_2
-         Filter: f_leak(b)
-   ->  Seq Scan on _hyper_9_50_chunk z1_3
-         Filter: f_leak(b)
-   ->  Seq Scan on _hyper_9_51_chunk z1_4
-         Filter: f_leak(b)
-(10 rows)
-
-DROP VIEW rls_view;
--- View and Table owners are different.
-SET SESSION AUTHORIZATION regress_rls_bob;
-CREATE VIEW rls_view AS SELECT * FROM z1 WHERE f_leak(b);
-GRANT SELECT ON rls_view TO regress_rls_alice;
--- Query as role that is not owner of view but is owner of table.
--- Should return records based on view owner policies.
-SET SESSION AUTHORIZATION regress_rls_alice;
-SELECT * FROM rls_view;
-NOTICE:  f_leak => bbb
-NOTICE:  f_leak => dad
- a |  b  
----+-----
- 2 | bbb
- 4 | dad
-(2 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM rls_view;
-                  QUERY PLAN                   
------------------------------------------------
- Custom Scan (ChunkAppend) on z1
-   Chunks excluded during startup: 0
-   ->  Seq Scan on z1 z1_1
-         Filter: (((a % 2) = 0) AND f_leak(b))
-   ->  Seq Scan on _hyper_9_49_chunk z1_2
-         Filter: (((a % 2) = 0) AND f_leak(b))
-   ->  Seq Scan on _hyper_9_50_chunk z1_3
-         Filter: (((a % 2) = 0) AND f_leak(b))
-   ->  Seq Scan on _hyper_9_51_chunk z1_4
-         Filter: (((a % 2) = 0) AND f_leak(b))
-(10 rows)
-
--- Query as role that is not owner of table but is owner of view.
--- Should return records based on view owner policies.
-SET SESSION AUTHORIZATION regress_rls_bob;
-SELECT * FROM rls_view;
-NOTICE:  f_leak => bbb
-NOTICE:  f_leak => dad
- a |  b  
----+-----
- 2 | bbb
- 4 | dad
-(2 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM rls_view;
-                  QUERY PLAN                   
------------------------------------------------
- Custom Scan (ChunkAppend) on z1
-   Chunks excluded during startup: 0
-   ->  Seq Scan on z1 z1_1
-         Filter: (((a % 2) = 0) AND f_leak(b))
-   ->  Seq Scan on _hyper_9_49_chunk z1_2
-         Filter: (((a % 2) = 0) AND f_leak(b))
-   ->  Seq Scan on _hyper_9_50_chunk z1_3
-         Filter: (((a % 2) = 0) AND f_leak(b))
-   ->  Seq Scan on _hyper_9_51_chunk z1_4
-         Filter: (((a % 2) = 0) AND f_leak(b))
-(10 rows)
-
--- Query as role that is not the owner of the table or view without permissions.
-SET SESSION AUTHORIZATION regress_rls_carol;
-SELECT * FROM rls_view; --fail - permission denied.
-ERROR:  permission denied for view rls_view
-EXPLAIN (COSTS OFF) SELECT * FROM rls_view; --fail - permission denied.
-ERROR:  permission denied for view rls_view
--- Query as role that is not the owner of the table or view with permissions.
-SET SESSION AUTHORIZATION regress_rls_bob;
-GRANT SELECT ON rls_view TO regress_rls_carol;
-SELECT * FROM rls_view;
-NOTICE:  f_leak => bbb
-NOTICE:  f_leak => dad
- a |  b  
----+-----
- 2 | bbb
- 4 | dad
-(2 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM rls_view;
-                  QUERY PLAN                   
------------------------------------------------
- Custom Scan (ChunkAppend) on z1
-   Chunks excluded during startup: 0
-   ->  Seq Scan on z1 z1_1
-         Filter: (((a % 2) = 0) AND f_leak(b))
-   ->  Seq Scan on _hyper_9_49_chunk z1_2
-         Filter: (((a % 2) = 0) AND f_leak(b))
-   ->  Seq Scan on _hyper_9_50_chunk z1_3
-         Filter: (((a % 2) = 0) AND f_leak(b))
-   ->  Seq Scan on _hyper_9_51_chunk z1_4
-         Filter: (((a % 2) = 0) AND f_leak(b))
-(10 rows)
-
-SET SESSION AUTHORIZATION regress_rls_bob;
-DROP VIEW rls_view;
---
--- Command specific
---
-SET SESSION AUTHORIZATION regress_rls_alice;
-CREATE TABLE x1 (a int, b text, c text);
-SELECT public.create_hypertable('x1', 'a', chunk_time_interval=>2);
-NOTICE:  adding not-null constraint to column "a"
-DETAIL:  Time dimensions cannot have NULL values.
-      create_hypertable       
-------------------------------
- (11,regress_rls_schema,x1,t)
-(1 row)
-
-GRANT ALL ON x1 TO PUBLIC;
-INSERT INTO x1 VALUES
-    (1, 'abc', 'regress_rls_bob'),
-    (2, 'bcd', 'regress_rls_bob'),
-    (3, 'cde', 'regress_rls_carol'),
-    (4, 'def', 'regress_rls_carol'),
-    (5, 'efg', 'regress_rls_bob'),
-    (6, 'fgh', 'regress_rls_bob'),
-    (7, 'fgh', 'regress_rls_carol'),
-    (8, 'fgh', 'regress_rls_carol');
-CREATE POLICY p0 ON x1 FOR ALL USING (c = current_user);
-CREATE POLICY p1 ON x1 FOR SELECT USING (a % 2 = 0);
-CREATE POLICY p2 ON x1 FOR INSERT WITH CHECK (a % 2 = 1);
-CREATE POLICY p3 ON x1 FOR UPDATE USING (a % 2 = 0);
-CREATE POLICY p4 ON x1 FOR DELETE USING (a < 8);
-ALTER TABLE x1 ENABLE ROW LEVEL SECURITY;
-SET SESSION AUTHORIZATION regress_rls_bob;
-SELECT * FROM x1 WHERE f_leak(b) ORDER BY a ASC;
-NOTICE:  f_leak => abc
-NOTICE:  f_leak => bcd
-NOTICE:  f_leak => def
-NOTICE:  f_leak => efg
-NOTICE:  f_leak => fgh
-NOTICE:  f_leak => fgh
- a |  b  |         c         
----+-----+-------------------
- 1 | abc | regress_rls_bob
- 2 | bcd | regress_rls_bob
- 4 | def | regress_rls_carol
- 5 | efg | regress_rls_bob
- 6 | fgh | regress_rls_bob
- 8 | fgh | regress_rls_carol
-(6 rows)
-
-UPDATE x1 SET b = b || '_updt' WHERE f_leak(b) RETURNING *;
-NOTICE:  f_leak => abc
-NOTICE:  f_leak => bcd
-NOTICE:  f_leak => def
-NOTICE:  f_leak => efg
-NOTICE:  f_leak => fgh
-NOTICE:  f_leak => fgh
- a |    b     |         c         
----+----------+-------------------
- 1 | abc_updt | regress_rls_bob
- 2 | bcd_updt | regress_rls_bob
- 4 | def_updt | regress_rls_carol
- 5 | efg_updt | regress_rls_bob
- 6 | fgh_updt | regress_rls_bob
- 8 | fgh_updt | regress_rls_carol
-(6 rows)
-
-SET SESSION AUTHORIZATION regress_rls_carol;
-SELECT * FROM x1 WHERE f_leak(b) ORDER BY a ASC;
-NOTICE:  f_leak => cde
-NOTICE:  f_leak => bcd_updt
-NOTICE:  f_leak => def_updt
-NOTICE:  f_leak => fgh
-NOTICE:  f_leak => fgh_updt
-NOTICE:  f_leak => fgh_updt
- a |    b     |         c         
----+----------+-------------------
- 2 | bcd_updt | regress_rls_bob
- 3 | cde      | regress_rls_carol
- 4 | def_updt | regress_rls_carol
- 6 | fgh_updt | regress_rls_bob
- 7 | fgh      | regress_rls_carol
- 8 | fgh_updt | regress_rls_carol
-(6 rows)
-
-UPDATE x1 SET b = b || '_updt' WHERE f_leak(b) RETURNING *;
-NOTICE:  f_leak => cde
-NOTICE:  f_leak => bcd_updt
-NOTICE:  f_leak => def_updt
-NOTICE:  f_leak => fgh
-NOTICE:  f_leak => fgh_updt
-NOTICE:  f_leak => fgh_updt
- a |       b       |         c         
----+---------------+-------------------
- 3 | cde_updt      | regress_rls_carol
- 2 | bcd_updt_updt | regress_rls_bob
- 4 | def_updt_updt | regress_rls_carol
- 7 | fgh_updt      | regress_rls_carol
- 6 | fgh_updt_updt | regress_rls_bob
- 8 | fgh_updt_updt | regress_rls_carol
-(6 rows)
-
-DELETE FROM x1 WHERE f_leak(b) RETURNING *;
-NOTICE:  f_leak => cde_updt
-NOTICE:  f_leak => bcd_updt_updt
-NOTICE:  f_leak => def_updt_updt
-NOTICE:  f_leak => fgh_updt
-NOTICE:  f_leak => fgh_updt_updt
-NOTICE:  f_leak => fgh_updt_updt
- a |       b       |         c         
----+---------------+-------------------
- 3 | cde_updt      | regress_rls_carol
- 2 | bcd_updt_updt | regress_rls_bob
- 4 | def_updt_updt | regress_rls_carol
- 7 | fgh_updt      | regress_rls_carol
- 6 | fgh_updt_updt | regress_rls_bob
- 8 | fgh_updt_updt | regress_rls_carol
-(6 rows)
-
---
--- Duplicate Policy Names
---
-SET SESSION AUTHORIZATION regress_rls_alice;
-CREATE TABLE y1 (a int, b text);
-SELECT public.create_hypertable('y1', 'a', chunk_time_interval=>2);
-NOTICE:  adding not-null constraint to column "a"
-DETAIL:  Time dimensions cannot have NULL values.
-      create_hypertable       
-------------------------------
- (12,regress_rls_schema,y1,t)
-(1 row)
-
-INSERT INTO y1 VALUES(1,2);
-CREATE TABLE y2 (a int, b text);
-SELECT public.create_hypertable('y2', 'a', chunk_time_interval=>2);
-NOTICE:  adding not-null constraint to column "a"
-DETAIL:  Time dimensions cannot have NULL values.
-      create_hypertable       
-------------------------------
- (13,regress_rls_schema,y2,t)
-(1 row)
-
-GRANT ALL ON y1, y2 TO regress_rls_bob;
-CREATE POLICY p1 ON y1 FOR ALL USING (a % 2 = 0);
-CREATE POLICY p2 ON y1 FOR SELECT USING (a > 2);
-CREATE POLICY p1 ON y1 FOR SELECT USING (a % 2 = 1);  --fail
-ERROR:  policy "p1" for table "y1" already exists
-CREATE POLICY p1 ON y2 FOR ALL USING (a % 2 = 0);  --OK
-ALTER TABLE y1 ENABLE ROW LEVEL SECURITY;
-ALTER TABLE y2 ENABLE ROW LEVEL SECURITY;
---
--- Expression structure with SBV
---
--- Create view as table owner.  RLS should NOT be applied.
-SET SESSION AUTHORIZATION regress_rls_alice;
-CREATE VIEW rls_sbv WITH (security_barrier) AS
-    SELECT * FROM y1 WHERE f_leak(b);
-EXPLAIN (COSTS OFF) SELECT * FROM rls_sbv WHERE (a = 1);
-                                  QUERY PLAN                                   
--------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on y1
-   Chunks excluded during startup: 0
-   ->  Seq Scan on y1 y1_1
-         Filter: (f_leak(b) AND (a = 1))
-   ->  Index Scan using _hyper_12_57_chunk_y1_a_idx on _hyper_12_57_chunk y1_2
-         Index Cond: (a = 1)
-         Filter: f_leak(b)
-(7 rows)
-
-DROP VIEW rls_sbv;
--- Create view as role that does not own table.  RLS should be applied.
-SET SESSION AUTHORIZATION regress_rls_bob;
-CREATE VIEW rls_sbv WITH (security_barrier) AS
-    SELECT * FROM y1 WHERE f_leak(b);
-EXPLAIN (COSTS OFF) SELECT * FROM rls_sbv WHERE (a = 1);
-                                  QUERY PLAN                                   
--------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on y1
-   Chunks excluded during startup: 0
-   ->  Seq Scan on y1 y1_1
-         Filter: ((a = 1) AND ((a > 2) OR ((a % 2) = 0)) AND f_leak(b))
-   ->  Index Scan using _hyper_12_57_chunk_y1_a_idx on _hyper_12_57_chunk y1_2
-         Index Cond: (a = 1)
-         Filter: (((a > 2) OR ((a % 2) = 0)) AND f_leak(b))
-(7 rows)
-
-DROP VIEW rls_sbv;
---
--- Expression structure
---
-SET SESSION AUTHORIZATION regress_rls_alice;
-INSERT INTO y2 (SELECT x, md5(x::text) FROM generate_series(0,20) x);
-CREATE POLICY p2 ON y2 USING (a % 3 = 0);
-CREATE POLICY p3 ON y2 USING (a % 4 = 0);
-SET SESSION AUTHORIZATION regress_rls_bob;
-SELECT * FROM y2 WHERE f_leak(b);
-NOTICE:  f_leak => cfcd208495d565ef66e7dff9f98764da
-NOTICE:  f_leak => c81e728d9d4c2f636f067f89cc14862c
-NOTICE:  f_leak => eccbc87e4b5ce2fe28308fd9f2a7baf3
-NOTICE:  f_leak => a87ff679a2f3e71d9181a67b7542122c
-NOTICE:  f_leak => 1679091c5a880faf6fb5e6087eb1b2dc
-NOTICE:  f_leak => c9f0f895fb98ab9159f51fd0297e236d
-NOTICE:  f_leak => 45c48cce2e2d7fbdea1afc51c7c6ad26
-NOTICE:  f_leak => d3d9446802a44259755d38e6d163e820
-NOTICE:  f_leak => c20ad4d76fe97759aa27a0c99bff6710
-NOTICE:  f_leak => aab3238922bcc25a6f606eb525ffdc56
-NOTICE:  f_leak => 9bf31c7ff062936a96d3c8bd1f8f2ff3
-NOTICE:  f_leak => c74d97b01eae257e44aa9d5bade97baf
-NOTICE:  f_leak => 6f4922f45568161a8cdf4ad2299f6d23
-NOTICE:  f_leak => 98f13708210194c475687be6106a3b84
- a  |                b                 
-----+----------------------------------
-  0 | cfcd208495d565ef66e7dff9f98764da
-  2 | c81e728d9d4c2f636f067f89cc14862c
-  3 | eccbc87e4b5ce2fe28308fd9f2a7baf3
-  4 | a87ff679a2f3e71d9181a67b7542122c
-  6 | 1679091c5a880faf6fb5e6087eb1b2dc
-  8 | c9f0f895fb98ab9159f51fd0297e236d
-  9 | 45c48cce2e2d7fbdea1afc51c7c6ad26
- 10 | d3d9446802a44259755d38e6d163e820
- 12 | c20ad4d76fe97759aa27a0c99bff6710
- 14 | aab3238922bcc25a6f606eb525ffdc56
- 15 | 9bf31c7ff062936a96d3c8bd1f8f2ff3
- 16 | c74d97b01eae257e44aa9d5bade97baf
- 18 | 6f4922f45568161a8cdf4ad2299f6d23
- 20 | 98f13708210194c475687be6106a3b84
-(14 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM y2 WHERE f_leak(b);
-                                    QUERY PLAN                                     
------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on y2
-   Chunks excluded during startup: 0
-   ->  Seq Scan on y2 y2_1
-         Filter: ((((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)) AND f_leak(b))
-   ->  Seq Scan on _hyper_13_58_chunk y2_2
-         Filter: ((((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)) AND f_leak(b))
-   ->  Seq Scan on _hyper_13_59_chunk y2_3
-         Filter: ((((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)) AND f_leak(b))
-   ->  Seq Scan on _hyper_13_60_chunk y2_4
-         Filter: ((((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)) AND f_leak(b))
-   ->  Seq Scan on _hyper_13_61_chunk y2_5
-         Filter: ((((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)) AND f_leak(b))
-   ->  Seq Scan on _hyper_13_62_chunk y2_6
-         Filter: ((((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)) AND f_leak(b))
-   ->  Seq Scan on _hyper_13_63_chunk y2_7
-         Filter: ((((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)) AND f_leak(b))
-   ->  Seq Scan on _hyper_13_64_chunk y2_8
-         Filter: ((((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)) AND f_leak(b))
-   ->  Seq Scan on _hyper_13_65_chunk y2_9
-         Filter: ((((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)) AND f_leak(b))
-   ->  Seq Scan on _hyper_13_66_chunk y2_10
-         Filter: ((((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)) AND f_leak(b))
-   ->  Seq Scan on _hyper_13_67_chunk y2_11
-         Filter: ((((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)) AND f_leak(b))
-   ->  Seq Scan on _hyper_13_68_chunk y2_12
-         Filter: ((((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)) AND f_leak(b))
-(26 rows)
-
---
--- Qual push-down of leaky functions, when not referring to table
---
-SELECT * FROM y2 WHERE f_leak('abc');
-NOTICE:  f_leak => abc
-NOTICE:  f_leak => abc
-NOTICE:  f_leak => abc
-NOTICE:  f_leak => abc
-NOTICE:  f_leak => abc
-NOTICE:  f_leak => abc
-NOTICE:  f_leak => abc
-NOTICE:  f_leak => abc
-NOTICE:  f_leak => abc
-NOTICE:  f_leak => abc
-NOTICE:  f_leak => abc
-NOTICE:  f_leak => abc
-NOTICE:  f_leak => abc
-NOTICE:  f_leak => abc
-NOTICE:  f_leak => abc
-NOTICE:  f_leak => abc
-NOTICE:  f_leak => abc
-NOTICE:  f_leak => abc
-NOTICE:  f_leak => abc
-NOTICE:  f_leak => abc
-NOTICE:  f_leak => abc
- a  |                b                 
-----+----------------------------------
-  0 | cfcd208495d565ef66e7dff9f98764da
-  2 | c81e728d9d4c2f636f067f89cc14862c
-  3 | eccbc87e4b5ce2fe28308fd9f2a7baf3
-  4 | a87ff679a2f3e71d9181a67b7542122c
-  6 | 1679091c5a880faf6fb5e6087eb1b2dc
-  8 | c9f0f895fb98ab9159f51fd0297e236d
-  9 | 45c48cce2e2d7fbdea1afc51c7c6ad26
- 10 | d3d9446802a44259755d38e6d163e820
- 12 | c20ad4d76fe97759aa27a0c99bff6710
- 14 | aab3238922bcc25a6f606eb525ffdc56
- 15 | 9bf31c7ff062936a96d3c8bd1f8f2ff3
- 16 | c74d97b01eae257e44aa9d5bade97baf
- 18 | 6f4922f45568161a8cdf4ad2299f6d23
- 20 | 98f13708210194c475687be6106a3b84
-(14 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM y2 WHERE f_leak('abc');
-                                         QUERY PLAN                                          
----------------------------------------------------------------------------------------------
- Custom Scan (ChunkAppend) on y2
-   Chunks excluded during startup: 0
-   ->  Seq Scan on y2 y2_1
-         Filter: (f_leak('abc'::text) AND (((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)))
-   ->  Seq Scan on _hyper_13_58_chunk y2_2
-         Filter: (f_leak('abc'::text) AND (((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)))
-   ->  Seq Scan on _hyper_13_59_chunk y2_3
-         Filter: (f_leak('abc'::text) AND (((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)))
-   ->  Seq Scan on _hyper_13_60_chunk y2_4
-         Filter: (f_leak('abc'::text) AND (((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)))
-   ->  Seq Scan on _hyper_13_61_chunk y2_5
-         Filter: (f_leak('abc'::text) AND (((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)))
-   ->  Seq Scan on _hyper_13_62_chunk y2_6
-         Filter: (f_leak('abc'::text) AND (((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)))
-   ->  Seq Scan on _hyper_13_63_chunk y2_7
-         Filter: (f_leak('abc'::text) AND (((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)))
-   ->  Seq Scan on _hyper_13_64_chunk y2_8
-         Filter: (f_leak('abc'::text) AND (((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)))
-   ->  Seq Scan on _hyper_13_65_chunk y2_9
-         Filter: (f_leak('abc'::text) AND (((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)))
-   ->  Seq Scan on _hyper_13_66_chunk y2_10
-         Filter: (f_leak('abc'::text) AND (((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)))
-   ->  Seq Scan on _hyper_13_67_chunk y2_11
-         Filter: (f_leak('abc'::text) AND (((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)))
-   ->  Seq Scan on _hyper_13_68_chunk y2_12
-         Filter: (f_leak('abc'::text) AND (((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)))
-(26 rows)
-
-CREATE TABLE test_qual_pushdown (
-    abc text
-);
-INSERT INTO test_qual_pushdown VALUES ('abc'),('def');
-SELECT * FROM y2 JOIN test_qual_pushdown ON (b = abc) WHERE f_leak(abc);
-NOTICE:  f_leak => abc
-NOTICE:  f_leak => def
- a | b | abc 
----+---+-----
-(0 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM y2 JOIN test_qual_pushdown ON (b = abc) WHERE f_leak(abc);
-                               QUERY PLAN                                
--------------------------------------------------------------------------
- Hash Join
-   Hash Cond: (y2.b = test_qual_pushdown.abc)
-   ->  Append
-         ->  Seq Scan on y2 y2_1
-               Filter: (((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0))
-         ->  Seq Scan on _hyper_13_58_chunk y2_2
-               Filter: (((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0))
-         ->  Seq Scan on _hyper_13_59_chunk y2_3
-               Filter: (((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0))
-         ->  Seq Scan on _hyper_13_60_chunk y2_4
-               Filter: (((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0))
-         ->  Seq Scan on _hyper_13_61_chunk y2_5
-               Filter: (((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0))
-         ->  Seq Scan on _hyper_13_62_chunk y2_6
-               Filter: (((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0))
-         ->  Seq Scan on _hyper_13_63_chunk y2_7
-               Filter: (((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0))
-         ->  Seq Scan on _hyper_13_64_chunk y2_8
-               Filter: (((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0))
-         ->  Seq Scan on _hyper_13_65_chunk y2_9
-               Filter: (((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0))
-         ->  Seq Scan on _hyper_13_66_chunk y2_10
-               Filter: (((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0))
-         ->  Seq Scan on _hyper_13_67_chunk y2_11
-               Filter: (((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0))
-         ->  Seq Scan on _hyper_13_68_chunk y2_12
-               Filter: (((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0))
-   ->  Hash
-         ->  Seq Scan on test_qual_pushdown
-               Filter: f_leak(abc)
-(30 rows)
-
-SELECT * FROM y2 JOIN test_qual_pushdown ON (b = abc) WHERE f_leak(b);
-NOTICE:  f_leak => cfcd208495d565ef66e7dff9f98764da
-NOTICE:  f_leak => c81e728d9d4c2f636f067f89cc14862c
-NOTICE:  f_leak => eccbc87e4b5ce2fe28308fd9f2a7baf3
-NOTICE:  f_leak => a87ff679a2f3e71d9181a67b7542122c
-NOTICE:  f_leak => 1679091c5a880faf6fb5e6087eb1b2dc
-NOTICE:  f_leak => c9f0f895fb98ab9159f51fd0297e236d
-NOTICE:  f_leak => 45c48cce2e2d7fbdea1afc51c7c6ad26
-NOTICE:  f_leak => d3d9446802a44259755d38e6d163e820
-NOTICE:  f_leak => c20ad4d76fe97759aa27a0c99bff6710
-NOTICE:  f_leak => aab3238922bcc25a6f606eb525ffdc56
-NOTICE:  f_leak => 9bf31c7ff062936a96d3c8bd1f8f2ff3
-NOTICE:  f_leak => c74d97b01eae257e44aa9d5bade97baf
-NOTICE:  f_leak => 6f4922f45568161a8cdf4ad2299f6d23
-NOTICE:  f_leak => 98f13708210194c475687be6106a3b84
- a | b | abc 
----+---+-----
-(0 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM y2 JOIN test_qual_pushdown ON (b = abc) WHERE f_leak(b);
-                                          QUERY PLAN                                           
------------------------------------------------------------------------------------------------
- Hash Join
-   Hash Cond: (test_qual_pushdown.abc = y2.b)
-   ->  Seq Scan on test_qual_pushdown
-   ->  Hash
-         ->  Custom Scan (ChunkAppend) on y2
-               Chunks excluded during startup: 0
-               ->  Seq Scan on y2 y2_1
-                     Filter: ((((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)) AND f_leak(b))
-               ->  Seq Scan on _hyper_13_58_chunk y2_2
-                     Filter: ((((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)) AND f_leak(b))
-               ->  Seq Scan on _hyper_13_59_chunk y2_3
-                     Filter: ((((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)) AND f_leak(b))
-               ->  Seq Scan on _hyper_13_60_chunk y2_4
-                     Filter: ((((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)) AND f_leak(b))
-               ->  Seq Scan on _hyper_13_61_chunk y2_5
-                     Filter: ((((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)) AND f_leak(b))
-               ->  Seq Scan on _hyper_13_62_chunk y2_6
-                     Filter: ((((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)) AND f_leak(b))
-               ->  Seq Scan on _hyper_13_63_chunk y2_7
-                     Filter: ((((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)) AND f_leak(b))
-               ->  Seq Scan on _hyper_13_64_chunk y2_8
-                     Filter: ((((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)) AND f_leak(b))
-               ->  Seq Scan on _hyper_13_65_chunk y2_9
-                     Filter: ((((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)) AND f_leak(b))
-               ->  Seq Scan on _hyper_13_66_chunk y2_10
-                     Filter: ((((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)) AND f_leak(b))
-               ->  Seq Scan on _hyper_13_67_chunk y2_11
-                     Filter: ((((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)) AND f_leak(b))
-               ->  Seq Scan on _hyper_13_68_chunk y2_12
-                     Filter: ((((a % 4) = 0) OR ((a % 3) = 0) OR ((a % 2) = 0)) AND f_leak(b))
-(30 rows)
-
-DROP TABLE test_qual_pushdown;
---
--- Plancache invalidate on user change.
---
-RESET SESSION AUTHORIZATION;
-\set VERBOSITY terse \\ -- suppress cascade details
-DROP TABLE t1 CASCADE;
-NOTICE:  drop cascades to 2 other objects
-\set VERBOSITY default
-CREATE TABLE t1 (a integer);
-SELECT public.create_hypertable('t1', 'a', chunk_time_interval=>2);
-NOTICE:  adding not-null constraint to column "a"
-DETAIL:  Time dimensions cannot have NULL values.
-      create_hypertable       
-------------------------------
- (14,regress_rls_schema,t1,t)
-(1 row)
-
-GRANT SELECT ON t1 TO regress_rls_bob, regress_rls_carol;
-CREATE POLICY p1 ON t1 TO regress_rls_bob USING ((a % 2) = 0);
-CREATE POLICY p2 ON t1 TO regress_rls_carol USING ((a % 4) = 0);
-ALTER TABLE t1 ENABLE ROW LEVEL SECURITY;
--- Prepare as regress_rls_bob
-SET ROLE regress_rls_bob;
-PREPARE role_inval AS SELECT * FROM t1;
--- Check plan
-EXPLAIN (COSTS OFF) EXECUTE role_inval;
-       QUERY PLAN        
--------------------------
- Seq Scan on t1
-   Filter: ((a % 2) = 0)
-(2 rows)
-
--- Change to regress_rls_carol
-SET ROLE regress_rls_carol;
--- Check plan- should be different
-EXPLAIN (COSTS OFF) EXECUTE role_inval;
-       QUERY PLAN        
--------------------------
- Seq Scan on t1
-   Filter: ((a % 4) = 0)
-(2 rows)
-
--- Change back to regress_rls_bob
-SET ROLE regress_rls_bob;
--- Check plan- should be back to original
-EXPLAIN (COSTS OFF) EXECUTE role_inval;
-       QUERY PLAN        
--------------------------
- Seq Scan on t1
-   Filter: ((a % 2) = 0)
-(2 rows)
-
---
--- CTE and RLS
---
-RESET SESSION AUTHORIZATION;
-DROP TABLE t1 CASCADE;
-CREATE TABLE t1 (a integer, b text);
-SELECT public.create_hypertable('t1', 'a', chunk_time_interval=>2);
-NOTICE:  adding not-null constraint to column "a"
-DETAIL:  Time dimensions cannot have NULL values.
-      create_hypertable       
-------------------------------
- (15,regress_rls_schema,t1,t)
-(1 row)
-
-CREATE POLICY p1 ON t1 USING (a % 2 = 0);
-ALTER TABLE t1 ENABLE ROW LEVEL SECURITY;
-GRANT ALL ON t1 TO regress_rls_bob;
-INSERT INTO t1 (SELECT x, md5(x::text) FROM generate_series(0,20) x);
-SET SESSION AUTHORIZATION regress_rls_bob;
-WITH cte1 AS (SELECT * FROM t1 WHERE f_leak(b)) SELECT * FROM cte1;
-NOTICE:  f_leak => cfcd208495d565ef66e7dff9f98764da
-NOTICE:  f_leak => c81e728d9d4c2f636f067f89cc14862c
-NOTICE:  f_leak => a87ff679a2f3e71d9181a67b7542122c
-NOTICE:  f_leak => 1679091c5a880faf6fb5e6087eb1b2dc
-NOTICE:  f_leak => c9f0f895fb98ab9159f51fd0297e236d
-NOTICE:  f_leak => d3d9446802a44259755d38e6d163e820
-NOTICE:  f_leak => c20ad4d76fe97759aa27a0c99bff6710
-NOTICE:  f_leak => aab3238922bcc25a6f606eb525ffdc56
-NOTICE:  f_leak => c74d97b01eae257e44aa9d5bade97baf
-NOTICE:  f_leak => 6f4922f45568161a8cdf4ad2299f6d23
-NOTICE:  f_leak => 98f13708210194c475687be6106a3b84
- a  |                b                 
-----+----------------------------------
-  0 | cfcd208495d565ef66e7dff9f98764da
-  2 | c81e728d9d4c2f636f067f89cc14862c
-  4 | a87ff679a2f3e71d9181a67b7542122c
-  6 | 1679091c5a880faf6fb5e6087eb1b2dc
-  8 | c9f0f895fb98ab9159f51fd0297e236d
- 10 | d3d9446802a44259755d38e6d163e820
- 12 | c20ad4d76fe97759aa27a0c99bff6710
- 14 | aab3238922bcc25a6f606eb525ffdc56
- 16 | c74d97b01eae257e44aa9d5bade97baf
- 18 | 6f4922f45568161a8cdf4ad2299f6d23
- 20 | 98f13708210194c475687be6106a3b84
-(11 rows)
-
-EXPLAIN (COSTS OFF) WITH cte1 AS (SELECT * FROM t1 WHERE f_leak(b)) SELECT * FROM cte1;
-                      QUERY PLAN                       
--------------------------------------------------------
- CTE Scan on cte1
-   CTE cte1
-     ->  Custom Scan (ChunkAppend) on t1
-           Chunks excluded during startup: 0
-           ->  Seq Scan on t1 t1_1
-                 Filter: (((a % 2) = 0) AND f_leak(b))
-           ->  Seq Scan on _hyper_15_69_chunk t1_2
-                 Filter: (((a % 2) = 0) AND f_leak(b))
-           ->  Seq Scan on _hyper_15_70_chunk t1_3
-                 Filter: (((a % 2) = 0) AND f_leak(b))
-           ->  Seq Scan on _hyper_15_71_chunk t1_4
-                 Filter: (((a % 2) = 0) AND f_leak(b))
-           ->  Seq Scan on _hyper_15_72_chunk t1_5
-                 Filter: (((a % 2) = 0) AND f_leak(b))
-           ->  Seq Scan on _hyper_15_73_chunk t1_6
-                 Filter: (((a % 2) = 0) AND f_leak(b))
-           ->  Seq Scan on _hyper_15_74_chunk t1_7
-                 Filter: (((a % 2) = 0) AND f_leak(b))
-           ->  Seq Scan on _hyper_15_75_chunk t1_8
-                 Filter: (((a % 2) = 0) AND f_leak(b))
-           ->  Seq Scan on _hyper_15_76_chunk t1_9
-                 Filter: (((a % 2) = 0) AND f_leak(b))
-           ->  Seq Scan on _hyper_15_77_chunk t1_10
-                 Filter: (((a % 2) = 0) AND f_leak(b))
-           ->  Seq Scan on _hyper_15_78_chunk t1_11
-                 Filter: (((a % 2) = 0) AND f_leak(b))
-           ->  Seq Scan on _hyper_15_79_chunk t1_12
-                 Filter: (((a % 2) = 0) AND f_leak(b))
-(28 rows)
-
-WITH cte1 AS (UPDATE t1 SET a = a + 1 RETURNING *) SELECT * FROM cte1; --fail
-ERROR:  new row violates row-level security policy for table "t1"
-WITH cte1 AS (UPDATE t1 SET a = a RETURNING *) SELECT * FROM cte1; --ok
- a  |                b                 
-----+----------------------------------
-  0 | cfcd208495d565ef66e7dff9f98764da
-  2 | c81e728d9d4c2f636f067f89cc14862c
-  4 | a87ff679a2f3e71d9181a67b7542122c
-  6 | 1679091c5a880faf6fb5e6087eb1b2dc
-  8 | c9f0f895fb98ab9159f51fd0297e236d
- 10 | d3d9446802a44259755d38e6d163e820
- 12 | c20ad4d76fe97759aa27a0c99bff6710
- 14 | aab3238922bcc25a6f606eb525ffdc56
- 16 | c74d97b01eae257e44aa9d5bade97baf
- 18 | 6f4922f45568161a8cdf4ad2299f6d23
- 20 | 98f13708210194c475687be6106a3b84
-(11 rows)
-
-WITH cte1 AS (INSERT INTO t1 VALUES (21, 'Fail') RETURNING *) SELECT * FROM cte1; --fail
-ERROR:  new row violates row-level security policy for table "t1"
-WITH cte1 AS (INSERT INTO t1 VALUES (20, 'Success') RETURNING *) SELECT * FROM cte1; --ok
- a  |    b    
-----+---------
- 20 | Success
-(1 row)
-
---
--- Rename Policy
---
-RESET SESSION AUTHORIZATION;
-ALTER POLICY p1 ON t1 RENAME TO p1; --fail
-ERROR:  policy "p1" for table "t1" already exists
-SELECT polname, relname
-    FROM pg_policy pol
-    JOIN pg_class pc ON (pc.oid = pol.polrelid)
-    WHERE relname = 't1';
- polname | relname 
----------+---------
- p1      | t1
-(1 row)
-
-ALTER POLICY p1 ON t1 RENAME TO p2; --ok
-SELECT polname, relname
-    FROM pg_policy pol
-    JOIN pg_class pc ON (pc.oid = pol.polrelid)
-    WHERE relname = 't1';
- polname | relname 
----------+---------
- p2      | t1
-(1 row)
-
---
--- Check INSERT SELECT
---
-SET SESSION AUTHORIZATION regress_rls_bob;
-CREATE TABLE t2 (a integer, b text);
-SELECT public.create_hypertable('t2', 'a', chunk_time_interval=>2);
-NOTICE:  adding not-null constraint to column "a"
-DETAIL:  Time dimensions cannot have NULL values.
-      create_hypertable       
-------------------------------
- (16,regress_rls_schema,t2,t)
-(1 row)
-
-INSERT INTO t2 (SELECT * FROM t1);
-EXPLAIN (COSTS OFF) INSERT INTO t2 (SELECT * FROM t1);
-                          QUERY PLAN                          
---------------------------------------------------------------
- Custom Scan (HypertableModify)
-   ->  Insert on t2
-         ->  Custom Scan (ChunkDispatch)
-               ->  Append
-                     ->  Seq Scan on t1 t1_1
-                           Filter: ((a % 2) = 0)
-                     ->  Seq Scan on _hyper_15_69_chunk t1_2
-                           Filter: ((a % 2) = 0)
-                     ->  Seq Scan on _hyper_15_70_chunk t1_3
-                           Filter: ((a % 2) = 0)
-                     ->  Seq Scan on _hyper_15_71_chunk t1_4
-                           Filter: ((a % 2) = 0)
-                     ->  Seq Scan on _hyper_15_72_chunk t1_5
-                           Filter: ((a % 2) = 0)
-                     ->  Seq Scan on _hyper_15_73_chunk t1_6
-                           Filter: ((a % 2) = 0)
-                     ->  Seq Scan on _hyper_15_74_chunk t1_7
-                           Filter: ((a % 2) = 0)
-                     ->  Seq Scan on _hyper_15_75_chunk t1_8
-                           Filter: ((a % 2) = 0)
-                     ->  Seq Scan on _hyper_15_76_chunk t1_9
-                           Filter: ((a % 2) = 0)
-                     ->  Seq Scan on _hyper_15_77_chunk t1_10
-                           Filter: ((a % 2) = 0)
-                     ->  Seq Scan on _hyper_15_78_chunk t1_11
-                           Filter: ((a % 2) = 0)
-                     ->  Seq Scan on _hyper_15_79_chunk t1_12
-                           Filter: ((a % 2) = 0)
-(28 rows)
-
-SELECT * FROM t2;
- a  |                b                 
-----+----------------------------------
-  0 | cfcd208495d565ef66e7dff9f98764da
-  2 | c81e728d9d4c2f636f067f89cc14862c
-  4 | a87ff679a2f3e71d9181a67b7542122c
-  6 | 1679091c5a880faf6fb5e6087eb1b2dc
-  8 | c9f0f895fb98ab9159f51fd0297e236d
- 10 | d3d9446802a44259755d38e6d163e820
- 12 | c20ad4d76fe97759aa27a0c99bff6710
- 14 | aab3238922bcc25a6f606eb525ffdc56
- 16 | c74d97b01eae257e44aa9d5bade97baf
- 18 | 6f4922f45568161a8cdf4ad2299f6d23
- 20 | 98f13708210194c475687be6106a3b84
- 20 | Success
-(12 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM t2;
-                 QUERY PLAN                 
---------------------------------------------
- Append
-   ->  Seq Scan on t2 t2_1
-   ->  Seq Scan on _hyper_16_80_chunk t2_2
-   ->  Seq Scan on _hyper_16_81_chunk t2_3
-   ->  Seq Scan on _hyper_16_82_chunk t2_4
-   ->  Seq Scan on _hyper_16_83_chunk t2_5
-   ->  Seq Scan on _hyper_16_84_chunk t2_6
-   ->  Seq Scan on _hyper_16_85_chunk t2_7
-   ->  Seq Scan on _hyper_16_86_chunk t2_8
-   ->  Seq Scan on _hyper_16_87_chunk t2_9
-   ->  Seq Scan on _hyper_16_88_chunk t2_10
-   ->  Seq Scan on _hyper_16_89_chunk t2_11
-   ->  Seq Scan on _hyper_16_90_chunk t2_12
-(13 rows)
-
-CREATE TABLE t3 AS SELECT * FROM t1;
-SELECT public.create_hypertable('t2', 'a', chunk_time_interval=>2);
-ERROR:  table "t2" is already a hypertable
-SELECT * FROM t3;
- a  |                b                 
-----+----------------------------------
-  0 | cfcd208495d565ef66e7dff9f98764da
-  2 | c81e728d9d4c2f636f067f89cc14862c
-  4 | a87ff679a2f3e71d9181a67b7542122c
-  6 | 1679091c5a880faf6fb5e6087eb1b2dc
-  8 | c9f0f895fb98ab9159f51fd0297e236d
- 10 | d3d9446802a44259755d38e6d163e820
- 12 | c20ad4d76fe97759aa27a0c99bff6710
- 14 | aab3238922bcc25a6f606eb525ffdc56
- 16 | c74d97b01eae257e44aa9d5bade97baf
- 18 | 6f4922f45568161a8cdf4ad2299f6d23
- 20 | 98f13708210194c475687be6106a3b84
- 20 | Success
-(12 rows)
-
-SELECT * INTO t4 FROM t1;
-SELECT * FROM t4;
- a  |                b                 
-----+----------------------------------
-  0 | cfcd208495d565ef66e7dff9f98764da
-  2 | c81e728d9d4c2f636f067f89cc14862c
-  4 | a87ff679a2f3e71d9181a67b7542122c
-  6 | 1679091c5a880faf6fb5e6087eb1b2dc
-  8 | c9f0f895fb98ab9159f51fd0297e236d
- 10 | d3d9446802a44259755d38e6d163e820
- 12 | c20ad4d76fe97759aa27a0c99bff6710
- 14 | aab3238922bcc25a6f606eb525ffdc56
- 16 | c74d97b01eae257e44aa9d5bade97baf
- 18 | 6f4922f45568161a8cdf4ad2299f6d23
- 20 | 98f13708210194c475687be6106a3b84
- 20 | Success
-(12 rows)
-
---
--- RLS with JOIN
---
-SET SESSION AUTHORIZATION regress_rls_alice;
-CREATE TABLE blog (id integer, author text, post text);
-SELECT public.create_hypertable('blog', 'id', chunk_time_interval=>2);
-NOTICE:  adding not-null constraint to column "id"
-DETAIL:  Time dimensions cannot have NULL values.
-       create_hypertable        
---------------------------------
- (17,regress_rls_schema,blog,t)
-(1 row)
-
-CREATE TABLE comment (blog_id integer, message text);
-SELECT public.create_hypertable('comment', 'blog_id', chunk_time_interval=>2);
-NOTICE:  adding not-null constraint to column "blog_id"
-DETAIL:  Time dimensions cannot have NULL values.
-         create_hypertable         
------------------------------------
- (18,regress_rls_schema,comment,t)
-(1 row)
-
-GRANT ALL ON blog, comment TO regress_rls_bob;
-CREATE POLICY blog_1 ON blog USING (id % 2 = 0);
-ALTER TABLE blog ENABLE ROW LEVEL SECURITY;
-INSERT INTO blog VALUES
-    (1, 'alice', 'blog #1'),
-    (2, 'bob', 'blog #1'),
-    (3, 'alice', 'blog #2'),
-    (4, 'alice', 'blog #3'),
-    (5, 'john', 'blog #1');
-INSERT INTO comment VALUES
-    (1, 'cool blog'),
-    (1, 'fun blog'),
-    (3, 'crazy blog'),
-    (5, 'what?'),
-    (4, 'insane!'),
-    (2, 'who did it?');
-SET SESSION AUTHORIZATION regress_rls_bob;
--- Check RLS JOIN with Non-RLS.
-SELECT id, author, message FROM blog JOIN comment ON id = blog_id;
- id | author |   message   
-----+--------+-------------
-  2 | bob    | who did it?
-  4 | alice  | insane!
-(2 rows)
-
--- Check Non-RLS JOIN with RLS.
-SELECT id, author, message FROM comment JOIN blog ON id = blog_id;
- id | author |   message   
-----+--------+-------------
-  2 | bob    | who did it?
-  4 | alice  | insane!
-(2 rows)
-
-SET SESSION AUTHORIZATION regress_rls_alice;
-CREATE POLICY comment_1 ON comment USING (blog_id < 4);
-ALTER TABLE comment ENABLE ROW LEVEL SECURITY;
-SET SESSION AUTHORIZATION regress_rls_bob;
--- Check RLS JOIN RLS
-SELECT id, author, message FROM blog JOIN comment ON id = blog_id;
- id | author |   message   
-----+--------+-------------
-  2 | bob    | who did it?
-(1 row)
-
-SELECT id, author, message FROM comment JOIN blog ON id = blog_id;
- id | author |   message   
-----+--------+-------------
-  2 | bob    | who did it?
-(1 row)
-
-SET SESSION AUTHORIZATION regress_rls_alice;
-DROP TABLE blog;
-DROP TABLE comment;
---
--- Default Deny Policy
---
-RESET SESSION AUTHORIZATION;
-DROP POLICY p2 ON t1;
-ALTER TABLE t1 OWNER TO regress_rls_alice;
--- Check that default deny does not apply to superuser.
-RESET SESSION AUTHORIZATION;
-SELECT * FROM t1;
- a  |                b                 
-----+----------------------------------
-  1 | c4ca4238a0b923820dcc509a6f75849b
-  0 | cfcd208495d565ef66e7dff9f98764da
-  3 | eccbc87e4b5ce2fe28308fd9f2a7baf3
-  2 | c81e728d9d4c2f636f067f89cc14862c
-  5 | e4da3b7fbbce2345d7772b0674a318d5
-  4 | a87ff679a2f3e71d9181a67b7542122c
-  7 | 8f14e45fceea167a5a36dedd4bea2543
-  6 | 1679091c5a880faf6fb5e6087eb1b2dc
-  9 | 45c48cce2e2d7fbdea1afc51c7c6ad26
-  8 | c9f0f895fb98ab9159f51fd0297e236d
- 11 | 6512bd43d9caa6e02c990b0a82652dca
- 10 | d3d9446802a44259755d38e6d163e820
- 13 | c51ce410c124a10e0db5e4b97fc2af39
- 12 | c20ad4d76fe97759aa27a0c99bff6710
- 15 | 9bf31c7ff062936a96d3c8bd1f8f2ff3
- 14 | aab3238922bcc25a6f606eb525ffdc56
- 17 | 70efdf2ec9b086079795c442636b55fb
- 16 | c74d97b01eae257e44aa9d5bade97baf
- 19 | 1f0e3dad99908345f7439f8ffabdffc4
- 18 | 6f4922f45568161a8cdf4ad2299f6d23
- 20 | 98f13708210194c475687be6106a3b84
- 20 | Success
-(22 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM t1;
-                 QUERY PLAN                 
---------------------------------------------
- Append
-   ->  Seq Scan on t1 t1_1
-   ->  Seq Scan on _hyper_15_69_chunk t1_2
-   ->  Seq Scan on _hyper_15_70_chunk t1_3
-   ->  Seq Scan on _hyper_15_71_chunk t1_4
-   ->  Seq Scan on _hyper_15_72_chunk t1_5
-   ->  Seq Scan on _hyper_15_73_chunk t1_6
-   ->  Seq Scan on _hyper_15_74_chunk t1_7
-   ->  Seq Scan on _hyper_15_75_chunk t1_8
-   ->  Seq Scan on _hyper_15_76_chunk t1_9
-   ->  Seq Scan on _hyper_15_77_chunk t1_10
-   ->  Seq Scan on _hyper_15_78_chunk t1_11
-   ->  Seq Scan on _hyper_15_79_chunk t1_12
-(13 rows)
-
--- Check that default deny does not apply to table owner.
-SET SESSION AUTHORIZATION regress_rls_alice;
-SELECT * FROM t1;
- a  |                b                 
-----+----------------------------------
-  1 | c4ca4238a0b923820dcc509a6f75849b
-  0 | cfcd208495d565ef66e7dff9f98764da
-  3 | eccbc87e4b5ce2fe28308fd9f2a7baf3
-  2 | c81e728d9d4c2f636f067f89cc14862c
-  5 | e4da3b7fbbce2345d7772b0674a318d5
-  4 | a87ff679a2f3e71d9181a67b7542122c
-  7 | 8f14e45fceea167a5a36dedd4bea2543
-  6 | 1679091c5a880faf6fb5e6087eb1b2dc
-  9 | 45c48cce2e2d7fbdea1afc51c7c6ad26
-  8 | c9f0f895fb98ab9159f51fd0297e236d
- 11 | 6512bd43d9caa6e02c990b0a82652dca
- 10 | d3d9446802a44259755d38e6d163e820
- 13 | c51ce410c124a10e0db5e4b97fc2af39
- 12 | c20ad4d76fe97759aa27a0c99bff6710
- 15 | 9bf31c7ff062936a96d3c8bd1f8f2ff3
- 14 | aab3238922bcc25a6f606eb525ffdc56
- 17 | 70efdf2ec9b086079795c442636b55fb
- 16 | c74d97b01eae257e44aa9d5bade97baf
- 19 | 1f0e3dad99908345f7439f8ffabdffc4
- 18 | 6f4922f45568161a8cdf4ad2299f6d23
- 20 | 98f13708210194c475687be6106a3b84
- 20 | Success
-(22 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM t1;
-                 QUERY PLAN                 
---------------------------------------------
- Append
-   ->  Seq Scan on t1 t1_1
-   ->  Seq Scan on _hyper_15_69_chunk t1_2
-   ->  Seq Scan on _hyper_15_70_chunk t1_3
-   ->  Seq Scan on _hyper_15_71_chunk t1_4
-   ->  Seq Scan on _hyper_15_72_chunk t1_5
-   ->  Seq Scan on _hyper_15_73_chunk t1_6
-   ->  Seq Scan on _hyper_15_74_chunk t1_7
-   ->  Seq Scan on _hyper_15_75_chunk t1_8
-   ->  Seq Scan on _hyper_15_76_chunk t1_9
-   ->  Seq Scan on _hyper_15_77_chunk t1_10
-   ->  Seq Scan on _hyper_15_78_chunk t1_11
-   ->  Seq Scan on _hyper_15_79_chunk t1_12
-(13 rows)
-
--- Check that default deny applies to non-owner/non-superuser when RLS on.
-SET SESSION AUTHORIZATION regress_rls_bob;
-SET row_security TO ON;
-SELECT * FROM t1;
- a | b 
----+---
-(0 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM t1;
-        QUERY PLAN        
---------------------------
- Result
-   One-Time Filter: false
-(2 rows)
-
-SET SESSION AUTHORIZATION regress_rls_bob;
-SELECT * FROM t1;
- a | b 
----+---
-(0 rows)
-
-EXPLAIN (COSTS OFF) SELECT * FROM t1;
-        QUERY PLAN        
---------------------------
- Result
-   One-Time Filter: false
-(2 rows)
-
---
--- COPY TO/FROM
---
-RESET SESSION AUTHORIZATION;
-DROP TABLE copy_t CASCADE;
-ERROR:  table "copy_t" does not exist
-CREATE TABLE copy_t (a integer, b text);
-SELECT public.create_hypertable('copy_t', 'a', chunk_time_interval=>2);
-NOTICE:  adding not-null constraint to column "a"
-DETAIL:  Time dimensions cannot have NULL values.
-        create_hypertable         
-----------------------------------
- (19,regress_rls_schema,copy_t,t)
-(1 row)
-
-CREATE POLICY p1 ON copy_t USING (a % 2 = 0);
-ALTER TABLE copy_t ENABLE ROW LEVEL SECURITY;
-GRANT ALL ON copy_t TO regress_rls_bob, regress_rls_exempt_user;
-INSERT INTO copy_t (SELECT x, md5(x::text) FROM generate_series(0,10) x);
--- Check COPY TO as Superuser/owner.
-RESET SESSION AUTHORIZATION;
-SET row_security TO OFF;
-COPY (SELECT * FROM copy_t ORDER BY a ASC) TO STDOUT WITH DELIMITER ',';
-0,cfcd208495d565ef66e7dff9f98764da
-1,c4ca4238a0b923820dcc509a6f75849b
-2,c81e728d9d4c2f636f067f89cc14862c
-3,eccbc87e4b5ce2fe28308fd9f2a7baf3
-4,a87ff679a2f3e71d9181a67b7542122c
-5,e4da3b7fbbce2345d7772b0674a318d5
-6,1679091c5a880faf6fb5e6087eb1b2dc
-7,8f14e45fceea167a5a36dedd4bea2543
-8,c9f0f895fb98ab9159f51fd0297e236d
-9,45c48cce2e2d7fbdea1afc51c7c6ad26
-10,d3d9446802a44259755d38e6d163e820
-SET row_security TO ON;
-COPY (SELECT * FROM copy_t ORDER BY a ASC) TO STDOUT WITH DELIMITER ',';
-0,cfcd208495d565ef66e7dff9f98764da
-1,c4ca4238a0b923820dcc509a6f75849b
-2,c81e728d9d4c2f636f067f89cc14862c
-3,eccbc87e4b5ce2fe28308fd9f2a7baf3
-4,a87ff679a2f3e71d9181a67b7542122c
-5,e4da3b7fbbce2345d7772b0674a318d5
-6,1679091c5a880faf6fb5e6087eb1b2dc
-7,8f14e45fceea167a5a36dedd4bea2543
-8,c9f0f895fb98ab9159f51fd0297e236d
-9,45c48cce2e2d7fbdea1afc51c7c6ad26
-10,d3d9446802a44259755d38e6d163e820
--- Check COPY TO as user with permissions.
-SET SESSION AUTHORIZATION regress_rls_bob;
-SET row_security TO OFF;
-COPY (SELECT * FROM copy_t ORDER BY a ASC) TO STDOUT WITH DELIMITER ','; --fail - would be affected by RLS
-ERROR:  query would be affected by row-level security policy for table "copy_t"
-SET row_security TO ON;
-COPY (SELECT * FROM copy_t ORDER BY a ASC) TO STDOUT WITH DELIMITER ','; --ok
-0,cfcd208495d565ef66e7dff9f98764da
-2,c81e728d9d4c2f636f067f89cc14862c
-4,a87ff679a2f3e71d9181a67b7542122c
-6,1679091c5a880faf6fb5e6087eb1b2dc
-8,c9f0f895fb98ab9159f51fd0297e236d
-10,d3d9446802a44259755d38e6d163e820
--- Check COPY TO as user with permissions and BYPASSRLS
-SET SESSION AUTHORIZATION regress_rls_exempt_user;
-SET row_security TO OFF;
-COPY (SELECT * FROM copy_t ORDER BY a ASC) TO STDOUT WITH DELIMITER ','; --ok
-0,cfcd208495d565ef66e7dff9f98764da
-1,c4ca4238a0b923820dcc509a6f75849b
-2,c81e728d9d4c2f636f067f89cc14862c
-3,eccbc87e4b5ce2fe28308fd9f2a7baf3
-4,a87ff679a2f3e71d9181a67b7542122c
-5,e4da3b7fbbce2345d7772b0674a318d5
-6,1679091c5a880faf6fb5e6087eb1b2dc
-7,8f14e45fceea167a5a36dedd4bea2543
-8,c9f0f895fb98ab9159f51fd0297e236d
-9,45c48cce2e2d7fbdea1afc51c7c6ad26
-10,d3d9446802a44259755d38e6d163e820
-SET row_security TO ON;
-COPY (SELECT * FROM copy_t ORDER BY a ASC) TO STDOUT WITH DELIMITER ','; --ok
-0,cfcd208495d565ef66e7dff9f98764da
-1,c4ca4238a0b923820dcc509a6f75849b
-2,c81e728d9d4c2f636f067f89cc14862c
-3,eccbc87e4b5ce2fe28308fd9f2a7baf3
-4,a87ff679a2f3e71d9181a67b7542122c
-5,e4da3b7fbbce2345d7772b0674a318d5
-6,1679091c5a880faf6fb5e6087eb1b2dc
-7,8f14e45fceea167a5a36dedd4bea2543
-8,c9f0f895fb98ab9159f51fd0297e236d
-9,45c48cce2e2d7fbdea1afc51c7c6ad26
-10,d3d9446802a44259755d38e6d163e820
--- Check COPY TO as user without permissions. SET row_security TO OFF;
-SET SESSION AUTHORIZATION regress_rls_carol;
-SET row_security TO OFF;
-COPY (SELECT * FROM copy_t ORDER BY a ASC) TO STDOUT WITH DELIMITER ','; --fail - would be affected by RLS
-ERROR:  query would be affected by row-level security policy for table "copy_t"
-SET row_security TO ON;
-COPY (SELECT * FROM copy_t ORDER BY a ASC) TO STDOUT WITH DELIMITER ','; --fail - permission denied
-ERROR:  permission denied for table copy_t
--- Check COPY relation TO; keep it just one row to avoid reordering issues
-RESET SESSION AUTHORIZATION;
-SET row_security TO ON;
-CREATE TABLE copy_rel_to (a integer, b text);
-SELECT public.create_hypertable('copy_rel_to', 'a', chunk_time_interval=>2);
-NOTICE:  adding not-null constraint to column "a"
-DETAIL:  Time dimensions cannot have NULL values.
-           create_hypertable           
----------------------------------------
- (20,regress_rls_schema,copy_rel_to,t)
-(1 row)
-
-CREATE POLICY p1 ON copy_rel_to USING (a % 2 = 0);
-ALTER TABLE copy_rel_to ENABLE ROW LEVEL SECURITY;
-GRANT ALL ON copy_rel_to TO regress_rls_bob, regress_rls_exempt_user;
-INSERT INTO copy_rel_to VALUES (1, md5('1'));
--- Check COPY TO as Superuser/owner.
-RESET SESSION AUTHORIZATION;
-SET row_security TO OFF;
-COPY (SELECT * FROM copy_rel_to) TO STDOUT WITH DELIMITER ',';
-1,c4ca4238a0b923820dcc509a6f75849b
-SET row_security TO ON;
-COPY (SELECT * FROM copy_rel_to) TO STDOUT WITH DELIMITER ',';
-1,c4ca4238a0b923820dcc509a6f75849b
--- Check COPY TO as user with permissions.
-SET SESSION AUTHORIZATION regress_rls_bob;
-SET row_security TO OFF;
-COPY (SELECT * FROM copy_rel_to) TO STDOUT WITH DELIMITER ','; --fail - would be affected by RLS
-ERROR:  query would be affected by row-level security policy for table "copy_rel_to"
-SET row_security TO ON;
-COPY (SELECT * FROM copy_rel_to) TO STDOUT WITH DELIMITER ','; --ok
--- Check COPY TO as user with permissions and BYPASSRLS
-SET SESSION AUTHORIZATION regress_rls_exempt_user;
-SET row_security TO OFF;
-COPY (SELECT * FROM copy_rel_to) TO STDOUT WITH DELIMITER ','; --ok
-1,c4ca4238a0b923820dcc509a6f75849b
-SET row_security TO ON;
-COPY (SELECT * FROM copy_rel_to) TO STDOUT WITH DELIMITER ','; --ok
-1,c4ca4238a0b923820dcc509a6f75849b
--- Check COPY TO as user without permissions. SET row_security TO OFF;
-SET SESSION AUTHORIZATION regress_rls_carol;
-SET row_security TO OFF;
-COPY (SELECT * FROM copy_rel_to) TO STDOUT WITH DELIMITER ','; --fail - permission denied
-ERROR:  query would be affected by row-level security policy for table "copy_rel_to"
-SET row_security TO ON;
-COPY (SELECT * FROM copy_rel_to) TO STDOUT WITH DELIMITER ','; --fail - permission denied
-ERROR:  permission denied for table copy_rel_to
--- Check COPY FROM as Superuser/owner.
-RESET SESSION AUTHORIZATION;
-SET row_security TO OFF;
-COPY copy_t FROM STDIN; --ok
-SET row_security TO ON;
-COPY copy_t FROM STDIN; --ok
--- Check COPY FROM as user with permissions.
-SET SESSION AUTHORIZATION regress_rls_bob;
-SET row_security TO OFF;
-COPY copy_t FROM STDIN; --fail - would be affected by RLS.
-ERROR:  query would be affected by row-level security policy for table "copy_t"
-SET row_security TO ON;
-COPY copy_t FROM STDIN; --fail - COPY FROM not supported by RLS.
-ERROR:  COPY FROM not supported with row-level security
-HINT:  Use INSERT statements instead.
--- Check COPY FROM as user with permissions and BYPASSRLS
-SET SESSION AUTHORIZATION regress_rls_exempt_user;
-SET row_security TO ON;
-COPY copy_t FROM STDIN; --ok
--- Check COPY FROM as user without permissions.
-SET SESSION AUTHORIZATION regress_rls_carol;
-SET row_security TO OFF;
-COPY copy_t FROM STDIN; --fail - permission denied.
-ERROR:  permission denied for table copy_t
-SET row_security TO ON;
-COPY copy_t FROM STDIN; --fail - permission denied.
-ERROR:  permission denied for table copy_t
-RESET SESSION AUTHORIZATION;
-DROP TABLE copy_t;
-DROP TABLE copy_rel_to CASCADE;
--- Check WHERE CURRENT OF
-SET SESSION AUTHORIZATION regress_rls_alice;
-CREATE TABLE current_check (currentid int, payload text, rlsuser text);
-SELECT public.create_hypertable('current_check', 'currentid', chunk_time_interval=>10);
-NOTICE:  adding not-null constraint to column "currentid"
-DETAIL:  Time dimensions cannot have NULL values.
-            create_hypertable            
------------------------------------------
- (21,regress_rls_schema,current_check,t)
-(1 row)
-
-GRANT ALL ON current_check TO PUBLIC;
-INSERT INTO current_check VALUES
-    (1, 'abc', 'regress_rls_bob'),
-    (2, 'bcd', 'regress_rls_bob'),
-    (3, 'cde', 'regress_rls_bob'),
-    (4, 'def', 'regress_rls_bob');
-CREATE POLICY p1 ON current_check FOR SELECT USING (currentid % 2 = 0);
-CREATE POLICY p2 ON current_check FOR DELETE USING (currentid = 4 AND rlsuser = current_user);
-CREATE POLICY p3 ON current_check FOR UPDATE USING (currentid = 4) WITH CHECK (rlsuser = current_user);
-ALTER TABLE current_check ENABLE ROW LEVEL SECURITY;
-SET SESSION AUTHORIZATION regress_rls_bob;
--- Can SELECT even rows
-SELECT * FROM current_check;
- currentid | payload |     rlsuser     
------------+---------+-----------------
-         2 | bcd     | regress_rls_bob
-         4 | def     | regress_rls_bob
-(2 rows)
-
--- Cannot UPDATE row 2
-UPDATE current_check SET payload = payload || '_new' WHERE currentid = 2 RETURNING *;
- currentid | payload | rlsuser 
------------+---------+---------
-(0 rows)
-
-BEGIN;
--- WHERE CURRENT OF does not work with custom scan nodes
--- so we have to disable chunk append here
-SET timescaledb.enable_chunk_append TO false;
-DECLARE current_check_cursor SCROLL CURSOR FOR SELECT * FROM current_check;
--- Returns rows that can be seen according to SELECT policy, like plain SELECT
--- above (even rows)
-FETCH ABSOLUTE 1 FROM current_check_cursor;
- currentid | payload |     rlsuser     
------------+---------+-----------------
-         2 | bcd     | regress_rls_bob
-(1 row)
-
--- Still cannot UPDATE row 2 through cursor
-UPDATE current_check SET payload = payload || '_new' WHERE CURRENT OF current_check_cursor RETURNING *;
- currentid | payload | rlsuser 
------------+---------+---------
-(0 rows)
-
--- Can update row 4 through cursor, which is the next visible row
-FETCH RELATIVE 1 FROM current_check_cursor;
- currentid | payload |     rlsuser     
------------+---------+-----------------
-         4 | def     | regress_rls_bob
-(1 row)
-
-UPDATE current_check SET payload = payload || '_new' WHERE CURRENT OF current_check_cursor RETURNING *;
- currentid | payload |     rlsuser     
------------+---------+-----------------
-         4 | def_new | regress_rls_bob
-(1 row)
-
-SELECT * FROM current_check;
- currentid | payload |     rlsuser     
------------+---------+-----------------
-         2 | bcd     | regress_rls_bob
-         4 | def_new | regress_rls_bob
-(2 rows)
-
--- Plan should be a subquery TID scan
-EXPLAIN (COSTS OFF) UPDATE current_check SET payload = payload WHERE CURRENT OF current_check_cursor;
-                            QUERY PLAN                             
--------------------------------------------------------------------
- Custom Scan (HypertableModify)
-   ->  Update on current_check
-         Update on _hyper_21_104_chunk current_check_1
-         ->  Tid Scan on _hyper_21_104_chunk current_check_1
-               TID Cond: CURRENT OF current_check_cursor
-               Filter: ((currentid = 4) AND ((currentid % 2) = 0))
-(6 rows)
-
--- Similarly can only delete row 4
-FETCH ABSOLUTE 1 FROM current_check_cursor;
- currentid | payload |     rlsuser     
------------+---------+-----------------
-         2 | bcd     | regress_rls_bob
-(1 row)
-
-DELETE FROM current_check WHERE CURRENT OF current_check_cursor RETURNING *;
- currentid | payload | rlsuser 
------------+---------+---------
-(0 rows)
-
-FETCH RELATIVE 1 FROM current_check_cursor;
- currentid | payload |     rlsuser     
------------+---------+-----------------
-         4 | def     | regress_rls_bob
-(1 row)
-
-DELETE FROM current_check WHERE CURRENT OF current_check_cursor RETURNING *;
- currentid | payload |     rlsuser     
------------+---------+-----------------
-         4 | def_new | regress_rls_bob
-(1 row)
-
-SELECT * FROM current_check;
- currentid | payload |     rlsuser     
------------+---------+-----------------
-         2 | bcd     | regress_rls_bob
-(1 row)
-
-RESET timescaledb.enable_chunk_append;
-COMMIT;
---
--- check pg_stats view filtering
---
-SET row_security TO ON;
-SET SESSION AUTHORIZATION regress_rls_alice;
-ANALYZE current_check;
--- Stats visible
-SELECT row_security_active('current_check');
- row_security_active 
----------------------
- f
-(1 row)
-
-SELECT attname, most_common_vals FROM pg_stats
-  WHERE tablename = 'current_check'
-  ORDER BY 1;
-  attname  | most_common_vals  
------------+-------------------
- currentid | 
- payload   | 
- rlsuser   | {regress_rls_bob}
-(3 rows)
-
-SET SESSION AUTHORIZATION regress_rls_bob;
--- Stats not visible
-SELECT row_security_active('current_check');
- row_security_active 
----------------------
- t
-(1 row)
-
-SELECT attname, most_common_vals FROM pg_stats
-  WHERE tablename = 'current_check'
-  ORDER BY 1;
- attname | most_common_vals 
----------+------------------
-(0 rows)
-
---
--- Collation support
---
-BEGIN;
-CREATE TABLE coll_t (c) AS VALUES ('bar'::text);
-CREATE POLICY coll_p ON coll_t USING (c < ('foo'::text COLLATE "C"));
-ALTER TABLE coll_t ENABLE ROW LEVEL SECURITY;
-GRANT SELECT ON coll_t TO regress_rls_alice;
-SELECT (string_to_array(polqual, ':'))[7] AS inputcollid FROM pg_policy WHERE polrelid = 'coll_t'::regclass;
-   inputcollid    
-------------------
- inputcollid 950 
-(1 row)
-
-SET SESSION AUTHORIZATION regress_rls_alice;
-SELECT * FROM coll_t;
-  c  
------
- bar
-(1 row)
-
-ROLLBACK;
---
--- Shared Object Dependencies
---
-RESET SESSION AUTHORIZATION;
-BEGIN;
-CREATE ROLE regress_rls_eve;
-CREATE ROLE regress_rls_frank;
-CREATE TABLE tbl1 (c) AS VALUES ('bar'::text);
-GRANT SELECT ON TABLE tbl1 TO regress_rls_eve;
-CREATE POLICY P ON tbl1 TO regress_rls_eve, regress_rls_frank USING (true);
-SELECT refclassid::regclass, deptype
-  FROM pg_depend
-  WHERE classid = 'pg_policy'::regclass
-  AND refobjid = 'tbl1'::regclass;
- refclassid | deptype 
-------------+---------
- pg_class   | a
-(1 row)
-
-SELECT refclassid::regclass, deptype
-  FROM pg_shdepend
-  WHERE classid = 'pg_policy'::regclass
-  AND refobjid IN ('regress_rls_eve'::regrole, 'regress_rls_frank'::regrole);
- refclassid | deptype 
-------------+---------
- pg_authid  | r
- pg_authid  | r
-(2 rows)
-
-SAVEPOINT q;
-DROP ROLE regress_rls_eve; --fails due to dependency on POLICY p
-ERROR:  role "regress_rls_eve" cannot be dropped because some objects depend on it
-DETAIL:  privileges for table tbl1
-target of policy p on table tbl1
-ROLLBACK TO q;
-ALTER POLICY p ON tbl1 TO regress_rls_frank USING (true);
-SAVEPOINT q;
-DROP ROLE regress_rls_eve; --fails due to dependency on GRANT SELECT
-ERROR:  role "regress_rls_eve" cannot be dropped because some objects depend on it
-DETAIL:  privileges for table tbl1
-ROLLBACK TO q;
-REVOKE ALL ON TABLE tbl1 FROM regress_rls_eve;
-SAVEPOINT q;
-DROP ROLE regress_rls_eve; --succeeds
-ROLLBACK TO q;
-SAVEPOINT q;
-DROP ROLE regress_rls_frank; --fails due to dependency on POLICY p
-ERROR:  role "regress_rls_frank" cannot be dropped because some objects depend on it
-DETAIL:  target of policy p on table tbl1
-ROLLBACK TO q;
-DROP POLICY p ON tbl1;
-SAVEPOINT q;
-DROP ROLE regress_rls_frank; -- succeeds
-ROLLBACK TO q;
-ROLLBACK; -- cleanup
---
--- Converting table to view
---
-BEGIN;
-CREATE TABLE t (c int);
-SELECT public.create_hypertable('t', 'c', chunk_time_interval=>2);
-NOTICE:  adding not-null constraint to column "c"
-DETAIL:  Time dimensions cannot have NULL values.
-      create_hypertable      
------------------------------
- (22,regress_rls_schema,t,t)
-(1 row)
-
-CREATE POLICY p ON t USING (c % 2 = 1);
-ALTER TABLE t ENABLE ROW LEVEL SECURITY;
-SAVEPOINT q;
-CREATE RULE "_RETURN" AS ON SELECT TO t DO INSTEAD
-  SELECT * FROM generate_series(1,5) t0(c); -- fails due to row level security enabled
-ERROR:  hypertables do not support rules
-ROLLBACK TO q;
-ALTER TABLE t DISABLE ROW LEVEL SECURITY;
-SAVEPOINT q;
-CREATE RULE "_RETURN" AS ON SELECT TO t DO INSTEAD
-  SELECT * FROM generate_series(1,5) t0(c); -- fails due to policy p on t
-ERROR:  hypertables do not support rules
-ROLLBACK TO q;
-DROP POLICY p ON t;
-CREATE RULE "_RETURN" AS ON SELECT TO t DO INSTEAD
-  SELECT * FROM generate_series(1,5) t0(c); -- succeeds
-ERROR:  hypertables do not support rules
-ROLLBACK;
---
--- Policy expression handling
---
-BEGIN;
-CREATE TABLE t (c) AS VALUES ('bar'::text);
-CREATE POLICY p ON t USING (max(c)); -- fails: aggregate functions are not allowed in policy expressions
-ERROR:  aggregate functions are not allowed in policy expressions
-ROLLBACK;
---
--- Non-target relations are only subject to SELECT policies
---
-SET SESSION AUTHORIZATION regress_rls_alice;
-CREATE TABLE r1 (a int);
-SELECT public.create_hypertable('r1', 'a', chunk_time_interval=>2);
-NOTICE:  adding not-null constraint to column "a"
-DETAIL:  Time dimensions cannot have NULL values.
-      create_hypertable       
-------------------------------
- (23,regress_rls_schema,r1,t)
-(1 row)
-
-CREATE TABLE r2 (a int);
-SELECT public.create_hypertable('r2', 'a', chunk_time_interval=>2);
-NOTICE:  adding not-null constraint to column "a"
-DETAIL:  Time dimensions cannot have NULL values.
-      create_hypertable       
-------------------------------
- (24,regress_rls_schema,r2,t)
-(1 row)
-
-INSERT INTO r1 VALUES (10), (20);
-INSERT INTO r2 VALUES (10), (20);
-GRANT ALL ON r1, r2 TO regress_rls_bob;
-CREATE POLICY p1 ON r1 USING (true);
-ALTER TABLE r1 ENABLE ROW LEVEL SECURITY;
-CREATE POLICY p1 ON r2 FOR SELECT USING (true);
-CREATE POLICY p2 ON r2 FOR INSERT WITH CHECK (false);
-CREATE POLICY p3 ON r2 FOR UPDATE USING (false);
-CREATE POLICY p4 ON r2 FOR DELETE USING (false);
-ALTER TABLE r2 ENABLE ROW LEVEL SECURITY;
-SET SESSION AUTHORIZATION regress_rls_bob;
-SELECT * FROM r1;
- a  
-----
- 10
- 20
-(2 rows)
-
-SELECT * FROM r2;
- a  
-----
- 10
- 20
-(2 rows)
-
--- r2 is read-only
-INSERT INTO r2 VALUES (2); -- Not allowed
-ERROR:  new row violates row-level security policy for table "r2"
-\pset tuples_only 1
-UPDATE r2 SET a = 2 RETURNING *; -- Updates nothing
-
-DELETE FROM r2 RETURNING *; -- Deletes nothing
-
-\pset tuples_only 0
--- r2 can be used as a non-target relation in DML
-INSERT INTO r1 SELECT a + 1 FROM r2 RETURNING *; -- OK
- a  
-----
- 11
- 21
-(2 rows)
-
-UPDATE r1 SET a = r2.a + 2 FROM r2 WHERE r1.a = r2.a RETURNING *; -- OK
-ERROR:  new row for relation "_hyper_23_105_chunk" violates check constraint "constraint_105"
-DELETE FROM r1 USING r2 WHERE r1.a = r2.a + 2 RETURNING *; -- OK
- a | a 
----+---
-(0 rows)
-
-SELECT * FROM r1;
- a  
-----
- 10
- 11
- 20
- 21
-(4 rows)
-
-SELECT * FROM r2;
- a  
-----
- 10
- 20
-(2 rows)
-
-SET SESSION AUTHORIZATION regress_rls_alice;
-DROP TABLE r1;
-DROP TABLE r2;
---
--- FORCE ROW LEVEL SECURITY applies RLS to owners too
---
-SET SESSION AUTHORIZATION regress_rls_alice;
-SET row_security = on;
-CREATE TABLE r1 (a int);
-SELECT public.create_hypertable('r1', 'a', chunk_time_interval=>2);
-NOTICE:  adding not-null constraint to column "a"
-DETAIL:  Time dimensions cannot have NULL values.
-      create_hypertable       
-------------------------------
- (25,regress_rls_schema,r1,t)
-(1 row)
-
-INSERT INTO r1 VALUES (10), (20);
-CREATE POLICY p1 ON r1 USING (false);
-ALTER TABLE r1 ENABLE ROW LEVEL SECURITY;
-ALTER TABLE r1 FORCE ROW LEVEL SECURITY;
--- No error, but no rows
-TABLE r1;
- a 
----
-(0 rows)
-
--- RLS error
-INSERT INTO r1 VALUES (1);
-ERROR:  new row violates row-level security policy for table "r1"
--- No error (unable to see any rows to update)
-UPDATE r1 SET a = 1;
-TABLE r1;
- a 
----
-(0 rows)
-
--- No error (unable to see any rows to delete)
-DELETE FROM r1;
-TABLE r1;
- a 
----
-(0 rows)
-
-SET row_security = off;
--- these all fail, would be affected by RLS
-TABLE r1;
-ERROR:  query would be affected by row-level security policy for table "r1"
-HINT:  To disable the policy for the table's owner, use ALTER TABLE NO FORCE ROW LEVEL SECURITY.
-UPDATE r1 SET a = 1;
-ERROR:  query would be affected by row-level security policy for table "r1"
-HINT:  To disable the policy for the table's owner, use ALTER TABLE NO FORCE ROW LEVEL SECURITY.
-DELETE FROM r1;
-ERROR:  query would be affected by row-level security policy for table "r1"
-HINT:  To disable the policy for the table's owner, use ALTER TABLE NO FORCE ROW LEVEL SECURITY.
-DROP TABLE r1;
---
--- FORCE ROW LEVEL SECURITY does not break RI
---
-SET SESSION AUTHORIZATION regress_rls_alice;
-SET row_security = on;
-CREATE TABLE r1 (a int PRIMARY KEY);
--- r1 is not a hypertable since r1.a is referenced by r2
-CREATE TABLE r2 (a int REFERENCES r1);
-SELECT public.create_hypertable('r2', 'a', chunk_time_interval=>2);
-NOTICE:  adding not-null constraint to column "a"
-DETAIL:  Time dimensions cannot have NULL values.
-      create_hypertable       
-------------------------------
- (26,regress_rls_schema,r2,t)
-(1 row)
-
-INSERT INTO r1 VALUES (10), (20);
-INSERT INTO r2 VALUES (10), (20);
--- Create policies on r2 which prevent the
--- owner from seeing any rows, but RI should
--- still see them.
-CREATE POLICY p1 ON r2 USING (false);
-ALTER TABLE r2 ENABLE ROW LEVEL SECURITY;
-ALTER TABLE r2 FORCE ROW LEVEL SECURITY;
--- Errors due to rows in r2
-DELETE FROM r1;
-ERROR:  update or delete on table "r1" violates foreign key constraint "113_23_r2_a_fkey" on table "_hyper_26_113_chunk"
-DETAIL:  Key (a)=(10) is still referenced from table "_hyper_26_113_chunk".
--- Reset r2 to no-RLS
-DROP POLICY p1 ON r2;
-ALTER TABLE r2 NO FORCE ROW LEVEL SECURITY;
-ALTER TABLE r2 DISABLE ROW LEVEL SECURITY;
--- clean out r2 for INSERT test below
-DELETE FROM r2;
--- Change r1 to not allow rows to be seen
-CREATE POLICY p1 ON r1 USING (false);
-ALTER TABLE r1 ENABLE ROW LEVEL SECURITY;
-ALTER TABLE r1 FORCE ROW LEVEL SECURITY;
--- No rows seen
-TABLE r1;
- a 
----
-(0 rows)
-
--- No error, RI still sees that row exists in r1
-INSERT INTO r2 VALUES (10);
-DROP TABLE r2;
-DROP TABLE r1;
--- Ensure cascaded DELETE works
-CREATE TABLE r1 (a int PRIMARY KEY);
--- r1 is not a hypertable since r1.a is referenced by r2
-CREATE TABLE r2 (a int REFERENCES r1 ON DELETE CASCADE);
-SELECT public.create_hypertable('r2', 'a', chunk_time_interval=>2);
-NOTICE:  adding not-null constraint to column "a"
-DETAIL:  Time dimensions cannot have NULL values.
-      create_hypertable       
-------------------------------
- (27,regress_rls_schema,r2,t)
-(1 row)
-
-INSERT INTO r1 VALUES (10), (20);
-INSERT INTO r2 VALUES (10), (20);
--- Create policies on r2 which prevent the
--- owner from seeing any rows, but RI should
--- still see them.
-CREATE POLICY p1 ON r2 USING (false);
-ALTER TABLE r2 ENABLE ROW LEVEL SECURITY;
-ALTER TABLE r2 FORCE ROW LEVEL SECURITY;
--- Deletes all records from both
-DELETE FROM r1;
--- Remove FORCE from r2
-ALTER TABLE r2 NO FORCE ROW LEVEL SECURITY;
--- As owner, we now bypass RLS
--- verify no rows in r2 now
-TABLE r2;
- a 
----
-(0 rows)
-
-DROP TABLE r2;
-DROP TABLE r1;
--- Ensure cascaded UPDATE works
-CREATE TABLE r1 (a int PRIMARY KEY);
--- r1 is not a hypertable since r1.a is referenced by r2
-CREATE TABLE r2 (a int REFERENCES r1 ON UPDATE CASCADE);
-SELECT public.create_hypertable('r2', 'a', chunk_time_interval=>2);
-NOTICE:  adding not-null constraint to column "a"
-DETAIL:  Time dimensions cannot have NULL values.
-      create_hypertable       
-------------------------------
- (28,regress_rls_schema,r2,t)
-(1 row)
-
-INSERT INTO r1 VALUES (10), (20);
-INSERT INTO r2 VALUES (10), (20);
--- Create policies on r2 which prevent the
--- owner from seeing any rows, but RI should
--- still see them.
-CREATE POLICY p1 ON r2 USING (false);
-ALTER TABLE r2 ENABLE ROW LEVEL SECURITY;
-ALTER TABLE r2 FORCE ROW LEVEL SECURITY;
--- Updates records in both
-UPDATE r1 SET a = a+5;
-ERROR:  new row for relation "_hyper_28_117_chunk" violates check constraint "constraint_117"
-DETAIL:  Failing row contains (15).
-CONTEXT:  SQL statement "UPDATE ONLY "_timescaledb_internal"."_hyper_28_117_chunk" SET "a" = $1 WHERE $2 OPERATOR(pg_catalog.=) "a""
--- Remove FORCE from r2
-ALTER TABLE r2 NO FORCE ROW LEVEL SECURITY;
--- As owner, we now bypass RLS
--- verify records in r2 updated
-TABLE r2;
- a  
-----
- 10
- 20
-(2 rows)
-
-DROP TABLE r2;
-DROP TABLE r1;
---
--- Test INSERT+RETURNING applies SELECT policies as
--- WithCheckOptions (meaning an error is thrown)
---
-SET SESSION AUTHORIZATION regress_rls_alice;
-SET row_security = on;
-CREATE TABLE r1 (a int);
-SELECT public.create_hypertable('r1', 'a', chunk_time_interval=>2);
-NOTICE:  adding not-null constraint to column "a"
-DETAIL:  Time dimensions cannot have NULL values.
-      create_hypertable       
-------------------------------
- (29,regress_rls_schema,r1,t)
-(1 row)
-
-CREATE POLICY p1 ON r1 FOR SELECT USING (false);
-CREATE POLICY p2 ON r1 FOR INSERT WITH CHECK (true);
-ALTER TABLE r1 ENABLE ROW LEVEL SECURITY;
-ALTER TABLE r1 FORCE ROW LEVEL SECURITY;
--- Works fine
-INSERT INTO r1 VALUES (10), (20);
--- No error, but no rows
-TABLE r1;
- a 
----
-(0 rows)
-
-SET row_security = off;
--- fail, would be affected by RLS
-TABLE r1;
-ERROR:  query would be affected by row-level security policy for table "r1"
-HINT:  To disable the policy for the table's owner, use ALTER TABLE NO FORCE ROW LEVEL SECURITY.
-SET row_security = on;
--- Error
-INSERT INTO r1 VALUES (10), (20) RETURNING *;
-ERROR:  new row violates row-level security policy for table "r1"
-DROP TABLE r1;
---
--- Test UPDATE+RETURNING applies SELECT policies as
--- WithCheckOptions (meaning an error is thrown)
---
-SET SESSION AUTHORIZATION regress_rls_alice;
-SET row_security = on;
-CREATE TABLE r1 (a int PRIMARY KEY);
-SELECT public.create_hypertable('r1', 'a', chunk_time_interval=>100);
-      create_hypertable       
-------------------------------
- (30,regress_rls_schema,r1,t)
-(1 row)
-
-CREATE POLICY p1 ON r1 FOR SELECT USING (a < 20);
-CREATE POLICY p2 ON r1 FOR UPDATE USING (a < 20) WITH CHECK (true);
-CREATE POLICY p3 ON r1 FOR INSERT WITH CHECK (true);
-INSERT INTO r1 VALUES (10);
-ALTER TABLE r1 ENABLE ROW LEVEL SECURITY;
-ALTER TABLE r1 FORCE ROW LEVEL SECURITY;
--- Works fine
-UPDATE r1 SET a = 30;
--- Show updated rows
-ALTER TABLE r1 NO FORCE ROW LEVEL SECURITY;
-TABLE r1;
- a  
-----
- 30
-(1 row)
-
--- reset value in r1 for test with RETURNING
-UPDATE r1 SET a = 10;
--- Verify row reset
-TABLE r1;
- a  
-----
- 10
-(1 row)
-
-ALTER TABLE r1 FORCE ROW LEVEL SECURITY;
--- Error
-UPDATE r1 SET a = 30 RETURNING *;
-ERROR:  new row violates row-level security policy for table "r1"
--- UPDATE path of INSERT ... ON CONFLICT DO UPDATE should also error out
-INSERT INTO r1 VALUES (10)
-    ON CONFLICT (a) DO UPDATE SET a = 30 RETURNING *;
-ERROR:  new row violates row-level security policy for table "r1"
--- Should still error out without RETURNING (use of arbiter always requires
--- SELECT permissions)
-INSERT INTO r1 VALUES (10)
-    ON CONFLICT (a) DO UPDATE SET a = 30;
-ERROR:  new row violates row-level security policy for table "r1"
--- ON CONFLICT ON CONSTRAINT
-INSERT INTO r1 VALUES (10)
-    ON CONFLICT ON CONSTRAINT r1_pkey DO UPDATE SET a = 30;
-ERROR:  new row violates row-level security policy for table "r1"
-DROP TABLE r1;
--- Check dependency handling
-RESET SESSION AUTHORIZATION;
-CREATE TABLE dep1 (c1 int);
-SELECT public.create_hypertable('dep1', 'c1', chunk_time_interval=>2);
-NOTICE:  adding not-null constraint to column "c1"
-DETAIL:  Time dimensions cannot have NULL values.
-       create_hypertable        
---------------------------------
- (31,regress_rls_schema,dep1,t)
-(1 row)
-
-CREATE TABLE dep2 (c1 int);
-SELECT public.create_hypertable('dep2', 'c1', chunk_time_interval=>2);
-NOTICE:  adding not-null constraint to column "c1"
-DETAIL:  Time dimensions cannot have NULL values.
-       create_hypertable        
---------------------------------
- (32,regress_rls_schema,dep2,t)
-(1 row)
-
-CREATE POLICY dep_p1 ON dep1 TO regress_rls_bob USING (c1 > (select max(dep2.c1) from dep2));
-ALTER POLICY dep_p1 ON dep1 TO regress_rls_bob,regress_rls_carol;
--- Should return one
-SELECT count(*) = 1 FROM pg_depend
-				   WHERE objid = (SELECT oid FROM pg_policy WHERE polname = 'dep_p1')
-					 AND refobjid = (SELECT oid FROM pg_class WHERE relname = 'dep2');
- ?column? 
-----------
- t
-(1 row)
-
-ALTER POLICY dep_p1 ON dep1 USING (true);
--- Should return one
-SELECT count(*) = 1 FROM pg_shdepend
-				   WHERE objid = (SELECT oid FROM pg_policy WHERE polname = 'dep_p1')
-					 AND refobjid = (SELECT oid FROM pg_authid WHERE rolname = 'regress_rls_bob');
- ?column? 
-----------
- t
-(1 row)
-
--- Should return one
-SELECT count(*) = 1 FROM pg_shdepend
-				   WHERE objid = (SELECT oid FROM pg_policy WHERE polname = 'dep_p1')
-					 AND refobjid = (SELECT oid FROM pg_authid WHERE rolname = 'regress_rls_carol');
- ?column? 
-----------
- t
-(1 row)
-
--- Should return zero
-SELECT count(*) = 0 FROM pg_depend
-				   WHERE objid = (SELECT oid FROM pg_policy WHERE polname = 'dep_p1')
-					 AND refobjid = (SELECT oid FROM pg_class WHERE relname = 'dep2');
- ?column? 
-----------
- t
-(1 row)
-
--- DROP OWNED BY testing
-RESET SESSION AUTHORIZATION;
-CREATE ROLE regress_rls_dob_role1;
-CREATE ROLE regress_rls_dob_role2;
-CREATE TABLE dob_t1 (c1 int);
-SELECT public.create_hypertable('dob_t1', 'c1', chunk_time_interval=>2);
-NOTICE:  adding not-null constraint to column "c1"
-DETAIL:  Time dimensions cannot have NULL values.
-        create_hypertable         
-----------------------------------
- (33,regress_rls_schema,dob_t1,t)
-(1 row)
-
-CREATE TABLE dob_t2 (c1 int) PARTITION BY RANGE (c1);
-CREATE POLICY p1 ON dob_t1 TO regress_rls_dob_role1 USING (true);
-DROP OWNED BY regress_rls_dob_role1;
-DROP POLICY p1 ON dob_t1; -- should fail, already gone
-ERROR:  policy "p1" for table "dob_t1" does not exist
-CREATE POLICY p1 ON dob_t1 TO regress_rls_dob_role1,regress_rls_dob_role2 USING (true);
-DROP OWNED BY regress_rls_dob_role1;
-DROP POLICY p1 ON dob_t1; -- should succeed
-CREATE POLICY p1 ON dob_t2 TO regress_rls_dob_role1,regress_rls_dob_role2 USING (true);
-DROP OWNED BY regress_rls_dob_role1;
-DROP POLICY p1 ON dob_t2; -- should succeed
-DROP USER regress_rls_dob_role1;
-DROP USER regress_rls_dob_role2;
---
--- Clean up objects
---
-RESET SESSION AUTHORIZATION;
-\set VERBOSITY terse \\ -- suppress cascade details
-DROP SCHEMA regress_rls_schema CASCADE;
-NOTICE:  drop cascades to 116 other objects
-\set VERBOSITY default
-DROP USER regress_rls_alice;
-DROP USER regress_rls_bob;
-DROP USER regress_rls_carol;
-DROP USER regress_rls_dave;
-DROP USER regress_rls_exempt_user;
-DROP ROLE regress_rls_group1;
-DROP ROLE regress_rls_group2;
--- Arrange to have a few policies left over, for testing
--- pg_dump/pg_restore
-CREATE SCHEMA regress_rls_schema;
-CREATE TABLE rls_tbl (c1 int);
-SELECT public.create_hypertable('rls_tbl', 'c1', chunk_time_interval=>2);
-NOTICE:  adding not-null constraint to column "c1"
-DETAIL:  Time dimensions cannot have NULL values.
-         create_hypertable         
------------------------------------
- (34,regress_rls_schema,rls_tbl,t)
-(1 row)
-
-ALTER TABLE rls_tbl ENABLE ROW LEVEL SECURITY;
-CREATE POLICY p1 ON rls_tbl USING (c1 > 5);
-CREATE POLICY p2 ON rls_tbl FOR SELECT USING (c1 <= 3);
-CREATE POLICY p3 ON rls_tbl FOR UPDATE USING (c1 <= 3) WITH CHECK (c1 > 5);
-CREATE POLICY p4 ON rls_tbl FOR DELETE USING (c1 <= 3);
-CREATE TABLE rls_tbl_force (c1 int);
-SELECT public.create_hypertable('rls_tbl_force', 'c1', chunk_time_interval=>2);
-NOTICE:  adding not-null constraint to column "c1"
-DETAIL:  Time dimensions cannot have NULL values.
-            create_hypertable            
------------------------------------------
- (35,regress_rls_schema,rls_tbl_force,t)
-(1 row)
-
-ALTER TABLE rls_tbl_force ENABLE ROW LEVEL SECURITY;
-ALTER TABLE rls_tbl_force FORCE ROW LEVEL SECURITY;
-CREATE POLICY p1 ON rls_tbl_force USING (c1 = 5) WITH CHECK (c1 < 5);
-CREATE POLICY p2 ON rls_tbl_force FOR SELECT USING (c1 = 8);
-CREATE POLICY p3 ON rls_tbl_force FOR UPDATE USING (c1 = 8) WITH CHECK (c1 >= 5);
-CREATE POLICY p4 ON rls_tbl_force FOR DELETE USING (c1 = 8);
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/size_utils.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/size_utils.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/size_utils.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/size_utils.out	2023-11-25 05:27:44.149022649 +0000
@@ -1,760 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-\ir include/insert_two_partitions.sql
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-CREATE TABLE PUBLIC."two_Partitions" (
-  "timeCustom" BIGINT NOT NULL,
-  device_id TEXT NOT NULL,
-  series_0 DOUBLE PRECISION NULL,
-  series_1 DOUBLE PRECISION NULL,
-  series_2 DOUBLE PRECISION NULL,
-  series_bool BOOLEAN NULL
-);
-CREATE INDEX ON PUBLIC."two_Partitions" (device_id, "timeCustom" DESC NULLS LAST) WHERE device_id IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, series_0) WHERE series_0 IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, series_1)  WHERE series_1 IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, series_2) WHERE series_2 IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, series_bool) WHERE series_bool IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, device_id);
-SELECT * FROM create_hypertable('"public"."two_Partitions"'::regclass, 'timeCustom'::name, 'device_id'::name, associated_schema_name=>'_timescaledb_internal'::text, number_partitions => 2, chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
- hypertable_id | schema_name |   table_name   | created 
----------------+-------------+----------------+---------
-             1 | public      | two_Partitions | t
-(1 row)
-
-\set QUIET off
-BEGIN;
-BEGIN
-\COPY public."two_Partitions" FROM 'data/ds1_dev1_1.tsv' NULL AS '';
-COPY 7
-COMMIT;
-COMMIT
-INSERT INTO public."two_Partitions"("timeCustom", device_id, series_0, series_1) VALUES
-(1257987600000000000, 'dev1', 1.5, 1),
-(1257987600000000000, 'dev1', 1.5, 2),
-(1257894000000000000, 'dev2', 1.5, 1),
-(1257894002000000000, 'dev1', 2.5, 3);
-INSERT 0 4
-INSERT INTO "two_Partitions"("timeCustom", device_id, series_0, series_1) VALUES
-(1257894000000000000, 'dev2', 1.5, 2);
-INSERT 0 1
-\set QUIET on
-SELECT * FROM hypertable_detailed_size('"public"."two_Partitions"');
- table_bytes | index_bytes | toast_bytes | total_bytes | node_name 
--------------+-------------+-------------+-------------+-----------
-       32768 |      475136 |       40960 |      548864 | 
-(1 row)
-
-SELECT * FROM hypertable_index_size('"public"."two_Partitions_device_id_timeCustom_idx"');
- hypertable_index_size 
------------------------
-                 73728
-(1 row)
-
-SELECT * FROM hypertable_index_size('"public"."two_Partitions_timeCustom_device_id_idx"');
- hypertable_index_size 
------------------------
-                 73728
-(1 row)
-
-SELECT * FROM hypertable_index_size('"public"."two_Partitions_timeCustom_idx"');
- hypertable_index_size 
------------------------
-                 73728
-(1 row)
-
-SELECT * FROM hypertable_index_size('"public"."two_Partitions_timeCustom_series_0_idx"');
- hypertable_index_size 
------------------------
-                 73728
-(1 row)
-
-SELECT * FROM hypertable_index_size('"public"."two_Partitions_timeCustom_series_1_idx"');
- hypertable_index_size 
------------------------
-                 73728
-(1 row)
-
-SELECT * FROM hypertable_index_size('"public"."two_Partitions_timeCustom_series_2_idx"');
- hypertable_index_size 
------------------------
-                 49152
-(1 row)
-
-SELECT * FROM hypertable_index_size('"public"."two_Partitions_timeCustom_series_bool_idx"');
- hypertable_index_size 
------------------------
-                 57344
-(1 row)
-
-SELECT * FROM chunks_detailed_size('"public"."two_Partitions"') order by chunk_name;
-     chunk_schema      |    chunk_name    | table_bytes | index_bytes | toast_bytes | total_bytes | node_name 
------------------------+------------------+-------------+-------------+-------------+-------------+-----------
- _timescaledb_internal | _hyper_1_1_chunk |        8192 |      114688 |        8192 |      131072 | 
- _timescaledb_internal | _hyper_1_2_chunk |        8192 |      106496 |        8192 |      122880 | 
- _timescaledb_internal | _hyper_1_3_chunk |        8192 |       98304 |        8192 |      114688 | 
- _timescaledb_internal | _hyper_1_4_chunk |        8192 |       98304 |        8192 |      114688 | 
-(4 rows)
-
-CREATE TABLE timestamp_partitioned(time TIMESTAMP, value TEXT);
-SELECT * FROM create_hypertable('timestamp_partitioned', 'time', 'value', 2);
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
- hypertable_id | schema_name |      table_name       | created 
----------------+-------------+-----------------------+---------
-             2 | public      | timestamp_partitioned | t
-(1 row)
-
-INSERT INTO timestamp_partitioned VALUES('2004-10-19 10:23:54', '10');
-INSERT INTO timestamp_partitioned VALUES('2004-12-19 10:23:54', '30');
-SELECT * FROM chunks_detailed_size('timestamp_partitioned') order by chunk_name;
-     chunk_schema      |    chunk_name    | table_bytes | index_bytes | toast_bytes | total_bytes | node_name 
------------------------+------------------+-------------+-------------+-------------+-------------+-----------
- _timescaledb_internal | _hyper_2_5_chunk |        8192 |       32768 |        8192 |       49152 | 
- _timescaledb_internal | _hyper_2_6_chunk |        8192 |       32768 |        8192 |       49152 | 
-(2 rows)
-
-CREATE TABLE timestamp_partitioned_2(time TIMESTAMP, value CHAR(9));
-SELECT * FROM create_hypertable('timestamp_partitioned_2', 'time', 'value', 2);
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
- hypertable_id | schema_name |       table_name        | created 
----------------+-------------+-------------------------+---------
-             3 | public      | timestamp_partitioned_2 | t
-(1 row)
-
-INSERT INTO timestamp_partitioned_2 VALUES('2004-10-19 10:23:54', '10');
-INSERT INTO timestamp_partitioned_2 VALUES('2004-12-19 10:23:54', '30');
-SELECT * FROM chunks_detailed_size('timestamp_partitioned_2') order by chunk_name;
-     chunk_schema      |    chunk_name    | table_bytes | index_bytes | toast_bytes | total_bytes | node_name 
------------------------+------------------+-------------+-------------+-------------+-------------+-----------
- _timescaledb_internal | _hyper_3_7_chunk |        8192 |       32768 |           0 |       40960 | 
- _timescaledb_internal | _hyper_3_8_chunk |        8192 |       32768 |           0 |       40960 | 
-(2 rows)
-
-CREATE TABLE toast_test(time TIMESTAMP, value TEXT);
--- Set storage type to EXTERNAL to prevent PostgreSQL from compressing my
--- easily compressable string and instead store it with TOAST
-ALTER TABLE toast_test ALTER COLUMN value SET STORAGE EXTERNAL;
-SELECT * FROM create_hypertable('toast_test', 'time');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
- hypertable_id | schema_name | table_name | created 
----------------+-------------+------------+---------
-             4 | public      | toast_test | t
-(1 row)
-
-INSERT INTO toast_test VALUES('2004-10-19 10:23:54', $$
-this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k. this must be over 2k.
-$$);
-SELECT * FROM chunks_detailed_size('toast_test');
-     chunk_schema      |    chunk_name    | table_bytes | index_bytes | toast_bytes | total_bytes | node_name 
------------------------+------------------+-------------+-------------+-------------+-------------+-----------
- _timescaledb_internal | _hyper_4_9_chunk |        8192 |       16384 |       24576 |       49152 | 
-(1 row)
-
---
--- Tests for approximate_row_count()
---
--- Regular table
---
-CREATE TABLE approx_count(time TIMESTAMP, value int);
-INSERT INTO approx_count VALUES('2004-01-01 10:00:01', 1);
-INSERT INTO approx_count VALUES('2004-01-01 10:00:02', 2);
-INSERT INTO approx_count VALUES('2004-01-01 10:00:03', 3);
-INSERT INTO approx_count VALUES('2004-01-01 10:00:04', 4);
-INSERT INTO approx_count VALUES('2004-01-01 10:00:05', 5);
-INSERT INTO approx_count VALUES('2004-01-01 10:00:06', 6);
-INSERT INTO approx_count VALUES('2004-01-01 10:00:07', 7);
-SELECT * FROM approximate_row_count('approx_count');
- approximate_row_count 
------------------------
-                     0
-(1 row)
-
-ANALYZE approx_count;
-SELECT count(*) FROM approx_count;
- count 
--------
-     7
-(1 row)
-
-SELECT * FROM approximate_row_count('approx_count');
- approximate_row_count 
------------------------
-                     7
-(1 row)
-
-DROP TABLE approx_count;
--- Regular table with basic inheritance
---
-CREATE TABLE approx_count(id int);
-CREATE TABLE approx_count_child(id2 int) INHERITS (approx_count);
-INSERT INTO approx_count_child VALUES(0);
-INSERT INTO approx_count VALUES(1);
-SELECT count(*) FROM approx_count;
- count 
--------
-     2
-(1 row)
-
-SELECT * FROM approximate_row_count('approx_count');
- approximate_row_count 
------------------------
-                     0
-(1 row)
-
-ANALYZE approx_count;
-SELECT * FROM approximate_row_count('approx_count');
- approximate_row_count 
------------------------
-                     1
-(1 row)
-
-ANALYZE approx_count_child;
-SELECT * FROM approximate_row_count('approx_count');
- approximate_row_count 
------------------------
-                     2
-(1 row)
-
-DROP TABLE approx_count CASCADE;
-NOTICE:  drop cascades to table approx_count_child
--- Regular table with nested inheritance
---
-CREATE TABLE approx_count(id int);
-CREATE TABLE approx_count_a(id2 int) INHERITS (approx_count);
-CREATE TABLE approx_count_b(id3 int) INHERITS (approx_count_a);
-CREATE TABLE approx_count_c(id4 int) INHERITS (approx_count_b);
-INSERT INTO approx_count_a VALUES(0);
-INSERT INTO approx_count_b VALUES(1);
-INSERT INTO approx_count_c VALUES(2);
-INSERT INTO approx_count VALUES(3);
-SELECT * FROM approximate_row_count('approx_count');
- approximate_row_count 
------------------------
-                     0
-(1 row)
-
-ANALYZE approx_count_a;
-ANALYZE approx_count_b;
-ANALYZE approx_count_c;
-ANALYZE approx_count;
-SELECT count(*) FROM approx_count;
- count 
--------
-     4
-(1 row)
-
-SELECT * FROM approximate_row_count('approx_count');
- approximate_row_count 
------------------------
-                     4
-(1 row)
-
-SELECT count(*) FROM approx_count_a;
- count 
--------
-     3
-(1 row)
-
-SELECT * FROM approximate_row_count('approx_count_a');
- approximate_row_count 
------------------------
-                     3
-(1 row)
-
-SELECT count(*) FROM approx_count_b;
- count 
--------
-     2
-(1 row)
-
-SELECT * FROM approximate_row_count('approx_count_b');
- approximate_row_count 
------------------------
-                     2
-(1 row)
-
-SELECT count(*) FROM approx_count_c;
- count 
--------
-     1
-(1 row)
-
-SELECT * FROM approximate_row_count('approx_count_c');
- approximate_row_count 
------------------------
-                     1
-(1 row)
-
-DROP TABLE approx_count CASCADE;
-NOTICE:  drop cascades to 3 other objects
--- table with declarative partitioning
---
-CREATE TABLE approx_count_dp(time TIMESTAMP, value int) PARTITION BY RANGE(time);
-CREATE TABLE approx_count_dp0 PARTITION OF approx_count_dp
-FOR VALUES FROM ('2004-01-01 00:00:00') TO ('2005-01-01 00:00:00');
-CREATE TABLE approx_count_dp1 PARTITION OF approx_count_dp
-FOR VALUES FROM ('2005-01-01 00:00:00') TO ('2006-01-01 00:00:00');
-CREATE TABLE approx_count_dp2 PARTITION OF approx_count_dp
-FOR VALUES FROM ('2006-01-01 00:00:00') TO ('2007-01-01 00:00:00');
-INSERT INTO approx_count_dp VALUES('2004-01-01 10:00:00', 1);
-INSERT INTO approx_count_dp VALUES('2004-01-01 11:00:00', 1);
-INSERT INTO approx_count_dp VALUES('2004-01-01 12:00:01', 1);
-INSERT INTO approx_count_dp VALUES('2005-01-01 10:00:00', 1);
-INSERT INTO approx_count_dp VALUES('2005-01-01 11:00:00', 1);
-INSERT INTO approx_count_dp VALUES('2005-01-01 12:00:01', 1);
-INSERT INTO approx_count_dp VALUES('2006-01-01 10:00:00', 1);
-INSERT INTO approx_count_dp VALUES('2006-01-01 11:00:00', 1);
-INSERT INTO approx_count_dp VALUES('2006-01-01 12:00:01', 1);
-SELECT count(*) FROM approx_count_dp;
- count 
--------
-     9
-(1 row)
-
-SELECT count(*) FROM approx_count_dp0;
- count 
--------
-     3
-(1 row)
-
-SELECT count(*) FROM approx_count_dp1;
- count 
--------
-     3
-(1 row)
-
-SELECT count(*) FROM approx_count_dp2;
- count 
--------
-     3
-(1 row)
-
-SELECT * FROM approximate_row_count('approx_count_dp');
- approximate_row_count 
------------------------
-                     0
-(1 row)
-
-ANALYZE approx_count_dp;
-SELECT * FROM approximate_row_count('approx_count_dp');
- approximate_row_count 
------------------------
-                     9
-(1 row)
-
-SELECT * FROM approximate_row_count('approx_count_dp0');
- approximate_row_count 
------------------------
-                     3
-(1 row)
-
-SELECT * FROM approximate_row_count('approx_count_dp1');
- approximate_row_count 
------------------------
-                     3
-(1 row)
-
-SELECT * FROM approximate_row_count('approx_count_dp2');
- approximate_row_count 
------------------------
-                     3
-(1 row)
-
-CREATE TABLE approx_count_dp_nested(time TIMESTAMP, device_id int, value int) PARTITION BY RANGE(time);
-CREATE TABLE approx_count_dp_nested_0 PARTITION OF approx_count_dp_nested FOR VALUES FROM ('2004-01-01 00:00:00') TO ('2005-01-01 00:00:00') PARTITION BY RANGE (device_id);
-CREATE TABLE approx_count_dp_nested_0_0 PARTITION OF approx_count_dp_nested_0 FOR VALUES FROM (0) TO (10);
-CREATE TABLE approx_count_dp_nested_0_1 PARTITION OF approx_count_dp_nested_0 FOR VALUES FROM (10) TO (20);
-CREATE TABLE approx_count_dp_nested_1 PARTITION OF approx_count_dp_nested FOR VALUES FROM ('2005-01-01 00:00:00') TO ('2006-01-01 00:00:00') PARTITION BY RANGE (device_id);
-CREATE TABLE approx_count_dp_nested_1_0 PARTITION OF approx_count_dp_nested_1 FOR VALUES FROM (0) TO (10);
-CREATE TABLE approx_count_dp_nested_1_1 PARTITION OF approx_count_dp_nested_1 FOR VALUES FROM (10) TO (20);
-INSERT INTO approx_count_dp_nested VALUES('2004-01-01 10:00:00', 1, 1);
-INSERT INTO approx_count_dp_nested VALUES('2004-01-01 10:00:00', 2, 1);
-INSERT INTO approx_count_dp_nested VALUES('2004-01-01 10:00:00', 3, 1);
-INSERT INTO approx_count_dp_nested VALUES('2004-01-01 10:00:00', 11, 1);
-INSERT INTO approx_count_dp_nested VALUES('2004-01-01 10:00:00', 12, 1);
-INSERT INTO approx_count_dp_nested VALUES('2004-01-01 10:00:00', 13, 1);
-INSERT INTO approx_count_dp_nested VALUES('2005-01-01 10:00:00', 1, 1);
-INSERT INTO approx_count_dp_nested VALUES('2005-01-01 10:00:00', 2, 1);
-INSERT INTO approx_count_dp_nested VALUES('2005-01-01 10:00:00', 3, 1);
-INSERT INTO approx_count_dp_nested VALUES('2005-01-01 10:00:00', 11, 1);
-INSERT INTO approx_count_dp_nested VALUES('2005-01-01 10:00:00', 12, 1);
-INSERT INTO approx_count_dp_nested VALUES('2005-01-01 10:00:00', 13, 1);
-SELECT * FROM approximate_row_count('approx_count_dp_nested');
- approximate_row_count 
------------------------
-                     0
-(1 row)
-
-ANALYZE approx_count_dp_nested;
-SELECT
-  (SELECT count(*) FROM approx_count_dp_nested) AS dp_nested,
-  (SELECT count(*) FROM approx_count_dp_nested_0) AS dp_nested_0,
-  (SELECT count(*) FROM approx_count_dp_nested_0_0) AS dp_nested_0_0,
-  (SELECT count(*) FROM approx_count_dp_nested_0_1) AS dp_nested_0_1,
-  (SELECT count(*) FROM approx_count_dp_nested_1) AS dp_nested_1,
-  (SELECT count(*) FROM approx_count_dp_nested_1_0) AS dp_nested_1_0,
-  (SELECT count(*) FROM approx_count_dp_nested_1_1) AS dp_nested_1_1
-UNION ALL
-SELECT
-  approximate_row_count('approx_count_dp_nested'),
-  approximate_row_count('approx_count_dp_nested_0'),
-  approximate_row_count('approx_count_dp_nested_0_0'),
-  approximate_row_count('approx_count_dp_nested_0_1'),
-  approximate_row_count('approx_count_dp_nested_1'),
-  approximate_row_count('approx_count_dp_nested_1_0'),
-  approximate_row_count('approx_count_dp_nested_1_1');
- dp_nested | dp_nested_0 | dp_nested_0_0 | dp_nested_0_1 | dp_nested_1 | dp_nested_1_0 | dp_nested_1_1 
------------+-------------+---------------+---------------+-------------+---------------+---------------
-        12 |           6 |             3 |             3 |           6 |             3 |             3
-        12 |           6 |             3 |             3 |           6 |             3 |             3
-(2 rows)
-
--- Hypertable
---
-CREATE TABLE approx_count(time TIMESTAMP, value int);
-SELECT * FROM create_hypertable('approx_count', 'time');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
- hypertable_id | schema_name |  table_name  | created 
----------------+-------------+--------------+---------
-             5 | public      | approx_count | t
-(1 row)
-
-INSERT INTO approx_count VALUES('2004-01-01 10:00:01', 1);
-INSERT INTO approx_count VALUES('2004-01-01 10:00:02', 2);
-INSERT INTO approx_count VALUES('2004-01-01 10:00:03', 3);
-INSERT INTO approx_count VALUES('2004-01-01 10:00:04', 4);
-INSERT INTO approx_count VALUES('2004-01-01 10:00:05', 5);
-INSERT INTO approx_count VALUES('2004-01-01 10:00:06', 6);
-INSERT INTO approx_count VALUES('2004-01-01 10:00:07', 7);
-INSERT INTO approx_count VALUES('2004-01-01 10:00:08', 8);
-INSERT INTO approx_count VALUES('2004-01-01 10:00:09', 9);
-INSERT INTO approx_count VALUES('2004-01-01 10:00:10', 10);
-SELECT count(*) FROM approx_count;
- count 
--------
-    10
-(1 row)
-
-SELECT * FROM approximate_row_count('approx_count');
- approximate_row_count 
------------------------
-                     0
-(1 row)
-
-ANALYZE approx_count;
-SELECT * FROM approximate_row_count('approx_count');
- approximate_row_count 
------------------------
-                    10
-(1 row)
-
-\set ON_ERROR_STOP 0
-SELECT * FROM approximate_row_count('unexisting');
-ERROR:  relation "unexisting" does not exist at character 37
-SELECT * FROM approximate_row_count();
-ERROR:  function approximate_row_count() does not exist at character 15
-SELECT * FROM approximate_row_count(NULL);
- approximate_row_count 
------------------------
-                      
-(1 row)
-
-\set ON_ERROR_STOP 1
--- Test size functions with invalid or non-existing OID
-SELECT * FROM hypertable_size(0);
- hypertable_size 
------------------
-                
-(1 row)
-
-SELECT * FROM hypertable_detailed_size(0) ORDER BY node_name;
- table_bytes | index_bytes | toast_bytes | total_bytes | node_name 
--------------+-------------+-------------+-------------+-----------
-(0 rows)
-
-SELECT * FROM chunks_detailed_size(0) ORDER BY node_name;
- chunk_schema | chunk_name | table_bytes | index_bytes | toast_bytes | total_bytes | node_name 
---------------+------------+-------------+-------------+-------------+-------------+-----------
-(0 rows)
-
-SELECT * FROM hypertable_compression_stats(0) ORDER BY node_name;
- total_chunks | number_compressed_chunks | before_compression_table_bytes | before_compression_index_bytes | before_compression_toast_bytes | before_compression_total_bytes | after_compression_table_bytes | after_compression_index_bytes | after_compression_toast_bytes | after_compression_total_bytes | node_name 
---------------+--------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+-----------
-(0 rows)
-
-SELECT * FROM chunk_compression_stats(0) ORDER BY node_name;
- chunk_schema | chunk_name | compression_status | before_compression_table_bytes | before_compression_index_bytes | before_compression_toast_bytes | before_compression_total_bytes | after_compression_table_bytes | after_compression_index_bytes | after_compression_toast_bytes | after_compression_total_bytes | node_name 
---------------+------------+--------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+-----------
-(0 rows)
-
-SELECT * FROM hypertable_index_size(0);
- hypertable_index_size 
------------------------
-                      
-(1 row)
-
-SELECT * FROM _timescaledb_internal.relation_size(0);
- total_size | heap_size | index_size | toast_size 
-------------+-----------+------------+------------
-            |           |            |           
-(1 row)
-
-SELECT * FROM hypertable_size(1);
- hypertable_size 
------------------
-                
-(1 row)
-
-SELECT * FROM hypertable_detailed_size(1) ORDER BY node_name;
- table_bytes | index_bytes | toast_bytes | total_bytes | node_name 
--------------+-------------+-------------+-------------+-----------
-(0 rows)
-
-SELECT * FROM chunks_detailed_size(1) ORDER BY node_name;
- chunk_schema | chunk_name | table_bytes | index_bytes | toast_bytes | total_bytes | node_name 
---------------+------------+-------------+-------------+-------------+-------------+-----------
-(0 rows)
-
-SELECT * FROM hypertable_compression_stats(1) ORDER BY node_name;
- total_chunks | number_compressed_chunks | before_compression_table_bytes | before_compression_index_bytes | before_compression_toast_bytes | before_compression_total_bytes | after_compression_table_bytes | after_compression_index_bytes | after_compression_toast_bytes | after_compression_total_bytes | node_name 
---------------+--------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+-----------
-(0 rows)
-
-SELECT * FROM chunk_compression_stats(1) ORDER BY node_name;
- chunk_schema | chunk_name | compression_status | before_compression_table_bytes | before_compression_index_bytes | before_compression_toast_bytes | before_compression_total_bytes | after_compression_table_bytes | after_compression_index_bytes | after_compression_toast_bytes | after_compression_total_bytes | node_name 
---------------+------------+--------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+-----------
-(0 rows)
-
-SELECT * FROM hypertable_index_size(1);
- hypertable_index_size 
------------------------
-                      
-(1 row)
-
-SELECT * FROM _timescaledb_internal.relation_size(1);
- total_size | heap_size | index_size | toast_size 
-------------+-----------+------------+------------
-          0 |         0 |          0 |          0
-(1 row)
-
--- Test size functions with NULL input
-SELECT * FROM hypertable_size(NULL);
- hypertable_size 
------------------
-                
-(1 row)
-
-SELECT * FROM hypertable_detailed_size(NULL) ORDER BY node_name;
- table_bytes | index_bytes | toast_bytes | total_bytes | node_name 
--------------+-------------+-------------+-------------+-----------
-(0 rows)
-
-SELECT * FROM chunks_detailed_size(NULL) ORDER BY node_name;
- chunk_schema | chunk_name | table_bytes | index_bytes | toast_bytes | total_bytes | node_name 
---------------+------------+-------------+-------------+-------------+-------------+-----------
-(0 rows)
-
-SELECT * FROM hypertable_compression_stats(NULL) ORDER BY node_name;
- total_chunks | number_compressed_chunks | before_compression_table_bytes | before_compression_index_bytes | before_compression_toast_bytes | before_compression_total_bytes | after_compression_table_bytes | after_compression_index_bytes | after_compression_toast_bytes | after_compression_total_bytes | node_name 
---------------+--------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+-----------
-(0 rows)
-
-SELECT * FROM chunk_compression_stats(NULL) ORDER BY node_name;
- chunk_schema | chunk_name | compression_status | before_compression_table_bytes | before_compression_index_bytes | before_compression_toast_bytes | before_compression_total_bytes | after_compression_table_bytes | after_compression_index_bytes | after_compression_toast_bytes | after_compression_total_bytes | node_name 
---------------+------------+--------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+-----------
-(0 rows)
-
-SELECT * FROM hypertable_index_size(NULL);
- hypertable_index_size 
------------------------
-                      
-(1 row)
-
-SELECT * FROM _timescaledb_internal.relation_size(NULL);
- total_size | heap_size | index_size | toast_size 
-------------+-----------+------------+------------
-            |           |            |           
-(1 row)
-
--- Test size functions on regular table
-CREATE TABLE hypersize(time timestamptz, device int);
-CREATE INDEX hypersize_time_idx ON hypersize (time);
-\set ON_ERROR_STOP 0
-\set VERBOSITY default
-\set SHOW_CONTEXT never
-SELECT pg_relation_size('hypersize'), pg_table_size('hypersize'), pg_indexes_size('hypersize'), pg_total_relation_size('hypersize'), pg_relation_size('hypersize_time_idx');
- pg_relation_size | pg_table_size | pg_indexes_size | pg_total_relation_size | pg_relation_size 
-------------------+---------------+-----------------+------------------------+------------------
-                0 |             0 |            8192 |                   8192 |             8192
-(1 row)
-
-SELECT * FROM _timescaledb_internal.relation_size('hypersize');
- total_size | heap_size | index_size | toast_size 
-------------+-----------+------------+------------
-       8192 |         0 |       8192 |          0
-(1 row)
-
-SELECT * FROM hypertable_size('hypersize');
- hypertable_size 
------------------
-                
-(1 row)
-
-SELECT * FROM hypertable_detailed_size('hypersize') ORDER BY node_name;
- table_bytes | index_bytes | toast_bytes | total_bytes | node_name 
--------------+-------------+-------------+-------------+-----------
-(0 rows)
-
-SELECT * FROM chunks_detailed_size('hypersize') ORDER BY node_name;
- chunk_schema | chunk_name | table_bytes | index_bytes | toast_bytes | total_bytes | node_name 
---------------+------------+-------------+-------------+-------------+-------------+-----------
-(0 rows)
-
-SELECT * FROM hypertable_compression_stats('hypersize') ORDER BY node_name;
- total_chunks | number_compressed_chunks | before_compression_table_bytes | before_compression_index_bytes | before_compression_toast_bytes | before_compression_total_bytes | after_compression_table_bytes | after_compression_index_bytes | after_compression_toast_bytes | after_compression_total_bytes | node_name 
---------------+--------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+-----------
-(0 rows)
-
-SELECT * FROM chunk_compression_stats('hypersize') ORDER BY node_name;
- chunk_schema | chunk_name | compression_status | before_compression_table_bytes | before_compression_index_bytes | before_compression_toast_bytes | before_compression_total_bytes | after_compression_table_bytes | after_compression_index_bytes | after_compression_toast_bytes | after_compression_total_bytes | node_name 
---------------+------------+--------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+-----------
-(0 rows)
-
-SELECT * FROM hypertable_index_size('hypersize_time_idx');
- hypertable_index_size 
------------------------
-                      
-(1 row)
-
-\set VERBOSITY terse
-\set ON_ERROR_STOP 1
--- Test size functions on empty hypertable
-SELECT * FROM create_hypertable('hypersize', 'time');
-NOTICE:  adding not-null constraint to column "time"
- hypertable_id | schema_name | table_name | created 
----------------+-------------+------------+---------
-             6 | public      | hypersize  | t
-(1 row)
-
-SELECT pg_relation_size('hypersize'), pg_table_size('hypersize'), pg_indexes_size('hypersize'), pg_total_relation_size('hypersize'), pg_relation_size('hypersize_time_idx');
- pg_relation_size | pg_table_size | pg_indexes_size | pg_total_relation_size | pg_relation_size 
-------------------+---------------+-----------------+------------------------+------------------
-                0 |             0 |            8192 |                   8192 |             8192
-(1 row)
-
-SELECT * FROM _timescaledb_internal.relation_size('hypersize');
- total_size | heap_size | index_size | toast_size 
-------------+-----------+------------+------------
-       8192 |         0 |       8192 |          0
-(1 row)
-
-SELECT * FROM hypertable_size('hypersize');
- hypertable_size 
------------------
-            8192
-(1 row)
-
-SELECT * FROM hypertable_detailed_size('hypersize') ORDER BY node_name;
- table_bytes | index_bytes | toast_bytes | total_bytes | node_name 
--------------+-------------+-------------+-------------+-----------
-           0 |        8192 |           0 |        8192 | 
-(1 row)
-
-SELECT * FROM chunks_detailed_size('hypersize') ORDER BY node_name;
- chunk_schema | chunk_name | table_bytes | index_bytes | toast_bytes | total_bytes | node_name 
---------------+------------+-------------+-------------+-------------+-------------+-----------
-(0 rows)
-
-SELECT * FROM hypertable_compression_stats('hypersize') ORDER BY node_name;
- total_chunks | number_compressed_chunks | before_compression_table_bytes | before_compression_index_bytes | before_compression_toast_bytes | before_compression_total_bytes | after_compression_table_bytes | after_compression_index_bytes | after_compression_toast_bytes | after_compression_total_bytes | node_name 
---------------+--------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+-----------
-(0 rows)
-
-SELECT * FROM chunk_compression_stats('hypersize') ORDER BY node_name;
- chunk_schema | chunk_name | compression_status | before_compression_table_bytes | before_compression_index_bytes | before_compression_toast_bytes | before_compression_total_bytes | after_compression_table_bytes | after_compression_index_bytes | after_compression_toast_bytes | after_compression_total_bytes | node_name 
---------------+------------+--------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+-----------
-(0 rows)
-
-SELECT * FROM hypertable_index_size('hypersize_time_idx');
- hypertable_index_size 
------------------------
-                  8192
-(1 row)
-
--- Test size functions on non-empty hypertable
-INSERT INTO hypersize VALUES('2021-02-25', 1);
-SELECT pg_relation_size('hypersize'), pg_table_size('hypersize'), pg_indexes_size('hypersize'), pg_total_relation_size('hypersize'), pg_relation_size('hypersize_time_idx');
- pg_relation_size | pg_table_size | pg_indexes_size | pg_total_relation_size | pg_relation_size 
-------------------+---------------+-----------------+------------------------+------------------
-                0 |             0 |            8192 |                   8192 |             8192
-(1 row)
-
-SELECT pg_relation_size(ch), pg_table_size(ch), pg_indexes_size(ch), pg_total_relation_size(ch)
-FROM show_chunks('hypersize') ch
-ORDER BY ch;
- pg_relation_size | pg_table_size | pg_indexes_size | pg_total_relation_size 
-------------------+---------------+-----------------+------------------------
-             8192 |          8192 |           16384 |                  24576
-(1 row)
-
-SELECT * FROM show_chunks('hypersize') ch JOIN LATERAL _timescaledb_internal.relation_size(ch) ON true;
-                   ch                    | total_size | heap_size | index_size | toast_size 
------------------------------------------+------------+-----------+------------+------------
- _timescaledb_internal._hyper_6_11_chunk |      24576 |      8192 |      16384 |          0
-(1 row)
-
-SELECT * FROM hypertable_size('hypersize');
- hypertable_size 
------------------
-           32768
-(1 row)
-
-SELECT * FROM hypertable_detailed_size('hypersize') ORDER BY node_name;
- table_bytes | index_bytes | toast_bytes | total_bytes | node_name 
--------------+-------------+-------------+-------------+-----------
-        8192 |       24576 |           0 |       32768 | 
-(1 row)
-
-SELECT * FROM chunks_detailed_size('hypersize') ORDER BY node_name;
-     chunk_schema      |    chunk_name     | table_bytes | index_bytes | toast_bytes | total_bytes | node_name 
------------------------+-------------------+-------------+-------------+-------------+-------------+-----------
- _timescaledb_internal | _hyper_6_11_chunk |        8192 |       16384 |           0 |       24576 | 
-(1 row)
-
-SELECT * FROM hypertable_compression_stats('hypersize') ORDER BY node_name;
- total_chunks | number_compressed_chunks | before_compression_table_bytes | before_compression_index_bytes | before_compression_toast_bytes | before_compression_total_bytes | after_compression_table_bytes | after_compression_index_bytes | after_compression_toast_bytes | after_compression_total_bytes | node_name 
---------------+--------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+-----------
-(0 rows)
-
-SELECT * FROM chunk_compression_stats('hypersize') ORDER BY node_name;
- chunk_schema | chunk_name | compression_status | before_compression_table_bytes | before_compression_index_bytes | before_compression_toast_bytes | before_compression_total_bytes | after_compression_table_bytes | after_compression_index_bytes | after_compression_toast_bytes | after_compression_total_bytes | node_name 
---------------+------------+--------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+-----------
-(0 rows)
-
-SELECT * FROM hypertable_index_size('hypersize_time_idx');
- hypertable_index_size 
------------------------
-                 24576
-(1 row)
-
--- github issue #4857
--- below procedure should not crash
-SET client_min_messages = ERROR;
-do
-$$
-DECLARE
-  o INT;
-BEGIN
-  FOR c IN 1..20 LOOP
-    ANALYZE;
-  END LOOP;
-END;
-$$;
-RESET client_min_messages;
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/sort_optimization.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/sort_optimization.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/sort_optimization.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/sort_optimization.out	2023-11-25 05:27:44.137022684 +0000
@@ -1,73 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-\set PREFIX 'EXPLAIN (COSTS OFF) '
-CREATE TABLE order_test(time int NOT NULL, device_id int, value float);
-CREATE INDEX ON order_test(time,device_id);
-CREATE INDEX ON order_test(device_id,time);
-SELECT create_hypertable('order_test','time',chunk_time_interval:=1000);
-    create_hypertable    
--------------------------
- (1,public,order_test,t)
-(1 row)
-
-INSERT INTO order_test SELECT 0,10,0.5;
-INSERT INTO order_test SELECT 1,9,0.5;
-INSERT INTO order_test SELECT 2,8,0.5;
--- we want to see here that index scans are possible for the chosen expressions
--- so we disable seqscan so we dont need to worry about other factors which would
--- make PostgreSQL prefer seqscan over index scan
-SET enable_seqscan TO off;
--- test sort optimization with single member order by
-SELECT time_bucket(10,time),device_id,value FROM order_test ORDER BY 1;
- time_bucket | device_id | value 
--------------+-----------+-------
-           0 |        10 |   0.5
-           0 |         9 |   0.5
-           0 |         8 |   0.5
-(3 rows)
-
--- should use index scan
-:PREFIX SELECT time_bucket(10,time),device_id,value FROM order_test ORDER BY 1;
-                                        QUERY PLAN                                        
-------------------------------------------------------------------------------------------
- Result
-   ->  Index Scan Backward using _hyper_1_1_chunk_order_test_time_idx on _hyper_1_1_chunk
-(2 rows)
-
--- test sort optimization with ordering by multiple columns and time_bucket not last
-SELECT time_bucket(10,time),device_id,value FROM order_test ORDER BY 1,2;
- time_bucket | device_id | value 
--------------+-----------+-------
-           0 |         8 |   0.5
-           0 |         9 |   0.5
-           0 |        10 |   0.5
-(3 rows)
-
--- must not use index scan
-:PREFIX SELECT time_bucket(10,time),device_id,value FROM order_test ORDER BY 1,2;
-                                     QUERY PLAN                                     
-------------------------------------------------------------------------------------
- Sort
-   Sort Key: (time_bucket(10, _hyper_1_1_chunk."time")), _hyper_1_1_chunk.device_id
-   ->  Result
-         ->  Seq Scan on _hyper_1_1_chunk
-(4 rows)
-
--- test sort optimization with ordering by multiple columns and time_bucket as last member
-SELECT time_bucket(10,time),device_id,value FROM order_test ORDER BY 2,1;
- time_bucket | device_id | value 
--------------+-----------+-------
-           0 |         8 |   0.5
-           0 |         9 |   0.5
-           0 |        10 |   0.5
-(3 rows)
-
--- should use index scan
-:PREFIX SELECT time_bucket(10,time),device_id,value FROM order_test ORDER BY 2,1;
-                                        QUERY PLAN                                         
--------------------------------------------------------------------------------------------
- Result
-   ->  Index Scan using _hyper_1_1_chunk_order_test_device_id_time_idx on _hyper_1_1_chunk
-(2 rows)
-
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/sql_query.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/sql_query.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/sql_query.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/sql_query.out	2023-11-25 05:27:44.153022637 +0000
@@ -1,296 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-\o /dev/null
-\ir include/insert_two_partitions.sql
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-CREATE TABLE PUBLIC."two_Partitions" (
-  "timeCustom" BIGINT NOT NULL,
-  device_id TEXT NOT NULL,
-  series_0 DOUBLE PRECISION NULL,
-  series_1 DOUBLE PRECISION NULL,
-  series_2 DOUBLE PRECISION NULL,
-  series_bool BOOLEAN NULL
-);
-CREATE INDEX ON PUBLIC."two_Partitions" (device_id, "timeCustom" DESC NULLS LAST) WHERE device_id IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, series_0) WHERE series_0 IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, series_1)  WHERE series_1 IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, series_2) WHERE series_2 IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, series_bool) WHERE series_bool IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, device_id);
-SELECT * FROM create_hypertable('"public"."two_Partitions"'::regclass, 'timeCustom'::name, 'device_id'::name, associated_schema_name=>'_timescaledb_internal'::text, number_partitions => 2, chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
-\set QUIET off
-BEGIN;
-\COPY public."two_Partitions" FROM 'data/ds1_dev1_1.tsv' NULL AS '';
-COMMIT;
-INSERT INTO public."two_Partitions"("timeCustom", device_id, series_0, series_1) VALUES
-(1257987600000000000, 'dev1', 1.5, 1),
-(1257987600000000000, 'dev1', 1.5, 2),
-(1257894000000000000, 'dev2', 1.5, 1),
-(1257894002000000000, 'dev1', 2.5, 3);
-INSERT INTO "two_Partitions"("timeCustom", device_id, series_0, series_1) VALUES
-(1257894000000000000, 'dev2', 1.5, 2);
-\set QUIET on
-\o
-SELECT * FROM PUBLIC."two_Partitions";
-     timeCustom      | device_id | series_0 | series_1 | series_2 | series_bool 
----------------------+-----------+----------+----------+----------+-------------
- 1257894000000000000 | dev1      |      1.5 |        1 |        2 | t
- 1257894000000000000 | dev1      |      1.5 |        2 |          | 
- 1257894000000001000 | dev1      |      2.5 |        3 |          | 
- 1257894001000000000 | dev1      |      3.5 |        4 |          | 
- 1257894002000000000 | dev1      |      5.5 |        6 |          | t
- 1257894002000000000 | dev1      |      5.5 |        7 |          | f
- 1257894002000000000 | dev1      |      2.5 |        3 |          | 
- 1257897600000000000 | dev1      |      4.5 |        5 |          | f
- 1257987600000000000 | dev1      |      1.5 |        1 |          | 
- 1257987600000000000 | dev1      |      1.5 |        2 |          | 
- 1257894000000000000 | dev2      |      1.5 |        1 |          | 
- 1257894000000000000 | dev2      |      1.5 |        2 |          | 
-(12 rows)
-
-EXPLAIN (verbose ON, costs off) SELECT * FROM PUBLIC."two_Partitions";
-                                                                                        QUERY PLAN                                                                                        
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Append
-   ->  Seq Scan on _timescaledb_internal._hyper_1_1_chunk
-         Output: _hyper_1_1_chunk."timeCustom", _hyper_1_1_chunk.device_id, _hyper_1_1_chunk.series_0, _hyper_1_1_chunk.series_1, _hyper_1_1_chunk.series_2, _hyper_1_1_chunk.series_bool
-   ->  Seq Scan on _timescaledb_internal._hyper_1_2_chunk
-         Output: _hyper_1_2_chunk."timeCustom", _hyper_1_2_chunk.device_id, _hyper_1_2_chunk.series_0, _hyper_1_2_chunk.series_1, _hyper_1_2_chunk.series_2, _hyper_1_2_chunk.series_bool
-   ->  Seq Scan on _timescaledb_internal._hyper_1_3_chunk
-         Output: _hyper_1_3_chunk."timeCustom", _hyper_1_3_chunk.device_id, _hyper_1_3_chunk.series_0, _hyper_1_3_chunk.series_1, _hyper_1_3_chunk.series_2, _hyper_1_3_chunk.series_bool
-   ->  Seq Scan on _timescaledb_internal._hyper_1_4_chunk
-         Output: _hyper_1_4_chunk."timeCustom", _hyper_1_4_chunk.device_id, _hyper_1_4_chunk.series_0, _hyper_1_4_chunk.series_1, _hyper_1_4_chunk.series_2, _hyper_1_4_chunk.series_bool
-(9 rows)
-
-\echo "The following queries should NOT scan two_Partitions._hyper_1_1_chunk"
-"The following queries should NOT scan two_Partitions._hyper_1_1_chunk"
-EXPLAIN (verbose ON, costs off) SELECT * FROM PUBLIC."two_Partitions" WHERE device_id = 'dev2';
-                                                                                     QUERY PLAN                                                                                     
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Index Scan using "_hyper_1_4_chunk_two_Partitions_device_id_timeCustom_idx" on _timescaledb_internal._hyper_1_4_chunk
-   Output: _hyper_1_4_chunk."timeCustom", _hyper_1_4_chunk.device_id, _hyper_1_4_chunk.series_0, _hyper_1_4_chunk.series_1, _hyper_1_4_chunk.series_2, _hyper_1_4_chunk.series_bool
-   Index Cond: (_hyper_1_4_chunk.device_id = 'dev2'::text)
-(3 rows)
-
-EXPLAIN (verbose ON, costs off) SELECT * FROM PUBLIC."two_Partitions" WHERE device_id = 'dev'||'2';
-                                                                                     QUERY PLAN                                                                                     
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Index Scan using "_hyper_1_4_chunk_two_Partitions_device_id_timeCustom_idx" on _timescaledb_internal._hyper_1_4_chunk
-   Output: _hyper_1_4_chunk."timeCustom", _hyper_1_4_chunk.device_id, _hyper_1_4_chunk.series_0, _hyper_1_4_chunk.series_1, _hyper_1_4_chunk.series_2, _hyper_1_4_chunk.series_bool
-   Index Cond: (_hyper_1_4_chunk.device_id = 'dev2'::text)
-(3 rows)
-
-EXPLAIN (verbose ON, costs off) SELECT * FROM PUBLIC."two_Partitions" WHERE 'dev'||'2' = device_id;
-                                                                                     QUERY PLAN                                                                                     
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Index Scan using "_hyper_1_4_chunk_two_Partitions_device_id_timeCustom_idx" on _timescaledb_internal._hyper_1_4_chunk
-   Output: _hyper_1_4_chunk."timeCustom", _hyper_1_4_chunk.device_id, _hyper_1_4_chunk.series_0, _hyper_1_4_chunk.series_1, _hyper_1_4_chunk.series_2, _hyper_1_4_chunk.series_bool
-   Index Cond: (_hyper_1_4_chunk.device_id = 'dev2'::text)
-(3 rows)
-
---test integer partition key
-CREATE TABLE "int_part"(time timestamp, object_id int, temp float);
-SELECT create_hypertable('"int_part"', 'time', 'object_id', 2);
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-   create_hypertable   
------------------------
- (2,public,int_part,t)
-(1 row)
-
-INSERT INTO "int_part" VALUES('2017-01-20T09:00:01', 1, 22.5);
-INSERT INTO "int_part" VALUES('2017-01-20T09:00:01', 2, 22.5);
---check that there are two chunks
-SELECT * FROM test.show_subtables('int_part');
-                 Child                  | Tablespace 
-----------------------------------------+------------
- _timescaledb_internal._hyper_2_5_chunk | 
- _timescaledb_internal._hyper_2_6_chunk | 
-(2 rows)
-
-SELECT * FROM "int_part" WHERE object_id = 1;
-           time           | object_id | temp 
---------------------------+-----------+------
- Fri Jan 20 09:00:01 2017 |         1 | 22.5
-(1 row)
-
---check that queries with IN/ANY/= work for the "time" column
-SELECT * FROM "int_part" WHERE time IN (NULL);
- time | object_id | temp 
-------+-----------+------
-(0 rows)
-
-SELECT * FROM "int_part" WHERE time = ANY (NULL);
- time | object_id | temp 
-------+-----------+------
-(0 rows)
-
-SELECT * FROM "int_part" WHERE time = NULL;
- time | object_id | temp 
-------+-----------+------
-(0 rows)
-
---make sure this touches only one partititon
-EXPLAIN (verbose ON, costs off) SELECT * FROM "int_part" WHERE object_id = 1;
-                                               QUERY PLAN                                                
----------------------------------------------------------------------------------------------------------
- Index Scan using _hyper_2_5_chunk_int_part_object_id_time_idx on _timescaledb_internal._hyper_2_5_chunk
-   Output: _hyper_2_5_chunk."time", _hyper_2_5_chunk.object_id, _hyper_2_5_chunk.temp
-   Index Cond: (_hyper_2_5_chunk.object_id = 1)
-(3 rows)
-
---Need to verify space partitions are currently pruned in this query
---EXPLAIN (verbose ON, costs off) SELECT * FROM "two_Partitions" WHERE device_id IN ('dev2', 'dev21');
-\echo "The following shows non-aggregated queries with time desc using merge append"
-"The following shows non-aggregated queries with time desc using merge append"
-EXPLAIN (verbose ON, costs off)SELECT * FROM PUBLIC."two_Partitions" ORDER BY "timeCustom" DESC NULLS LAST limit 2;
-                                                                                              QUERY PLAN                                                                                              
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Limit
-   Output: "two_Partitions"."timeCustom", "two_Partitions".device_id, "two_Partitions".series_0, "two_Partitions".series_1, "two_Partitions".series_2, "two_Partitions".series_bool
-   ->  Custom Scan (ChunkAppend) on public."two_Partitions"
-         Output: "two_Partitions"."timeCustom", "two_Partitions".device_id, "two_Partitions".series_0, "two_Partitions".series_1, "two_Partitions".series_2, "two_Partitions".series_bool
-         Order: "two_Partitions"."timeCustom" DESC NULLS LAST
-         Startup Exclusion: false
-         Runtime Exclusion: false
-         ->  Index Scan using "_hyper_1_3_chunk_two_Partitions_timeCustom_device_id_idx" on _timescaledb_internal._hyper_1_3_chunk
-               Output: _hyper_1_3_chunk."timeCustom", _hyper_1_3_chunk.device_id, _hyper_1_3_chunk.series_0, _hyper_1_3_chunk.series_1, _hyper_1_3_chunk.series_2, _hyper_1_3_chunk.series_bool
-         ->  Index Scan using "_hyper_1_2_chunk_two_Partitions_timeCustom_device_id_idx" on _timescaledb_internal._hyper_1_2_chunk
-               Output: _hyper_1_2_chunk."timeCustom", _hyper_1_2_chunk.device_id, _hyper_1_2_chunk.series_0, _hyper_1_2_chunk.series_1, _hyper_1_2_chunk.series_2, _hyper_1_2_chunk.series_bool
-         ->  Merge Append
-               Sort Key: _hyper_1_4_chunk."timeCustom" DESC NULLS LAST
-               ->  Index Scan using "_hyper_1_4_chunk_two_Partitions_timeCustom_device_id_idx" on _timescaledb_internal._hyper_1_4_chunk
-                     Output: _hyper_1_4_chunk."timeCustom", _hyper_1_4_chunk.device_id, _hyper_1_4_chunk.series_0, _hyper_1_4_chunk.series_1, _hyper_1_4_chunk.series_2, _hyper_1_4_chunk.series_bool
-               ->  Index Scan using "_hyper_1_1_chunk_two_Partitions_timeCustom_device_id_idx" on _timescaledb_internal._hyper_1_1_chunk
-                     Output: _hyper_1_1_chunk."timeCustom", _hyper_1_1_chunk.device_id, _hyper_1_1_chunk.series_0, _hyper_1_1_chunk.series_1, _hyper_1_1_chunk.series_2, _hyper_1_1_chunk.series_bool
-(17 rows)
-
---shows that more specific indexes are used if the WHERE clauses "match", uses the series_1 index here.
-EXPLAIN (verbose ON, costs off)SELECT * FROM PUBLIC."two_Partitions" WHERE series_1 IS NOT NULL ORDER BY "timeCustom" DESC NULLS LAST limit 2;
-                                                                                              QUERY PLAN                                                                                              
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Limit
-   Output: "two_Partitions"."timeCustom", "two_Partitions".device_id, "two_Partitions".series_0, "two_Partitions".series_1, "two_Partitions".series_2, "two_Partitions".series_bool
-   ->  Custom Scan (ChunkAppend) on public."two_Partitions"
-         Output: "two_Partitions"."timeCustom", "two_Partitions".device_id, "two_Partitions".series_0, "two_Partitions".series_1, "two_Partitions".series_2, "two_Partitions".series_bool
-         Order: "two_Partitions"."timeCustom" DESC NULLS LAST
-         Startup Exclusion: false
-         Runtime Exclusion: false
-         ->  Index Scan using "_hyper_1_3_chunk_two_Partitions_timeCustom_series_1_idx" on _timescaledb_internal._hyper_1_3_chunk
-               Output: _hyper_1_3_chunk."timeCustom", _hyper_1_3_chunk.device_id, _hyper_1_3_chunk.series_0, _hyper_1_3_chunk.series_1, _hyper_1_3_chunk.series_2, _hyper_1_3_chunk.series_bool
-         ->  Index Scan using "_hyper_1_2_chunk_two_Partitions_timeCustom_series_1_idx" on _timescaledb_internal._hyper_1_2_chunk
-               Output: _hyper_1_2_chunk."timeCustom", _hyper_1_2_chunk.device_id, _hyper_1_2_chunk.series_0, _hyper_1_2_chunk.series_1, _hyper_1_2_chunk.series_2, _hyper_1_2_chunk.series_bool
-         ->  Merge Append
-               Sort Key: _hyper_1_4_chunk."timeCustom" DESC NULLS LAST
-               ->  Index Scan using "_hyper_1_4_chunk_two_Partitions_timeCustom_series_1_idx" on _timescaledb_internal._hyper_1_4_chunk
-                     Output: _hyper_1_4_chunk."timeCustom", _hyper_1_4_chunk.device_id, _hyper_1_4_chunk.series_0, _hyper_1_4_chunk.series_1, _hyper_1_4_chunk.series_2, _hyper_1_4_chunk.series_bool
-               ->  Index Scan using "_hyper_1_1_chunk_two_Partitions_timeCustom_series_1_idx" on _timescaledb_internal._hyper_1_1_chunk
-                     Output: _hyper_1_1_chunk."timeCustom", _hyper_1_1_chunk.device_id, _hyper_1_1_chunk.series_0, _hyper_1_1_chunk.series_1, _hyper_1_1_chunk.series_2, _hyper_1_1_chunk.series_bool
-(17 rows)
-
---here the "match" is implication series_1 > 1 => series_1 IS NOT NULL
-EXPLAIN (verbose ON, costs off)SELECT * FROM PUBLIC."two_Partitions" WHERE series_1 > 1 ORDER BY "timeCustom" DESC NULLS LAST limit 2;
-                                                                                              QUERY PLAN                                                                                              
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Limit
-   Output: "two_Partitions"."timeCustom", "two_Partitions".device_id, "two_Partitions".series_0, "two_Partitions".series_1, "two_Partitions".series_2, "two_Partitions".series_bool
-   ->  Custom Scan (ChunkAppend) on public."two_Partitions"
-         Output: "two_Partitions"."timeCustom", "two_Partitions".device_id, "two_Partitions".series_0, "two_Partitions".series_1, "two_Partitions".series_2, "two_Partitions".series_bool
-         Order: "two_Partitions"."timeCustom" DESC NULLS LAST
-         Startup Exclusion: false
-         Runtime Exclusion: false
-         ->  Index Scan using "_hyper_1_3_chunk_two_Partitions_timeCustom_series_1_idx" on _timescaledb_internal._hyper_1_3_chunk
-               Output: _hyper_1_3_chunk."timeCustom", _hyper_1_3_chunk.device_id, _hyper_1_3_chunk.series_0, _hyper_1_3_chunk.series_1, _hyper_1_3_chunk.series_2, _hyper_1_3_chunk.series_bool
-               Index Cond: (_hyper_1_3_chunk.series_1 > '1'::double precision)
-         ->  Index Scan using "_hyper_1_2_chunk_two_Partitions_timeCustom_series_1_idx" on _timescaledb_internal._hyper_1_2_chunk
-               Output: _hyper_1_2_chunk."timeCustom", _hyper_1_2_chunk.device_id, _hyper_1_2_chunk.series_0, _hyper_1_2_chunk.series_1, _hyper_1_2_chunk.series_2, _hyper_1_2_chunk.series_bool
-               Index Cond: (_hyper_1_2_chunk.series_1 > '1'::double precision)
-         ->  Merge Append
-               Sort Key: _hyper_1_4_chunk."timeCustom" DESC NULLS LAST
-               ->  Index Scan using "_hyper_1_4_chunk_two_Partitions_timeCustom_series_1_idx" on _timescaledb_internal._hyper_1_4_chunk
-                     Output: _hyper_1_4_chunk."timeCustom", _hyper_1_4_chunk.device_id, _hyper_1_4_chunk.series_0, _hyper_1_4_chunk.series_1, _hyper_1_4_chunk.series_2, _hyper_1_4_chunk.series_bool
-                     Index Cond: (_hyper_1_4_chunk.series_1 > '1'::double precision)
-               ->  Index Scan using "_hyper_1_1_chunk_two_Partitions_timeCustom_series_1_idx" on _timescaledb_internal._hyper_1_1_chunk
-                     Output: _hyper_1_1_chunk."timeCustom", _hyper_1_1_chunk.device_id, _hyper_1_1_chunk.series_0, _hyper_1_1_chunk.series_1, _hyper_1_1_chunk.series_2, _hyper_1_1_chunk.series_bool
-                     Index Cond: (_hyper_1_1_chunk.series_1 > '1'::double precision)
-(21 rows)
-
---note that without time transform things work too
-EXPLAIN (verbose ON, costs off)SELECT "timeCustom" t, min(series_0) FROM PUBLIC."two_Partitions" GROUP BY t ORDER BY t DESC NULLS LAST limit 2;
-                                                                  QUERY PLAN                                                                   
------------------------------------------------------------------------------------------------------------------------------------------------
- Limit
-   Output: "two_Partitions"."timeCustom", (min("two_Partitions".series_0))
-   ->  GroupAggregate
-         Output: "two_Partitions"."timeCustom", min("two_Partitions".series_0)
-         Group Key: "two_Partitions"."timeCustom"
-         ->  Custom Scan (ChunkAppend) on public."two_Partitions"
-               Output: "two_Partitions"."timeCustom", "two_Partitions".series_0
-               Order: "two_Partitions"."timeCustom" DESC NULLS LAST
-               Startup Exclusion: false
-               Runtime Exclusion: false
-               ->  Index Scan using "_hyper_1_3_chunk_two_Partitions_timeCustom_device_id_idx" on _timescaledb_internal._hyper_1_3_chunk
-                     Output: _hyper_1_3_chunk."timeCustom", _hyper_1_3_chunk.series_0
-               ->  Index Scan using "_hyper_1_2_chunk_two_Partitions_timeCustom_device_id_idx" on _timescaledb_internal._hyper_1_2_chunk
-                     Output: _hyper_1_2_chunk."timeCustom", _hyper_1_2_chunk.series_0
-               ->  Merge Append
-                     Sort Key: _hyper_1_4_chunk."timeCustom" DESC NULLS LAST
-                     ->  Index Scan using "_hyper_1_4_chunk_two_Partitions_timeCustom_device_id_idx" on _timescaledb_internal._hyper_1_4_chunk
-                           Output: _hyper_1_4_chunk."timeCustom", _hyper_1_4_chunk.series_0
-                     ->  Index Scan using "_hyper_1_1_chunk_two_Partitions_timeCustom_device_id_idx" on _timescaledb_internal._hyper_1_1_chunk
-                           Output: _hyper_1_1_chunk."timeCustom", _hyper_1_1_chunk.series_0
-(20 rows)
-
---The query should still use the index on timeCustom, even though the GROUP BY/ORDER BY is on the transformed time 't'.
---However, current query plans show that it does not.
-EXPLAIN (verbose ON, costs off)SELECT "timeCustom"/10 t, min(series_0) FROM PUBLIC."two_Partitions" GROUP BY t ORDER BY t DESC NULLS LAST limit 2;
-                                            QUERY PLAN                                            
---------------------------------------------------------------------------------------------------
- Limit
-   Output: ((_hyper_1_1_chunk."timeCustom" / 10)), (min(_hyper_1_1_chunk.series_0))
-   ->  Sort
-         Output: ((_hyper_1_1_chunk."timeCustom" / 10)), (min(_hyper_1_1_chunk.series_0))
-         Sort Key: ((_hyper_1_1_chunk."timeCustom" / 10)) DESC NULLS LAST
-         ->  HashAggregate
-               Output: ((_hyper_1_1_chunk."timeCustom" / 10)), min(_hyper_1_1_chunk.series_0)
-               Group Key: (_hyper_1_1_chunk."timeCustom" / 10)
-               ->  Result
-                     Output: (_hyper_1_1_chunk."timeCustom" / 10), _hyper_1_1_chunk.series_0
-                     ->  Append
-                           ->  Seq Scan on _timescaledb_internal._hyper_1_1_chunk
-                                 Output: _hyper_1_1_chunk."timeCustom", _hyper_1_1_chunk.series_0
-                           ->  Seq Scan on _timescaledb_internal._hyper_1_2_chunk
-                                 Output: _hyper_1_2_chunk."timeCustom", _hyper_1_2_chunk.series_0
-                           ->  Seq Scan on _timescaledb_internal._hyper_1_3_chunk
-                                 Output: _hyper_1_3_chunk."timeCustom", _hyper_1_3_chunk.series_0
-                           ->  Seq Scan on _timescaledb_internal._hyper_1_4_chunk
-                                 Output: _hyper_1_4_chunk."timeCustom", _hyper_1_4_chunk.series_0
-(19 rows)
-
-EXPLAIN (verbose ON, costs off)SELECT "timeCustom"%10 t, min(series_0) FROM PUBLIC."two_Partitions" GROUP BY t ORDER BY t DESC NULLS LAST limit 2;
-                                               QUERY PLAN                                               
---------------------------------------------------------------------------------------------------------
- Limit
-   Output: ((_hyper_1_1_chunk."timeCustom" % '10'::bigint)), (min(_hyper_1_1_chunk.series_0))
-   ->  Sort
-         Output: ((_hyper_1_1_chunk."timeCustom" % '10'::bigint)), (min(_hyper_1_1_chunk.series_0))
-         Sort Key: ((_hyper_1_1_chunk."timeCustom" % '10'::bigint)) DESC NULLS LAST
-         ->  HashAggregate
-               Output: ((_hyper_1_1_chunk."timeCustom" % '10'::bigint)), min(_hyper_1_1_chunk.series_0)
-               Group Key: (_hyper_1_1_chunk."timeCustom" % '10'::bigint)
-               ->  Result
-                     Output: (_hyper_1_1_chunk."timeCustom" % '10'::bigint), _hyper_1_1_chunk.series_0
-                     ->  Append
-                           ->  Seq Scan on _timescaledb_internal._hyper_1_1_chunk
-                                 Output: _hyper_1_1_chunk."timeCustom", _hyper_1_1_chunk.series_0
-                           ->  Seq Scan on _timescaledb_internal._hyper_1_2_chunk
-                                 Output: _hyper_1_2_chunk."timeCustom", _hyper_1_2_chunk.series_0
-                           ->  Seq Scan on _timescaledb_internal._hyper_1_3_chunk
-                                 Output: _hyper_1_3_chunk."timeCustom", _hyper_1_3_chunk.series_0
-                           ->  Seq Scan on _timescaledb_internal._hyper_1_4_chunk
-                                 Output: _hyper_1_4_chunk."timeCustom", _hyper_1_4_chunk.series_0
-(19 rows)
-
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/tableam.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/tableam.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/tableam.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/tableam.out	2023-11-25 05:27:44.137022684 +0000
@@ -1,39 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
--- Test support for setting table access method on hypertables
-\c :TEST_DBNAME :ROLE_SUPERUSER
--- create a new access method that reuses the heap handler
-CREATE ACCESS METHOD testam TYPE TABLE HANDLER heap_tableam_handler;
-SET ROLE :ROLE_DEFAULT_PERM_USER;
-CREATE TABLE testam (time timestamptz, device int, temp float) USING testam;
-SELECT create_hypertable('testam', 'time', 'device', 2);
-NOTICE:  adding not-null constraint to column "time"
-  create_hypertable  
----------------------
- (1,public,testam,t)
-(1 row)
-
--- show that the hypertable is using the 'testam' table access method
-SELECT amname AS hypertable_amname
-FROM pg_class cl, pg_am am
-WHERE cl.oid = 'testam'::regclass
-AND cl.relam = am.oid;
- hypertable_amname 
--------------------
- testam
-(1 row)
-
--- insert data to create a chunk
-INSERT INTO testam VALUES('2020-01-22:11:30', 1, 29.3);
--- make sure the table access method for a chunk is the same as the
--- hypertable root
-SELECT amname AS chunk_amname
-FROM pg_class cl, pg_am am, show_chunks('testam') ch
-WHERE cl.oid = ch
-AND cl.relam = am.oid;
- chunk_amname 
---------------
- testam
-(1 row)
-
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/timestamp.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/timestamp.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/timestamp.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/timestamp.out	2023-11-25 05:27:44.149022649 +0000
@@ -1,2021 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
--- Utility function for grouping/slotting time with a given interval.
-CREATE OR REPLACE FUNCTION date_group(
-    field           timestamp,
-    group_interval  interval
-)
-    RETURNS timestamp LANGUAGE SQL STABLE AS
-$BODY$
-    SELECT to_timestamp((EXTRACT(EPOCH from $1)::int /
-        EXTRACT(EPOCH from group_interval)::int) *
-        EXTRACT(EPOCH from group_interval)::int)::timestamp;
-$BODY$;
-CREATE TABLE PUBLIC."testNs" (
-  "timeCustom" TIMESTAMP NOT NULL,
-  device_id TEXT NOT NULL,
-  series_0 DOUBLE PRECISION NULL,
-  series_1 DOUBLE PRECISION NULL,
-  series_2 DOUBLE PRECISION NULL,
-  series_bool BOOLEAN NULL
-);
-CREATE INDEX ON PUBLIC."testNs" (device_id, "timeCustom" DESC NULLS LAST) WHERE device_id IS NOT NULL;
-\c :TEST_DBNAME :ROLE_SUPERUSER
-CREATE SCHEMA "testNs" AUTHORIZATION :ROLE_DEFAULT_PERM_USER;
-\c :TEST_DBNAME :ROLE_DEFAULT_PERM_USER
-SELECT * FROM create_hypertable('"public"."testNs"', 'timeCustom', 'device_id', 2, associated_schema_name=>'testNs' );
-WARNING:  column type "timestamp without time zone" used for "timeCustom" does not follow best practices
- hypertable_id | schema_name | table_name | created 
----------------+-------------+------------+---------
-             1 | public      | testNs     | t
-(1 row)
-
-\c :TEST_DBNAME
-INSERT INTO PUBLIC."testNs"("timeCustom", device_id, series_0, series_1) VALUES
-('2009-11-12T01:00:00+00:00', 'dev1', 1.5, 1),
-('2009-11-12T01:00:00+00:00', 'dev1', 1.5, 2),
-('2009-11-10T23:00:02+00:00', 'dev1', 2.5, 3);
-INSERT INTO PUBLIC."testNs"("timeCustom", device_id, series_0, series_1) VALUES
-('2009-11-10T23:00:00+00:00', 'dev2', 1.5, 1),
-('2009-11-10T23:00:00+00:00', 'dev2', 1.5, 2);
-SELECT * FROM PUBLIC."testNs";
-        timeCustom        | device_id | series_0 | series_1 | series_2 | series_bool 
---------------------------+-----------+----------+----------+----------+-------------
- Thu Nov 12 01:00:00 2009 | dev1      |      1.5 |        1 |          | 
- Thu Nov 12 01:00:00 2009 | dev1      |      1.5 |        2 |          | 
- Tue Nov 10 23:00:02 2009 | dev1      |      2.5 |        3 |          | 
- Tue Nov 10 23:00:00 2009 | dev2      |      1.5 |        1 |          | 
- Tue Nov 10 23:00:00 2009 | dev2      |      1.5 |        2 |          | 
-(5 rows)
-
-SET client_min_messages = WARNING;
-\echo 'The next 2 queries will differ in output between UTC and EST since the mod is on the 100th hour UTC'
-The next 2 queries will differ in output between UTC and EST since the mod is on the 100th hour UTC
-SET timezone = 'UTC';
-SELECT date_group("timeCustom", '100 days') AS time, sum(series_0)
-FROM PUBLIC."testNs" GROUP BY time ORDER BY time ASC;
-           time           | sum 
---------------------------+-----
- Sun Sep 13 00:00:00 2009 | 8.5
-(1 row)
-
-SET timezone = 'EST';
-SELECT date_group("timeCustom", '100 days') AS time, sum(series_0)
-FROM PUBLIC."testNs" GROUP BY time ORDER BY time ASC;
-           time           | sum 
---------------------------+-----
- Sat Sep 12 19:00:00 2009 | 8.5
-(1 row)
-
-\echo 'The rest of the queries will be the same in output between UTC and EST'
-The rest of the queries will be the same in output between UTC and EST
-SET timezone = 'UTC';
-SELECT date_group("timeCustom", '1 day') AS time, sum(series_0)
-FROM PUBLIC."testNs" GROUP BY time ORDER BY time ASC;
-           time           | sum 
---------------------------+-----
- Tue Nov 10 00:00:00 2009 | 5.5
- Thu Nov 12 00:00:00 2009 |   3
-(2 rows)
-
-SET timezone = 'EST';
-SELECT date_group("timeCustom", '1 day') AS time, sum(series_0)
-FROM PUBLIC."testNs" GROUP BY time ORDER BY time ASC;
-           time           | sum 
---------------------------+-----
- Mon Nov 09 19:00:00 2009 | 5.5
- Wed Nov 11 19:00:00 2009 |   3
-(2 rows)
-
-SET timezone = 'UTC';
-SELECT *
-FROM PUBLIC."testNs"
-WHERE "timeCustom" >= TIMESTAMP '2009-11-10T23:00:00'
-AND "timeCustom" < TIMESTAMP '2009-11-12T01:00:00' ORDER BY "timeCustom" DESC, device_id, series_1;
-        timeCustom        | device_id | series_0 | series_1 | series_2 | series_bool 
---------------------------+-----------+----------+----------+----------+-------------
- Tue Nov 10 23:00:02 2009 | dev1      |      2.5 |        3 |          | 
- Tue Nov 10 23:00:00 2009 | dev2      |      1.5 |        1 |          | 
- Tue Nov 10 23:00:00 2009 | dev2      |      1.5 |        2 |          | 
-(3 rows)
-
-SET timezone = 'EST';
-SELECT *
-FROM PUBLIC."testNs"
-WHERE "timeCustom" >= TIMESTAMP '2009-11-10T23:00:00'
-AND "timeCustom" < TIMESTAMP '2009-11-12T01:00:00' ORDER BY "timeCustom" DESC, device_id, series_1;
-        timeCustom        | device_id | series_0 | series_1 | series_2 | series_bool 
---------------------------+-----------+----------+----------+----------+-------------
- Tue Nov 10 23:00:02 2009 | dev1      |      2.5 |        3 |          | 
- Tue Nov 10 23:00:00 2009 | dev2      |      1.5 |        1 |          | 
- Tue Nov 10 23:00:00 2009 | dev2      |      1.5 |        2 |          | 
-(3 rows)
-
-SET timezone = 'UTC';
-SELECT date_group("timeCustom", '1 day') AS time, sum(series_0)
-FROM PUBLIC."testNs" GROUP BY time ORDER BY time ASC LIMIT 2;
-           time           | sum 
---------------------------+-----
- Tue Nov 10 00:00:00 2009 | 5.5
- Thu Nov 12 00:00:00 2009 |   3
-(2 rows)
-
-SET timezone = 'EST';
-SELECT date_group("timeCustom", '1 day') AS time, sum(series_0)
-FROM PUBLIC."testNs" GROUP BY time ORDER BY time ASC LIMIT 2;
-           time           | sum 
---------------------------+-----
- Mon Nov 09 19:00:00 2009 | 5.5
- Wed Nov 11 19:00:00 2009 |   3
-(2 rows)
-
-------------------------------------
--- Test time conversion functions --
-------------------------------------
-\set ON_ERROR_STOP 0
-SET timezone = 'UTC';
--- Conversion to timestamp using Postgres built-in function taking
--- double. Gives inaccurate result on Postgres <= 9.6.2. Accurate on
--- Postgres >= 9.6.3.
-SELECT to_timestamp(1486480176.236538);
-            to_timestamp             
--------------------------------------
- Tue Feb 07 15:09:36.236538 2017 UTC
-(1 row)
-
--- extension-specific version taking microsecond UNIX timestamp
-SELECT _timescaledb_internal.to_timestamp(1486480176236538);
-            to_timestamp             
--------------------------------------
- Tue Feb 07 15:09:36.236538 2017 UTC
-(1 row)
-
--- Should be the inverse of the statement above.
-SELECT _timescaledb_internal.to_unix_microseconds('2017-02-07 15:09:36.236538+00');
- to_unix_microseconds 
-----------------------
-     1486480176236538
-(1 row)
-
--- For timestamps, BIGINT MAX represents +Infinity and BIGINT MIN
--- -Infinity. We keep this notion for UNIX epoch time:
-SELECT _timescaledb_internal.to_unix_microseconds('+infinity');
-ERROR:  invalid input syntax for type timestamp with time zone: "+infinity" at character 51
-SELECT _timescaledb_internal.to_timestamp(9223372036854775807);
- to_timestamp 
---------------
- infinity
-(1 row)
-
-SELECT _timescaledb_internal.to_unix_microseconds('-infinity');
- to_unix_microseconds 
-----------------------
- -9223372036854775808
-(1 row)
-
-SELECT _timescaledb_internal.to_timestamp(-9223372036854775808);
- to_timestamp 
---------------
- -infinity
-(1 row)
-
--- In UNIX microseconds, the largest bigint value below infinity
--- (BIGINT MAX) is smaller than internal date upper bound and should
--- therefore be OK. Further, converting to the internal postgres epoch
--- cannot overflow a 64-bit INTEGER since the postgres epoch is at a
--- later date compared to the UNIX epoch, and is therefore represented
--- by a smaller number
-SELECT _timescaledb_internal.to_timestamp(9223372036854775806);
-             to_timestamp              
----------------------------------------
- Sun Jan 10 04:00:54.775806 294247 UTC
-(1 row)
-
--- Julian day zero is -210866803200000000 microseconds from UNIX epoch
-SELECT _timescaledb_internal.to_timestamp(-210866803200000000);
-          to_timestamp           
----------------------------------
- Mon Nov 24 00:00:00 4714 UTC BC
-(1 row)
-
-\set VERBOSITY default
--- Going beyond Julian day zero should give out-of-range error
-SELECT _timescaledb_internal.to_timestamp(-210866803200000001);
-ERROR:  timestamp out of range
--- Lower bound on date (should return the Julian day zero UNIX timestamp above)
-SELECT _timescaledb_internal.to_unix_microseconds('4714-11-24 00:00:00+00 BC');
- to_unix_microseconds 
-----------------------
-  -210866803200000000
-(1 row)
-
--- Going beyond lower bound on date should return out-of-range
-SELECT _timescaledb_internal.to_unix_microseconds('4714-11-23 23:59:59.999999+00 BC');
-ERROR:  timestamp out of range: "4714-11-23 23:59:59.999999+00 BC"
-LINE 1: SELECT _timescaledb_internal.to_unix_microseconds('4714-11-2...
-                                                          ^
--- The upper bound for Postgres TIMESTAMPTZ
-SELECT timestamp '294276-12-31 23:59:59.999999+00';
-             timestamp             
------------------------------------
- Sun Dec 31 23:59:59.999999 294276
-(1 row)
-
--- Going beyond the upper bound, should fail
-SELECT timestamp '294276-12-31 23:59:59.999999+00' + interval '1 us';
-ERROR:  timestamp out of range
--- Cannot represent the upper bound timestamp with a UNIX microsecond timestamp
--- since the Postgres epoch is at a later date than the UNIX epoch.
-SELECT _timescaledb_internal.to_unix_microseconds('294276-12-31 23:59:59.999999+00');
-ERROR:  timestamp out of range
--- Subtracting the difference between the two epochs (10957 days) should bring
--- us within range.
-SELECT timestamp '294276-12-31 23:59:59.999999+00' - interval '10957 days';
-             ?column?              
------------------------------------
- Fri Jan 01 23:59:59.999999 294247
-(1 row)
-
-SELECT _timescaledb_internal.to_unix_microseconds('294247-01-01 23:59:59.999999');
- to_unix_microseconds 
-----------------------
-  9223371331199999999
-(1 row)
-
--- Adding one microsecond should take us out-of-range again
-SELECT timestamp '294247-01-01 23:59:59.999999' + interval '1 us';
-          ?column?          
-----------------------------
- Sat Jan 02 00:00:00 294247
-(1 row)
-
-SELECT _timescaledb_internal.to_unix_microseconds(timestamp '294247-01-01 23:59:59.999999' + interval '1 us');
-ERROR:  timestamp out of range
---no time_bucketing of dates not by integer # of days
-SELECT time_bucket('1 hour', DATE '2012-01-01');
-ERROR:  interval must not have sub-day precision
-SELECT time_bucket('25 hour', DATE '2012-01-01');
-ERROR:  interval must be a multiple of a day
-\set ON_ERROR_STOP 1
-SELECT time_bucket(INTERVAL '1 day', TIMESTAMP '2011-01-02 01:01:01');
-       time_bucket        
---------------------------
- Sun Jan 02 00:00:00 2011
-(1 row)
-
-SELECT time, time_bucket(INTERVAL '2 day ', time)
-FROM unnest(ARRAY[
-    TIMESTAMP '2011-01-01 01:01:01',
-    TIMESTAMP '2011-01-02 01:01:01',
-    TIMESTAMP '2011-01-03 01:01:01',
-    TIMESTAMP '2011-01-04 01:01:01'
-    ]) AS time;
-           time           |       time_bucket        
---------------------------+--------------------------
- Sat Jan 01 01:01:01 2011 | Sat Jan 01 00:00:00 2011
- Sun Jan 02 01:01:01 2011 | Sat Jan 01 00:00:00 2011
- Mon Jan 03 01:01:01 2011 | Mon Jan 03 00:00:00 2011
- Tue Jan 04 01:01:01 2011 | Mon Jan 03 00:00:00 2011
-(4 rows)
-
-SELECT int_def, time_bucket(int_def,TIMESTAMP '2011-01-02 01:01:01.111')
-FROM unnest(ARRAY[
-    INTERVAL '1 millisecond',
-    INTERVAL '1 second',
-    INTERVAL '1 minute',
-    INTERVAL '1 hour',
-    INTERVAL '1 day',
-    INTERVAL '2 millisecond',
-    INTERVAL '2 second',
-    INTERVAL '2 minute',
-    INTERVAL '2 hour',
-    INTERVAL '2 day'
-    ]) AS int_def;
-   int_def    |         time_bucket          
---------------+------------------------------
- @ 0.001 secs | Sun Jan 02 01:01:01.111 2011
- @ 1 sec      | Sun Jan 02 01:01:01 2011
- @ 1 min      | Sun Jan 02 01:01:00 2011
- @ 1 hour     | Sun Jan 02 01:00:00 2011
- @ 1 day      | Sun Jan 02 00:00:00 2011
- @ 0.002 secs | Sun Jan 02 01:01:01.11 2011
- @ 2 secs     | Sun Jan 02 01:01:00 2011
- @ 2 mins     | Sun Jan 02 01:00:00 2011
- @ 2 hours    | Sun Jan 02 00:00:00 2011
- @ 2 days     | Sat Jan 01 00:00:00 2011
-(10 rows)
-
-\set ON_ERROR_STOP 0
-SELECT time_bucket(INTERVAL '1 year 1d',TIMESTAMP '2011-01-02 01:01:01.111');
-ERROR:  month intervals cannot have day or time component
-SELECT time_bucket(INTERVAL '1 month 1 minute',TIMESTAMP '2011-01-02 01:01:01.111');
-ERROR:  month intervals cannot have day or time component
-\set ON_ERROR_STOP 1
-SELECT time, time_bucket(INTERVAL '5 minute', time)
-FROM unnest(ARRAY[
-    TIMESTAMP '1970-01-01 00:59:59.999999',
-    TIMESTAMP '1970-01-01 01:01:00',
-    TIMESTAMP '1970-01-01 01:04:59.999999',
-    TIMESTAMP '1970-01-01 01:05:00'
-    ]) AS time;
-              time               |       time_bucket        
----------------------------------+--------------------------
- Thu Jan 01 00:59:59.999999 1970 | Thu Jan 01 00:55:00 1970
- Thu Jan 01 01:01:00 1970        | Thu Jan 01 01:00:00 1970
- Thu Jan 01 01:04:59.999999 1970 | Thu Jan 01 01:00:00 1970
- Thu Jan 01 01:05:00 1970        | Thu Jan 01 01:05:00 1970
-(4 rows)
-
-SELECT time, time_bucket(INTERVAL '5 minute', time)
-FROM unnest(ARRAY[
-    TIMESTAMP '2011-01-02 01:04:59.999999',
-    TIMESTAMP '2011-01-02 01:05:00',
-    TIMESTAMP '2011-01-02 01:09:59.999999',
-    TIMESTAMP '2011-01-02 01:10:00'
-    ]) AS time;
-              time               |       time_bucket        
----------------------------------+--------------------------
- Sun Jan 02 01:04:59.999999 2011 | Sun Jan 02 01:00:00 2011
- Sun Jan 02 01:05:00 2011        | Sun Jan 02 01:05:00 2011
- Sun Jan 02 01:09:59.999999 2011 | Sun Jan 02 01:05:00 2011
- Sun Jan 02 01:10:00 2011        | Sun Jan 02 01:10:00 2011
-(4 rows)
-
---offset with interval
-SELECT time, time_bucket(INTERVAL '5 minute', time ,  INTERVAL '2 minutes')
-FROM unnest(ARRAY[
-    TIMESTAMP '2011-01-02 01:01:59.999999',
-    TIMESTAMP '2011-01-02 01:02:00',
-    TIMESTAMP '2011-01-02 01:06:59.999999',
-    TIMESTAMP '2011-01-02 01:07:00'
-    ]) AS time;
-              time               |       time_bucket        
----------------------------------+--------------------------
- Sun Jan 02 01:01:59.999999 2011 | Sun Jan 02 00:57:00 2011
- Sun Jan 02 01:02:00 2011        | Sun Jan 02 01:02:00 2011
- Sun Jan 02 01:06:59.999999 2011 | Sun Jan 02 01:02:00 2011
- Sun Jan 02 01:07:00 2011        | Sun Jan 02 01:07:00 2011
-(4 rows)
-
-SELECT time, time_bucket(INTERVAL '5 minute', time , - INTERVAL '2 minutes')
-FROM unnest(ARRAY[
-    TIMESTAMP '2011-01-02 01:02:59.999999',
-    TIMESTAMP '2011-01-02 01:03:00',
-    TIMESTAMP '2011-01-02 01:07:59.999999',
-    TIMESTAMP '2011-01-02 01:08:00'
-    ]) AS time;
-              time               |       time_bucket        
----------------------------------+--------------------------
- Sun Jan 02 01:02:59.999999 2011 | Sun Jan 02 00:58:00 2011
- Sun Jan 02 01:03:00 2011        | Sun Jan 02 01:03:00 2011
- Sun Jan 02 01:07:59.999999 2011 | Sun Jan 02 01:03:00 2011
- Sun Jan 02 01:08:00 2011        | Sun Jan 02 01:08:00 2011
-(4 rows)
-
---offset with infinity
--- timestamp
-SELECT time, time_bucket(INTERVAL '1 week', time, INTERVAL '1 day')
-FROM unnest(ARRAY[
-    timestamp '-Infinity',
-    timestamp 'Infinity'
-    ]) AS time;
-   time    | time_bucket 
------------+-------------
- -infinity | -infinity
- infinity  | infinity
-(2 rows)
-
--- timestamptz
-SELECT time, time_bucket(INTERVAL '1 week', time, INTERVAL '1 day')
-FROM unnest(ARRAY[
-    timestamp with time zone '-Infinity',
-    timestamp with time zone 'Infinity'
-    ]) AS time;
-   time    | time_bucket 
------------+-------------
- -infinity | -infinity
- infinity  | infinity
-(2 rows)
-
--- Date
-SELECT date, time_bucket(INTERVAL '1 week', date, INTERVAL '1 day')
-FROM unnest(ARRAY[
-    date '-Infinity',
-    date 'Infinity'
-    ]) AS date;
-   date    | time_bucket 
------------+-------------
- -infinity | -infinity
- infinity  | infinity
-(2 rows)
-
---example to align with an origin
-SELECT time, time_bucket(INTERVAL '5 minute', time - (TIMESTAMP '2011-01-02 00:02:00' - TIMESTAMP 'epoch')) +  (TIMESTAMP '2011-01-02 00:02:00'-TIMESTAMP 'epoch')
-FROM unnest(ARRAY[
-    TIMESTAMP '2011-01-02 01:01:59.999999',
-    TIMESTAMP '2011-01-02 01:02:00',
-    TIMESTAMP '2011-01-02 01:06:59.999999',
-    TIMESTAMP '2011-01-02 01:07:00'
-    ]) AS time;
-              time               |         ?column?         
----------------------------------+--------------------------
- Sun Jan 02 01:01:59.999999 2011 | Sun Jan 02 00:57:00 2011
- Sun Jan 02 01:02:00 2011        | Sun Jan 02 01:02:00 2011
- Sun Jan 02 01:06:59.999999 2011 | Sun Jan 02 01:02:00 2011
- Sun Jan 02 01:07:00 2011        | Sun Jan 02 01:07:00 2011
-(4 rows)
-
---rounding version
-SELECT time, time_bucket(INTERVAL '5 minute', time , - INTERVAL '2.5 minutes') + INTERVAL '2 minutes 30 seconds'
-FROM unnest(ARRAY[
-    TIMESTAMP '2011-01-02 01:05:01',
-    TIMESTAMP '2011-01-02 01:07:29',
-    TIMESTAMP '2011-01-02 01:02:30',
-    TIMESTAMP '2011-01-02 01:07:30',
-    TIMESTAMP '2011-01-02 01:02:29'
-    ]) AS time;
-           time           |         ?column?         
---------------------------+--------------------------
- Sun Jan 02 01:05:01 2011 | Sun Jan 02 01:05:00 2011
- Sun Jan 02 01:07:29 2011 | Sun Jan 02 01:05:00 2011
- Sun Jan 02 01:02:30 2011 | Sun Jan 02 01:05:00 2011
- Sun Jan 02 01:07:30 2011 | Sun Jan 02 01:10:00 2011
- Sun Jan 02 01:02:29 2011 | Sun Jan 02 01:00:00 2011
-(5 rows)
-
---time_bucket with timezone should mimick date_trunc
-SET timezone TO 'UTC';
-SELECT time, time_bucket(INTERVAL '1 hour', time), date_trunc('hour', time)
-FROM unnest(ARRAY[
-    TIMESTAMP WITH TIME ZONE '2011-01-02 01:01:01',
-    TIMESTAMP WITH TIME ZONE '2011-01-02 01:01:01+01',
-    TIMESTAMP WITH TIME ZONE '2011-01-02 01:01:01+02'
-    ]) AS time;
-             time             |         time_bucket          |          date_trunc          
-------------------------------+------------------------------+------------------------------
- Sun Jan 02 01:01:01 2011 UTC | Sun Jan 02 01:00:00 2011 UTC | Sun Jan 02 01:00:00 2011 UTC
- Sun Jan 02 00:01:01 2011 UTC | Sun Jan 02 00:00:00 2011 UTC | Sun Jan 02 00:00:00 2011 UTC
- Sat Jan 01 23:01:01 2011 UTC | Sat Jan 01 23:00:00 2011 UTC | Sat Jan 01 23:00:00 2011 UTC
-(3 rows)
-
-SELECT time, time_bucket(INTERVAL '1 day', time), date_trunc('day', time)
-FROM unnest(ARRAY[
-    TIMESTAMP WITH TIME ZONE '2011-01-02 01:01:01',
-    TIMESTAMP WITH TIME ZONE '2011-01-02 01:01:01+01',
-    TIMESTAMP WITH TIME ZONE '2011-01-02 01:01:01+02'
-    ]) AS time;
-             time             |         time_bucket          |          date_trunc          
-------------------------------+------------------------------+------------------------------
- Sun Jan 02 01:01:01 2011 UTC | Sun Jan 02 00:00:00 2011 UTC | Sun Jan 02 00:00:00 2011 UTC
- Sun Jan 02 00:01:01 2011 UTC | Sun Jan 02 00:00:00 2011 UTC | Sun Jan 02 00:00:00 2011 UTC
- Sat Jan 01 23:01:01 2011 UTC | Sat Jan 01 00:00:00 2011 UTC | Sat Jan 01 00:00:00 2011 UTC
-(3 rows)
-
---what happens with a local tz
-SET timezone TO 'America/New_York';
-SELECT time, time_bucket(INTERVAL '1 hour', time), date_trunc('hour', time)
-FROM unnest(ARRAY[
-    TIMESTAMP WITH TIME ZONE '2011-01-02 01:01:01',
-    TIMESTAMP WITH TIME ZONE '2011-01-02 01:01:01+01',
-    TIMESTAMP WITH TIME ZONE '2011-01-02 01:01:01+02'
-    ]) AS time;
-             time             |         time_bucket          |          date_trunc          
-------------------------------+------------------------------+------------------------------
- Sun Jan 02 01:01:01 2011 EST | Sun Jan 02 01:00:00 2011 EST | Sun Jan 02 01:00:00 2011 EST
- Sat Jan 01 19:01:01 2011 EST | Sat Jan 01 19:00:00 2011 EST | Sat Jan 01 19:00:00 2011 EST
- Sat Jan 01 18:01:01 2011 EST | Sat Jan 01 18:00:00 2011 EST | Sat Jan 01 18:00:00 2011 EST
-(3 rows)
-
---Note the timestamp tz input is aligned with UTC day /not/ local day. different than date_trunc.
-SELECT time, time_bucket(INTERVAL '1 day', time), date_trunc('day', time)
-FROM unnest(ARRAY[
-    TIMESTAMP WITH TIME ZONE '2011-01-02 01:01:01',
-    TIMESTAMP WITH TIME ZONE '2011-01-03 01:01:01+01',
-    TIMESTAMP WITH TIME ZONE '2011-01-04 01:01:01+02'
-    ]) AS time;
-             time             |         time_bucket          |          date_trunc          
-------------------------------+------------------------------+------------------------------
- Sun Jan 02 01:01:01 2011 EST | Sat Jan 01 19:00:00 2011 EST | Sun Jan 02 00:00:00 2011 EST
- Sun Jan 02 19:01:01 2011 EST | Sun Jan 02 19:00:00 2011 EST | Sun Jan 02 00:00:00 2011 EST
- Mon Jan 03 18:01:01 2011 EST | Sun Jan 02 19:00:00 2011 EST | Mon Jan 03 00:00:00 2011 EST
-(3 rows)
-
---can force local bucketing with simple cast.
-SELECT time, time_bucket(INTERVAL '1 day', time::timestamp), date_trunc('day', time)
-FROM unnest(ARRAY[
-    TIMESTAMP WITH TIME ZONE '2011-01-02 01:01:01',
-    TIMESTAMP WITH TIME ZONE '2011-01-03 01:01:01+01',
-    TIMESTAMP WITH TIME ZONE '2011-01-04 01:01:01+02'
-    ]) AS time;
-             time             |       time_bucket        |          date_trunc          
-------------------------------+--------------------------+------------------------------
- Sun Jan 02 01:01:01 2011 EST | Sun Jan 02 00:00:00 2011 | Sun Jan 02 00:00:00 2011 EST
- Sun Jan 02 19:01:01 2011 EST | Sun Jan 02 00:00:00 2011 | Sun Jan 02 00:00:00 2011 EST
- Mon Jan 03 18:01:01 2011 EST | Mon Jan 03 00:00:00 2011 | Mon Jan 03 00:00:00 2011 EST
-(3 rows)
-
---can also use interval to correct
-SELECT time, time_bucket(INTERVAL '1 day', time, -INTERVAL '19 hours'), date_trunc('day', time)
-FROM unnest(ARRAY[
-    TIMESTAMP WITH TIME ZONE '2011-01-02 01:01:01',
-    TIMESTAMP WITH TIME ZONE '2011-01-03 01:01:01+01',
-    TIMESTAMP WITH TIME ZONE '2011-01-04 01:01:01+02'
-    ]) AS time;
-             time             |         time_bucket          |          date_trunc          
-------------------------------+------------------------------+------------------------------
- Sun Jan 02 01:01:01 2011 EST | Sun Jan 02 00:00:00 2011 EST | Sun Jan 02 00:00:00 2011 EST
- Sun Jan 02 19:01:01 2011 EST | Sun Jan 02 00:00:00 2011 EST | Sun Jan 02 00:00:00 2011 EST
- Mon Jan 03 18:01:01 2011 EST | Mon Jan 03 00:00:00 2011 EST | Mon Jan 03 00:00:00 2011 EST
-(3 rows)
-
---dst: same local hour bucketed as two different hours.
-SELECT time, time_bucket(INTERVAL '1 hour', time), date_trunc('hour', time)
-FROM unnest(ARRAY[
-    TIMESTAMP WITH TIME ZONE '2017-11-05 12:05:00+07',
-    TIMESTAMP WITH TIME ZONE '2017-11-05 13:05:00+07'
-    ]) AS time;
-             time             |         time_bucket          |          date_trunc          
-------------------------------+------------------------------+------------------------------
- Sun Nov 05 01:05:00 2017 EDT | Sun Nov 05 01:00:00 2017 EDT | Sun Nov 05 01:00:00 2017 EDT
- Sun Nov 05 01:05:00 2017 EST | Sun Nov 05 01:00:00 2017 EST | Sun Nov 05 01:00:00 2017 EST
-(2 rows)
-
---local alignment changes when bucketing by UTC across dst boundary
-SELECT time, time_bucket(INTERVAL '2 hour', time)
-FROM unnest(ARRAY[
-    TIMESTAMP WITH TIME ZONE '2017-11-05 10:05:00+07',
-    TIMESTAMP WITH TIME ZONE '2017-11-05 12:05:00+07',
-    TIMESTAMP WITH TIME ZONE '2017-11-05 13:05:00+07',
-    TIMESTAMP WITH TIME ZONE '2017-11-05 15:05:00+07'
-    ]) AS time;
-             time             |         time_bucket          
-------------------------------+------------------------------
- Sat Nov 04 23:05:00 2017 EDT | Sat Nov 04 22:00:00 2017 EDT
- Sun Nov 05 01:05:00 2017 EDT | Sun Nov 05 00:00:00 2017 EDT
- Sun Nov 05 01:05:00 2017 EST | Sun Nov 05 01:00:00 2017 EST
- Sun Nov 05 03:05:00 2017 EST | Sun Nov 05 03:00:00 2017 EST
-(4 rows)
-
---local alignment is preserved when bucketing by local time across DST boundary.
-SELECT time, time_bucket(INTERVAL '2 hour', time::timestamp)
-FROM unnest(ARRAY[
-    TIMESTAMP WITH TIME ZONE '2017-11-05 10:05:00+07',
-    TIMESTAMP WITH TIME ZONE '2017-11-05 12:05:00+07',
-    TIMESTAMP WITH TIME ZONE '2017-11-05 13:05:00+07',
-    TIMESTAMP WITH TIME ZONE '2017-11-05 15:05:00+07'
-    ]) AS time;
-             time             |       time_bucket        
-------------------------------+--------------------------
- Sat Nov 04 23:05:00 2017 EDT | Sat Nov 04 22:00:00 2017
- Sun Nov 05 01:05:00 2017 EDT | Sun Nov 05 00:00:00 2017
- Sun Nov 05 01:05:00 2017 EST | Sun Nov 05 00:00:00 2017
- Sun Nov 05 03:05:00 2017 EST | Sun Nov 05 02:00:00 2017
-(4 rows)
-
-SELECT time,
-    time_bucket(10::smallint, time) AS time_bucket_smallint,
-    time_bucket(10::int, time) AS time_bucket_int,
-    time_bucket(10::bigint, time) AS time_bucket_bigint
-FROM unnest(ARRAY[
-     '-11',
-     '-10',
-      '-9',
-      '-1',
-       '0',
-       '1',
-      '99',
-     '100',
-     '109',
-     '110'
-    ]::smallint[]) AS time;
- time | time_bucket_smallint | time_bucket_int | time_bucket_bigint 
-------+----------------------+-----------------+--------------------
-  -11 |                  -20 |             -20 |                -20
-  -10 |                  -10 |             -10 |                -10
-   -9 |                  -10 |             -10 |                -10
-   -1 |                  -10 |             -10 |                -10
-    0 |                    0 |               0 |                  0
-    1 |                    0 |               0 |                  0
-   99 |                   90 |              90 |                 90
-  100 |                  100 |             100 |                100
-  109 |                  100 |             100 |                100
-  110 |                  110 |             110 |                110
-(10 rows)
-
-SELECT time,
-    time_bucket(10::smallint, time, 2::smallint) AS time_bucket_smallint,
-    time_bucket(10::int, time, 2::int) AS time_bucket_int,
-    time_bucket(10::bigint, time, 2::bigint) AS time_bucket_bigint
-FROM unnest(ARRAY[
-      '-9',
-      '-8',
-      '-7',
-       '1',
-       '2',
-       '3',
-     '101',
-     '102',
-     '111',
-     '112'
-    ]::smallint[]) AS time;
- time | time_bucket_smallint | time_bucket_int | time_bucket_bigint 
-------+----------------------+-----------------+--------------------
-   -9 |                  -18 |             -18 |                -18
-   -8 |                   -8 |              -8 |                 -8
-   -7 |                   -8 |              -8 |                 -8
-    1 |                   -8 |              -8 |                 -8
-    2 |                    2 |               2 |                  2
-    3 |                    2 |               2 |                  2
-  101 |                   92 |              92 |                 92
-  102 |                  102 |             102 |                102
-  111 |                  102 |             102 |                102
-  112 |                  112 |             112 |                112
-(10 rows)
-
-SELECT time,
-    time_bucket(10::smallint, time, -2::smallint) AS time_bucket_smallint,
-    time_bucket(10::int, time, -2::int) AS time_bucket_int,
-    time_bucket(10::bigint, time, -2::bigint) AS time_bucket_bigint
-FROM unnest(ARRAY[
-    '-13',
-    '-12',
-    '-11',
-     '-3',
-     '-2',
-     '-1',
-     '97',
-     '98',
-    '107',
-    '108'
-    ]::smallint[]) AS time;
- time | time_bucket_smallint | time_bucket_int | time_bucket_bigint 
-------+----------------------+-----------------+--------------------
-  -13 |                  -22 |             -22 |                -22
-  -12 |                  -12 |             -12 |                -12
-  -11 |                  -12 |             -12 |                -12
-   -3 |                  -12 |             -12 |                -12
-   -2 |                   -2 |              -2 |                 -2
-   -1 |                   -2 |              -2 |                 -2
-   97 |                   88 |              88 |                 88
-   98 |                   98 |              98 |                 98
-  107 |                   98 |              98 |                 98
-  108 |                  108 |             108 |                108
-(10 rows)
-
-\set ON_ERROR_STOP 0
-SELECT time_bucket(10::smallint, '-32768'::smallint);
-ERROR:  timestamp out of range
-SELECT time_bucket(10::smallint, '-32761'::smallint);
-ERROR:  timestamp out of range
-select time_bucket(10::smallint, '-32768'::smallint, 1000::smallint);
-ERROR:  timestamp out of range
-select time_bucket(10::smallint, '-32768'::smallint, '32767'::smallint);
-ERROR:  timestamp out of range
-select time_bucket(10::smallint, '32767'::smallint, '-32768'::smallint);
-ERROR:  timestamp out of range
-\set ON_ERROR_STOP 1
-SELECT time, time_bucket(10::smallint, time)
-FROM unnest(ARRAY[
-    '-32760',
-    '-32759',
-    '32767'
-    ]::smallint[]) AS time;
-  time  | time_bucket 
---------+-------------
- -32760 |      -32760
- -32759 |      -32760
-  32767 |       32760
-(3 rows)
-
-\set ON_ERROR_STOP 0
-SELECT time_bucket(10::int, '-2147483648'::int);
-ERROR:  timestamp out of range
-SELECT time_bucket(10::int, '-2147483641'::int);
-ERROR:  timestamp out of range
-SELECT time_bucket(1000::int, '-2147483000'::int, 1::int);
-ERROR:  timestamp out of range
-SELECT time_bucket(1000::int, '-2147483648'::int, '2147483647'::int);
-ERROR:  timestamp out of range
-SELECT time_bucket(1000::int, '2147483647'::int, '-2147483648'::int);
-ERROR:  timestamp out of range
-\set ON_ERROR_STOP 1
-SELECT time, time_bucket(10::int, time)
-FROM unnest(ARRAY[
-    '-2147483640',
-    '-2147483639',
-    '2147483647'
-    ]::int[]) AS time;
-    time     | time_bucket 
--------------+-------------
- -2147483640 | -2147483640
- -2147483639 | -2147483640
-  2147483647 |  2147483640
-(3 rows)
-
-\set ON_ERROR_STOP 0
-SELECT time_bucket(10::bigint, '-9223372036854775808'::bigint);
-ERROR:  timestamp out of range
-SELECT time_bucket(10::bigint, '-9223372036854775801'::bigint);
-ERROR:  timestamp out of range
-SELECT time_bucket(1000::bigint, '-9223372036854775000'::bigint, 1::bigint);
-ERROR:  timestamp out of range
-SELECT time_bucket(1000::bigint, '-9223372036854775808'::bigint, '9223372036854775807'::bigint);
-ERROR:  timestamp out of range
-SELECT time_bucket(1000::bigint, '9223372036854775807'::bigint, '-9223372036854775808'::bigint);
-ERROR:  timestamp out of range
-\set ON_ERROR_STOP 1
-SELECT time, time_bucket(10::bigint, time)
-FROM unnest(ARRAY[
-    '-9223372036854775800',
-    '-9223372036854775799',
-    '9223372036854775807'
-    ]::bigint[]) AS time;
-         time         |     time_bucket      
-----------------------+----------------------
- -9223372036854775800 | -9223372036854775800
- -9223372036854775799 | -9223372036854775800
-  9223372036854775807 |  9223372036854775800
-(3 rows)
-
-SELECT time, time_bucket(INTERVAL '1 day', time::date)
-FROM unnest(ARRAY[
-    date '2017-11-05',
-    date '2017-11-06'
-    ]) AS time;
-    time    | time_bucket 
-------------+-------------
- 11-05-2017 | 11-05-2017
- 11-06-2017 | 11-06-2017
-(2 rows)
-
-SELECT time, time_bucket(INTERVAL '4 day', time::date)
-FROM unnest(ARRAY[
-    date '2017-11-04',
-    date '2017-11-05',
-    date '2017-11-08',
-    date '2017-11-09'
-    ]) AS time;
-    time    | time_bucket 
-------------+-------------
- 11-04-2017 | 11-01-2017
- 11-05-2017 | 11-05-2017
- 11-08-2017 | 11-05-2017
- 11-09-2017 | 11-09-2017
-(4 rows)
-
-SELECT time, time_bucket(INTERVAL '4 day', time::date, INTERVAL '2 day')
-FROM unnest(ARRAY[
-    date '2017-11-06',
-    date '2017-11-07',
-    date '2017-11-10',
-    date '2017-11-11'
-    ]) AS time;
-    time    | time_bucket 
-------------+-------------
- 11-06-2017 | 11-03-2017
- 11-07-2017 | 11-07-2017
- 11-10-2017 | 11-07-2017
- 11-11-2017 | 11-11-2017
-(4 rows)
-
--- 2019-09-24 is a Monday, and we want to ensure that time_bucket returns the week starting with a Monday as date_trunc does,
--- Rather than a Saturday which is the date of the PostgreSQL epoch
-SELECT time, time_bucket(INTERVAL '1 week', time::date)
-FROM unnest(ARRAY[
-    date '2018-09-16',
-    date '2018-09-17',
-    date '2018-09-23',
-    date '2018-09-24'
-    ]) AS time;
-    time    | time_bucket 
-------------+-------------
- 09-16-2018 | 09-10-2018
- 09-17-2018 | 09-17-2018
- 09-23-2018 | 09-17-2018
- 09-24-2018 | 09-24-2018
-(4 rows)
-
-SELECT time, time_bucket(INTERVAL '1 week', time)
-FROM unnest(ARRAY[
-    timestamp without time zone '2018-09-16',
-    timestamp without time zone '2018-09-17',
-    timestamp without time zone '2018-09-23',
-    timestamp without time zone '2018-09-24'
-    ]) AS time;
-           time           |       time_bucket        
---------------------------+--------------------------
- Sun Sep 16 00:00:00 2018 | Mon Sep 10 00:00:00 2018
- Mon Sep 17 00:00:00 2018 | Mon Sep 17 00:00:00 2018
- Sun Sep 23 00:00:00 2018 | Mon Sep 17 00:00:00 2018
- Mon Sep 24 00:00:00 2018 | Mon Sep 24 00:00:00 2018
-(4 rows)
-
-SELECT time, time_bucket(INTERVAL '1 week', time)
-FROM unnest(ARRAY[
-    timestamp with time zone '2018-09-16',
-    timestamp with time zone '2018-09-17',
-    timestamp with time zone '2018-09-23',
-    timestamp with time zone '2018-09-24'
-    ]) AS time;
-             time             |         time_bucket          
-------------------------------+------------------------------
- Sun Sep 16 00:00:00 2018 EDT | Sun Sep 09 20:00:00 2018 EDT
- Mon Sep 17 00:00:00 2018 EDT | Sun Sep 16 20:00:00 2018 EDT
- Sun Sep 23 00:00:00 2018 EDT | Sun Sep 16 20:00:00 2018 EDT
- Mon Sep 24 00:00:00 2018 EDT | Sun Sep 23 20:00:00 2018 EDT
-(4 rows)
-
-SELECT time, time_bucket(INTERVAL '1 week', time)
-FROM unnest(ARRAY[
-    timestamp with time zone '-Infinity',
-    timestamp with time zone 'Infinity'
-    ]) AS time;
-   time    | time_bucket 
------------+-------------
- -infinity | -infinity
- infinity  | infinity
-(2 rows)
-
-SELECT time, time_bucket(INTERVAL '1 week', time)
-FROM unnest(ARRAY[
-    timestamp without time zone '-Infinity',
-    timestamp without time zone 'Infinity'
-    ]) AS time;
-   time    | time_bucket 
------------+-------------
- -infinity | -infinity
- infinity  | infinity
-(2 rows)
-
-SELECT time, time_bucket(INTERVAL '1 week', time), date_trunc('week', time) = time_bucket(INTERVAL '1 week', time)
-FROM unnest(ARRAY[
-    timestamp without time zone '4714-11-24 01:01:01.0 BC',
-    timestamp without time zone '294276-12-31 23:59:59.9999'
-    ]) AS time;
-              time               |         time_bucket         | ?column? 
----------------------------------+-----------------------------+----------
- Mon Nov 24 01:01:01 4714 BC     | Mon Nov 24 00:00:00 4714 BC | t
- Sun Dec 31 23:59:59.9999 294276 | Mon Dec 25 00:00:00 294276  | t
-(2 rows)
-
---1000 years later weeks still align.
-SELECT time, time_bucket(INTERVAL '1 week', time), date_trunc('week', time) = time_bucket(INTERVAL '1 week', time)
-FROM unnest(ARRAY[
-    timestamp without time zone '3018-09-14',
-    timestamp without time zone '3018-09-20',
-    timestamp without time zone '3018-09-21',
-    timestamp without time zone '3018-09-22'
-    ]) AS time;
-           time           |       time_bucket        | ?column? 
---------------------------+--------------------------+----------
- Mon Sep 14 00:00:00 3018 | Mon Sep 14 00:00:00 3018 | t
- Sun Sep 20 00:00:00 3018 | Mon Sep 14 00:00:00 3018 | t
- Mon Sep 21 00:00:00 3018 | Mon Sep 21 00:00:00 3018 | t
- Tue Sep 22 00:00:00 3018 | Mon Sep 21 00:00:00 3018 | t
-(4 rows)
-
---weeks align for timestamptz as well if cast to local time, (but not if done at UTC).
-SELECT time, date_trunc('week', time) = time_bucket(INTERVAL '1 week', time),  date_trunc('week', time) = time_bucket(INTERVAL '1 week', time::timestamp)
-FROM unnest(ARRAY[
-    timestamp with time zone '3018-09-14',
-    timestamp with time zone '3018-09-20',
-    timestamp with time zone '3018-09-21',
-    timestamp with time zone '3018-09-22'
-    ]) AS time;
-             time             | ?column? | ?column? 
-------------------------------+----------+----------
- Mon Sep 14 00:00:00 3018 EDT | f        | t
- Sun Sep 20 00:00:00 3018 EDT | f        | t
- Mon Sep 21 00:00:00 3018 EDT | f        | t
- Tue Sep 22 00:00:00 3018 EDT | f        | t
-(4 rows)
-
---check functions with origin
---note that the default origin is at 0 UTC, using origin parameter it is easy to provide a EDT origin point
-\x
-SELECT time, time_bucket(INTERVAL '1 week', time) no_epoch,
-             time_bucket(INTERVAL '1 week', time::timestamp) no_epoch_local,
-             time_bucket(INTERVAL '1 week', time) = time_bucket(INTERVAL '1 week', time, timestamptz '2000-01-03 00:00:00+0') always_true,
-             time_bucket(INTERVAL '1 week', time, timestamptz '2000-01-01 00:00:00+0') pg_epoch,
-             time_bucket(INTERVAL '1 week', time, timestamptz 'epoch') unix_epoch,
-             time_bucket(INTERVAL '1 week', time, timestamptz '3018-09-13') custom_1,
-             time_bucket(INTERVAL '1 week', time, timestamptz '3018-09-14') custom_2
-FROM unnest(ARRAY[
-    timestamp with time zone '2000-01-01 00:00:00+0'- interval '1 second',
-    timestamp with time zone '2000-01-01 00:00:00+0',
-    timestamp with time zone '2000-01-03 00:00:00+0'- interval '1 second',
-    timestamp with time zone '2000-01-03 00:00:00+0',
-    timestamp with time zone '2000-01-01',
-    timestamp with time zone '2000-01-02',
-    timestamp with time zone '2000-01-03',
-    timestamp with time zone '3018-09-12',
-    timestamp with time zone '3018-09-13',
-    timestamp with time zone '3018-09-14',
-    timestamp with time zone '3018-09-15'
-    ]) AS time;
--[ RECORD 1 ]--+-----------------------------
-time           | Fri Dec 31 18:59:59 1999 EST
-no_epoch       | Sun Dec 26 19:00:00 1999 EST
-no_epoch_local | Mon Dec 27 00:00:00 1999
-always_true    | t
-pg_epoch       | Fri Dec 24 19:00:00 1999 EST
-unix_epoch     | Wed Dec 29 19:00:00 1999 EST
-custom_1       | Sat Dec 25 23:00:00 1999 EST
-custom_2       | Sun Dec 26 23:00:00 1999 EST
--[ RECORD 2 ]--+-----------------------------
-time           | Fri Dec 31 19:00:00 1999 EST
-no_epoch       | Sun Dec 26 19:00:00 1999 EST
-no_epoch_local | Mon Dec 27 00:00:00 1999
-always_true    | t
-pg_epoch       | Fri Dec 31 19:00:00 1999 EST
-unix_epoch     | Wed Dec 29 19:00:00 1999 EST
-custom_1       | Sat Dec 25 23:00:00 1999 EST
-custom_2       | Sun Dec 26 23:00:00 1999 EST
--[ RECORD 3 ]--+-----------------------------
-time           | Sun Jan 02 18:59:59 2000 EST
-no_epoch       | Sun Dec 26 19:00:00 1999 EST
-no_epoch_local | Mon Dec 27 00:00:00 1999
-always_true    | t
-pg_epoch       | Fri Dec 31 19:00:00 1999 EST
-unix_epoch     | Wed Dec 29 19:00:00 1999 EST
-custom_1       | Sat Jan 01 23:00:00 2000 EST
-custom_2       | Sun Dec 26 23:00:00 1999 EST
--[ RECORD 4 ]--+-----------------------------
-time           | Sun Jan 02 19:00:00 2000 EST
-no_epoch       | Sun Jan 02 19:00:00 2000 EST
-no_epoch_local | Mon Dec 27 00:00:00 1999
-always_true    | t
-pg_epoch       | Fri Dec 31 19:00:00 1999 EST
-unix_epoch     | Wed Dec 29 19:00:00 1999 EST
-custom_1       | Sat Jan 01 23:00:00 2000 EST
-custom_2       | Sun Dec 26 23:00:00 1999 EST
--[ RECORD 5 ]--+-----------------------------
-time           | Sat Jan 01 00:00:00 2000 EST
-no_epoch       | Sun Dec 26 19:00:00 1999 EST
-no_epoch_local | Mon Dec 27 00:00:00 1999
-always_true    | t
-pg_epoch       | Fri Dec 31 19:00:00 1999 EST
-unix_epoch     | Wed Dec 29 19:00:00 1999 EST
-custom_1       | Sat Dec 25 23:00:00 1999 EST
-custom_2       | Sun Dec 26 23:00:00 1999 EST
--[ RECORD 6 ]--+-----------------------------
-time           | Sun Jan 02 00:00:00 2000 EST
-no_epoch       | Sun Dec 26 19:00:00 1999 EST
-no_epoch_local | Mon Dec 27 00:00:00 1999
-always_true    | t
-pg_epoch       | Fri Dec 31 19:00:00 1999 EST
-unix_epoch     | Wed Dec 29 19:00:00 1999 EST
-custom_1       | Sat Jan 01 23:00:00 2000 EST
-custom_2       | Sun Dec 26 23:00:00 1999 EST
--[ RECORD 7 ]--+-----------------------------
-time           | Mon Jan 03 00:00:00 2000 EST
-no_epoch       | Sun Jan 02 19:00:00 2000 EST
-no_epoch_local | Mon Jan 03 00:00:00 2000
-always_true    | t
-pg_epoch       | Fri Dec 31 19:00:00 1999 EST
-unix_epoch     | Wed Dec 29 19:00:00 1999 EST
-custom_1       | Sat Jan 01 23:00:00 2000 EST
-custom_2       | Sun Jan 02 23:00:00 2000 EST
--[ RECORD 8 ]--+-----------------------------
-time           | Sat Sep 12 00:00:00 3018 EDT
-no_epoch       | Sun Sep 06 20:00:00 3018 EDT
-no_epoch_local | Mon Sep 07 00:00:00 3018
-always_true    | t
-pg_epoch       | Fri Sep 11 20:00:00 3018 EDT
-unix_epoch     | Wed Sep 09 20:00:00 3018 EDT
-custom_1       | Sun Sep 06 00:00:00 3018 EDT
-custom_2       | Mon Sep 07 00:00:00 3018 EDT
--[ RECORD 9 ]--+-----------------------------
-time           | Sun Sep 13 00:00:00 3018 EDT
-no_epoch       | Sun Sep 06 20:00:00 3018 EDT
-no_epoch_local | Mon Sep 07 00:00:00 3018
-always_true    | t
-pg_epoch       | Fri Sep 11 20:00:00 3018 EDT
-unix_epoch     | Wed Sep 09 20:00:00 3018 EDT
-custom_1       | Sun Sep 13 00:00:00 3018 EDT
-custom_2       | Mon Sep 07 00:00:00 3018 EDT
--[ RECORD 10 ]-+-----------------------------
-time           | Mon Sep 14 00:00:00 3018 EDT
-no_epoch       | Sun Sep 13 20:00:00 3018 EDT
-no_epoch_local | Mon Sep 14 00:00:00 3018
-always_true    | t
-pg_epoch       | Fri Sep 11 20:00:00 3018 EDT
-unix_epoch     | Wed Sep 09 20:00:00 3018 EDT
-custom_1       | Sun Sep 13 00:00:00 3018 EDT
-custom_2       | Mon Sep 14 00:00:00 3018 EDT
--[ RECORD 11 ]-+-----------------------------
-time           | Tue Sep 15 00:00:00 3018 EDT
-no_epoch       | Sun Sep 13 20:00:00 3018 EDT
-no_epoch_local | Mon Sep 14 00:00:00 3018
-always_true    | t
-pg_epoch       | Fri Sep 11 20:00:00 3018 EDT
-unix_epoch     | Wed Sep 09 20:00:00 3018 EDT
-custom_1       | Sun Sep 13 00:00:00 3018 EDT
-custom_2       | Mon Sep 14 00:00:00 3018 EDT
-
-SELECT time, time_bucket(INTERVAL '1 week', time) no_epoch,
-             time_bucket(INTERVAL '1 week', time) = time_bucket(INTERVAL '1 week', time, timestamp '2000-01-03 00:00:00') always_true,
-             time_bucket(INTERVAL '1 week', time, timestamp '2000-01-01 00:00:00+0') pg_epoch,
-             time_bucket(INTERVAL '1 week', time, timestamp 'epoch') unix_epoch,
-             time_bucket(INTERVAL '1 week', time, timestamp '3018-09-13') custom_1,
-             time_bucket(INTERVAL '1 week', time, timestamp '3018-09-14') custom_2
-FROM unnest(ARRAY[
-    timestamp without time zone '2000-01-01 00:00:00'- interval '1 second',
-    timestamp without time zone '2000-01-01 00:00:00',
-    timestamp without time zone '2000-01-03 00:00:00'- interval '1 second',
-    timestamp without time zone '2000-01-03 00:00:00',
-    timestamp without time zone '2000-01-01',
-    timestamp without time zone '2000-01-02',
-    timestamp without time zone '2000-01-03',
-    timestamp without time zone '3018-09-12',
-    timestamp without time zone '3018-09-13',
-    timestamp without time zone '3018-09-14',
-    timestamp without time zone '3018-09-15'
-    ]) AS time;
--[ RECORD 1 ]-------------------------
-time        | Fri Dec 31 23:59:59 1999
-no_epoch    | Mon Dec 27 00:00:00 1999
-always_true | t
-pg_epoch    | Sat Dec 25 00:00:00 1999
-unix_epoch  | Thu Dec 30 00:00:00 1999
-custom_1    | Sun Dec 26 00:00:00 1999
-custom_2    | Mon Dec 27 00:00:00 1999
--[ RECORD 2 ]-------------------------
-time        | Sat Jan 01 00:00:00 2000
-no_epoch    | Mon Dec 27 00:00:00 1999
-always_true | t
-pg_epoch    | Sat Jan 01 00:00:00 2000
-unix_epoch  | Thu Dec 30 00:00:00 1999
-custom_1    | Sun Dec 26 00:00:00 1999
-custom_2    | Mon Dec 27 00:00:00 1999
--[ RECORD 3 ]-------------------------
-time        | Sun Jan 02 23:59:59 2000
-no_epoch    | Mon Dec 27 00:00:00 1999
-always_true | t
-pg_epoch    | Sat Jan 01 00:00:00 2000
-unix_epoch  | Thu Dec 30 00:00:00 1999
-custom_1    | Sun Jan 02 00:00:00 2000
-custom_2    | Mon Dec 27 00:00:00 1999
--[ RECORD 4 ]-------------------------
-time        | Mon Jan 03 00:00:00 2000
-no_epoch    | Mon Jan 03 00:00:00 2000
-always_true | t
-pg_epoch    | Sat Jan 01 00:00:00 2000
-unix_epoch  | Thu Dec 30 00:00:00 1999
-custom_1    | Sun Jan 02 00:00:00 2000
-custom_2    | Mon Jan 03 00:00:00 2000
--[ RECORD 5 ]-------------------------
-time        | Sat Jan 01 00:00:00 2000
-no_epoch    | Mon Dec 27 00:00:00 1999
-always_true | t
-pg_epoch    | Sat Jan 01 00:00:00 2000
-unix_epoch  | Thu Dec 30 00:00:00 1999
-custom_1    | Sun Dec 26 00:00:00 1999
-custom_2    | Mon Dec 27 00:00:00 1999
--[ RECORD 6 ]-------------------------
-time        | Sun Jan 02 00:00:00 2000
-no_epoch    | Mon Dec 27 00:00:00 1999
-always_true | t
-pg_epoch    | Sat Jan 01 00:00:00 2000
-unix_epoch  | Thu Dec 30 00:00:00 1999
-custom_1    | Sun Jan 02 00:00:00 2000
-custom_2    | Mon Dec 27 00:00:00 1999
--[ RECORD 7 ]-------------------------
-time        | Mon Jan 03 00:00:00 2000
-no_epoch    | Mon Jan 03 00:00:00 2000
-always_true | t
-pg_epoch    | Sat Jan 01 00:00:00 2000
-unix_epoch  | Thu Dec 30 00:00:00 1999
-custom_1    | Sun Jan 02 00:00:00 2000
-custom_2    | Mon Jan 03 00:00:00 2000
--[ RECORD 8 ]-------------------------
-time        | Sat Sep 12 00:00:00 3018
-no_epoch    | Mon Sep 07 00:00:00 3018
-always_true | t
-pg_epoch    | Sat Sep 12 00:00:00 3018
-unix_epoch  | Thu Sep 10 00:00:00 3018
-custom_1    | Sun Sep 06 00:00:00 3018
-custom_2    | Mon Sep 07 00:00:00 3018
--[ RECORD 9 ]-------------------------
-time        | Sun Sep 13 00:00:00 3018
-no_epoch    | Mon Sep 07 00:00:00 3018
-always_true | t
-pg_epoch    | Sat Sep 12 00:00:00 3018
-unix_epoch  | Thu Sep 10 00:00:00 3018
-custom_1    | Sun Sep 13 00:00:00 3018
-custom_2    | Mon Sep 07 00:00:00 3018
--[ RECORD 10 ]------------------------
-time        | Mon Sep 14 00:00:00 3018
-no_epoch    | Mon Sep 14 00:00:00 3018
-always_true | t
-pg_epoch    | Sat Sep 12 00:00:00 3018
-unix_epoch  | Thu Sep 10 00:00:00 3018
-custom_1    | Sun Sep 13 00:00:00 3018
-custom_2    | Mon Sep 14 00:00:00 3018
--[ RECORD 11 ]------------------------
-time        | Tue Sep 15 00:00:00 3018
-no_epoch    | Mon Sep 14 00:00:00 3018
-always_true | t
-pg_epoch    | Sat Sep 12 00:00:00 3018
-unix_epoch  | Thu Sep 10 00:00:00 3018
-custom_1    | Sun Sep 13 00:00:00 3018
-custom_2    | Mon Sep 14 00:00:00 3018
-
-SELECT time, time_bucket(INTERVAL '1 week', time) no_epoch,
-             time_bucket(INTERVAL '1 week', time) = time_bucket(INTERVAL '1 week', time, date '2000-01-03') always_true,
-             time_bucket(INTERVAL '1 week', time, date '2000-01-01') pg_epoch,
-             time_bucket(INTERVAL '1 week', time, (timestamp 'epoch')::date) unix_epoch,
-             time_bucket(INTERVAL '1 week', time, date '3018-09-13') custom_1,
-             time_bucket(INTERVAL '1 week', time, date '3018-09-14') custom_2
-FROM unnest(ARRAY[
-    date '1999-12-31',
-    date '2000-01-01',
-    date '2000-01-02',
-    date '2000-01-03',
-    date '3018-09-12',
-    date '3018-09-13',
-    date '3018-09-14',
-    date '3018-09-15'
-    ]) AS time;
--[ RECORD 1 ]-----------
-time        | 12-31-1999
-no_epoch    | 12-27-1999
-always_true | t
-pg_epoch    | 12-25-1999
-unix_epoch  | 12-30-1999
-custom_1    | 12-26-1999
-custom_2    | 12-27-1999
--[ RECORD 2 ]-----------
-time        | 01-01-2000
-no_epoch    | 12-27-1999
-always_true | t
-pg_epoch    | 01-01-2000
-unix_epoch  | 12-30-1999
-custom_1    | 12-26-1999
-custom_2    | 12-27-1999
--[ RECORD 3 ]-----------
-time        | 01-02-2000
-no_epoch    | 12-27-1999
-always_true | t
-pg_epoch    | 01-01-2000
-unix_epoch  | 12-30-1999
-custom_1    | 01-02-2000
-custom_2    | 12-27-1999
--[ RECORD 4 ]-----------
-time        | 01-03-2000
-no_epoch    | 01-03-2000
-always_true | t
-pg_epoch    | 01-01-2000
-unix_epoch  | 12-30-1999
-custom_1    | 01-02-2000
-custom_2    | 01-03-2000
--[ RECORD 5 ]-----------
-time        | 09-12-3018
-no_epoch    | 09-07-3018
-always_true | t
-pg_epoch    | 09-12-3018
-unix_epoch  | 09-10-3018
-custom_1    | 09-06-3018
-custom_2    | 09-07-3018
--[ RECORD 6 ]-----------
-time        | 09-13-3018
-no_epoch    | 09-07-3018
-always_true | t
-pg_epoch    | 09-12-3018
-unix_epoch  | 09-10-3018
-custom_1    | 09-13-3018
-custom_2    | 09-07-3018
--[ RECORD 7 ]-----------
-time        | 09-14-3018
-no_epoch    | 09-14-3018
-always_true | t
-pg_epoch    | 09-12-3018
-unix_epoch  | 09-10-3018
-custom_1    | 09-13-3018
-custom_2    | 09-14-3018
--[ RECORD 8 ]-----------
-time        | 09-15-3018
-no_epoch    | 09-14-3018
-always_true | t
-pg_epoch    | 09-12-3018
-unix_epoch  | 09-10-3018
-custom_1    | 09-13-3018
-custom_2    | 09-14-3018
-
-\x
---really old origin works if date around that time
-SELECT time, time_bucket(INTERVAL '1 week', time, timestamp without time zone '4710-11-24 01:01:01.0 BC')
-FROM unnest(ARRAY[
-    timestamp without time zone '4710-11-24 01:01:01.0 BC',
-    timestamp without time zone '4710-11-25 01:01:01.0 BC',
-    timestamp without time zone '2001-01-01',
-    timestamp without time zone '3001-01-01'
-    ]) AS time;
-            time             |         time_bucket         
------------------------------+-----------------------------
- Sat Nov 24 01:01:01 4710 BC | Sat Nov 24 01:01:01 4710 BC
- Sun Nov 25 01:01:01 4710 BC | Sat Nov 24 01:01:01 4710 BC
- Mon Jan 01 00:00:00 2001    | Sat Dec 30 01:01:01 2000
- Thu Jan 01 00:00:00 3001    | Sat Dec 27 01:01:01 3000
-(4 rows)
-
-SELECT time, time_bucket(INTERVAL '1 week', time, timestamp without time zone '294270-12-30 23:59:59.9999')
-FROM unnest(ARRAY[
-    timestamp without time zone '294270-12-29 23:59:59.9999',
-    timestamp without time zone '294270-12-30 23:59:59.9999',
-    timestamp without time zone '294270-12-31 23:59:59.9999',
-    timestamp without time zone '2001-01-01',
-    timestamp without time zone '3001-01-01'
-    ]) AS time;
-              time               |           time_bucket           
----------------------------------+---------------------------------
- Thu Dec 29 23:59:59.9999 294270 | Fri Dec 23 23:59:59.9999 294270
- Fri Dec 30 23:59:59.9999 294270 | Fri Dec 30 23:59:59.9999 294270
- Sat Dec 31 23:59:59.9999 294270 | Fri Dec 30 23:59:59.9999 294270
- Mon Jan 01 00:00:00 2001        | Fri Dec 29 23:59:59.9999 2000
- Thu Jan 01 00:00:00 3001        | Fri Dec 26 23:59:59.9999 3000
-(5 rows)
-
-\set ON_ERROR_STOP 0
---really old origin + very new data + long period errors
-SELECT time, time_bucket(INTERVAL '100000 day', time, timestamp without time zone '4710-11-24 01:01:01.0 BC')
-FROM unnest(ARRAY[
-    timestamp without time zone '294270-12-31 23:59:59.9999'
-    ]) AS time;
-ERROR:  timestamp out of range
-SELECT time, time_bucket(INTERVAL '100000 day', time, timestamp with time zone '4710-11-25 01:01:01.0 BC')
-FROM unnest(ARRAY[
-    timestamp with time zone '294270-12-30 23:59:59.9999'
-    ]) AS time;
-ERROR:  timestamp out of range
---really high origin + old data + long period errors out
-SELECT time, time_bucket(INTERVAL '10000000 day', time, timestamp without time zone '294270-12-31 23:59:59.9999')
-FROM unnest(ARRAY[
-    timestamp without time zone '4710-11-24 01:01:01.0 BC'
-    ]) AS time;
-ERROR:  timestamp out of range
-SELECT time, time_bucket(INTERVAL '10000000 day', time, timestamp with time zone '294270-12-31 23:59:59.9999')
-FROM unnest(ARRAY[
-    timestamp with time zone '4710-11-24 01:01:01.0 BC'
-    ]) AS time;
-ERROR:  timestamp out of range
-\set ON_ERROR_STOP 1
--------------------------------------------
---- Test time_bucket with month periods ---
--------------------------------------------
-SET datestyle TO ISO;
-SELECT
-  time::date,
-  time_bucket('1 month', time::date) AS "1m",
-  time_bucket('2 month', time::date) AS "2m",
-  time_bucket('3 month', time::date) AS "3m",
-  time_bucket('1 month', time::date, '2000-02-01'::date) AS "1m origin",
-  time_bucket('2 month', time::date, '2000-02-01'::date) AS "2m origin",
-  time_bucket('3 month', time::date, '2000-02-01'::date) AS "3m origin"
-FROM generate_series('1990-01-03'::date,'1990-06-03'::date,'1month'::interval) time;
-    time    |     1m     |     2m     |     3m     | 1m origin  | 2m origin  | 3m origin  
-------------+------------+------------+------------+------------+------------+------------
- 1990-01-03 | 1990-01-01 | 1990-01-01 | 1990-01-01 | 1990-01-01 | 1989-12-01 | 1989-11-01
- 1990-02-03 | 1990-02-01 | 1990-01-01 | 1990-01-01 | 1990-02-01 | 1990-02-01 | 1990-02-01
- 1990-03-03 | 1990-03-01 | 1990-03-01 | 1990-01-01 | 1990-03-01 | 1990-02-01 | 1990-02-01
- 1990-04-03 | 1990-04-01 | 1990-03-01 | 1990-04-01 | 1990-04-01 | 1990-04-01 | 1990-02-01
- 1990-05-03 | 1990-05-01 | 1990-05-01 | 1990-04-01 | 1990-05-01 | 1990-04-01 | 1990-05-01
- 1990-06-03 | 1990-06-01 | 1990-05-01 | 1990-04-01 | 1990-06-01 | 1990-06-01 | 1990-05-01
-(6 rows)
-
-SELECT
-  time,
-  time_bucket('1 month', time) AS "1m",
-  time_bucket('2 month', time) AS "2m",
-  time_bucket('3 month', time) AS "3m",
-  time_bucket('1 month', time, '2000-02-01'::timestamp) AS "1m origin",
-  time_bucket('2 month', time, '2000-02-01'::timestamp) AS "2m origin",
-  time_bucket('3 month', time, '2000-02-01'::timestamp) AS "3m origin"
-FROM generate_series('1990-01-03'::timestamp,'1990-06-03'::timestamp,'1month'::interval) time;
-        time         |         1m          |         2m          |         3m          |      1m origin      |      2m origin      |      3m origin      
----------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------
- 1990-01-03 00:00:00 | 1990-01-01 00:00:00 | 1990-01-01 00:00:00 | 1990-01-01 00:00:00 | 1990-01-01 00:00:00 | 1989-12-01 00:00:00 | 1989-11-01 00:00:00
- 1990-02-03 00:00:00 | 1990-02-01 00:00:00 | 1990-01-01 00:00:00 | 1990-01-01 00:00:00 | 1990-02-01 00:00:00 | 1990-02-01 00:00:00 | 1990-02-01 00:00:00
- 1990-03-03 00:00:00 | 1990-03-01 00:00:00 | 1990-03-01 00:00:00 | 1990-01-01 00:00:00 | 1990-03-01 00:00:00 | 1990-02-01 00:00:00 | 1990-02-01 00:00:00
- 1990-04-03 00:00:00 | 1990-04-01 00:00:00 | 1990-03-01 00:00:00 | 1990-04-01 00:00:00 | 1990-04-01 00:00:00 | 1990-04-01 00:00:00 | 1990-02-01 00:00:00
- 1990-05-03 00:00:00 | 1990-05-01 00:00:00 | 1990-05-01 00:00:00 | 1990-04-01 00:00:00 | 1990-05-01 00:00:00 | 1990-04-01 00:00:00 | 1990-05-01 00:00:00
- 1990-06-03 00:00:00 | 1990-06-01 00:00:00 | 1990-05-01 00:00:00 | 1990-04-01 00:00:00 | 1990-06-01 00:00:00 | 1990-06-01 00:00:00 | 1990-05-01 00:00:00
-(6 rows)
-
-SELECT
-  time,
-  time_bucket('1 month', time) AS "1m",
-  time_bucket('2 month', time) AS "2m",
-  time_bucket('3 month', time) AS "3m",
-  time_bucket('1 month', time, '2000-02-01'::timestamptz) AS "1m origin",
-  time_bucket('2 month', time, '2000-02-01'::timestamptz) AS "2m origin",
-  time_bucket('3 month', time, '2000-02-01'::timestamptz) AS "3m origin"
-FROM generate_series('1990-01-03'::timestamptz,'1990-06-03'::timestamptz,'1month'::interval) time;
-          time          |           1m           |           2m           |           3m           |       1m origin        |       2m origin        |       3m origin        
-------------------------+------------------------+------------------------+------------------------+------------------------+------------------------+------------------------
- 1990-01-03 00:00:00-05 | 1989-12-31 19:00:00-05 | 1989-12-31 19:00:00-05 | 1989-12-31 19:00:00-05 | 1989-12-31 19:00:00-05 | 1989-11-30 19:00:00-05 | 1989-10-31 19:00:00-05
- 1990-02-03 00:00:00-05 | 1990-01-31 19:00:00-05 | 1989-12-31 19:00:00-05 | 1989-12-31 19:00:00-05 | 1990-01-31 19:00:00-05 | 1990-01-31 19:00:00-05 | 1990-01-31 19:00:00-05
- 1990-03-03 00:00:00-05 | 1990-02-28 19:00:00-05 | 1990-02-28 19:00:00-05 | 1989-12-31 19:00:00-05 | 1990-02-28 19:00:00-05 | 1990-01-31 19:00:00-05 | 1990-01-31 19:00:00-05
- 1990-04-03 00:00:00-04 | 1990-03-31 19:00:00-05 | 1990-02-28 19:00:00-05 | 1990-03-31 19:00:00-05 | 1990-03-31 19:00:00-05 | 1990-03-31 19:00:00-05 | 1990-01-31 19:00:00-05
- 1990-05-03 00:00:00-04 | 1990-04-30 20:00:00-04 | 1990-04-30 20:00:00-04 | 1990-03-31 19:00:00-05 | 1990-04-30 20:00:00-04 | 1990-03-31 19:00:00-05 | 1990-04-30 20:00:00-04
- 1990-06-03 00:00:00-04 | 1990-05-31 20:00:00-04 | 1990-04-30 20:00:00-04 | 1990-03-31 19:00:00-05 | 1990-05-31 20:00:00-04 | 1990-05-31 20:00:00-04 | 1990-04-30 20:00:00-04
-(6 rows)
-
----------------------------------------
---- Test time_bucket with timezones ---
----------------------------------------
--- test NULL args
-SELECT
-time_bucket(NULL::interval,now(),'Europe/Berlin'),
-time_bucket('1day',NULL::timestamptz,'Europe/Berlin'),
-time_bucket('1day',now(),NULL::text),
-time_bucket('1day','2020-02-03','Europe/Berlin',NULL),
-time_bucket('1day','2020-02-03','Europe/Berlin','2020-04-01',NULL),
-time_bucket('1day','2020-02-03','Europe/Berlin',NULL,NULL),
-time_bucket('1day','2020-02-03','Europe/Berlin',"offset":=NULL::interval),
-time_bucket('1day','2020-02-03','Europe/Berlin',origin:=NULL::timestamptz);
- time_bucket | time_bucket | time_bucket |      time_bucket       |      time_bucket       |      time_bucket       |      time_bucket       |      time_bucket       
--------------+-------------+-------------+------------------------+------------------------+------------------------+------------------------+------------------------
-             |             |             | 2020-02-02 18:00:00-05 | 2020-02-03 00:00:00-05 | 2020-02-02 18:00:00-05 | 2020-02-02 18:00:00-05 | 2020-02-02 18:00:00-05
-(1 row)
-
-SET datestyle TO ISO;
-SELECT
-  time_bucket('1day', ts) AS "UTC",
-  time_bucket('1day', ts, 'Europe/Berlin') AS "Berlin",
-  time_bucket('1day', ts, 'Europe/London') AS "London",
-  time_bucket('1day', ts, 'America/New_York') AS "New York",
-  time_bucket('1day', ts, 'PST') AS "PST",
-  time_bucket('1day', ts, current_setting('timezone')) AS "current"
-FROM generate_series('1999-12-31 17:00'::timestamptz,'2000-01-02 3:00'::timestamptz, '1hour'::interval) ts;
-          UTC           |         Berlin         |         London         |        New York        |          PST           |        current         
-------------------------+------------------------+------------------------+------------------------+------------------------+------------------------
- 1999-12-30 19:00:00-05 | 1999-12-30 18:00:00-05 | 1999-12-30 19:00:00-05 | 1999-12-31 00:00:00-05 | 1999-12-31 03:00:00-05 | 1999-12-31 00:00:00-05
- 1999-12-30 19:00:00-05 | 1999-12-31 18:00:00-05 | 1999-12-30 19:00:00-05 | 1999-12-31 00:00:00-05 | 1999-12-31 03:00:00-05 | 1999-12-31 00:00:00-05
- 1999-12-31 19:00:00-05 | 1999-12-31 18:00:00-05 | 1999-12-31 19:00:00-05 | 1999-12-31 00:00:00-05 | 1999-12-31 03:00:00-05 | 1999-12-31 00:00:00-05
- 1999-12-31 19:00:00-05 | 1999-12-31 18:00:00-05 | 1999-12-31 19:00:00-05 | 1999-12-31 00:00:00-05 | 1999-12-31 03:00:00-05 | 1999-12-31 00:00:00-05
- 1999-12-31 19:00:00-05 | 1999-12-31 18:00:00-05 | 1999-12-31 19:00:00-05 | 1999-12-31 00:00:00-05 | 1999-12-31 03:00:00-05 | 1999-12-31 00:00:00-05
- 1999-12-31 19:00:00-05 | 1999-12-31 18:00:00-05 | 1999-12-31 19:00:00-05 | 1999-12-31 00:00:00-05 | 1999-12-31 03:00:00-05 | 1999-12-31 00:00:00-05
- 1999-12-31 19:00:00-05 | 1999-12-31 18:00:00-05 | 1999-12-31 19:00:00-05 | 1999-12-31 00:00:00-05 | 1999-12-31 03:00:00-05 | 1999-12-31 00:00:00-05
- 1999-12-31 19:00:00-05 | 1999-12-31 18:00:00-05 | 1999-12-31 19:00:00-05 | 2000-01-01 00:00:00-05 | 1999-12-31 03:00:00-05 | 2000-01-01 00:00:00-05
- 1999-12-31 19:00:00-05 | 1999-12-31 18:00:00-05 | 1999-12-31 19:00:00-05 | 2000-01-01 00:00:00-05 | 1999-12-31 03:00:00-05 | 2000-01-01 00:00:00-05
- 1999-12-31 19:00:00-05 | 1999-12-31 18:00:00-05 | 1999-12-31 19:00:00-05 | 2000-01-01 00:00:00-05 | 1999-12-31 03:00:00-05 | 2000-01-01 00:00:00-05
- 1999-12-31 19:00:00-05 | 1999-12-31 18:00:00-05 | 1999-12-31 19:00:00-05 | 2000-01-01 00:00:00-05 | 2000-01-01 03:00:00-05 | 2000-01-01 00:00:00-05
- 1999-12-31 19:00:00-05 | 1999-12-31 18:00:00-05 | 1999-12-31 19:00:00-05 | 2000-01-01 00:00:00-05 | 2000-01-01 03:00:00-05 | 2000-01-01 00:00:00-05
- 1999-12-31 19:00:00-05 | 1999-12-31 18:00:00-05 | 1999-12-31 19:00:00-05 | 2000-01-01 00:00:00-05 | 2000-01-01 03:00:00-05 | 2000-01-01 00:00:00-05
- 1999-12-31 19:00:00-05 | 1999-12-31 18:00:00-05 | 1999-12-31 19:00:00-05 | 2000-01-01 00:00:00-05 | 2000-01-01 03:00:00-05 | 2000-01-01 00:00:00-05
- 1999-12-31 19:00:00-05 | 1999-12-31 18:00:00-05 | 1999-12-31 19:00:00-05 | 2000-01-01 00:00:00-05 | 2000-01-01 03:00:00-05 | 2000-01-01 00:00:00-05
- 1999-12-31 19:00:00-05 | 1999-12-31 18:00:00-05 | 1999-12-31 19:00:00-05 | 2000-01-01 00:00:00-05 | 2000-01-01 03:00:00-05 | 2000-01-01 00:00:00-05
- 1999-12-31 19:00:00-05 | 1999-12-31 18:00:00-05 | 1999-12-31 19:00:00-05 | 2000-01-01 00:00:00-05 | 2000-01-01 03:00:00-05 | 2000-01-01 00:00:00-05
- 1999-12-31 19:00:00-05 | 1999-12-31 18:00:00-05 | 1999-12-31 19:00:00-05 | 2000-01-01 00:00:00-05 | 2000-01-01 03:00:00-05 | 2000-01-01 00:00:00-05
- 1999-12-31 19:00:00-05 | 1999-12-31 18:00:00-05 | 1999-12-31 19:00:00-05 | 2000-01-01 00:00:00-05 | 2000-01-01 03:00:00-05 | 2000-01-01 00:00:00-05
- 1999-12-31 19:00:00-05 | 1999-12-31 18:00:00-05 | 1999-12-31 19:00:00-05 | 2000-01-01 00:00:00-05 | 2000-01-01 03:00:00-05 | 2000-01-01 00:00:00-05
- 1999-12-31 19:00:00-05 | 1999-12-31 18:00:00-05 | 1999-12-31 19:00:00-05 | 2000-01-01 00:00:00-05 | 2000-01-01 03:00:00-05 | 2000-01-01 00:00:00-05
- 1999-12-31 19:00:00-05 | 1999-12-31 18:00:00-05 | 1999-12-31 19:00:00-05 | 2000-01-01 00:00:00-05 | 2000-01-01 03:00:00-05 | 2000-01-01 00:00:00-05
- 1999-12-31 19:00:00-05 | 1999-12-31 18:00:00-05 | 1999-12-31 19:00:00-05 | 2000-01-01 00:00:00-05 | 2000-01-01 03:00:00-05 | 2000-01-01 00:00:00-05
- 1999-12-31 19:00:00-05 | 1999-12-31 18:00:00-05 | 1999-12-31 19:00:00-05 | 2000-01-01 00:00:00-05 | 2000-01-01 03:00:00-05 | 2000-01-01 00:00:00-05
- 1999-12-31 19:00:00-05 | 1999-12-31 18:00:00-05 | 1999-12-31 19:00:00-05 | 2000-01-01 00:00:00-05 | 2000-01-01 03:00:00-05 | 2000-01-01 00:00:00-05
- 1999-12-31 19:00:00-05 | 2000-01-01 18:00:00-05 | 1999-12-31 19:00:00-05 | 2000-01-01 00:00:00-05 | 2000-01-01 03:00:00-05 | 2000-01-01 00:00:00-05
- 2000-01-01 19:00:00-05 | 2000-01-01 18:00:00-05 | 2000-01-01 19:00:00-05 | 2000-01-01 00:00:00-05 | 2000-01-01 03:00:00-05 | 2000-01-01 00:00:00-05
- 2000-01-01 19:00:00-05 | 2000-01-01 18:00:00-05 | 2000-01-01 19:00:00-05 | 2000-01-01 00:00:00-05 | 2000-01-01 03:00:00-05 | 2000-01-01 00:00:00-05
- 2000-01-01 19:00:00-05 | 2000-01-01 18:00:00-05 | 2000-01-01 19:00:00-05 | 2000-01-01 00:00:00-05 | 2000-01-01 03:00:00-05 | 2000-01-01 00:00:00-05
- 2000-01-01 19:00:00-05 | 2000-01-01 18:00:00-05 | 2000-01-01 19:00:00-05 | 2000-01-01 00:00:00-05 | 2000-01-01 03:00:00-05 | 2000-01-01 00:00:00-05
- 2000-01-01 19:00:00-05 | 2000-01-01 18:00:00-05 | 2000-01-01 19:00:00-05 | 2000-01-01 00:00:00-05 | 2000-01-01 03:00:00-05 | 2000-01-01 00:00:00-05
- 2000-01-01 19:00:00-05 | 2000-01-01 18:00:00-05 | 2000-01-01 19:00:00-05 | 2000-01-02 00:00:00-05 | 2000-01-01 03:00:00-05 | 2000-01-02 00:00:00-05
- 2000-01-01 19:00:00-05 | 2000-01-01 18:00:00-05 | 2000-01-01 19:00:00-05 | 2000-01-02 00:00:00-05 | 2000-01-01 03:00:00-05 | 2000-01-02 00:00:00-05
- 2000-01-01 19:00:00-05 | 2000-01-01 18:00:00-05 | 2000-01-01 19:00:00-05 | 2000-01-02 00:00:00-05 | 2000-01-01 03:00:00-05 | 2000-01-02 00:00:00-05
- 2000-01-01 19:00:00-05 | 2000-01-01 18:00:00-05 | 2000-01-01 19:00:00-05 | 2000-01-02 00:00:00-05 | 2000-01-02 03:00:00-05 | 2000-01-02 00:00:00-05
-(35 rows)
-
-SELECT
-  time_bucket('1month', ts) AS "UTC",
-  time_bucket('1month', ts, 'Europe/Berlin') AS "Berlin",
-  time_bucket('1month', ts, 'America/New_York') AS "New York",
-  time_bucket('1month', ts, current_setting('timezone')) AS "current",
-  time_bucket('2month', ts, current_setting('timezone')) AS "2m",
-  time_bucket('2month', ts, current_setting('timezone'), '2000-02-01'::timestamp) AS "2m origin",
-  time_bucket('2month', ts, current_setting('timezone'), "offset":='14 day'::interval) AS "2m offset",
-  time_bucket('2month', ts, current_setting('timezone'), '2000-02-01'::timestamp, '7 day'::interval) AS "2m offset + origin"
-FROM generate_series('1999-12-01'::timestamptz,'2000-09-01'::timestamptz, '9 day'::interval) ts;
-          UTC           |         Berlin         |        New York        |        current         |           2m           |       2m origin        |       2m offset        |   2m offset + origin   
-------------------------+------------------------+------------------------+------------------------+------------------------+------------------------+------------------------+------------------------
- 1999-11-30 19:00:00-05 | 1999-11-30 18:00:00-05 | 1999-12-01 00:00:00-05 | 1999-12-01 00:00:00-05 | 1999-11-01 00:00:00-05 | 1999-12-01 00:00:00-05 | 1999-11-15 00:00:00-05 | 1999-10-08 00:00:00-04
- 1999-11-30 19:00:00-05 | 1999-11-30 18:00:00-05 | 1999-12-01 00:00:00-05 | 1999-12-01 00:00:00-05 | 1999-11-01 00:00:00-05 | 1999-12-01 00:00:00-05 | 1999-11-15 00:00:00-05 | 1999-12-08 00:00:00-05
- 1999-11-30 19:00:00-05 | 1999-11-30 18:00:00-05 | 1999-12-01 00:00:00-05 | 1999-12-01 00:00:00-05 | 1999-11-01 00:00:00-05 | 1999-12-01 00:00:00-05 | 1999-11-15 00:00:00-05 | 1999-12-08 00:00:00-05
- 1999-11-30 19:00:00-05 | 1999-11-30 18:00:00-05 | 1999-12-01 00:00:00-05 | 1999-12-01 00:00:00-05 | 1999-11-01 00:00:00-05 | 1999-12-01 00:00:00-05 | 1999-11-15 00:00:00-05 | 1999-12-08 00:00:00-05
- 1999-12-31 19:00:00-05 | 1999-12-31 18:00:00-05 | 2000-01-01 00:00:00-05 | 2000-01-01 00:00:00-05 | 2000-01-01 00:00:00-05 | 1999-12-01 00:00:00-05 | 1999-11-15 00:00:00-05 | 1999-12-08 00:00:00-05
- 1999-12-31 19:00:00-05 | 1999-12-31 18:00:00-05 | 2000-01-01 00:00:00-05 | 2000-01-01 00:00:00-05 | 2000-01-01 00:00:00-05 | 1999-12-01 00:00:00-05 | 2000-01-15 00:00:00-05 | 1999-12-08 00:00:00-05
- 1999-12-31 19:00:00-05 | 1999-12-31 18:00:00-05 | 2000-01-01 00:00:00-05 | 2000-01-01 00:00:00-05 | 2000-01-01 00:00:00-05 | 1999-12-01 00:00:00-05 | 2000-01-15 00:00:00-05 | 1999-12-08 00:00:00-05
- 2000-01-31 19:00:00-05 | 2000-01-31 18:00:00-05 | 2000-02-01 00:00:00-05 | 2000-02-01 00:00:00-05 | 2000-01-01 00:00:00-05 | 2000-02-01 00:00:00-05 | 2000-01-15 00:00:00-05 | 1999-12-08 00:00:00-05
- 2000-01-31 19:00:00-05 | 2000-01-31 18:00:00-05 | 2000-02-01 00:00:00-05 | 2000-02-01 00:00:00-05 | 2000-01-01 00:00:00-05 | 2000-02-01 00:00:00-05 | 2000-01-15 00:00:00-05 | 2000-02-08 00:00:00-05
- 2000-01-31 19:00:00-05 | 2000-01-31 18:00:00-05 | 2000-02-01 00:00:00-05 | 2000-02-01 00:00:00-05 | 2000-01-01 00:00:00-05 | 2000-02-01 00:00:00-05 | 2000-01-15 00:00:00-05 | 2000-02-08 00:00:00-05
- 2000-01-31 19:00:00-05 | 2000-01-31 18:00:00-05 | 2000-02-01 00:00:00-05 | 2000-02-01 00:00:00-05 | 2000-01-01 00:00:00-05 | 2000-02-01 00:00:00-05 | 2000-01-15 00:00:00-05 | 2000-02-08 00:00:00-05
- 2000-02-29 19:00:00-05 | 2000-02-29 18:00:00-05 | 2000-03-01 00:00:00-05 | 2000-03-01 00:00:00-05 | 2000-03-01 00:00:00-05 | 2000-02-01 00:00:00-05 | 2000-01-15 00:00:00-05 | 2000-02-08 00:00:00-05
- 2000-02-29 19:00:00-05 | 2000-02-29 18:00:00-05 | 2000-03-01 00:00:00-05 | 2000-03-01 00:00:00-05 | 2000-03-01 00:00:00-05 | 2000-02-01 00:00:00-05 | 2000-03-15 00:00:00-05 | 2000-02-08 00:00:00-05
- 2000-02-29 19:00:00-05 | 2000-02-29 18:00:00-05 | 2000-03-01 00:00:00-05 | 2000-03-01 00:00:00-05 | 2000-03-01 00:00:00-05 | 2000-02-01 00:00:00-05 | 2000-03-15 00:00:00-05 | 2000-02-08 00:00:00-05
- 2000-03-31 19:00:00-05 | 2000-03-31 17:00:00-05 | 2000-04-01 00:00:00-05 | 2000-04-01 00:00:00-05 | 2000-03-01 00:00:00-05 | 2000-04-01 00:00:00-05 | 2000-03-15 00:00:00-05 | 2000-02-08 00:00:00-05
- 2000-03-31 19:00:00-05 | 2000-03-31 17:00:00-05 | 2000-04-01 00:00:00-05 | 2000-04-01 00:00:00-05 | 2000-03-01 00:00:00-05 | 2000-04-01 00:00:00-05 | 2000-03-15 00:00:00-05 | 2000-04-08 00:00:00-04
- 2000-03-31 19:00:00-05 | 2000-03-31 17:00:00-05 | 2000-04-01 00:00:00-05 | 2000-04-01 00:00:00-05 | 2000-03-01 00:00:00-05 | 2000-04-01 00:00:00-05 | 2000-03-15 00:00:00-05 | 2000-04-08 00:00:00-04
- 2000-04-30 20:00:00-04 | 2000-04-30 18:00:00-04 | 2000-05-01 00:00:00-04 | 2000-05-01 00:00:00-04 | 2000-05-01 00:00:00-04 | 2000-04-01 00:00:00-05 | 2000-03-15 00:00:00-05 | 2000-04-08 00:00:00-04
- 2000-04-30 20:00:00-04 | 2000-04-30 18:00:00-04 | 2000-05-01 00:00:00-04 | 2000-05-01 00:00:00-04 | 2000-05-01 00:00:00-04 | 2000-04-01 00:00:00-05 | 2000-03-15 00:00:00-05 | 2000-04-08 00:00:00-04
- 2000-04-30 20:00:00-04 | 2000-04-30 18:00:00-04 | 2000-05-01 00:00:00-04 | 2000-05-01 00:00:00-04 | 2000-05-01 00:00:00-04 | 2000-04-01 00:00:00-05 | 2000-05-15 00:00:00-04 | 2000-04-08 00:00:00-04
- 2000-04-30 20:00:00-04 | 2000-04-30 18:00:00-04 | 2000-05-01 00:00:00-04 | 2000-05-01 00:00:00-04 | 2000-05-01 00:00:00-04 | 2000-04-01 00:00:00-05 | 2000-05-15 00:00:00-04 | 2000-04-08 00:00:00-04
- 2000-05-31 20:00:00-04 | 2000-05-31 18:00:00-04 | 2000-06-01 00:00:00-04 | 2000-06-01 00:00:00-04 | 2000-05-01 00:00:00-04 | 2000-06-01 00:00:00-04 | 2000-05-15 00:00:00-04 | 2000-04-08 00:00:00-04
- 2000-05-31 20:00:00-04 | 2000-05-31 18:00:00-04 | 2000-06-01 00:00:00-04 | 2000-06-01 00:00:00-04 | 2000-05-01 00:00:00-04 | 2000-06-01 00:00:00-04 | 2000-05-15 00:00:00-04 | 2000-06-08 00:00:00-04
- 2000-05-31 20:00:00-04 | 2000-05-31 18:00:00-04 | 2000-06-01 00:00:00-04 | 2000-06-01 00:00:00-04 | 2000-05-01 00:00:00-04 | 2000-06-01 00:00:00-04 | 2000-05-15 00:00:00-04 | 2000-06-08 00:00:00-04
- 2000-06-30 20:00:00-04 | 2000-06-30 18:00:00-04 | 2000-07-01 00:00:00-04 | 2000-07-01 00:00:00-04 | 2000-07-01 00:00:00-04 | 2000-06-01 00:00:00-04 | 2000-05-15 00:00:00-04 | 2000-06-08 00:00:00-04
- 2000-06-30 20:00:00-04 | 2000-06-30 18:00:00-04 | 2000-07-01 00:00:00-04 | 2000-07-01 00:00:00-04 | 2000-07-01 00:00:00-04 | 2000-06-01 00:00:00-04 | 2000-05-15 00:00:00-04 | 2000-06-08 00:00:00-04
- 2000-06-30 20:00:00-04 | 2000-06-30 18:00:00-04 | 2000-07-01 00:00:00-04 | 2000-07-01 00:00:00-04 | 2000-07-01 00:00:00-04 | 2000-06-01 00:00:00-04 | 2000-07-15 00:00:00-04 | 2000-06-08 00:00:00-04
- 2000-06-30 20:00:00-04 | 2000-06-30 18:00:00-04 | 2000-07-01 00:00:00-04 | 2000-07-01 00:00:00-04 | 2000-07-01 00:00:00-04 | 2000-06-01 00:00:00-04 | 2000-07-15 00:00:00-04 | 2000-06-08 00:00:00-04
- 2000-07-31 20:00:00-04 | 2000-07-31 18:00:00-04 | 2000-08-01 00:00:00-04 | 2000-08-01 00:00:00-04 | 2000-07-01 00:00:00-04 | 2000-08-01 00:00:00-04 | 2000-07-15 00:00:00-04 | 2000-08-08 00:00:00-04
- 2000-07-31 20:00:00-04 | 2000-07-31 18:00:00-04 | 2000-08-01 00:00:00-04 | 2000-08-01 00:00:00-04 | 2000-07-01 00:00:00-04 | 2000-08-01 00:00:00-04 | 2000-07-15 00:00:00-04 | 2000-08-08 00:00:00-04
- 2000-07-31 20:00:00-04 | 2000-07-31 18:00:00-04 | 2000-08-01 00:00:00-04 | 2000-08-01 00:00:00-04 | 2000-07-01 00:00:00-04 | 2000-08-01 00:00:00-04 | 2000-07-15 00:00:00-04 | 2000-08-08 00:00:00-04
-(31 rows)
-
-RESET datestyle;
-------------------------------------------------------------
---- Test timescaledb_experimental.time_bucket_ng function --
-------------------------------------------------------------
--- not supported functionality
-\set ON_ERROR_STOP 0
-SELECT timescaledb_experimental.time_bucket_ng('1 hour', '2001-02-03' :: date) AS result;
-ERROR:  interval must be either days and weeks, or months and years
-SELECT timescaledb_experimental.time_bucket_ng('0 days', '2001-02-03' :: date) AS result;
-ERROR:  interval must be at least one day
-SELECT timescaledb_experimental.time_bucket_ng('1 month', '2001-02-03' :: date, origin => '2000-01-02') AS result;
-ERROR:  origin must be the first day of the month
-HINT:  When using timestamptz-version of the function, 'origin' is converted to provided 'timezone'.
-SELECT timescaledb_experimental.time_bucket_ng('1 month', '2000-01-02' :: date, origin => '2001-01-01') AS result;
-   result   
-------------
- 01-01-2000
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng('1 day', '2000-01-02' :: date, origin => '2001-01-01') AS result;
-ERROR:  origin must be before the given date
-SELECT timescaledb_experimental.time_bucket_ng('1 month 3 hours', '2021-11-22' :: timestamp) AS result;
-ERROR:  interval can't combine months with minutes or hours
--- timestamp is less than the default 'origin' value
-SELECT timescaledb_experimental.time_bucket_ng('1 day', '1999-01-01 12:34:56 MSK' :: timestamptz, timezone => 'MSK');
-ERROR:  origin must be before the given date
--- 'origin' in Europe/Moscow timezone is not the first day of the month at given time zone (UTC in this case)
-select timescaledb_experimental.time_bucket_ng('1 month', '2021-07-12 12:34:56 Europe/Moscow' :: timestamptz, origin => '2021-06-01 00:00:00 Europe/Moscow' :: timestamptz, timezone => 'UTC');
-ERROR:  origin must be the first day of the month
-HINT:  When using timestamptz-version of the function, 'origin' is converted to provided 'timezone'.
-\set ON_ERROR_STOP 1
--- wrappers
-SELECT timescaledb_experimental.time_bucket_ng('1 year', '2021-11-22' :: timestamp) AS result;
-          result          
---------------------------
- Fri Jan 01 00:00:00 2021
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng('1 year', '2021-11-22' :: timestamptz) AS result;
-            result            
-------------------------------
- Fri Jan 01 00:00:00 2021 EST
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng('1 year', '2021-11-22' :: timestamp, origin => '2021-06-01') AS result;
-          result          
---------------------------
- Tue Jun 01 00:00:00 2021
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng('1 year', '2021-11-22' :: timestamptz, origin => '2021-06-01') AS result;
-            result            
-------------------------------
- Tue Jun 01 00:00:00 2021 EDT
-(1 row)
-
--- null argument
-SELECT timescaledb_experimental.time_bucket_ng('1 year', null :: date) AS result;
- result 
---------
- 
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng('1 year', null :: timestamp) AS result;
- result 
---------
- 
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng('1 year', null :: timestamptz) AS result;
- result 
---------
- 
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng('1 year', null :: timestamptz, timezone => 'Europe/Moscow') AS result;
- result 
---------
- 
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng('1 year', null :: date, origin => '2021-06-01') AS result;
- result 
---------
- 
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng('1 year', null :: timestamp, origin => '2021-06-01') AS result;
- result 
---------
- 
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng('1 year', null :: timestamptz, origin => '2021-06-01') AS result;
- result 
---------
- 
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng('1 year', null :: timestamptz, origin => '2021-06-01', timezone => 'Europe/Moscow') AS result;
- result 
---------
- 
-(1 row)
-
--- null interval
-SELECT timescaledb_experimental.time_bucket_ng(null, '2021-07-12' :: date) AS result;
- result 
---------
- 
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng(null, '2021-07-12 12:34:56' :: timestamp) AS result;
- result 
---------
- 
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng(null, '2021-07-12 12:34:56' :: timestamptz) AS result;
- result 
---------
- 
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng(null, '2021-07-12 12:34:56' :: timestamptz, 'Europe/Moscow') AS result;
- result 
---------
- 
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng(null, '2021-07-12' :: date, origin => '2021-06-01') AS result;
- result 
---------
- 
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng(null, '2021-07-12 12:34:56' :: timestamp, origin => '2021-06-01') AS result;
- result 
---------
- 
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng(null, '2021-07-12 12:34:56' :: timestamptz, origin => '2021-06-01') AS result;
- result 
---------
- 
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng(null, '2021-07-12 12:34:56' :: timestamptz, origin => '2021-06-01', timezone => 'Europe/Moscow') AS result;
- result 
---------
- 
-(1 row)
-
--- null origin
-SELECT timescaledb_experimental.time_bucket_ng('1 year', '2021-07-12' :: date, origin => null) AS result;
- result 
---------
- 
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng('1 year', '2021-07-12 12:34:56' :: timestamp, origin => null) AS result;
- result 
---------
- 
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng('1 year', '2021-07-12 12:34:56' :: timestamptz, origin => null) AS result;
- result 
---------
- 
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng('1 year', '2021-07-12 12:34:56' :: timestamptz, origin => null, timezone => 'Europe/Moscow') AS result;
- result 
---------
- 
-(1 row)
-
--- infinity argument
-SELECT timescaledb_experimental.time_bucket_ng('1 year', 'infinity' :: date) AS result;
-  result  
-----------
- infinity
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng('1 year', 'infinity' :: timestamp) AS result;
-  result  
-----------
- infinity
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng('1 year', 'infinity' :: timestamptz) AS result;
-  result  
-----------
- infinity
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng('1 year', 'infinity' :: timestamptz, timezone => 'Europe/Moscow') AS result;
-  result  
-----------
- infinity
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng('1 year', 'infinity' :: date, origin => '2021-06-01') AS result;
-  result  
-----------
- infinity
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng('1 year', 'infinity' :: timestamp, origin => '2021-06-01') AS result;
-  result  
-----------
- infinity
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng('1 year', 'infinity' :: timestamptz, origin => '2021-06-01') AS result;
-  result  
-----------
- infinity
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng('1 year', 'infinity' :: timestamptz, origin => '2021-06-01', timezone => 'Europe/Moscow') AS result;
-  result  
-----------
- infinity
-(1 row)
-
--- test for specific code path: hours/minutes/seconds interval and timestamp argument
-SELECT timescaledb_experimental.time_bucket_ng('12 hours', 'infinity' :: timestamp) AS result;
-  result  
-----------
- infinity
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng('12 hours', 'infinity' :: timestamp, origin => '2021-06-01') AS result;
-  result  
-----------
- infinity
-(1 row)
-
--- infinite origin
-SELECT timescaledb_experimental.time_bucket_ng('1 year', '2021-07-12' :: date, origin => 'infinity') AS result;
-  result  
-----------
- infinity
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng('1 year', '2021-07-12 12:34:56' :: timestamp, origin => 'infinity') AS result;
-  result  
-----------
- infinity
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng('1 year', '2021-07-12 12:34:56' :: timestamptz, origin => 'infinity') AS result;
-  result  
-----------
- infinity
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng('1 year', '2021-07-12 12:34:56' :: timestamptz, origin => 'infinity', timezone => 'Europe/Moscow') AS result;
-  result  
-----------
- infinity
-(1 row)
-
--- test for specific code path: hours/minutes/seconds interval and timestamp argument
-SELECT timescaledb_experimental.time_bucket_ng('12 hours', '2021-07-12 12:34:56' :: timestamp, origin => 'infinity') AS result;
-  result  
-----------
- infinity
-(1 row)
-
--- test for invalid timezone argument
-SELECT timescaledb_experimental.time_bucket_ng('1 year', '2021-07-12 12:34:56' :: timestamptz, timezone => null) AS result;
- result 
---------
- 
-(1 row)
-
-\set ON_ERROR_STOP 0
-SELECT timescaledb_experimental.time_bucket_ng('1 year', '2021-07-12 12:34:56' :: timestamptz, timezone => 'Europe/Ololondon') AS result;
-ERROR:  time zone "Europe/Ololondon" not recognized
-\set ON_ERROR_STOP 1
--- Make sure time_bucket_ng() supports seconds, minutes, and hours.
--- We happen to know that the internal implementation is the same
--- as for time_bucket(), thus there is no reason to execute all the tests
--- we already have for time_bucket(). These two functions will most likely
--- be merged eventually anyway.
-SELECT timescaledb_experimental.time_bucket_ng('30 seconds', '2021-07-12 12:34:56' :: timestamp) AS result;
-          result          
---------------------------
- Mon Jul 12 12:34:30 2021
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng('15 minutes', '2021-07-12 12:34:56' :: timestamp) AS result;
-          result          
---------------------------
- Mon Jul 12 12:30:00 2021
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng('6 hours', '2021-07-12 12:34:56' :: timestamp) AS result;
-          result          
---------------------------
- Mon Jul 12 12:00:00 2021
-(1 row)
-
--- Same as above, but with provided 'origin' argument.
-SELECT timescaledb_experimental.time_bucket_ng('30 seconds', '2021-07-12 12:34:56' :: timestamp, origin => '2021-07-12 12:10:00') AS result;
-          result          
---------------------------
- Mon Jul 12 12:34:30 2021
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng('15 minutes', '2021-07-12 12:34:56' :: timestamp, origin => '2021-07-12 12:10:00') AS result;
-          result          
---------------------------
- Mon Jul 12 12:25:00 2021
-(1 row)
-
-SELECT timescaledb_experimental.time_bucket_ng('6 hours', '2021-07-12 12:34:56' :: timestamp, origin => '2021-07-12 12:10:00') AS result;
-          result          
---------------------------
- Mon Jul 12 12:10:00 2021
-(1 row)
-
--- N days / weeks buckets
-SELECT  to_char(d, 'YYYY-MM-DD') AS d,
-        to_char(timescaledb_experimental.time_bucket_ng('1 day', d),  'YYYY-MM-DD') AS d1,
-        to_char(timescaledb_experimental.time_bucket_ng('2 days', d),  'YYYY-MM-DD') AS d2,
-        to_char(timescaledb_experimental.time_bucket_ng('3 days', d),  'YYYY-MM-DD') AS d3,
-        to_char(timescaledb_experimental.time_bucket_ng('1 week', d),  'YYYY-MM-DD') AS w1,
-        to_char(timescaledb_experimental.time_bucket_ng('1 week 2 days', d),  'YYYY-MM-DD') AS w1d2
-FROM generate_series('2020-01-01' :: date, '2020-01-12', '1 day') AS ts,
-     unnest(array[ts :: date]) AS d;
-     d      |     d1     |     d2     |     d3     |     w1     |    w1d2    
-------------+------------+------------+------------+------------+------------
- 2020-01-01 | 2020-01-01 | 2019-12-31 | 2020-01-01 | 2019-12-28 | 2019-12-26
- 2020-01-02 | 2020-01-02 | 2020-01-02 | 2020-01-01 | 2019-12-28 | 2019-12-26
- 2020-01-03 | 2020-01-03 | 2020-01-02 | 2020-01-01 | 2019-12-28 | 2019-12-26
- 2020-01-04 | 2020-01-04 | 2020-01-04 | 2020-01-04 | 2020-01-04 | 2020-01-04
- 2020-01-05 | 2020-01-05 | 2020-01-04 | 2020-01-04 | 2020-01-04 | 2020-01-04
- 2020-01-06 | 2020-01-06 | 2020-01-06 | 2020-01-04 | 2020-01-04 | 2020-01-04
- 2020-01-07 | 2020-01-07 | 2020-01-06 | 2020-01-07 | 2020-01-04 | 2020-01-04
- 2020-01-08 | 2020-01-08 | 2020-01-08 | 2020-01-07 | 2020-01-04 | 2020-01-04
- 2020-01-09 | 2020-01-09 | 2020-01-08 | 2020-01-07 | 2020-01-04 | 2020-01-04
- 2020-01-10 | 2020-01-10 | 2020-01-10 | 2020-01-10 | 2020-01-04 | 2020-01-04
- 2020-01-11 | 2020-01-11 | 2020-01-10 | 2020-01-10 | 2020-01-11 | 2020-01-04
- 2020-01-12 | 2020-01-12 | 2020-01-12 | 2020-01-10 | 2020-01-11 | 2020-01-04
-(12 rows)
-
--- N days / weeks buckets with given 'origin'
-SELECT  to_char(d, 'YYYY-MM-DD') AS d,
-        to_char(timescaledb_experimental.time_bucket_ng('1 day', d, origin => '2020-01-01'), 'YYYY-MM-DD') AS d1,
-        to_char(timescaledb_experimental.time_bucket_ng('2 days', d, origin => '2020-01-01'), 'YYYY-MM-DD') AS d2,
-        to_char(timescaledb_experimental.time_bucket_ng('3 days', d, origin => '2020-01-01'), 'YYYY-MM-DD') AS d3,
-        to_char(timescaledb_experimental.time_bucket_ng('1 week', d, origin => '2020-01-01'), 'YYYY-MM-DD') AS w1,
-        to_char(timescaledb_experimental.time_bucket_ng('1 week 2 days', d, origin => '2020-01-01'), 'YYYY-MM-DD') AS w1d2
-FROM generate_series('2020-01-01' :: date, '2020-01-12', '1 day') AS ts,
-     unnest(array[ts :: date]) AS d;
-     d      |     d1     |     d2     |     d3     |     w1     |    w1d2    
-------------+------------+------------+------------+------------+------------
- 2020-01-01 | 2020-01-01 | 2020-01-01 | 2020-01-01 | 2020-01-01 | 2020-01-01
- 2020-01-02 | 2020-01-02 | 2020-01-01 | 2020-01-01 | 2020-01-01 | 2020-01-01
- 2020-01-03 | 2020-01-03 | 2020-01-03 | 2020-01-01 | 2020-01-01 | 2020-01-01
- 2020-01-04 | 2020-01-04 | 2020-01-03 | 2020-01-04 | 2020-01-01 | 2020-01-01
- 2020-01-05 | 2020-01-05 | 2020-01-05 | 2020-01-04 | 2020-01-01 | 2020-01-01
- 2020-01-06 | 2020-01-06 | 2020-01-05 | 2020-01-04 | 2020-01-01 | 2020-01-01
- 2020-01-07 | 2020-01-07 | 2020-01-07 | 2020-01-07 | 2020-01-01 | 2020-01-01
- 2020-01-08 | 2020-01-08 | 2020-01-07 | 2020-01-07 | 2020-01-08 | 2020-01-01
- 2020-01-09 | 2020-01-09 | 2020-01-09 | 2020-01-07 | 2020-01-08 | 2020-01-01
- 2020-01-10 | 2020-01-10 | 2020-01-09 | 2020-01-10 | 2020-01-08 | 2020-01-10
- 2020-01-11 | 2020-01-11 | 2020-01-11 | 2020-01-10 | 2020-01-08 | 2020-01-10
- 2020-01-12 | 2020-01-12 | 2020-01-11 | 2020-01-10 | 2020-01-08 | 2020-01-10
-(12 rows)
-
--- N month buckets
-SELECT  to_char(d, 'YYYY-MM-DD') AS d,
-        to_char(timescaledb_experimental.time_bucket_ng('1 month', d), 'YYYY-MM-DD') AS m1,
-        to_char(timescaledb_experimental.time_bucket_ng('2 month', d), 'YYYY-MM-DD') AS m2,
-        to_char(timescaledb_experimental.time_bucket_ng('3 month', d), 'YYYY-MM-DD') AS m3,
-        to_char(timescaledb_experimental.time_bucket_ng('4 month', d), 'YYYY-MM-DD') AS m4,
-        to_char(timescaledb_experimental.time_bucket_ng('5 month', d), 'YYYY-MM-DD') AS m5
-FROM generate_series('2020-01-01' :: date, '2020-12-01', '1 month') AS ts,
-     unnest(array[ts :: date]) AS d;
-     d      |     m1     |     m2     |     m3     |     m4     |     m5     
-------------+------------+------------+------------+------------+------------
- 2020-01-01 | 2020-01-01 | 2020-01-01 | 2020-01-01 | 2020-01-01 | 2020-01-01
- 2020-02-01 | 2020-02-01 | 2020-01-01 | 2020-01-01 | 2020-01-01 | 2020-01-01
- 2020-03-01 | 2020-03-01 | 2020-03-01 | 2020-01-01 | 2020-01-01 | 2020-01-01
- 2020-04-01 | 2020-04-01 | 2020-03-01 | 2020-04-01 | 2020-01-01 | 2020-01-01
- 2020-05-01 | 2020-05-01 | 2020-05-01 | 2020-04-01 | 2020-05-01 | 2020-01-01
- 2020-06-01 | 2020-06-01 | 2020-05-01 | 2020-04-01 | 2020-05-01 | 2020-06-01
- 2020-07-01 | 2020-07-01 | 2020-07-01 | 2020-07-01 | 2020-05-01 | 2020-06-01
- 2020-08-01 | 2020-08-01 | 2020-07-01 | 2020-07-01 | 2020-05-01 | 2020-06-01
- 2020-09-01 | 2020-09-01 | 2020-09-01 | 2020-07-01 | 2020-09-01 | 2020-06-01
- 2020-10-01 | 2020-10-01 | 2020-09-01 | 2020-10-01 | 2020-09-01 | 2020-06-01
- 2020-11-01 | 2020-11-01 | 2020-11-01 | 2020-10-01 | 2020-09-01 | 2020-11-01
- 2020-12-01 | 2020-12-01 | 2020-11-01 | 2020-10-01 | 2020-09-01 | 2020-11-01
-(12 rows)
-
--- N month buckets with given 'origin'
-SELECT  to_char(d, 'YYYY-MM-DD') AS d,
-        to_char(timescaledb_experimental.time_bucket_ng('1 month', d, origin => '2019-05-01'), 'YYYY-MM-DD') AS m1,
-        to_char(timescaledb_experimental.time_bucket_ng('2 month', d, origin => '2019-05-01'), 'YYYY-MM-DD') AS m2,
-        to_char(timescaledb_experimental.time_bucket_ng('3 month', d, origin => '2019-05-01'), 'YYYY-MM-DD') AS m3,
-        to_char(timescaledb_experimental.time_bucket_ng('4 month', d, origin => '2019-05-01'), 'YYYY-MM-DD') AS m4,
-        to_char(timescaledb_experimental.time_bucket_ng('5 month', d, origin => '2019-05-01'), 'YYYY-MM-DD') AS m5
-FROM generate_series('2020-01-01' :: date, '2020-12-01', '1 month') AS ts,
-     unnest(array[ts :: date]) AS d;
-     d      |     m1     |     m2     |     m3     |     m4     |     m5     
-------------+------------+------------+------------+------------+------------
- 2020-01-01 | 2020-01-01 | 2020-01-01 | 2019-11-01 | 2020-01-01 | 2019-10-01
- 2020-02-01 | 2020-02-01 | 2020-01-01 | 2020-02-01 | 2020-01-01 | 2019-10-01
- 2020-03-01 | 2020-03-01 | 2020-03-01 | 2020-02-01 | 2020-01-01 | 2020-03-01
- 2020-04-01 | 2020-04-01 | 2020-03-01 | 2020-02-01 | 2020-01-01 | 2020-03-01
- 2020-05-01 | 2020-05-01 | 2020-05-01 | 2020-05-01 | 2020-05-01 | 2020-03-01
- 2020-06-01 | 2020-06-01 | 2020-05-01 | 2020-05-01 | 2020-05-01 | 2020-03-01
- 2020-07-01 | 2020-07-01 | 2020-07-01 | 2020-05-01 | 2020-05-01 | 2020-03-01
- 2020-08-01 | 2020-08-01 | 2020-07-01 | 2020-08-01 | 2020-05-01 | 2020-08-01
- 2020-09-01 | 2020-09-01 | 2020-09-01 | 2020-08-01 | 2020-09-01 | 2020-08-01
- 2020-10-01 | 2020-10-01 | 2020-09-01 | 2020-08-01 | 2020-09-01 | 2020-08-01
- 2020-11-01 | 2020-11-01 | 2020-11-01 | 2020-11-01 | 2020-09-01 | 2020-08-01
- 2020-12-01 | 2020-12-01 | 2020-11-01 | 2020-11-01 | 2020-09-01 | 2020-08-01
-(12 rows)
-
--- N years / N years, M month buckets
-SELECT  to_char(d, 'YYYY-MM-DD') AS d,
-        to_char(timescaledb_experimental.time_bucket_ng('1 year', d), 'YYYY-MM-DD') AS y1,
-        to_char(timescaledb_experimental.time_bucket_ng('1 year 6 month', d), 'YYYY-MM-DD') AS y1m6,
-        to_char(timescaledb_experimental.time_bucket_ng('2 years', d), 'YYYY-MM-DD') AS y2,
-        to_char(timescaledb_experimental.time_bucket_ng('2 years 6 month', d), 'YYYY-MM-DD') AS y2m6,
-        to_char(timescaledb_experimental.time_bucket_ng('3 years', d), 'YYYY-MM-DD') AS y3
-FROM generate_series('2015-01-01' :: date, '2020-12-01', '6 month') AS ts,
-     unnest(array[ts :: date]) AS d;
-     d      |     y1     |    y1m6    |     y2     |    y2m6    |     y3     
-------------+------------+------------+------------+------------+------------
- 2015-01-01 | 2015-01-01 | 2015-01-01 | 2014-01-01 | 2015-01-01 | 2015-01-01
- 2015-07-01 | 2015-01-01 | 2015-01-01 | 2014-01-01 | 2015-01-01 | 2015-01-01
- 2016-01-01 | 2016-01-01 | 2015-01-01 | 2016-01-01 | 2015-01-01 | 2015-01-01
- 2016-07-01 | 2016-01-01 | 2016-07-01 | 2016-01-01 | 2015-01-01 | 2015-01-01
- 2017-01-01 | 2017-01-01 | 2016-07-01 | 2016-01-01 | 2015-01-01 | 2015-01-01
- 2017-07-01 | 2017-01-01 | 2016-07-01 | 2016-01-01 | 2017-07-01 | 2015-01-01
- 2018-01-01 | 2018-01-01 | 2018-01-01 | 2018-01-01 | 2017-07-01 | 2018-01-01
- 2018-07-01 | 2018-01-01 | 2018-01-01 | 2018-01-01 | 2017-07-01 | 2018-01-01
- 2019-01-01 | 2019-01-01 | 2018-01-01 | 2018-01-01 | 2017-07-01 | 2018-01-01
- 2019-07-01 | 2019-01-01 | 2019-07-01 | 2018-01-01 | 2017-07-01 | 2018-01-01
- 2020-01-01 | 2020-01-01 | 2019-07-01 | 2020-01-01 | 2020-01-01 | 2018-01-01
- 2020-07-01 | 2020-01-01 | 2019-07-01 | 2020-01-01 | 2020-01-01 | 2018-01-01
-(12 rows)
-
--- N years / N years, M month buckets with given 'origin'
-SELECT  to_char(d, 'YYYY-MM-DD') AS d,
-        to_char(timescaledb_experimental.time_bucket_ng('1 year', d, origin => '2000-06-01'), 'YYYY-MM-DD') AS y1,
-        to_char(timescaledb_experimental.time_bucket_ng('1 year 6 month', d, origin => '2000-06-01'), 'YYYY-MM-DD') AS y1m6,
-        to_char(timescaledb_experimental.time_bucket_ng('2 years', d, origin => '2000-06-01'), 'YYYY-MM-DD') AS y2,
-        to_char(timescaledb_experimental.time_bucket_ng('2 years 6 month', d, origin => '2000-06-01'), 'YYYY-MM-DD') AS y2m6,
-        to_char(timescaledb_experimental.time_bucket_ng('3 years', d, origin => '2000-06-01'), 'YYYY-MM-DD') AS y3
-FROM generate_series('2015-01-01' :: date, '2020-12-01', '6 month') AS ts,
-     unnest(array[ts :: date]) AS d;
-     d      |     y1     |    y1m6    |     y2     |    y2m6    |     y3     
-------------+------------+------------+------------+------------+------------
- 2015-01-01 | 2014-06-01 | 2013-12-01 | 2014-06-01 | 2012-12-01 | 2012-06-01
- 2015-07-01 | 2015-06-01 | 2015-06-01 | 2014-06-01 | 2015-06-01 | 2015-06-01
- 2016-01-01 | 2015-06-01 | 2015-06-01 | 2014-06-01 | 2015-06-01 | 2015-06-01
- 2016-07-01 | 2016-06-01 | 2015-06-01 | 2016-06-01 | 2015-06-01 | 2015-06-01
- 2017-01-01 | 2016-06-01 | 2016-12-01 | 2016-06-01 | 2015-06-01 | 2015-06-01
- 2017-07-01 | 2017-06-01 | 2016-12-01 | 2016-06-01 | 2015-06-01 | 2015-06-01
- 2018-01-01 | 2017-06-01 | 2016-12-01 | 2016-06-01 | 2017-12-01 | 2015-06-01
- 2018-07-01 | 2018-06-01 | 2018-06-01 | 2018-06-01 | 2017-12-01 | 2018-06-01
- 2019-01-01 | 2018-06-01 | 2018-06-01 | 2018-06-01 | 2017-12-01 | 2018-06-01
- 2019-07-01 | 2019-06-01 | 2018-06-01 | 2018-06-01 | 2017-12-01 | 2018-06-01
- 2020-01-01 | 2019-06-01 | 2019-12-01 | 2018-06-01 | 2017-12-01 | 2018-06-01
- 2020-07-01 | 2020-06-01 | 2019-12-01 | 2020-06-01 | 2020-06-01 | 2018-06-01
-(12 rows)
-
--- Test timezones support with different bucket sizes
-BEGIN;
--- Timestamptz type is displayed in the session timezone.
--- To get consistent results during the test we temporary set the session
--- timezone to the known one.
-SET TIME ZONE '+00';
--- Moscow is UTC+3 in the year 2021. Let's say you are dealing with '1 day' bucket.
--- In order to calculate the beginning of the bucket you have to take LOCAL
--- Moscow time and throw away the time. You will get the midnight. The new day
--- starts 3 hours EARLIER in Moscow than in UTC+0 time zone, thus resulting
--- timestamp will be 3 hours LESS than for UTC+0.
-SELECT bs, tz, to_char(ts_out, 'YYYY-MM-DD HH24:MI:SS TZ') as res
-FROM unnest(array['Europe/Moscow', 'UTC']) as tz,
-     unnest(array['12 hours', '1 day', '1 month', '4 months', '1 year']) as bs,
-     unnest(array['2021-07-12 12:34:56 Europe/Moscow' :: timestamptz]) as ts_in,
-     unnest(array[timescaledb_experimental.time_bucket_ng(bs :: interval, ts_in, timezone => tz)]) as ts_out
-ORDER BY tz, bs :: interval;
-    bs    |      tz       |           res           
-----------+---------------+-------------------------
- 12 hours | Europe/Moscow | 2021-07-12 09:00:00 +00
- 1 day    | Europe/Moscow | 2021-07-11 21:00:00 +00
- 1 month  | Europe/Moscow | 2021-06-30 21:00:00 +00
- 4 months | Europe/Moscow | 2021-04-30 21:00:00 +00
- 1 year   | Europe/Moscow | 2020-12-31 21:00:00 +00
- 12 hours | UTC           | 2021-07-12 00:00:00 +00
- 1 day    | UTC           | 2021-07-12 00:00:00 +00
- 1 month  | UTC           | 2021-07-01 00:00:00 +00
- 4 months | UTC           | 2021-05-01 00:00:00 +00
- 1 year   | UTC           | 2021-01-01 00:00:00 +00
-(10 rows)
-
--- Same as above, but with 'origin'
-SELECT bs, tz, to_char(ts_out, 'YYYY-MM-DD HH24:MI:SS TZ') as res
-FROM unnest(array['Europe/Moscow']) as tz,
-     unnest(array['12 hours', '1 day', '1 month', '4 months', '1 year']) as bs,
-     unnest(array['2021-07-12 12:34:56 Europe/Moscow' :: timestamptz]) as ts_in,
-     unnest(array['2021-06-01 00:00:00 Europe/Moscow' :: timestamptz]) as origin_in,
-     unnest(array[timescaledb_experimental.time_bucket_ng(bs :: interval, ts_in, origin => origin_in, timezone => tz)]) as ts_out
-ORDER BY tz, bs :: interval;
-    bs    |      tz       |           res           
-----------+---------------+-------------------------
- 12 hours | Europe/Moscow | 2021-07-12 09:00:00 +00
- 1 day    | Europe/Moscow | 2021-07-11 21:00:00 +00
- 1 month  | Europe/Moscow | 2021-06-30 21:00:00 +00
- 4 months | Europe/Moscow | 2021-05-31 21:00:00 +00
- 1 year   | Europe/Moscow | 2021-05-31 21:00:00 +00
-(5 rows)
-
--- Overwritten origin allows to work with dates earlier than the default origin
-SELECT to_char(timescaledb_experimental.time_bucket_ng('1 day', '1999-01-01 12:34:56 MSK' :: timestamptz, origin => '1900-01-01 00:00:00 MSK', timezone => 'MSK'), 'YYYY-MM-DD HH24:MI:SS TZ');
-         to_char         
--------------------------
- 1998-12-31 21:00:00 +00
-(1 row)
-
--- Restore previously used time zone.
-ROLLBACK;
--------------------------------------
---- Test time input functions --
--------------------------------------
-\c :TEST_DBNAME :ROLE_SUPERUSER
-CREATE OR REPLACE FUNCTION test.interval_to_internal(coltype REGTYPE, value ANYELEMENT = NULL::BIGINT) RETURNS BIGINT
-AS :MODULE_PATHNAME, 'ts_dimension_interval_to_internal_test' LANGUAGE C VOLATILE;
-\c :TEST_DBNAME :ROLE_DEFAULT_PERM_USER
-SELECT test.interval_to_internal('TIMESTAMP'::regtype, INTERVAL '1 day');
- interval_to_internal 
-----------------------
-          86400000000
-(1 row)
-
-SELECT test.interval_to_internal('TIMESTAMP'::regtype, 86400000000);
- interval_to_internal 
-----------------------
-          86400000000
-(1 row)
-
----should give warning
-SELECT test.interval_to_internal('TIMESTAMP'::regtype, 86400);
-WARNING:  unexpected interval: smaller than one second
-HINT:  The interval is specified in microseconds.
- interval_to_internal 
-----------------------
-                86400
-(1 row)
-
-SELECT test.interval_to_internal('TIMESTAMP'::regtype);
- interval_to_internal 
-----------------------
-         604800000000
-(1 row)
-
-SELECT test.interval_to_internal('BIGINT'::regtype, 2147483649::bigint);
- interval_to_internal 
-----------------------
-           2147483649
-(1 row)
-
-\set VERBOSITY terse
-\set ON_ERROR_STOP 0
-SELECT test.interval_to_internal('INT'::regtype);
-ERROR:  integer dimensions require an explicit interval
-SELECT test.interval_to_internal('INT'::regtype, 2147483649::bigint);
-ERROR:  invalid interval: must be between 1 and 2147483647
-SELECT test.interval_to_internal('SMALLINT'::regtype, 32768::bigint);
-ERROR:  invalid interval: must be between 1 and 32767
-SELECT test.interval_to_internal('TEXT'::regtype, 32768::bigint);
-ERROR:  invalid type for dimension "testcol"
-SELECT test.interval_to_internal('INT'::regtype, INTERVAL '1 day');
-ERROR:  invalid interval type for integer dimension
-\set ON_ERROR_STOP 1
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/triggers.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/triggers.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/triggers.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/triggers.out	2023-11-25 05:27:44.133022695 +0000
@@ -1,445 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-CREATE TABLE hyper (
-  time BIGINT NOT NULL,
-  device_id TEXT NOT NULL,
-  sensor_1 NUMERIC NULL DEFAULT 1
-);
-CREATE OR REPLACE FUNCTION test_trigger()
-    RETURNS TRIGGER LANGUAGE PLPGSQL AS
-$BODY$
-DECLARE
-    cnt INTEGER;
-BEGIN
-    SELECT count(*) INTO cnt FROM hyper;
-    RAISE WARNING 'FIRING trigger when: % level: % op: % cnt: % trigger_name %',
-        tg_when, tg_level, tg_op, cnt, tg_name;
-
-    IF TG_OP = 'DELETE' THEN
-        RETURN OLD;
-    END IF;
-    RETURN NEW;
-END
-$BODY$;
--- row triggers: BEFORE
-CREATE TRIGGER _0_test_trigger_insert
-    BEFORE INSERT ON hyper
-    FOR EACH ROW EXECUTE FUNCTION test_trigger();
-CREATE TRIGGER _0_test_trigger_update
-    BEFORE UPDATE ON hyper
-    FOR EACH ROW EXECUTE FUNCTION test_trigger();
-CREATE TRIGGER _0_test_trigger_delete
-    BEFORE delete ON hyper
-    FOR EACH ROW EXECUTE FUNCTION test_trigger();
-CREATE TRIGGER z_test_trigger_all
-    BEFORE INSERT OR UPDATE OR DELETE ON hyper
-    FOR EACH ROW EXECUTE FUNCTION test_trigger();
--- row triggers: AFTER
-CREATE TRIGGER _0_test_trigger_insert_after
-    AFTER INSERT ON hyper
-    FOR EACH ROW EXECUTE FUNCTION test_trigger();
-CREATE TRIGGER _0_test_trigger_insert_after_when_dev1
-    AFTER INSERT ON hyper
-    FOR EACH ROW
-    WHEN (NEW.device_id = 'dev1')
-    EXECUTE FUNCTION test_trigger();
-CREATE TRIGGER _0_test_trigger_update_after
-    AFTER UPDATE ON hyper
-    FOR EACH ROW EXECUTE FUNCTION test_trigger();
-CREATE TRIGGER _0_test_trigger_delete_after
-    AFTER delete ON hyper
-    FOR EACH ROW EXECUTE FUNCTION test_trigger();
-CREATE TRIGGER z_test_trigger_all_after
-    AFTER INSERT OR UPDATE OR DELETE ON hyper
-    FOR EACH ROW EXECUTE FUNCTION test_trigger();
--- statement triggers: BEFORE
-CREATE TRIGGER _0_test_trigger_insert_s_before
-    BEFORE INSERT ON hyper
-    FOR EACH STATEMENT EXECUTE FUNCTION test_trigger();
-CREATE TRIGGER _0_test_trigger_update_s_before
-    BEFORE UPDATE ON hyper
-    FOR EACH STATEMENT EXECUTE FUNCTION test_trigger();
-CREATE TRIGGER _0_test_trigger_delete_s_before
-    BEFORE DELETE ON hyper
-    FOR EACH STATEMENT EXECUTE FUNCTION test_trigger();
--- statement triggers: AFTER
-CREATE TRIGGER _0_test_trigger_insert_s_after
-    AFTER INSERT ON hyper
-    FOR EACH STATEMENT EXECUTE FUNCTION test_trigger();
-CREATE TRIGGER _0_test_trigger_update_s_after
-    AFTER UPDATE ON hyper
-    FOR EACH STATEMENT EXECUTE FUNCTION test_trigger();
-CREATE TRIGGER _0_test_trigger_delete_s_after
-    AFTER DELETE ON hyper
-    FOR EACH STATEMENT EXECUTE FUNCTION test_trigger();
--- CONSTRAINT TRIGGER
-CREATE CONSTRAINT TRIGGER _0_test_trigger_constraint_insert
-  AFTER INSERT ON hyper FOR EACH ROW EXECUTE FUNCTION test_trigger();
-CREATE CONSTRAINT TRIGGER _0_test_trigger_constraint_update
-  AFTER UPDATE ON hyper FOR EACH ROW EXECUTE FUNCTION test_trigger();
-CREATE CONSTRAINT TRIGGER _0_test_trigger_constraint_delete
-  AFTER DELETE ON hyper FOR EACH ROW EXECUTE FUNCTION test_trigger();
-SELECT * FROM create_hypertable('hyper', 'time', chunk_time_interval => 10);
- hypertable_id | schema_name | table_name | created 
----------------+-------------+------------+---------
-             1 | public      | hyper      | t
-(1 row)
-
---test triggers before create_hypertable
-INSERT INTO hyper(time, device_id,sensor_1) VALUES
-(1257987600000000000, 'dev1', 1);
-WARNING:  FIRING trigger when: BEFORE level: STATEMENT op: INSERT cnt: 0 trigger_name _0_test_trigger_insert_s_before
-WARNING:  FIRING trigger when: BEFORE level: ROW op: INSERT cnt: 0 trigger_name _0_test_trigger_insert
-WARNING:  FIRING trigger when: BEFORE level: ROW op: INSERT cnt: 0 trigger_name z_test_trigger_all
-WARNING:  FIRING trigger when: AFTER level: ROW op: INSERT cnt: 1 trigger_name _0_test_trigger_constraint_insert
-WARNING:  FIRING trigger when: AFTER level: ROW op: INSERT cnt: 1 trigger_name _0_test_trigger_insert_after
-WARNING:  FIRING trigger when: AFTER level: ROW op: INSERT cnt: 1 trigger_name _0_test_trigger_insert_after_when_dev1
-WARNING:  FIRING trigger when: AFTER level: ROW op: INSERT cnt: 1 trigger_name z_test_trigger_all_after
-WARNING:  FIRING trigger when: AFTER level: STATEMENT op: INSERT cnt: 1 trigger_name _0_test_trigger_insert_s_after
-INSERT INTO hyper(time, device_id,sensor_1) VALUES
-(1257987700000000000, 'dev2', 1), (1257987800000000000, 'dev2', 1);
-WARNING:  FIRING trigger when: BEFORE level: STATEMENT op: INSERT cnt: 1 trigger_name _0_test_trigger_insert_s_before
-WARNING:  FIRING trigger when: BEFORE level: ROW op: INSERT cnt: 1 trigger_name _0_test_trigger_insert
-WARNING:  FIRING trigger when: BEFORE level: ROW op: INSERT cnt: 1 trigger_name z_test_trigger_all
-WARNING:  FIRING trigger when: BEFORE level: ROW op: INSERT cnt: 2 trigger_name _0_test_trigger_insert
-WARNING:  FIRING trigger when: BEFORE level: ROW op: INSERT cnt: 2 trigger_name z_test_trigger_all
-WARNING:  FIRING trigger when: AFTER level: ROW op: INSERT cnt: 3 trigger_name _0_test_trigger_constraint_insert
-WARNING:  FIRING trigger when: AFTER level: ROW op: INSERT cnt: 3 trigger_name _0_test_trigger_insert_after
-WARNING:  FIRING trigger when: AFTER level: ROW op: INSERT cnt: 3 trigger_name z_test_trigger_all_after
-WARNING:  FIRING trigger when: AFTER level: ROW op: INSERT cnt: 3 trigger_name _0_test_trigger_constraint_insert
-WARNING:  FIRING trigger when: AFTER level: ROW op: INSERT cnt: 3 trigger_name _0_test_trigger_insert_after
-WARNING:  FIRING trigger when: AFTER level: ROW op: INSERT cnt: 3 trigger_name z_test_trigger_all_after
-WARNING:  FIRING trigger when: AFTER level: STATEMENT op: INSERT cnt: 3 trigger_name _0_test_trigger_insert_s_after
-UPDATE hyper SET sensor_1 = 2;
-WARNING:  FIRING trigger when: BEFORE level: STATEMENT op: UPDATE cnt: 3 trigger_name _0_test_trigger_update_s_before
-WARNING:  FIRING trigger when: BEFORE level: ROW op: UPDATE cnt: 3 trigger_name _0_test_trigger_update
-WARNING:  FIRING trigger when: BEFORE level: ROW op: UPDATE cnt: 3 trigger_name z_test_trigger_all
-WARNING:  FIRING trigger when: BEFORE level: ROW op: UPDATE cnt: 3 trigger_name _0_test_trigger_update
-WARNING:  FIRING trigger when: BEFORE level: ROW op: UPDATE cnt: 3 trigger_name z_test_trigger_all
-WARNING:  FIRING trigger when: BEFORE level: ROW op: UPDATE cnt: 3 trigger_name _0_test_trigger_update
-WARNING:  FIRING trigger when: BEFORE level: ROW op: UPDATE cnt: 3 trigger_name z_test_trigger_all
-WARNING:  FIRING trigger when: AFTER level: ROW op: UPDATE cnt: 3 trigger_name _0_test_trigger_constraint_update
-WARNING:  FIRING trigger when: AFTER level: ROW op: UPDATE cnt: 3 trigger_name _0_test_trigger_update_after
-WARNING:  FIRING trigger when: AFTER level: ROW op: UPDATE cnt: 3 trigger_name z_test_trigger_all_after
-WARNING:  FIRING trigger when: AFTER level: ROW op: UPDATE cnt: 3 trigger_name _0_test_trigger_constraint_update
-WARNING:  FIRING trigger when: AFTER level: ROW op: UPDATE cnt: 3 trigger_name _0_test_trigger_update_after
-WARNING:  FIRING trigger when: AFTER level: ROW op: UPDATE cnt: 3 trigger_name z_test_trigger_all_after
-WARNING:  FIRING trigger when: AFTER level: ROW op: UPDATE cnt: 3 trigger_name _0_test_trigger_constraint_update
-WARNING:  FIRING trigger when: AFTER level: ROW op: UPDATE cnt: 3 trigger_name _0_test_trigger_update_after
-WARNING:  FIRING trigger when: AFTER level: ROW op: UPDATE cnt: 3 trigger_name z_test_trigger_all_after
-WARNING:  FIRING trigger when: AFTER level: STATEMENT op: UPDATE cnt: 3 trigger_name _0_test_trigger_update_s_after
-DELETE FROM hyper;
-WARNING:  FIRING trigger when: BEFORE level: STATEMENT op: DELETE cnt: 3 trigger_name _0_test_trigger_delete_s_before
-WARNING:  FIRING trigger when: BEFORE level: ROW op: DELETE cnt: 3 trigger_name _0_test_trigger_delete
-WARNING:  FIRING trigger when: BEFORE level: ROW op: DELETE cnt: 3 trigger_name z_test_trigger_all
-WARNING:  FIRING trigger when: BEFORE level: ROW op: DELETE cnt: 2 trigger_name _0_test_trigger_delete
-WARNING:  FIRING trigger when: BEFORE level: ROW op: DELETE cnt: 2 trigger_name z_test_trigger_all
-WARNING:  FIRING trigger when: BEFORE level: ROW op: DELETE cnt: 1 trigger_name _0_test_trigger_delete
-WARNING:  FIRING trigger when: BEFORE level: ROW op: DELETE cnt: 1 trigger_name z_test_trigger_all
-WARNING:  FIRING trigger when: AFTER level: ROW op: DELETE cnt: 0 trigger_name _0_test_trigger_constraint_delete
-WARNING:  FIRING trigger when: AFTER level: ROW op: DELETE cnt: 0 trigger_name _0_test_trigger_delete_after
-WARNING:  FIRING trigger when: AFTER level: ROW op: DELETE cnt: 0 trigger_name z_test_trigger_all_after
-WARNING:  FIRING trigger when: AFTER level: ROW op: DELETE cnt: 0 trigger_name _0_test_trigger_constraint_delete
-WARNING:  FIRING trigger when: AFTER level: ROW op: DELETE cnt: 0 trigger_name _0_test_trigger_delete_after
-WARNING:  FIRING trigger when: AFTER level: ROW op: DELETE cnt: 0 trigger_name z_test_trigger_all_after
-WARNING:  FIRING trigger when: AFTER level: ROW op: DELETE cnt: 0 trigger_name _0_test_trigger_constraint_delete
-WARNING:  FIRING trigger when: AFTER level: ROW op: DELETE cnt: 0 trigger_name _0_test_trigger_delete_after
-WARNING:  FIRING trigger when: AFTER level: ROW op: DELETE cnt: 0 trigger_name z_test_trigger_all_after
-WARNING:  FIRING trigger when: AFTER level: STATEMENT op: DELETE cnt: 0 trigger_name _0_test_trigger_delete_s_after
---test drop trigger
-DROP TRIGGER _0_test_trigger_insert ON hyper;
-DROP TRIGGER _0_test_trigger_insert_s_before ON hyper;
-DROP TRIGGER _0_test_trigger_insert_after ON hyper;
-DROP TRIGGER _0_test_trigger_insert_s_after ON hyper;
-INSERT INTO hyper(time, device_id,sensor_1) VALUES
-(1257987600000000000, 'dev1', 1);
-WARNING:  FIRING trigger when: BEFORE level: ROW op: INSERT cnt: 0 trigger_name z_test_trigger_all
-WARNING:  FIRING trigger when: AFTER level: ROW op: INSERT cnt: 1 trigger_name _0_test_trigger_constraint_insert
-WARNING:  FIRING trigger when: AFTER level: ROW op: INSERT cnt: 1 trigger_name _0_test_trigger_insert_after_when_dev1
-WARNING:  FIRING trigger when: AFTER level: ROW op: INSERT cnt: 1 trigger_name z_test_trigger_all_after
-INSERT INTO hyper(time, device_id,sensor_1) VALUES
-(1257987700000000000, 'dev2', 1), (1257987800000000000, 'dev2', 1);
-WARNING:  FIRING trigger when: BEFORE level: ROW op: INSERT cnt: 1 trigger_name z_test_trigger_all
-WARNING:  FIRING trigger when: BEFORE level: ROW op: INSERT cnt: 2 trigger_name z_test_trigger_all
-WARNING:  FIRING trigger when: AFTER level: ROW op: INSERT cnt: 3 trigger_name _0_test_trigger_constraint_insert
-WARNING:  FIRING trigger when: AFTER level: ROW op: INSERT cnt: 3 trigger_name z_test_trigger_all_after
-WARNING:  FIRING trigger when: AFTER level: ROW op: INSERT cnt: 3 trigger_name _0_test_trigger_constraint_insert
-WARNING:  FIRING trigger when: AFTER level: ROW op: INSERT cnt: 3 trigger_name z_test_trigger_all_after
-DROP TRIGGER _0_test_trigger_update ON hyper;
-DROP TRIGGER _0_test_trigger_update_s_before ON hyper;
-DROP TRIGGER _0_test_trigger_update_after ON hyper;
-DROP TRIGGER _0_test_trigger_update_s_after ON hyper;
-UPDATE hyper SET sensor_1 = 2;
-WARNING:  FIRING trigger when: BEFORE level: ROW op: UPDATE cnt: 3 trigger_name z_test_trigger_all
-WARNING:  FIRING trigger when: BEFORE level: ROW op: UPDATE cnt: 3 trigger_name z_test_trigger_all
-WARNING:  FIRING trigger when: BEFORE level: ROW op: UPDATE cnt: 3 trigger_name z_test_trigger_all
-WARNING:  FIRING trigger when: AFTER level: ROW op: UPDATE cnt: 3 trigger_name _0_test_trigger_constraint_update
-WARNING:  FIRING trigger when: AFTER level: ROW op: UPDATE cnt: 3 trigger_name z_test_trigger_all_after
-WARNING:  FIRING trigger when: AFTER level: ROW op: UPDATE cnt: 3 trigger_name _0_test_trigger_constraint_update
-WARNING:  FIRING trigger when: AFTER level: ROW op: UPDATE cnt: 3 trigger_name z_test_trigger_all_after
-WARNING:  FIRING trigger when: AFTER level: ROW op: UPDATE cnt: 3 trigger_name _0_test_trigger_constraint_update
-WARNING:  FIRING trigger when: AFTER level: ROW op: UPDATE cnt: 3 trigger_name z_test_trigger_all_after
-DROP TRIGGER _0_test_trigger_delete ON hyper;
-DROP TRIGGER _0_test_trigger_delete_s_before ON hyper;
-DROP TRIGGER _0_test_trigger_delete_after ON hyper;
-DROP TRIGGER _0_test_trigger_delete_s_after ON hyper;
-DELETE FROM hyper;
-WARNING:  FIRING trigger when: BEFORE level: ROW op: DELETE cnt: 3 trigger_name z_test_trigger_all
-WARNING:  FIRING trigger when: BEFORE level: ROW op: DELETE cnt: 2 trigger_name z_test_trigger_all
-WARNING:  FIRING trigger when: BEFORE level: ROW op: DELETE cnt: 1 trigger_name z_test_trigger_all
-WARNING:  FIRING trigger when: AFTER level: ROW op: DELETE cnt: 0 trigger_name _0_test_trigger_constraint_delete
-WARNING:  FIRING trigger when: AFTER level: ROW op: DELETE cnt: 0 trigger_name z_test_trigger_all_after
-WARNING:  FIRING trigger when: AFTER level: ROW op: DELETE cnt: 0 trigger_name _0_test_trigger_constraint_delete
-WARNING:  FIRING trigger when: AFTER level: ROW op: DELETE cnt: 0 trigger_name z_test_trigger_all_after
-WARNING:  FIRING trigger when: AFTER level: ROW op: DELETE cnt: 0 trigger_name _0_test_trigger_constraint_delete
-WARNING:  FIRING trigger when: AFTER level: ROW op: DELETE cnt: 0 trigger_name z_test_trigger_all_after
-DROP TRIGGER z_test_trigger_all ON hyper;
-DROP TRIGGER z_test_trigger_all_after ON hyper;
---test create trigger on hypertable
--- row triggers: BEFORE
-CREATE TRIGGER _0_test_trigger_insert
-    BEFORE INSERT ON hyper
-    FOR EACH ROW EXECUTE FUNCTION test_trigger();
-CREATE TRIGGER _0_test_trigger_update
-    BEFORE UPDATE ON hyper
-    FOR EACH ROW EXECUTE FUNCTION test_trigger();
-CREATE TRIGGER _0_test_trigger_delete
-    BEFORE delete ON hyper
-    FOR EACH ROW EXECUTE FUNCTION test_trigger();
-CREATE TRIGGER z_test_trigger_all
-    BEFORE INSERT OR UPDATE OR DELETE ON hyper
-    FOR EACH ROW EXECUTE FUNCTION test_trigger();
--- row triggers: AFTER
-CREATE TRIGGER _0_test_trigger_insert_after
-    AFTER INSERT ON hyper
-    FOR EACH ROW EXECUTE FUNCTION test_trigger();
-CREATE TRIGGER _0_test_trigger_update_after
-    AFTER UPDATE ON hyper
-    FOR EACH ROW EXECUTE FUNCTION test_trigger();
-CREATE TRIGGER _0_test_trigger_delete_after
-    AFTER delete ON hyper
-    FOR EACH ROW EXECUTE FUNCTION test_trigger();
-CREATE TRIGGER z_test_trigger_all_after
-    AFTER INSERT OR UPDATE OR DELETE ON hyper
-    FOR EACH ROW EXECUTE FUNCTION test_trigger();
--- statement triggers: BEFORE
-CREATE TRIGGER _0_test_trigger_insert_s_before
-    BEFORE INSERT ON hyper
-    FOR EACH STATEMENT EXECUTE FUNCTION test_trigger();
-CREATE TRIGGER _0_test_trigger_update_s_before
-    BEFORE UPDATE ON hyper
-    FOR EACH STATEMENT EXECUTE FUNCTION test_trigger();
-CREATE TRIGGER _0_test_trigger_delete_s_before
-    BEFORE DELETE ON hyper
-    FOR EACH STATEMENT EXECUTE FUNCTION test_trigger();
--- statement triggers: AFTER
-CREATE TRIGGER _0_test_trigger_insert_s_after
-    AFTER INSERT ON hyper
-    FOR EACH STATEMENT EXECUTE FUNCTION test_trigger();
-CREATE TRIGGER _0_test_trigger_update_s_after
-    AFTER UPDATE ON hyper
-    FOR EACH STATEMENT EXECUTE FUNCTION test_trigger();
-CREATE TRIGGER _0_test_trigger_delete_s_after
-    AFTER DELETE ON hyper
-    FOR EACH STATEMENT EXECUTE FUNCTION test_trigger();
-INSERT INTO hyper(time, device_id,sensor_1) VALUES
-(1257987600000000000, 'dev1', 1);
-WARNING:  FIRING trigger when: BEFORE level: STATEMENT op: INSERT cnt: 0 trigger_name _0_test_trigger_insert_s_before
-WARNING:  FIRING trigger when: BEFORE level: ROW op: INSERT cnt: 0 trigger_name _0_test_trigger_insert
-WARNING:  FIRING trigger when: BEFORE level: ROW op: INSERT cnt: 0 trigger_name z_test_trigger_all
-WARNING:  FIRING trigger when: AFTER level: ROW op: INSERT cnt: 1 trigger_name _0_test_trigger_constraint_insert
-WARNING:  FIRING trigger when: AFTER level: ROW op: INSERT cnt: 1 trigger_name _0_test_trigger_insert_after
-WARNING:  FIRING trigger when: AFTER level: ROW op: INSERT cnt: 1 trigger_name _0_test_trigger_insert_after_when_dev1
-WARNING:  FIRING trigger when: AFTER level: ROW op: INSERT cnt: 1 trigger_name z_test_trigger_all_after
-WARNING:  FIRING trigger when: AFTER level: STATEMENT op: INSERT cnt: 1 trigger_name _0_test_trigger_insert_s_after
-INSERT INTO hyper(time, device_id,sensor_1) VALUES
-(1257987700000000000, 'dev2', 1), (1257987800000000000, 'dev2', 1);
-WARNING:  FIRING trigger when: BEFORE level: STATEMENT op: INSERT cnt: 1 trigger_name _0_test_trigger_insert_s_before
-WARNING:  FIRING trigger when: BEFORE level: ROW op: INSERT cnt: 1 trigger_name _0_test_trigger_insert
-WARNING:  FIRING trigger when: BEFORE level: ROW op: INSERT cnt: 1 trigger_name z_test_trigger_all
-WARNING:  FIRING trigger when: BEFORE level: ROW op: INSERT cnt: 2 trigger_name _0_test_trigger_insert
-WARNING:  FIRING trigger when: BEFORE level: ROW op: INSERT cnt: 2 trigger_name z_test_trigger_all
-WARNING:  FIRING trigger when: AFTER level: ROW op: INSERT cnt: 3 trigger_name _0_test_trigger_constraint_insert
-WARNING:  FIRING trigger when: AFTER level: ROW op: INSERT cnt: 3 trigger_name _0_test_trigger_insert_after
-WARNING:  FIRING trigger when: AFTER level: ROW op: INSERT cnt: 3 trigger_name z_test_trigger_all_after
-WARNING:  FIRING trigger when: AFTER level: ROW op: INSERT cnt: 3 trigger_name _0_test_trigger_constraint_insert
-WARNING:  FIRING trigger when: AFTER level: ROW op: INSERT cnt: 3 trigger_name _0_test_trigger_insert_after
-WARNING:  FIRING trigger when: AFTER level: ROW op: INSERT cnt: 3 trigger_name z_test_trigger_all_after
-WARNING:  FIRING trigger when: AFTER level: STATEMENT op: INSERT cnt: 3 trigger_name _0_test_trigger_insert_s_after
-UPDATE hyper SET sensor_1 = 2;
-WARNING:  FIRING trigger when: BEFORE level: STATEMENT op: UPDATE cnt: 3 trigger_name _0_test_trigger_update_s_before
-WARNING:  FIRING trigger when: BEFORE level: ROW op: UPDATE cnt: 3 trigger_name _0_test_trigger_update
-WARNING:  FIRING trigger when: BEFORE level: ROW op: UPDATE cnt: 3 trigger_name z_test_trigger_all
-WARNING:  FIRING trigger when: BEFORE level: ROW op: UPDATE cnt: 3 trigger_name _0_test_trigger_update
-WARNING:  FIRING trigger when: BEFORE level: ROW op: UPDATE cnt: 3 trigger_name z_test_trigger_all
-WARNING:  FIRING trigger when: BEFORE level: ROW op: UPDATE cnt: 3 trigger_name _0_test_trigger_update
-WARNING:  FIRING trigger when: BEFORE level: ROW op: UPDATE cnt: 3 trigger_name z_test_trigger_all
-WARNING:  FIRING trigger when: AFTER level: ROW op: UPDATE cnt: 3 trigger_name _0_test_trigger_constraint_update
-WARNING:  FIRING trigger when: AFTER level: ROW op: UPDATE cnt: 3 trigger_name _0_test_trigger_update_after
-WARNING:  FIRING trigger when: AFTER level: ROW op: UPDATE cnt: 3 trigger_name z_test_trigger_all_after
-WARNING:  FIRING trigger when: AFTER level: ROW op: UPDATE cnt: 3 trigger_name _0_test_trigger_constraint_update
-WARNING:  FIRING trigger when: AFTER level: ROW op: UPDATE cnt: 3 trigger_name _0_test_trigger_update_after
-WARNING:  FIRING trigger when: AFTER level: ROW op: UPDATE cnt: 3 trigger_name z_test_trigger_all_after
-WARNING:  FIRING trigger when: AFTER level: ROW op: UPDATE cnt: 3 trigger_name _0_test_trigger_constraint_update
-WARNING:  FIRING trigger when: AFTER level: ROW op: UPDATE cnt: 3 trigger_name _0_test_trigger_update_after
-WARNING:  FIRING trigger when: AFTER level: ROW op: UPDATE cnt: 3 trigger_name z_test_trigger_all_after
-WARNING:  FIRING trigger when: AFTER level: STATEMENT op: UPDATE cnt: 3 trigger_name _0_test_trigger_update_s_after
-DELETE FROM hyper;
-WARNING:  FIRING trigger when: BEFORE level: STATEMENT op: DELETE cnt: 3 trigger_name _0_test_trigger_delete_s_before
-WARNING:  FIRING trigger when: BEFORE level: ROW op: DELETE cnt: 3 trigger_name _0_test_trigger_delete
-WARNING:  FIRING trigger when: BEFORE level: ROW op: DELETE cnt: 3 trigger_name z_test_trigger_all
-WARNING:  FIRING trigger when: BEFORE level: ROW op: DELETE cnt: 2 trigger_name _0_test_trigger_delete
-WARNING:  FIRING trigger when: BEFORE level: ROW op: DELETE cnt: 2 trigger_name z_test_trigger_all
-WARNING:  FIRING trigger when: BEFORE level: ROW op: DELETE cnt: 1 trigger_name _0_test_trigger_delete
-WARNING:  FIRING trigger when: BEFORE level: ROW op: DELETE cnt: 1 trigger_name z_test_trigger_all
-WARNING:  FIRING trigger when: AFTER level: ROW op: DELETE cnt: 0 trigger_name _0_test_trigger_constraint_delete
-WARNING:  FIRING trigger when: AFTER level: ROW op: DELETE cnt: 0 trigger_name _0_test_trigger_delete_after
-WARNING:  FIRING trigger when: AFTER level: ROW op: DELETE cnt: 0 trigger_name z_test_trigger_all_after
-WARNING:  FIRING trigger when: AFTER level: ROW op: DELETE cnt: 0 trigger_name _0_test_trigger_constraint_delete
-WARNING:  FIRING trigger when: AFTER level: ROW op: DELETE cnt: 0 trigger_name _0_test_trigger_delete_after
-WARNING:  FIRING trigger when: AFTER level: ROW op: DELETE cnt: 0 trigger_name z_test_trigger_all_after
-WARNING:  FIRING trigger when: AFTER level: ROW op: DELETE cnt: 0 trigger_name _0_test_trigger_constraint_delete
-WARNING:  FIRING trigger when: AFTER level: ROW op: DELETE cnt: 0 trigger_name _0_test_trigger_delete_after
-WARNING:  FIRING trigger when: AFTER level: ROW op: DELETE cnt: 0 trigger_name z_test_trigger_all_after
-WARNING:  FIRING trigger when: AFTER level: STATEMENT op: DELETE cnt: 0 trigger_name _0_test_trigger_delete_s_after
-CREATE TABLE vehicles (
-  vehicle_id INTEGER PRIMARY KEY,
-  vin_number CHAR(17),
-  last_checkup TIMESTAMP
-);
-CREATE TABLE color (
-  color_id INTEGER PRIMARY KEY,
-  notes text
-);
-CREATE TABLE location (
-  time TIMESTAMP NOT NULL,
-  vehicle_id INTEGER REFERENCES vehicles (vehicle_id),
-  color_id INTEGER, --no reference since gonna populate a hypertable
-  latitude FLOAT,
-  longitude FLOAT
-);
-CREATE OR REPLACE FUNCTION create_vehicle_trigger_fn()
-    RETURNS TRIGGER LANGUAGE PLPGSQL AS
-$BODY$
-BEGIN
-    INSERT INTO vehicles VALUES(NEW.vehicle_id, NULL, NULL) ON CONFLICT DO NOTHING;
-    RETURN NEW;
-END
-$BODY$;
-CREATE OR REPLACE FUNCTION create_color_trigger_fn()
-    RETURNS TRIGGER LANGUAGE PLPGSQL AS
-$BODY$
-BEGIN
-    --test subtxns within triggers
-    BEGIN
-        INSERT INTO color VALUES(NEW.color_id, 'n/a');
-    EXCEPTION WHEN unique_violation THEN
-            -- Nothing to do, just continue
-    END;
-    RETURN NEW;
-END
-$BODY$;
-CREATE TRIGGER create_color_trigger
-    BEFORE INSERT OR UPDATE ON location
-    FOR EACH ROW EXECUTE FUNCTION create_color_trigger_fn();
-SELECT create_hypertable('location', 'time');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-   create_hypertable   
------------------------
- (2,public,location,t)
-(1 row)
-
---make color also a hypertable
-SELECT create_hypertable('color', 'color_id', chunk_time_interval=>10);
- create_hypertable  
---------------------
- (3,public,color,t)
-(1 row)
-
--- Test that we can create and use triggers with another user
-GRANT TRIGGER, INSERT, SELECT, UPDATE ON location TO :ROLE_DEFAULT_PERM_USER_2;
-GRANT SELECT, INSERT, UPDATE ON color TO :ROLE_DEFAULT_PERM_USER_2;
-GRANT SELECT, INSERT, UPDATE ON vehicles TO :ROLE_DEFAULT_PERM_USER_2;
-\c :TEST_DBNAME :ROLE_DEFAULT_PERM_USER_2;
-CREATE TRIGGER create_vehicle_trigger
-    BEFORE INSERT OR UPDATE ON location
-    FOR EACH ROW EXECUTE FUNCTION create_vehicle_trigger_fn();
-INSERT INTO location VALUES('2017-01-01 01:02:03', 1, 1, 40.7493226,-73.9771259);
-INSERT INTO location VALUES('2017-01-01 01:02:04', 1, 20, 24.7493226,-73.9771259);
-INSERT INTO location VALUES('2017-01-01 01:02:03', 23, 1, 40.7493226,-73.9771269);
-INSERT INTO location VALUES('2017-01-01 01:02:03', 53, 20, 40.7493226,-73.9771269);
-UPDATE location SET vehicle_id = 52 WHERE vehicle_id = 53;
-SELECT * FROM location;
-           time           | vehicle_id | color_id |  latitude  |  longitude  
---------------------------+------------+----------+------------+-------------
- Sun Jan 01 01:02:03 2017 |          1 |        1 | 40.7493226 | -73.9771259
- Sun Jan 01 01:02:04 2017 |          1 |       20 | 24.7493226 | -73.9771259
- Sun Jan 01 01:02:03 2017 |         23 |        1 | 40.7493226 | -73.9771269
- Sun Jan 01 01:02:03 2017 |         52 |       20 | 40.7493226 | -73.9771269
-(4 rows)
-
-SELECT * FROM vehicles;
- vehicle_id | vin_number | last_checkup 
-------------+------------+--------------
-          1 |            | 
-         23 |            | 
-         53 |            | 
-         52 |            | 
-(4 rows)
-
-SELECT * FROM color;
- color_id | notes 
-----------+-------
-        1 | n/a
-       20 | n/a
-(2 rows)
-
--- switch back to default user to run some dropping tests
-\c :TEST_DBNAME :ROLE_DEFAULT_PERM_USER;
-\set ON_ERROR_STOP 0
--- test that disable trigger is disallowed
-ALTER TABLE location DISABLE TRIGGER create_vehicle_trigger;
-ERROR:  hypertables do not support  enabling or disabling triggers.
-\set ON_ERROR_STOP 1
--- test that drop trigger works
-DROP TRIGGER create_color_trigger ON location;
-DROP TRIGGER create_vehicle_trigger ON location;
--- test that drop trigger doesn't cause leftovers that mean that dropping chunks or hypertables no longer works
-SELECT count(1) FROM pg_depend d WHERE d.classid = 'pg_trigger'::regclass AND NOT EXISTS (SELECT 1 FROM pg_trigger WHERE oid = d.objid);
- count 
--------
-     0
-(1 row)
-
-DROP TABLE location;
--- test triggers with transition tables
--- test creating hypertable from table with triggers with transition tables
-CREATE TABLE transition_test(time timestamptz NOT NULL);
-CREATE TRIGGER t1 AFTER INSERT ON transition_test REFERENCING NEW TABLE AS new_trans FOR EACH STATEMENT EXECUTE FUNCTION test_trigger();
-\set ON_ERROR_STOP 0
-SELECT create_hypertable('transition_test','time');
-ERROR:  hypertables do not support transition tables in triggers
-\set ON_ERROR_STOP 1
-DROP TRIGGER t1 ON transition_test;
-SELECT create_hypertable('transition_test','time');
-      create_hypertable       
-------------------------------
- (4,public,transition_test,t)
-(1 row)
-
--- test creating trigger with transition tables on existing hypertable
-\set ON_ERROR_STOP 0
-CREATE TRIGGER t2 AFTER INSERT ON transition_test REFERENCING NEW TABLE AS new_trans FOR EACH STATEMENT EXECUTE FUNCTION test_trigger();
-ERROR:  trigger with transition tables not supported on hypertables
-CREATE TRIGGER t3 AFTER UPDATE ON transition_test REFERENCING NEW TABLE AS new_trans OLD TABLE AS old_trans FOR EACH STATEMENT EXECUTE FUNCTION test_trigger();
-ERROR:  trigger with transition tables not supported on hypertables
-CREATE TRIGGER t4 AFTER DELETE ON transition_test REFERENCING OLD TABLE AS old_trans FOR EACH STATEMENT EXECUTE FUNCTION test_trigger();
-ERROR:  trigger with transition tables not supported on hypertables
-CREATE TRIGGER t2 AFTER INSERT ON transition_test REFERENCING NEW TABLE AS new_trans FOR EACH ROW EXECUTE FUNCTION test_trigger();
-ERROR:  trigger with transition tables not supported on hypertables
-CREATE TRIGGER t3 AFTER UPDATE ON transition_test REFERENCING NEW TABLE AS new_trans OLD TABLE AS old_trans FOR EACH ROW EXECUTE FUNCTION test_trigger();
-ERROR:  trigger with transition tables not supported on hypertables
-CREATE TRIGGER t4 AFTER DELETE ON transition_test REFERENCING OLD TABLE AS old_trans FOR EACH ROW EXECUTE FUNCTION test_trigger();
-ERROR:  trigger with transition tables not supported on hypertables
-\set ON_ERROR_STOP 1
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/truncate.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/truncate.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/truncate.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/truncate.out	2023-11-25 05:27:44.141022672 +0000
@@ -1,269 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-\o /dev/null
-\ir include/insert_two_partitions.sql
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-CREATE TABLE PUBLIC."two_Partitions" (
-  "timeCustom" BIGINT NOT NULL,
-  device_id TEXT NOT NULL,
-  series_0 DOUBLE PRECISION NULL,
-  series_1 DOUBLE PRECISION NULL,
-  series_2 DOUBLE PRECISION NULL,
-  series_bool BOOLEAN NULL
-);
-CREATE INDEX ON PUBLIC."two_Partitions" (device_id, "timeCustom" DESC NULLS LAST) WHERE device_id IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, series_0) WHERE series_0 IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, series_1)  WHERE series_1 IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, series_2) WHERE series_2 IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, series_bool) WHERE series_bool IS NOT NULL;
-CREATE INDEX ON PUBLIC."two_Partitions" ("timeCustom" DESC NULLS LAST, device_id);
-SELECT * FROM create_hypertable('"public"."two_Partitions"'::regclass, 'timeCustom'::name, 'device_id'::name, associated_schema_name=>'_timescaledb_internal'::text, number_partitions => 2, chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
-\set QUIET off
-BEGIN;
-\COPY public."two_Partitions" FROM 'data/ds1_dev1_1.tsv' NULL AS '';
-COMMIT;
-INSERT INTO public."two_Partitions"("timeCustom", device_id, series_0, series_1) VALUES
-(1257987600000000000, 'dev1', 1.5, 1),
-(1257987600000000000, 'dev1', 1.5, 2),
-(1257894000000000000, 'dev2', 1.5, 1),
-(1257894002000000000, 'dev1', 2.5, 3);
-INSERT INTO "two_Partitions"("timeCustom", device_id, series_0, series_1) VALUES
-(1257894000000000000, 'dev2', 1.5, 2);
-\set QUIET on
-\o
-SELECT * FROM _timescaledb_catalog.hypertable;
- id | schema_name |   table_name   | associated_schema_name | associated_table_prefix | num_dimensions | chunk_sizing_func_schema |  chunk_sizing_func_name  | chunk_target_size | compression_state | compressed_hypertable_id | replication_factor 
-----+-------------+----------------+------------------------+-------------------------+----------------+--------------------------+--------------------------+-------------------+-------------------+--------------------------+--------------------
-  1 | public      | two_Partitions | _timescaledb_internal  | _hyper_1                |              2 | _timescaledb_internal    | calculate_chunk_interval |                 0 |                 0 |                          |                   
-(1 row)
-
-SELECT * FROM _timescaledb_catalog.chunk;
- id | hypertable_id |      schema_name      |    table_name    | compressed_chunk_id | dropped | status | osm_chunk 
-----+---------------+-----------------------+------------------+---------------------+---------+--------+-----------
-  1 |             1 | _timescaledb_internal | _hyper_1_1_chunk |                     | f       |      0 | f
-  2 |             1 | _timescaledb_internal | _hyper_1_2_chunk |                     | f       |      0 | f
-  3 |             1 | _timescaledb_internal | _hyper_1_3_chunk |                     | f       |      0 | f
-  4 |             1 | _timescaledb_internal | _hyper_1_4_chunk |                     | f       |      0 | f
-(4 rows)
-
-SELECT * FROM test.show_subtables('"two_Partitions"');
-                 Child                  | Tablespace 
-----------------------------------------+------------
- _timescaledb_internal._hyper_1_1_chunk | 
- _timescaledb_internal._hyper_1_2_chunk | 
- _timescaledb_internal._hyper_1_3_chunk | 
- _timescaledb_internal._hyper_1_4_chunk | 
-(4 rows)
-
-SELECT * FROM "two_Partitions";
-     timeCustom      | device_id | series_0 | series_1 | series_2 | series_bool 
----------------------+-----------+----------+----------+----------+-------------
- 1257894000000000000 | dev1      |      1.5 |        1 |        2 | t
- 1257894000000000000 | dev1      |      1.5 |        2 |          | 
- 1257894000000001000 | dev1      |      2.5 |        3 |          | 
- 1257894001000000000 | dev1      |      3.5 |        4 |          | 
- 1257894002000000000 | dev1      |      5.5 |        6 |          | t
- 1257894002000000000 | dev1      |      5.5 |        7 |          | f
- 1257894002000000000 | dev1      |      2.5 |        3 |          | 
- 1257897600000000000 | dev1      |      4.5 |        5 |          | f
- 1257987600000000000 | dev1      |      1.5 |        1 |          | 
- 1257987600000000000 | dev1      |      1.5 |        2 |          | 
- 1257894000000000000 | dev2      |      1.5 |        1 |          | 
- 1257894000000000000 | dev2      |      1.5 |        2 |          | 
-(12 rows)
-
-SET client_min_messages = WARNING;
-TRUNCATE "two_Partitions";
-SELECT * FROM _timescaledb_catalog.hypertable;
- id | schema_name |   table_name   | associated_schema_name | associated_table_prefix | num_dimensions | chunk_sizing_func_schema |  chunk_sizing_func_name  | chunk_target_size | compression_state | compressed_hypertable_id | replication_factor 
-----+-------------+----------------+------------------------+-------------------------+----------------+--------------------------+--------------------------+-------------------+-------------------+--------------------------+--------------------
-  1 | public      | two_Partitions | _timescaledb_internal  | _hyper_1                |              2 | _timescaledb_internal    | calculate_chunk_interval |                 0 |                 0 |                          |                   
-(1 row)
-
-SELECT * FROM _timescaledb_catalog.chunk;
- id | hypertable_id | schema_name | table_name | compressed_chunk_id | dropped | status | osm_chunk 
-----+---------------+-------------+------------+---------------------+---------+--------+-----------
-(0 rows)
-
--- should be empty
-SELECT * FROM test.show_subtables('"two_Partitions"');
- Child | Tablespace 
--------+------------
-(0 rows)
-
-SELECT * FROM "two_Partitions";
- timeCustom | device_id | series_0 | series_1 | series_2 | series_bool 
-------------+-----------+----------+----------+----------+-------------
-(0 rows)
-
-INSERT INTO public."two_Partitions"("timeCustom", device_id, series_0, series_1) VALUES
-(1257987600000000000, 'dev1', 1.5, 1),
-(1257987600000000000, 'dev1', 1.5, 2),
-(1257894000000000000, 'dev2', 1.5, 1),
-(1257894002000000000, 'dev1', 2.5, 3);
-SELECT * FROM _timescaledb_catalog.chunk;
- id | hypertable_id |      schema_name      |    table_name    | compressed_chunk_id | dropped | status | osm_chunk 
-----+---------------+-----------------------+------------------+---------------------+---------+--------+-----------
-  5 |             1 | _timescaledb_internal | _hyper_1_5_chunk |                     | f       |      0 | f
-  6 |             1 | _timescaledb_internal | _hyper_1_6_chunk |                     | f       |      0 | f
-  7 |             1 | _timescaledb_internal | _hyper_1_7_chunk |                     | f       |      0 | f
-(3 rows)
-
-CREATE VIEW dependent_view AS SELECT * FROM _timescaledb_internal._hyper_1_5_chunk;
-CREATE OR REPLACE FUNCTION test_trigger()
-    RETURNS TRIGGER LANGUAGE PLPGSQL AS
-$BODY$
-DECLARE
-    cnt INTEGER;
-BEGIN
-    RAISE WARNING 'FIRING trigger when: % level: % op: % cnt: % trigger_name %',
-          tg_when, tg_level, tg_op, cnt, tg_name;
-    IF TG_OP = 'DELETE' THEN
-        RETURN OLD;
-    END IF;
-    RETURN NEW;
-END
-$BODY$;
--- test truncate on a chunk
-CREATE TRIGGER _test_truncate_before
-    BEFORE TRUNCATE ON _timescaledb_internal._hyper_1_5_chunk
-    FOR EACH STATEMENT EXECUTE FUNCTION test_trigger();
-CREATE TRIGGER _test_truncate_after
-    AFTER TRUNCATE ON _timescaledb_internal._hyper_1_5_chunk
-    FOR EACH STATEMENT EXECUTE FUNCTION test_trigger();
-\set ON_ERROR_STOP 0
-TRUNCATE "two_Partitions";
-WARNING:  FIRING trigger when: BEFORE level: STATEMENT op: TRUNCATE cnt: <NULL> trigger_name _test_truncate_before
-WARNING:  FIRING trigger when: AFTER level: STATEMENT op: TRUNCATE cnt: <NULL> trigger_name _test_truncate_after
-ERROR:  cannot drop table _timescaledb_internal._hyper_1_5_chunk because other objects depend on it
--- cannot TRUNCATE ONLY a hypertable
-TRUNCATE ONLY "two_Partitions" CASCADE;
-ERROR:  cannot truncate only a hypertable
-\set ON_ERROR_STOP 1
--- create a regular table to make sure we can truncate it in the same call
-CREATE TABLE truncate_normal (color int);
-INSERT INTO truncate_normal VALUES (1);
-SELECT * FROM truncate_normal;
- color 
--------
-     1
-(1 row)
-
--- fix for bug #3580
-\set ON_ERROR_STOP 0
-TRUNCATE nonexistentrelation;
-ERROR:  relation "nonexistentrelation" does not exist
-\set ON_ERROR_STOP 1
-CREATE TABLE truncate_nested (color int);
-INSERT INTO truncate_nested VALUES (2);
-SELECT * FROM truncate_normal, truncate_nested;
- color | color 
--------+-------
-     1 |     2
-(1 row)
-
-CREATE FUNCTION fn_truncate_nested()
-RETURNS trigger LANGUAGE plpgsql
-AS $$
-BEGIN
-    TRUNCATE truncate_nested;
-    RETURN NEW;
-END;
-$$;
-CREATE TRIGGER tg_truncate_nested
-    BEFORE TRUNCATE ON truncate_normal
-    FOR EACH STATEMENT EXECUTE FUNCTION fn_truncate_nested();
-TRUNCATE truncate_normal;
-SELECT * FROM truncate_normal, truncate_nested;
- color | color 
--------+-------
-(0 rows)
-
-INSERT INTO truncate_normal VALUES (3);
-INSERT INTO truncate_nested VALUES (4);
-SELECT * FROM truncate_normal, truncate_nested;
- color | color 
--------+-------
-     3 |     4
-(1 row)
-
-TRUNCATE truncate_normal;
-SELECT * FROM truncate_normal, truncate_nested;
- color | color 
--------+-------
-(0 rows)
-
-INSERT INTO truncate_normal VALUES (5);
-INSERT INTO truncate_nested VALUES (6);
-SELECT * FROM truncate_normal, truncate_nested;
- color | color 
--------+-------
-     5 |     6
-(1 row)
-
-SELECT * FROM test.show_subtables('"two_Partitions"');
-                 Child                  | Tablespace 
-----------------------------------------+------------
- _timescaledb_internal._hyper_1_5_chunk | 
- _timescaledb_internal._hyper_1_6_chunk | 
- _timescaledb_internal._hyper_1_7_chunk | 
-(3 rows)
-
-TRUNCATE "two_Partitions", truncate_normal CASCADE;
-WARNING:  FIRING trigger when: BEFORE level: STATEMENT op: TRUNCATE cnt: <NULL> trigger_name _test_truncate_before
-WARNING:  FIRING trigger when: AFTER level: STATEMENT op: TRUNCATE cnt: <NULL> trigger_name _test_truncate_after
--- should be empty
-SELECT * FROM test.show_subtables('"two_Partitions"');
- Child | Tablespace 
--------+------------
-(0 rows)
-
-SELECT * FROM "two_Partitions";
- timeCustom | device_id | series_0 | series_1 | series_2 | series_bool 
-------------+-----------+----------+----------+----------+-------------
-(0 rows)
-
-SELECT * FROM truncate_normal, truncate_nested;
- color | color 
--------+-------
-(0 rows)
-
--- test TRUNCATE can be performed by a user
--- with TRUNCATE privilege who is not table owner
-\c :TEST_DBNAME :ROLE_SUPERUSER
-CREATE ROLE owner WITH LOGIN;
-CREATE ROLE truncator WITH LOGIN;
-CREATE DATABASE test_trunc_ht OWNER owner;
-\c test_trunc_ht :ROLE_SUPERUSER
-SET client_min_messages = ERROR;
-CREATE EXTENSION timescaledb;
-RESET client_min_messages;
-\c test_trunc_ht owner
-CREATE TABLE test_hypertable (time TIMESTAMP WITHOUT TIME ZONE NOT NULL, value DOUBLE PRECISION);
-SELECT create_hypertable('test_hypertable', 'time');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-      create_hypertable       
-------------------------------
- (1,public,test_hypertable,t)
-(1 row)
-
--- fail since we don't have TRUNCATE privileges yet
-\set ON_ERROR_STOP 0
-\c test_trunc_ht truncator
-TRUNCATE TABLE test_hypertable;
-ERROR:  permission denied for table test_hypertable
-\set ON_ERROR_STOP 1
-\c test_trunc_ht owner
-GRANT TRUNCATE ON test_hypertable TO truncator;
--- now succeed after privilege was granted
-\c test_trunc_ht truncator;
-TRUNCATE TABLE test_hypertable;
-\c :TEST_DBNAME :ROLE_SUPERUSER
--- set client_min_messages to ERROR to suppress warnings about orphaned files
-SET client_min_messages TO ERROR;
-DROP DATABASE test_trunc_ht;
-DROP ROLE owner;
-DROP ROLE truncator;
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/trusted_extension.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/trusted_extension.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/trusted_extension.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/trusted_extension.out	2023-11-25 05:27:44.149022649 +0000
@@ -1,56 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-\c :TEST_DBNAME :ROLE_SUPERUSER
-CREATE DATABASE trusted_test;
-GRANT CREATE ON DATABASE trusted_test TO :ROLE_1;
-\c trusted_test :ROLE_READ_ONLY
-\set ON_ERROR_STOP 0
-CREATE EXTENSION timescaledb;
-ERROR:  permission denied to create extension "timescaledb"
-\set ON_ERROR_STOP 1
-\c trusted_test :ROLE_1
--- user shouldn't have superuser privilege
-SELECT rolsuper FROM pg_roles WHERE rolname=user;
- rolsuper 
-----------
- f
-(1 row)
-
-SET client_min_messages TO ERROR;
-CREATE EXTENSION timescaledb;
-RESET client_min_messages;
-CREATE TABLE t(time timestamptz);
-SELECT create_hypertable('t','time');
-NOTICE:  adding not-null constraint to column "time"
- create_hypertable 
--------------------
- (1,public,t,t)
-(1 row)
-
-INSERT INTO t VALUES ('2000-01-01'), ('2001-01-01');
-SELECT * FROM t ORDER BY 1;
-             time             
-------------------------------
- Sat Jan 01 00:00:00 2000 PST
- Mon Jan 01 00:00:00 2001 PST
-(2 rows)
-
-SELECT * FROM timescaledb_information.hypertables;
- hypertable_schema | hypertable_name |    owner    | num_dimensions | num_chunks | compression_enabled | is_distributed | replication_factor | data_nodes | tablespaces 
--------------------+-----------------+-------------+----------------+------------+---------------------+----------------+--------------------+------------+-------------
- public            | t               | test_role_1 |              1 |          2 | f                   | f              |                    |            | 
-(1 row)
-
-\dt+ _timescaledb_internal._hyper_*
-                                            List of relations
-        Schema         |       Name       | Type  |    Owner    | Persistence |    Size    | Description 
------------------------+------------------+-------+-------------+-------------+------------+-------------
- _timescaledb_internal | _hyper_1_1_chunk | table | test_role_1 | permanent   | 8192 bytes | 
- _timescaledb_internal | _hyper_1_2_chunk | table | test_role_1 | permanent   | 8192 bytes | 
-(2 rows)
-
-DROP EXTENSION timescaledb CASCADE;
-NOTICE:  drop cascades to 3 other objects
-\c :TEST_DBNAME :ROLE_SUPERUSER
-DROP DATABASE trusted_test;
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/update-15.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/update-15.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/update-15.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/update-15.out	2023-11-25 05:27:44.129022707 +0000
@@ -1,164 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-\o /dev/null
-\ir include/insert_single.sql
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-CREATE TABLE PUBLIC."one_Partition" (
-  "timeCustom" BIGINT NOT NULL,
-  device_id TEXT NOT NULL,
-  series_0 DOUBLE PRECISION NULL,
-  series_1 DOUBLE PRECISION NULL,
-  series_2 DOUBLE PRECISION NULL,
-  series_bool BOOLEAN NULL
-);
-CREATE INDEX ON PUBLIC."one_Partition" (device_id, "timeCustom" DESC NULLS LAST) WHERE device_id IS NOT NULL;
-CREATE INDEX ON PUBLIC."one_Partition" ("timeCustom" DESC NULLS LAST, series_0) WHERE series_0 IS NOT NULL;
-CREATE INDEX ON PUBLIC."one_Partition" ("timeCustom" DESC NULLS LAST, series_1)  WHERE series_1 IS NOT NULL;
-CREATE INDEX ON PUBLIC."one_Partition" ("timeCustom" DESC NULLS LAST, series_2) WHERE series_2 IS NOT NULL;
-CREATE INDEX ON PUBLIC."one_Partition" ("timeCustom" DESC NULLS LAST, series_bool) WHERE series_bool IS NOT NULL;
-\c :DBNAME :ROLE_SUPERUSER
-CREATE SCHEMA "one_Partition" AUTHORIZATION :ROLE_DEFAULT_PERM_USER;
-\c :DBNAME :ROLE_DEFAULT_PERM_USER;
-SELECT * FROM create_hypertable('"public"."one_Partition"', 'timeCustom', associated_schema_name=>'one_Partition', chunk_time_interval=>_timescaledb_internal.interval_to_usec('1 month'));
---output command tags
-\set QUIET off
-BEGIN;
-\COPY "one_Partition" FROM 'data/ds1_dev1_1.tsv' NULL AS '';
-COMMIT;
-INSERT INTO "one_Partition"("timeCustom", device_id, series_0, series_1) VALUES
-(1257987600000000000, 'dev1', 1.5, 1),
-(1257987600000000000, 'dev1', 1.5, 2),
-(1257894000000000000, 'dev2', 1.5, 1),
-(1257894002000000000, 'dev1', 2.5, 3);
-INSERT INTO "one_Partition"("timeCustom", device_id, series_0, series_1) VALUES
-(1257894000000000000, 'dev2', 1.5, 2);
-\set QUIET on
-\o
--- Make sure UPDATE isn't optimized if it includes Append plans
--- Need to turn of nestloop to make append appear the same on PG96 and PG10
-set enable_nestloop = 'off';
-CREATE OR REPLACE FUNCTION series_val()
-RETURNS integer LANGUAGE PLPGSQL STABLE AS
-$BODY$
-BEGIN
-    RETURN 5;
-END;
-$BODY$;
--- ConstraintAwareAppend applied for SELECT
-EXPLAIN (costs off)
-SELECT FROM "one_Partition"
-WHERE series_1 IN (SELECT series_1 FROM "one_Partition" WHERE series_1 > series_val());
-                                                                  QUERY PLAN                                                                   
------------------------------------------------------------------------------------------------------------------------------------------------
- Hash Join
-   Hash Cond: ("one_Partition".series_1 = "one_Partition_1".series_1)
-   ->  Custom Scan (ChunkAppend) on "one_Partition"
-         Chunks excluded during startup: 0
-         ->  Index Only Scan using "_hyper_1_1_chunk_one_Partition_timeCustom_series_1_idx" on _hyper_1_1_chunk
-               Index Cond: (series_1 > (series_val())::double precision)
-         ->  Index Only Scan using "_hyper_1_2_chunk_one_Partition_timeCustom_series_1_idx" on _hyper_1_2_chunk
-               Index Cond: (series_1 > (series_val())::double precision)
-         ->  Index Only Scan using "_hyper_1_3_chunk_one_Partition_timeCustom_series_1_idx" on _hyper_1_3_chunk
-               Index Cond: (series_1 > (series_val())::double precision)
-   ->  Hash
-         ->  HashAggregate
-               Group Key: "one_Partition_1".series_1
-               ->  Custom Scan (ChunkAppend) on "one_Partition" "one_Partition_1"
-                     Chunks excluded during startup: 0
-                     ->  Index Only Scan using "_hyper_1_1_chunk_one_Partition_timeCustom_series_1_idx" on _hyper_1_1_chunk _hyper_1_1_chunk_1
-                           Index Cond: (series_1 > (series_val())::double precision)
-                     ->  Index Only Scan using "_hyper_1_2_chunk_one_Partition_timeCustom_series_1_idx" on _hyper_1_2_chunk _hyper_1_2_chunk_1
-                           Index Cond: (series_1 > (series_val())::double precision)
-                     ->  Index Only Scan using "_hyper_1_3_chunk_one_Partition_timeCustom_series_1_idx" on _hyper_1_3_chunk _hyper_1_3_chunk_1
-                           Index Cond: (series_1 > (series_val())::double precision)
-(21 rows)
-
--- ConstraintAwareAppend NOT applied for UPDATE
-EXPLAIN (costs off)
-UPDATE "one_Partition"
-SET series_1 = 8
-WHERE series_1 IN (SELECT series_1 FROM "one_Partition" WHERE series_1 > series_val());
-                                                                     QUERY PLAN                                                                      
------------------------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (HypertableModify)
-   ->  Update on "one_Partition"
-         Update on _hyper_1_1_chunk "one_Partition_2"
-         Update on _hyper_1_2_chunk "one_Partition_3"
-         Update on _hyper_1_3_chunk "one_Partition_4"
-         ->  Hash Join
-               Hash Cond: ("one_Partition".series_1 = "one_Partition_1".series_1)
-               ->  Append
-                     ->  Seq Scan on _hyper_1_1_chunk "one_Partition_2"
-                     ->  Seq Scan on _hyper_1_2_chunk "one_Partition_3"
-                     ->  Seq Scan on _hyper_1_3_chunk "one_Partition_4"
-               ->  Hash
-                     ->  HashAggregate
-                           Group Key: "one_Partition_1".series_1
-                           ->  Append
-                                 ->  Index Scan using "_hyper_1_1_chunk_one_Partition_timeCustom_series_1_idx" on _hyper_1_1_chunk "one_Partition_5"
-                                       Index Cond: (series_1 > (series_val())::double precision)
-                                 ->  Index Scan using "_hyper_1_2_chunk_one_Partition_timeCustom_series_1_idx" on _hyper_1_2_chunk "one_Partition_6"
-                                       Index Cond: (series_1 > (series_val())::double precision)
-                                 ->  Index Scan using "_hyper_1_3_chunk_one_Partition_timeCustom_series_1_idx" on _hyper_1_3_chunk "one_Partition_7"
-                                       Index Cond: (series_1 > (series_val())::double precision)
-(21 rows)
-
-SELECT * FROM "one_Partition" ORDER BY "timeCustom", device_id, series_0, series_1, series_2;
-     timeCustom      | device_id | series_0 | series_1 | series_2 | series_bool 
----------------------+-----------+----------+----------+----------+-------------
- 1257894000000000000 | dev1      |      1.5 |        1 |        2 | t
- 1257894000000000000 | dev1      |      1.5 |        2 |          | 
- 1257894000000000000 | dev2      |      1.5 |        1 |          | 
- 1257894000000000000 | dev2      |      1.5 |        2 |          | 
- 1257894000000001000 | dev1      |      2.5 |        3 |          | 
- 1257894001000000000 | dev1      |      3.5 |        4 |          | 
- 1257894002000000000 | dev1      |      2.5 |        3 |          | 
- 1257894002000000000 | dev1      |      5.5 |        6 |          | t
- 1257894002000000000 | dev1      |      5.5 |        7 |          | f
- 1257897600000000000 | dev1      |      4.5 |        5 |          | f
- 1257987600000000000 | dev1      |      1.5 |        1 |          | 
- 1257987600000000000 | dev1      |      1.5 |        2 |          | 
-(12 rows)
-
-UPDATE "one_Partition"
-SET series_1 = 8
-WHERE series_1 IN (SELECT series_1 FROM "one_Partition" WHERE series_1 > series_val());
-SELECT * FROM "one_Partition" ORDER BY "timeCustom", device_id, series_0, series_1, series_2;
-     timeCustom      | device_id | series_0 | series_1 | series_2 | series_bool 
----------------------+-----------+----------+----------+----------+-------------
- 1257894000000000000 | dev1      |      1.5 |        1 |        2 | t
- 1257894000000000000 | dev1      |      1.5 |        2 |          | 
- 1257894000000000000 | dev2      |      1.5 |        1 |          | 
- 1257894000000000000 | dev2      |      1.5 |        2 |          | 
- 1257894000000001000 | dev1      |      2.5 |        3 |          | 
- 1257894001000000000 | dev1      |      3.5 |        4 |          | 
- 1257894002000000000 | dev1      |      2.5 |        3 |          | 
- 1257894002000000000 | dev1      |      5.5 |        8 |          | f
- 1257894002000000000 | dev1      |      5.5 |        8 |          | t
- 1257897600000000000 | dev1      |      4.5 |        5 |          | f
- 1257987600000000000 | dev1      |      1.5 |        1 |          | 
- 1257987600000000000 | dev1      |      1.5 |        2 |          | 
-(12 rows)
-
-UPDATE "one_Partition" SET series_1 = 47;
-UPDATE "one_Partition" SET series_bool = true;
-SELECT * FROM "one_Partition" ORDER BY "timeCustom", device_id, series_0, series_1, series_2;
-     timeCustom      | device_id | series_0 | series_1 | series_2 | series_bool 
----------------------+-----------+----------+----------+----------+-------------
- 1257894000000000000 | dev1      |      1.5 |       47 |        2 | t
- 1257894000000000000 | dev1      |      1.5 |       47 |          | t
- 1257894000000000000 | dev2      |      1.5 |       47 |          | t
- 1257894000000000000 | dev2      |      1.5 |       47 |          | t
- 1257894000000001000 | dev1      |      2.5 |       47 |          | t
- 1257894001000000000 | dev1      |      3.5 |       47 |          | t
- 1257894002000000000 | dev1      |      2.5 |       47 |          | t
- 1257894002000000000 | dev1      |      5.5 |       47 |          | t
- 1257894002000000000 | dev1      |      5.5 |       47 |          | t
- 1257897600000000000 | dev1      |      4.5 |       47 |          | t
- 1257987600000000000 | dev1      |      1.5 |       47 |          | t
- 1257987600000000000 | dev1      |      1.5 |       47 |          | t
-(12 rows)
-
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/upsert.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/upsert.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/upsert.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/upsert.out	2023-11-25 05:27:44.133022695 +0000
@@ -1,624 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-CREATE TABLE upsert_test(time timestamp PRIMARY KEY, temp float, color text);
-SELECT create_hypertable('upsert_test', 'time');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-    create_hypertable     
---------------------------
- (1,public,upsert_test,t)
-(1 row)
-
-INSERT INTO upsert_test VALUES ('2017-01-20T09:00:01', 22.5, 'yellow') RETURNING *;
-           time           | temp | color  
---------------------------+------+--------
- Fri Jan 20 09:00:01 2017 | 22.5 | yellow
-(1 row)
-
-INSERT INTO upsert_test VALUES ('2017-01-20T09:00:01', 23.8, 'yellow') ON CONFLICT (time)
-DO UPDATE SET temp = 23.8 RETURNING *;
-           time           | temp | color  
---------------------------+------+--------
- Fri Jan 20 09:00:01 2017 | 23.8 | yellow
-(1 row)
-
-INSERT INTO upsert_test VALUES ('2017-01-20T09:00:01', 78.4, 'yellow') ON CONFLICT DO NOTHING;
-SELECT * FROM upsert_test;
-           time           | temp | color  
---------------------------+------+--------
- Fri Jan 20 09:00:01 2017 | 23.8 | yellow
-(1 row)
-
--- Test ON CONFLICT ON CONSTRAINT
-INSERT INTO upsert_test VALUES ('2017-01-20T09:00:01', 12.3, 'yellow') ON CONFLICT ON CONSTRAINT upsert_test_pkey
-DO UPDATE SET temp = 12.3 RETURNING time, temp, color;
-           time           | temp | color  
---------------------------+------+--------
- Fri Jan 20 09:00:01 2017 | 12.3 | yellow
-(1 row)
-
--- Test that update generates error on conflicts
-\set ON_ERROR_STOP 0
-INSERT INTO upsert_test VALUES ('2017-01-21T09:00:01', 22.5, 'yellow') RETURNING *;
-           time           | temp | color  
---------------------------+------+--------
- Sat Jan 21 09:00:01 2017 | 22.5 | yellow
-(1 row)
-
-UPDATE upsert_test SET time = '2017-01-20T09:00:01';
-ERROR:  duplicate key value violates unique constraint "1_1_upsert_test_pkey"
-\set ON_ERROR_STOP 1
--- Test with UNIQUE index on multiple columns instead of PRIMARY KEY constraint
-CREATE TABLE upsert_test_unique(time timestamp, temp float, color text);
-SELECT create_hypertable('upsert_test_unique', 'time');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-        create_hypertable        
----------------------------------
- (2,public,upsert_test_unique,t)
-(1 row)
-
-CREATE UNIQUE INDEX time_color_idx ON upsert_test_unique (time, color);
-INSERT INTO upsert_test_unique VALUES ('2017-01-20T09:00:01', 22.5, 'yellow') RETURNING *;
-           time           | temp | color  
---------------------------+------+--------
- Fri Jan 20 09:00:01 2017 | 22.5 | yellow
-(1 row)
-
-INSERT INTO upsert_test_unique VALUES ('2017-01-20T09:00:01', 21.2, 'brown');
-SELECT * FROM upsert_test_unique ORDER BY time, color DESC;
-           time           | temp | color  
---------------------------+------+--------
- Fri Jan 20 09:00:01 2017 | 22.5 | yellow
- Fri Jan 20 09:00:01 2017 | 21.2 | brown
-(2 rows)
-
-INSERT INTO upsert_test_unique VALUES ('2017-01-20T09:00:01', 31.8, 'yellow') ON CONFLICT (time, color)
-DO UPDATE SET temp = 31.8;
-INSERT INTO upsert_test_unique VALUES ('2017-01-20T09:00:01', 54.3, 'yellow') ON CONFLICT DO NOTHING;
-SELECT * FROM upsert_test_unique ORDER BY time, color DESC;
-           time           | temp | color  
---------------------------+------+--------
- Fri Jan 20 09:00:01 2017 | 31.8 | yellow
- Fri Jan 20 09:00:01 2017 | 21.2 | brown
-(2 rows)
-
--- Test with multiple UNIQUE indexes
-CREATE TABLE upsert_test_multi_unique(time timestamp, temp float, color text);
-SELECT create_hypertable('upsert_test_multi_unique', 'time');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-           create_hypertable           
----------------------------------------
- (3,public,upsert_test_multi_unique,t)
-(1 row)
-
-ALTER TABLE upsert_test_multi_unique ADD CONSTRAINT multi_time_temp UNIQUE (time, temp);
-CREATE UNIQUE INDEX multi_time_color_idx ON upsert_test_multi_unique (time, color);
-INSERT INTO upsert_test_multi_unique VALUES ('2017-01-20T09:00:01', 25.9, 'yellow');
-INSERT INTO upsert_test_multi_unique VALUES ('2017-01-21T09:00:01', 25.9, 'yellow');
-INSERT INTO upsert_test_multi_unique VALUES ('2017-01-20T09:00:01', 23.5, 'brown');
-INSERT INTO upsert_test_multi_unique VALUES ('2017-01-20T09:00:01', 25.9, 'purple') ON CONFLICT DO NOTHING;
-SELECT * FROM upsert_test_multi_unique ORDER BY time, color DESC;
-           time           | temp | color  
---------------------------+------+--------
- Fri Jan 20 09:00:01 2017 | 25.9 | yellow
- Fri Jan 20 09:00:01 2017 | 23.5 | brown
- Sat Jan 21 09:00:01 2017 | 25.9 | yellow
-(3 rows)
-
-INSERT INTO upsert_test_multi_unique VALUES ('2017-01-20T09:00:01', 25.9, 'blue') ON CONFLICT (time, temp)
-DO UPDATE SET color = 'blue';
-INSERT INTO upsert_test_multi_unique VALUES ('2017-01-20T09:00:01', 23.5, 'orange') ON CONFLICT ON CONSTRAINT multi_time_temp
-DO UPDATE SET color = excluded.color;
-SELECT * FROM upsert_test_multi_unique ORDER BY time, color DESC;
-           time           | temp | color  
---------------------------+------+--------
- Fri Jan 20 09:00:01 2017 | 23.5 | orange
- Fri Jan 20 09:00:01 2017 | 25.9 | blue
- Sat Jan 21 09:00:01 2017 | 25.9 | yellow
-(3 rows)
-
-INSERT INTO upsert_test_multi_unique VALUES ('2017-01-21T09:00:01', 45.7, 'yellow') ON CONFLICT (time, color)
-DO UPDATE SET temp = 45.7;
-SELECT * FROM upsert_test_multi_unique ORDER BY time, color DESC;
-           time           | temp | color  
---------------------------+------+--------
- Fri Jan 20 09:00:01 2017 | 23.5 | orange
- Fri Jan 20 09:00:01 2017 | 25.9 | blue
- Sat Jan 21 09:00:01 2017 | 45.7 | yellow
-(3 rows)
-
-\set ON_ERROR_STOP 0
--- Here the constraint in the ON CONFLICT clause is not the one that is
--- actually violated by the INSERT, so it should still fail.
-INSERT INTO upsert_test_multi_unique VALUES ('2017-01-20T09:00:01', 23.5, 'purple') ON CONFLICT (time, color)
-DO UPDATE set temp = 23.5;
-ERROR:  duplicate key value violates unique constraint "3_2_multi_time_temp"
-INSERT INTO upsert_test_multi_unique VALUES ('2017-01-20T09:00:01', 22.5, 'orange') ON CONFLICT ON CONSTRAINT multi_time_temp
-DO UPDATE set color = 'orange';
-ERROR:  duplicate key value violates unique constraint "_hyper_3_3_chunk_multi_time_color_idx"
-\set ON_ERROR_STOP 1
-CREATE TABLE upsert_test_space(time timestamp, device_id_1 char(20), to_drop int, temp float, color text);
---drop two columns; create one.
-ALTER TABLE upsert_test_space DROP to_drop;
-ALTER TABLE upsert_test_space DROP device_id_1, ADD device_id char(20);
-ALTER TABLE upsert_test_space ADD CONSTRAINT time_space_constraint UNIQUE (time, device_id);
-SELECT create_hypertable('upsert_test_space', 'time', 'device_id', 2, partitioning_func=>'_timescaledb_internal.get_partition_for_key'::regproc);
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-       create_hypertable        
---------------------------------
- (4,public,upsert_test_space,t)
-(1 row)
-
-INSERT INTO upsert_test_space (time, device_id, temp, color) VALUES ('2017-01-20T09:00:01', 'dev1', 25.9, 'yellow') RETURNING *;
-           time           | temp | color  |      device_id       
---------------------------+------+--------+----------------------
- Fri Jan 20 09:00:01 2017 | 25.9 | yellow | dev1                
-(1 row)
-
-INSERT INTO upsert_test_space (time, device_id, temp, color) VALUES ('2017-01-20T09:00:01', 'dev2', 25.9, 'yellow');
-INSERT INTO upsert_test_space (time, device_id, temp, color) VALUES ('2017-01-20T09:00:01', 'dev1', 23.5, 'green') ON CONFLICT (time, device_id)
-DO UPDATE SET color = excluded.color;
-INSERT INTO upsert_test_space (time, device_id, temp, color) VALUES ('2017-01-20T09:00:01', 'dev1', 23.5, 'orange') ON CONFLICT ON CONSTRAINT time_space_constraint
-DO UPDATE SET color = excluded.color;
-INSERT INTO upsert_test_space (time, device_id, temp, color) VALUES ('2017-01-20T09:00:01', 'dev2', 23.5, 'orange3') ON CONFLICT (time, device_id)
-DO UPDATE SET color = excluded.color||' (originally '|| upsert_test_space.color ||')' RETURNING *;
-           time           | temp |            color            |      device_id       
---------------------------+------+-----------------------------+----------------------
- Fri Jan 20 09:00:01 2017 | 25.9 | orange3 (originally yellow) | dev2                
-(1 row)
-
-INSERT INTO upsert_test_space (time, device_id, temp, color) VALUES ('2017-01-20T09:00:01', 'dev3', 23.5, 'orange3.1') ON CONFLICT (time, device_id)
-DO UPDATE SET color = excluded.color||' (originally '|| upsert_test_space.color ||')' RETURNING *;
-           time           | temp |   color   |      device_id       
---------------------------+------+-----------+----------------------
- Fri Jan 20 09:00:01 2017 | 23.5 | orange3.1 | dev3                
-(1 row)
-
-INSERT INTO upsert_test_space (time, device_id, temp, color) VALUES ('2017-01-20T09:00:01', 'dev2', 23.5, 'orange4') ON CONFLICT (time, device_id)
-DO NOTHING RETURNING *;
- time | temp | color | device_id 
-------+------+-------+-----------
-(0 rows)
-
-INSERT INTO upsert_test_space (time, device_id, temp, color) VALUES ('2017-01-20T09:00:01', 'dev4', 23.5, 'orange5') ON CONFLICT (time, device_id)
-DO NOTHING RETURNING *;
-           time           | temp |  color  |      device_id       
---------------------------+------+---------+----------------------
- Fri Jan 20 09:00:01 2017 | 23.5 | orange5 | dev4                
-(1 row)
-
-INSERT INTO upsert_test_space (time, device_id, temp, color) VALUES ('2017-01-20T09:00:01', 'dev5', 23.5, 'orange5') ON CONFLICT (time, device_id)
-DO NOTHING RETURNING *;
-           time           | temp |  color  |      device_id       
---------------------------+------+---------+----------------------
- Fri Jan 20 09:00:01 2017 | 23.5 | orange5 | dev5                
-(1 row)
-
-INSERT INTO upsert_test_space (time, device_id, temp, color) VALUES ('2017-01-20T09:00:01', 'dev5', 23.5, 'orange6') ON CONFLICT ON CONSTRAINT time_space_constraint
-DO NOTHING RETURNING *;
- time | temp | color | device_id 
-------+------+-------+-----------
-(0 rows)
-
---restore a column with the same name as a previously deleted one;
-ALTER TABLE upsert_test_space ADD device_id_1 char(20);
-INSERT INTO upsert_test_space (time, device_id, temp, color, device_id_1) VALUES ('2017-01-20T09:00:01', 'dev4', 23.5, 'orange5.1', 'dev-id-1') ON CONFLICT (time, device_id)
-DO UPDATE SET color = excluded.color||' (originally '|| upsert_test_space.color ||')' RETURNING *;
-           time           | temp |             color              |      device_id       | device_id_1 
---------------------------+------+--------------------------------+----------------------+-------------
- Fri Jan 20 09:00:01 2017 | 23.5 | orange5.1 (originally orange5) | dev4                 | 
-(1 row)
-
-INSERT INTO upsert_test_space (time, device_id, temp, color) VALUES ('2017-01-20T09:00:01', 'dev5', 23.5, 'orange6') ON CONFLICT (time, device_id)
-DO UPDATE SET color = excluded.color WHERE upsert_test_space.temp < 20 RETURNING *;
- time | temp | color | device_id | device_id_1 
-------+------+-------+-----------+-------------
-(0 rows)
-
-INSERT INTO upsert_test_space (time, device_id, temp, color) VALUES ('2017-01-20T09:00:01', 'dev5', 23.5, 'orange7') ON CONFLICT (time, device_id)
-DO UPDATE SET color = excluded.color WHERE excluded.temp < 20 RETURNING *;
- time | temp | color | device_id | device_id_1 
-------+------+-------+-----------+-------------
-(0 rows)
-
-INSERT INTO upsert_test_space (time, device_id, temp, color) VALUES ('2017-01-20T09:00:01', 'dev5', 3.5, 'orange7') ON CONFLICT (time, device_id)
-DO UPDATE SET color = excluded.color, temp=excluded.temp WHERE excluded.temp < 20 RETURNING *;
-           time           | temp |  color  |      device_id       | device_id_1 
---------------------------+------+---------+----------------------+-------------
- Fri Jan 20 09:00:01 2017 |  3.5 | orange7 | dev5                 | 
-(1 row)
-
-INSERT INTO upsert_test_space (time, device_id, temp, color) VALUES ('2017-01-20T09:00:01', 'dev5', 43.5, 'orange8') ON CONFLICT (time, device_id)
-DO UPDATE SET color = excluded.color WHERE upsert_test_space.temp < 20 RETURNING *;
-           time           | temp |  color  |      device_id       | device_id_1 
---------------------------+------+---------+----------------------+-------------
- Fri Jan 20 09:00:01 2017 |  3.5 | orange8 | dev5                 | 
-(1 row)
-
-INSERT INTO upsert_test_space (time, device_id, temp, color, device_id_1) VALUES ('2017-01-20T09:00:01', 'dev5', 43.5, 'orange8', 'device-id-1-new') ON CONFLICT (time, device_id)
-DO UPDATE SET device_id_1 = excluded.device_id_1 RETURNING *;
-           time           | temp |  color  |      device_id       |     device_id_1      
---------------------------+------+---------+----------------------+----------------------
- Fri Jan 20 09:00:01 2017 |  3.5 | orange8 | dev5                 | device-id-1-new     
-(1 row)
-
-INSERT INTO upsert_test_space (time, device_id, temp, color, device_id_1) VALUES ('2017-01-20T09:00:01', 'dev5', 43.5, 'orange8', 'device-id-1-new') ON CONFLICT (time, device_id)
-DO UPDATE SET device_id_1 = 'device-id-1-new-2', color = 'orange9'  RETURNING *;
-           time           | temp |  color  |      device_id       |     device_id_1      
---------------------------+------+---------+----------------------+----------------------
- Fri Jan 20 09:00:01 2017 |  3.5 | orange9 | dev5                 | device-id-1-new-2   
-(1 row)
-
-SELECT * FROM upsert_test_space;
-           time           | temp |             color              |      device_id       |     device_id_1      
---------------------------+------+--------------------------------+----------------------+----------------------
- Fri Jan 20 09:00:01 2017 | 25.9 | orange                         | dev1                 | 
- Fri Jan 20 09:00:01 2017 | 25.9 | orange3 (originally yellow)    | dev2                 | 
- Fri Jan 20 09:00:01 2017 | 23.5 | orange3.1                      | dev3                 | 
- Fri Jan 20 09:00:01 2017 | 23.5 | orange5.1 (originally orange5) | dev4                 | 
- Fri Jan 20 09:00:01 2017 |  3.5 | orange9                        | dev5                 | device-id-1-new-2   
-(5 rows)
-
-ALTER TABLE upsert_test_space DROP device_id_1, ADD device_id_2 char(20);
-INSERT INTO upsert_test_space (time, device_id, temp, color, device_id_2) VALUES ('2017-01-20T09:00:01', 'dev5', 43.5, 'orange8', 'device-id-2')
-ON CONFLICT (time, device_id)
-DO UPDATE SET device_id_2 = 'device-id-2-new', color = 'orange10' RETURNING *;
-           time           | temp |  color   |      device_id       |     device_id_2      
---------------------------+------+----------+----------------------+----------------------
- Fri Jan 20 09:00:01 2017 |  3.5 | orange10 | dev5                 | device-id-2-new     
-(1 row)
-
---test inserting to to a chunk already in the chunk dispatch cache again.
-INSERT INTO upsert_test_space as current (time, device_id, temp, color, device_id_2) VALUES ('2017-01-20T09:00:01', 'dev5', 43.5, 'orange8', 'device-id-2'),
-('2018-01-20T09:00:01', 'dev5', 43.5, 'orange8', 'device-id-2'),
-('2017-01-20T09:00:01', 'dev3', 43.5, 'orange7', 'device-id-2'),
-('2018-01-21T09:00:01', 'dev5', 43.5, 'orange9', 'device-id-2')
-ON CONFLICT (time, device_id)
-DO UPDATE SET device_id_2 = coalesce(excluded.device_id_2,current.device_id_2), color = coalesce(excluded.color,current.color) RETURNING *;
-           time           | temp |  color  |      device_id       |     device_id_2      
---------------------------+------+---------+----------------------+----------------------
- Fri Jan 20 09:00:01 2017 |  3.5 | orange8 | dev5                 | device-id-2         
- Sat Jan 20 09:00:01 2018 | 43.5 | orange8 | dev5                 | device-id-2         
- Fri Jan 20 09:00:01 2017 | 23.5 | orange7 | dev3                 | device-id-2         
- Sun Jan 21 09:00:01 2018 | 43.5 | orange9 | dev5                 | device-id-2         
-(4 rows)
-
-WITH CTE AS (
-    INSERT INTO upsert_test_multi_unique
-    VALUES ('2017-01-20T09:00:01', 25.9, 'purple')
-    ON CONFLICT DO NOTHING
-    RETURNING *
-) SELECT 1;
- ?column? 
-----------
-        1
-(1 row)
-
-WITH CTE AS (
-    INSERT INTO upsert_test_multi_unique
-    VALUES ('2017-01-20T09:00:01', 25.9, 'purple'),
-    ('2017-01-20T09:00:01', 29.9, 'purple1')
-    ON CONFLICT DO NOTHING
-    RETURNING *
-) SELECT * FROM CTE;
-           time           | temp |  color  
---------------------------+------+---------
- Fri Jan 20 09:00:01 2017 | 29.9 | purple1
-(1 row)
-
-WITH CTE AS (
-    INSERT INTO upsert_test_multi_unique
-    VALUES ('2017-01-20T09:00:01', 25.9, 'blue')
-    ON CONFLICT (time, temp) DO UPDATE SET color = 'blue'
-    RETURNING *
-)
-SELECT * FROM CTE;
-           time           | temp | color 
---------------------------+------+-------
- Fri Jan 20 09:00:01 2017 | 25.9 | blue
-(1 row)
-
---test error conditions when an index is dropped on a chunk
-DROP INDEX _timescaledb_internal._hyper_3_3_chunk_multi_time_color_idx;
---everything is ok if not used as an arbiter index
-INSERT INTO upsert_test_multi_unique
-VALUES ('2017-01-20T09:00:01', 25.9, 'purple')
-ON CONFLICT DO NOTHING
-RETURNING *;
- time | temp | color 
-------+------+-------
-(0 rows)
-
---errors out if used as an arbiter index
-\set ON_ERROR_STOP 0
-INSERT INTO upsert_test_multi_unique
-VALUES ('2017-01-20T09:00:01', 25.9, 'purple')
-ON CONFLICT (time, color) DO NOTHING
-RETURNING *;
-ERROR:  could not find arbiter index for hypertable index "multi_time_color_idx" on chunk "_hyper_3_3_chunk"
-\set ON_ERROR_STOP 1
---create table with one chunk that has a tup_conv_map and one that does not
---to ensure this, create a chunk before altering the table this chunk will not have a tup_conv_map
-CREATE TABLE upsert_test_diffchunk(time timestamp, device_id char(20), to_drop int, temp float, color text);
-SELECT create_hypertable('upsert_test_diffchunk', 'time', chunk_time_interval=> interval '1 month');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-         create_hypertable          
-------------------------------------
- (5,public,upsert_test_diffchunk,t)
-(1 row)
-
-CREATE UNIQUE INDEX time_device_idx ON upsert_test_diffchunk (time, device_id);
---this is the chunk with no tup_conv_map
-INSERT INTO upsert_test_diffchunk (time, device_id, temp, color) VALUES ('2017-01-20T09:00:01', 'dev1', 25.9, 'yellow') RETURNING *;
-           time           |      device_id       | to_drop | temp | color  
---------------------------+----------------------+---------+------+--------
- Fri Jan 20 09:00:01 2017 | dev1                 |         | 25.9 | yellow
-(1 row)
-
-INSERT INTO upsert_test_diffchunk (time, device_id, temp, color) VALUES ('2017-01-20T09:00:01', 'dev2', 25.9, 'yellow') RETURNING *;
-           time           |      device_id       | to_drop | temp | color  
---------------------------+----------------------+---------+------+--------
- Fri Jan 20 09:00:01 2017 | dev2                 |         | 25.9 | yellow
-(1 row)
-
---alter the table
-ALTER TABLE upsert_test_diffchunk DROP to_drop;
-ALTER TABLE upsert_test_diffchunk ADD device_id_2 char(20);
---new chunk that does have a tup conv map
-INSERT INTO upsert_test_diffchunk (time, device_id, temp, color) VALUES ('2019-01-20T09:00:01', 'dev1', 23.5, 'orange') ;
-INSERT INTO upsert_test_diffchunk (time, device_id, temp, color) VALUES ('2019-01-20T09:00:01', 'dev2', 23.5, 'orange') ;
-select * from upsert_test_diffchunk order by time, device_id;
-           time           |      device_id       | temp | color  | device_id_2 
---------------------------+----------------------+------+--------+-------------
- Fri Jan 20 09:00:01 2017 | dev1                 | 25.9 | yellow | 
- Fri Jan 20 09:00:01 2017 | dev2                 | 25.9 | yellow | 
- Sun Jan 20 09:00:01 2019 | dev1                 | 23.5 | orange | 
- Sun Jan 20 09:00:01 2019 | dev2                 | 23.5 | orange | 
-(4 rows)
-
---make sure current works
-INSERT INTO upsert_test_diffchunk as current (time, device_id, temp, color, device_id_2) VALUES
-('2019-01-20T09:00:01', 'dev1', 43.5, 'orange2', 'device-id-2'),
-('2017-01-20T09:00:01', 'dev1', 43.5, 'yellow2', 'device-id-2'),
-('2019-01-20T09:00:01', 'dev2', 43.5, 'orange2', 'device-id-2')
-ON CONFLICT (time, device_id)
-DO UPDATE SET
-device_id_2 = coalesce(excluded.device_id_2,current.device_id_2),
-temp = coalesce(excluded.temp,current.temp) ,
-color = coalesce(excluded.color,current.color);
-select * from upsert_test_diffchunk order by time, device_id;
-           time           |      device_id       | temp |  color  |     device_id_2      
---------------------------+----------------------+------+---------+----------------------
- Fri Jan 20 09:00:01 2017 | dev1                 | 43.5 | yellow2 | device-id-2         
- Fri Jan 20 09:00:01 2017 | dev2                 | 25.9 | yellow  | 
- Sun Jan 20 09:00:01 2019 | dev1                 | 43.5 | orange2 | device-id-2         
- Sun Jan 20 09:00:01 2019 | dev2                 | 43.5 | orange2 | device-id-2         
-(4 rows)
-
---arbiter index tests
-CREATE TABLE upsert_test_arbiter(time timestamp, to_drop int);
-SELECT create_hypertable('upsert_test_arbiter', 'time', chunk_time_interval=> interval '1 month');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-        create_hypertable         
-----------------------------------
- (6,public,upsert_test_arbiter,t)
-(1 row)
-
---this is the chunk with no tup_conv_map
-INSERT INTO upsert_test_arbiter (time, to_drop) VALUES ('2017-01-20T09:00:01', 1) RETURNING *;
-           time           | to_drop 
---------------------------+---------
- Fri Jan 20 09:00:01 2017 |       1
-(1 row)
-
-INSERT INTO upsert_test_arbiter (time, to_drop) VALUES ('2017-01-21T09:00:01', 2) RETURNING *;
-           time           | to_drop 
---------------------------+---------
- Sat Jan 21 09:00:01 2017 |       2
-(1 row)
-
-INSERT INTO upsert_test_arbiter (time, to_drop) VALUES ('2017-03-20T09:00:01', 3) RETURNING *;
-           time           | to_drop 
---------------------------+---------
- Mon Mar 20 09:00:01 2017 |       3
-(1 row)
-
---alter the table
-ALTER TABLE upsert_test_arbiter DROP to_drop;
-ALTER TABLE upsert_test_arbiter ADD device_id char(20) DEFAULT 'dev1';
-CREATE UNIQUE INDEX arbiter_time_device_idx ON upsert_test_arbiter (time, device_id);
-INSERT INTO upsert_test_arbiter as current (time, device_id) VALUES
-    ('2018-01-21T09:00:01', 'dev1'),
-    ('2017-01-20T09:00:01', 'dev1'),
-    ('2017-01-21T09:00:01', 'dev2'),
-    ('2018-01-21T09:00:01', 'dev2')
- ON CONFLICT (time, device_id) DO UPDATE SET device_id = coalesce(excluded.device_id,current.device_id)
-RETURNING *;
-           time           |      device_id       
---------------------------+----------------------
- Sun Jan 21 09:00:01 2018 | dev1                
- Fri Jan 20 09:00:01 2017 | dev1                
- Sat Jan 21 09:00:01 2017 | dev2                
- Sun Jan 21 09:00:01 2018 | dev2                
-(4 rows)
-
-with cte as (
-INSERT INTO upsert_test_arbiter (time, device_id) VALUES
-    ('2017-01-21T09:00:01', 'dev2'),
-    ('2018-01-21T09:00:01', 'dev2')
- ON CONFLICT (time, device_id) DO UPDATE SET device_id = 'dev3'
-RETURNING *)
-select * from cte;
-           time           |      device_id       
---------------------------+----------------------
- Sat Jan 21 09:00:01 2017 | dev3                
- Sun Jan 21 09:00:01 2018 | dev3                
-(2 rows)
-
--- test ON CONFLICT with prepared statements
-CREATE TABLE prepared_test(time timestamptz PRIMARY KEY, value float CHECK(value > 0));
-SELECT create_hypertable('prepared_test','time');
-     create_hypertable      
-----------------------------
- (7,public,prepared_test,t)
-(1 row)
-
-CREATE TABLE source_data(time timestamptz PRIMARY KEY, value float);
-INSERT INTO source_data VALUES('2000-01-01',0.5), ('2001-01-01',0.5);
--- at some point PostgreSQL will turn the plan into a generic plan
--- so we execute the prepared statement 10 times
--- check that an error in the prepared statement does not lead to the plan becoming unusable
-PREPARE prep_insert_select AS INSERT INTO prepared_test select * from source_data ON CONFLICT (time) DO UPDATE SET value = EXCLUDED.value;
-EXECUTE prep_insert_select;
-EXECUTE prep_insert_select;
-EXECUTE prep_insert_select;
-EXECUTE prep_insert_select;
-EXECUTE prep_insert_select;
-EXECUTE prep_insert_select;
-EXECUTE prep_insert_select;
-EXECUTE prep_insert_select;
-EXECUTE prep_insert_select;
-EXECUTE prep_insert_select;
---this insert will create an invalid tuple in source_data
---so that future calls to prep_insert_select will fail
-INSERT INTO source_data VALUES('2000-01-02',-0.5);
-\set ON_ERROR_STOP 0
-EXECUTE prep_insert_select;
-ERROR:  new row for relation "_hyper_7_11_chunk" violates check constraint "prepared_test_value_check"
-EXECUTE prep_insert_select;
-ERROR:  new row for relation "_hyper_7_11_chunk" violates check constraint "prepared_test_value_check"
-\set ON_ERROR_STOP 1
-DELETE FROM source_data WHERE value <= 0;
-EXECUTE prep_insert_select;
-PREPARE prep_insert AS INSERT INTO prepared_test VALUES('2000-01-01',0.5) ON CONFLICT (time) DO UPDATE SET value = EXCLUDED.value;
--- at some point PostgreSQL will turn the plan into a generic plan
--- so we execute the prepared statement 10 times
-EXECUTE prep_insert;
-EXECUTE prep_insert;
-EXECUTE prep_insert;
-EXECUTE prep_insert;
-EXECUTE prep_insert;
-EXECUTE prep_insert;
-EXECUTE prep_insert;
-EXECUTE prep_insert;
-EXECUTE prep_insert;
-EXECUTE prep_insert;
-SELECT * FROM prepared_test;
-             time             | value 
-------------------------------+-------
- Sat Jan 01 00:00:00 2000 PST |   0.5
- Mon Jan 01 00:00:00 2001 PST |   0.5
-(2 rows)
-
-DELETE FROM prepared_test;
--- test ON CONFLICT with functions
-CREATE OR REPLACE FUNCTION test_upsert(t timestamptz, v float) RETURNS VOID AS $sql$
-BEGIN
-INSERT INTO prepared_test VALUES(t,v) ON CONFLICT (time) DO UPDATE SET value = EXCLUDED.value;
-END;
-$sql$ LANGUAGE PLPGSQL;
--- at some point PostgreSQL will turn the plan into a generic plan
--- so we execute the function 10 times
-SELECT counter,test_upsert('2000-01-01',0.5) FROM generate_series(1,10) AS g(counter);
- counter | test_upsert 
----------+-------------
-       1 | 
-       2 | 
-       3 | 
-       4 | 
-       5 | 
-       6 | 
-       7 | 
-       8 | 
-       9 | 
-      10 | 
-(10 rows)
-
-SELECT * FROM prepared_test;
-             time             | value 
-------------------------------+-------
- Sat Jan 01 00:00:00 2000 PST |   0.5
-(1 row)
-
-DELETE FROM prepared_test;
--- at some point PostgreSQL will turn the plan into a generic plan
--- so we execute the function 10 times
-SELECT counter,test_upsert('2000-01-01',0.5) FROM generate_series(1,10) AS g(counter);
- counter | test_upsert 
----------+-------------
-       1 | 
-       2 | 
-       3 | 
-       4 | 
-       5 | 
-       6 | 
-       7 | 
-       8 | 
-       9 | 
-      10 | 
-(10 rows)
-
-SELECT * FROM prepared_test;
-             time             | value 
-------------------------------+-------
- Sat Jan 01 00:00:00 2000 PST |   0.5
-(1 row)
-
-DELETE FROM prepared_test;
--- run it again to ensure INSERT path is still working as well
-SELECT counter,test_upsert('2000-01-01',0.5) FROM generate_series(1,10) AS g(counter);
- counter | test_upsert 
----------+-------------
-       1 | 
-       2 | 
-       3 | 
-       4 | 
-       5 | 
-       6 | 
-       7 | 
-       8 | 
-       9 | 
-      10 | 
-(10 rows)
-
-SELECT * FROM prepared_test;
-             time             | value 
-------------------------------+-------
- Sat Jan 01 00:00:00 2000 PST |   0.5
-(1 row)
-
-DELETE FROM prepared_test;
--- test ON CONFLICT with functions
-CREATE OR REPLACE FUNCTION test_upsert2(t timestamptz, v float) RETURNS VOID AS $sql$
-BEGIN
-INSERT INTO prepared_test VALUES(t,v) ON CONFLICT (time) DO UPDATE SET value = prepared_test.value + 1.0;
-END;
-$sql$ LANGUAGE PLPGSQL;
--- at some point PostgreSQL will turn the plan into a generic plan
--- so we execute the function 10 times
-SELECT counter,test_upsert2('2000-01-01',1.0) FROM generate_series(1,10) AS g(counter);
- counter | test_upsert2 
----------+--------------
-       1 | 
-       2 | 
-       3 | 
-       4 | 
-       5 | 
-       6 | 
-       7 | 
-       8 | 
-       9 | 
-      10 | 
-(10 rows)
-
-SELECT * FROM prepared_test;
-             time             | value 
-------------------------------+-------
- Sat Jan 01 00:00:00 2000 PST |    10
-(1 row)
-
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/util.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/util.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/util.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/util.out	2023-11-25 05:27:49.297007649 +0000
@@ -1,4 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-\set ECHO errors
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/vacuum.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/vacuum.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/vacuum.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/vacuum.out	2023-11-25 05:27:49.297007649 +0000
@@ -1,228 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
-CREATE TABLE vacuum_test(time timestamp, temp float);
--- create hypertable with three chunks
-SELECT create_hypertable('vacuum_test', 'time', chunk_time_interval => 2628000000000, create_default_indexes => false);
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-    create_hypertable     
---------------------------
- (1,public,vacuum_test,t)
-(1 row)
-
-INSERT INTO vacuum_test VALUES ('2017-01-20T16:00:01', 17.5),
-                               ('2017-01-21T16:00:01', 19.1),
-                               ('2017-04-20T16:00:01', 89.5),
-                               ('2017-04-21T16:00:01', 17.1),
-                               ('2017-06-20T16:00:01', 18.5),
-                               ('2017-06-21T16:00:01', 11.0);
--- no stats
-SELECT tablename, attname, histogram_bounds, n_distinct FROM pg_stats
-WHERE schemaname = '_timescaledb_internal' AND tablename LIKE '_hyper_%_chunk'
-ORDER BY tablename, attname, array_to_string(histogram_bounds, ',');
- tablename | attname | histogram_bounds | n_distinct 
------------+---------+------------------+------------
-(0 rows)
-
-SELECT tablename, attname, histogram_bounds, n_distinct FROM pg_stats
-WHERE schemaname = 'public' AND tablename LIKE 'vacuum_test'
-ORDER BY tablename, attname, array_to_string(histogram_bounds, ',');
- tablename | attname | histogram_bounds | n_distinct 
------------+---------+------------------+------------
-(0 rows)
-
-VACUUM ANALYZE vacuum_test;
--- stats should exist for all three chunks
-SELECT tablename, attname, histogram_bounds, n_distinct FROM pg_stats
-WHERE schemaname = '_timescaledb_internal' AND tablename LIKE '_hyper_%_chunk'
-ORDER BY tablename, attname, array_to_string(histogram_bounds, ',');
-    tablename     | attname |                    histogram_bounds                     | n_distinct 
-------------------+---------+---------------------------------------------------------+------------
- _hyper_1_1_chunk | temp    | {17.5,19.1}                                             |         -1
- _hyper_1_1_chunk | time    | {"Fri Jan 20 16:00:01 2017","Sat Jan 21 16:00:01 2017"} |         -1
- _hyper_1_2_chunk | temp    | {17.1,89.5}                                             |         -1
- _hyper_1_2_chunk | time    | {"Thu Apr 20 16:00:01 2017","Fri Apr 21 16:00:01 2017"} |         -1
- _hyper_1_3_chunk | temp    | {11,18.5}                                               |         -1
- _hyper_1_3_chunk | time    | {"Tue Jun 20 16:00:01 2017","Wed Jun 21 16:00:01 2017"} |         -1
-(6 rows)
-
--- stats should exist on parent hypertable
-SELECT tablename, attname, histogram_bounds, n_distinct FROM pg_stats
-WHERE schemaname = 'public' AND tablename LIKE 'vacuum_test'
-ORDER BY tablename, attname, array_to_string(histogram_bounds, ',');
-  tablename  | attname |                                                                          histogram_bounds                                                                           | n_distinct 
--------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------
- vacuum_test | temp    | {11,17.1,17.5,18.5,19.1,89.5}                                                                                                                                       |         -1
- vacuum_test | time    | {"Fri Jan 20 16:00:01 2017","Sat Jan 21 16:00:01 2017","Thu Apr 20 16:00:01 2017","Fri Apr 21 16:00:01 2017","Tue Jun 20 16:00:01 2017","Wed Jun 21 16:00:01 2017"} |         -1
-(2 rows)
-
-DROP TABLE vacuum_test;
---test plain analyze (no_vacuum)
-CREATE TABLE analyze_test(time timestamp, temp float);
-SELECT create_hypertable('analyze_test', 'time', chunk_time_interval => 2628000000000, create_default_indexes => false);
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-     create_hypertable     
----------------------------
- (2,public,analyze_test,t)
-(1 row)
-
-INSERT INTO analyze_test VALUES ('2017-01-20T16:00:01', 17.5),
-                               ('2017-01-21T16:00:01', 19.1),
-                               ('2017-04-20T16:00:01', 89.5),
-                               ('2017-04-21T16:00:01', 17.1),
-                               ('2017-06-20T16:00:01', 18.5),
-                               ('2017-06-21T16:00:01', 11.0);
--- no stats
-SELECT tablename, attname, histogram_bounds, n_distinct FROM pg_stats
-WHERE schemaname = '_timescaledb_internal' AND tablename LIKE '_hyper_%_chunk'
-ORDER BY tablename, attname, array_to_string(histogram_bounds, ',');
- tablename | attname | histogram_bounds | n_distinct 
------------+---------+------------------+------------
-(0 rows)
-
-SELECT tablename, attname, histogram_bounds, n_distinct FROM pg_stats
-WHERE schemaname = 'public' AND tablename LIKE 'analyze_test'
-ORDER BY tablename, attname, array_to_string(histogram_bounds, ',');
- tablename | attname | histogram_bounds | n_distinct 
------------+---------+------------------+------------
-(0 rows)
-
-ANALYZE VERBOSE analyze_test;
-INFO:  analyzing "_timescaledb_internal._hyper_2_4_chunk"
-INFO:  "_hyper_2_4_chunk": scanned 1 of 1 pages, containing 2 live rows and 0 dead rows; 2 rows in sample, 2 estimated total rows
-INFO:  analyzing "_timescaledb_internal._hyper_2_5_chunk"
-INFO:  "_hyper_2_5_chunk": scanned 1 of 1 pages, containing 2 live rows and 0 dead rows; 2 rows in sample, 2 estimated total rows
-INFO:  analyzing "_timescaledb_internal._hyper_2_6_chunk"
-INFO:  "_hyper_2_6_chunk": scanned 1 of 1 pages, containing 2 live rows and 0 dead rows; 2 rows in sample, 2 estimated total rows
-INFO:  analyzing "public.analyze_test"
-INFO:  "analyze_test": scanned 0 of 0 pages, containing 0 live rows and 0 dead rows; 0 rows in sample, 0 estimated total rows
-INFO:  analyzing "public.analyze_test" inheritance tree
-INFO:  "_hyper_2_4_chunk": scanned 1 of 1 pages, containing 2 live rows and 0 dead rows; 2 rows in sample, 2 estimated total rows
-INFO:  "_hyper_2_5_chunk": scanned 1 of 1 pages, containing 2 live rows and 0 dead rows; 2 rows in sample, 2 estimated total rows
-INFO:  "_hyper_2_6_chunk": scanned 1 of 1 pages, containing 2 live rows and 0 dead rows; 2 rows in sample, 2 estimated total rows
--- stats should exist for all three chunks
-SELECT tablename, attname, histogram_bounds, n_distinct FROM pg_stats
-WHERE schemaname = '_timescaledb_internal' AND tablename LIKE '_hyper_%_chunk'
-ORDER BY tablename, attname, array_to_string(histogram_bounds, ',');
-    tablename     | attname |                    histogram_bounds                     | n_distinct 
-------------------+---------+---------------------------------------------------------+------------
- _hyper_2_4_chunk | temp    | {17.5,19.1}                                             |         -1
- _hyper_2_4_chunk | time    | {"Fri Jan 20 16:00:01 2017","Sat Jan 21 16:00:01 2017"} |         -1
- _hyper_2_5_chunk | temp    | {17.1,89.5}                                             |         -1
- _hyper_2_5_chunk | time    | {"Thu Apr 20 16:00:01 2017","Fri Apr 21 16:00:01 2017"} |         -1
- _hyper_2_6_chunk | temp    | {11,18.5}                                               |         -1
- _hyper_2_6_chunk | time    | {"Tue Jun 20 16:00:01 2017","Wed Jun 21 16:00:01 2017"} |         -1
-(6 rows)
-
--- stats should exist on parent hypertable
-SELECT tablename, attname, histogram_bounds, n_distinct FROM pg_stats
-WHERE schemaname = 'public' AND tablename LIKE 'analyze_test'
-ORDER BY tablename, attname, array_to_string(histogram_bounds, ',');
-  tablename   | attname |                                                                          histogram_bounds                                                                           | n_distinct 
---------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------
- analyze_test | temp    | {11,17.1,17.5,18.5,19.1,89.5}                                                                                                                                       |         -1
- analyze_test | time    | {"Fri Jan 20 16:00:01 2017","Sat Jan 21 16:00:01 2017","Thu Apr 20 16:00:01 2017","Fri Apr 21 16:00:01 2017","Tue Jun 20 16:00:01 2017","Wed Jun 21 16:00:01 2017"} |         -1
-(2 rows)
-
-DROP TABLE analyze_test;
--- Run vacuum on a normal (non-hypertable) table
-CREATE TABLE vacuum_norm(time timestamp, temp float);
-INSERT INTO vacuum_norm VALUES ('2017-01-20T09:00:01', 17.5),
-                               ('2017-01-21T09:00:01', 19.1),
-                               ('2017-04-20T09:00:01', 89.5),
-                               ('2017-04-21T09:00:01', 17.1),
-                               ('2017-06-20T09:00:01', 18.5),
-                               ('2017-06-21T09:00:01', 11.0);
-VACUUM ANALYZE vacuum_norm;
-DROP TABLE vacuum_norm;
---Similar to normal vacuum tests, but PG11 introduced ability to vacuum multiple tables at once, we make sure that works for hypertables as well.
-CREATE TABLE vacuum_test(time timestamp, temp float);
--- create hypertable with three chunks
-SELECT create_hypertable('vacuum_test', 'time', chunk_time_interval => 2628000000000, create_default_indexes => false);
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-    create_hypertable     
---------------------------
- (3,public,vacuum_test,t)
-(1 row)
-
-INSERT INTO vacuum_test VALUES ('2017-01-20T16:00:01', 17.5),
-                               ('2017-01-21T16:00:01', 19.1),
-                               ('2017-04-20T16:00:01', 89.5),
-                               ('2017-04-21T16:00:01', 17.1),
-                               ('2017-06-20T16:00:01', 18.5),
-                               ('2017-06-21T16:00:01', 11.0);
-CREATE TABLE analyze_test(time timestamp, temp float);
-SELECT create_hypertable('analyze_test', 'time', chunk_time_interval => 2628000000000, create_default_indexes => false);
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-NOTICE:  adding not-null constraint to column "time"
-     create_hypertable     
----------------------------
- (4,public,analyze_test,t)
-(1 row)
-
-INSERT INTO analyze_test VALUES ('2017-01-20T16:00:01', 17.5),
-                               ('2017-01-21T16:00:01', 19.1),
-                               ('2017-04-20T16:00:01', 89.5),
-                               ('2017-04-21T16:00:01', 17.1),
-                               ('2017-06-20T16:00:01', 18.5),
-                               ('2017-06-21T16:00:01', 11.0);
-CREATE TABLE vacuum_norm(time timestamp, temp float);
-INSERT INTO vacuum_norm VALUES ('2017-01-20T09:00:01', 17.5),
-                               ('2017-01-21T09:00:01', 19.1),
-                               ('2017-04-20T09:00:01', 89.5),
-                               ('2017-04-21T09:00:01', 17.1),
-                               ('2017-06-20T09:00:01', 18.5),
-                               ('2017-06-21T09:00:01', 11.0);
--- no stats
-SELECT tablename, attname, histogram_bounds, n_distinct FROM pg_stats
-WHERE schemaname = '_timescaledb_internal' AND tablename LIKE '_hyper_%_chunk'
-ORDER BY tablename, attname, array_to_string(histogram_bounds, ',');
- tablename | attname | histogram_bounds | n_distinct 
------------+---------+------------------+------------
-(0 rows)
-
-SELECT tablename, attname, histogram_bounds, n_distinct FROM pg_stats
-WHERE schemaname = 'public'
-ORDER BY tablename, attname, array_to_string(histogram_bounds, ',');
- tablename | attname | histogram_bounds | n_distinct 
------------+---------+------------------+------------
-(0 rows)
-
-VACUUM ANALYZE vacuum_norm, vacuum_test, analyze_test;
--- stats should exist for all 6 chunks
-SELECT tablename, attname, histogram_bounds, n_distinct FROM pg_stats
-WHERE schemaname = '_timescaledb_internal' AND tablename LIKE '_hyper_%_chunk'
-ORDER BY tablename, attname, array_to_string(histogram_bounds, ',');
-     tablename     | attname |                    histogram_bounds                     | n_distinct 
--------------------+---------+---------------------------------------------------------+------------
- _hyper_3_7_chunk  | temp    | {17.5,19.1}                                             |         -1
- _hyper_3_7_chunk  | time    | {"Fri Jan 20 16:00:01 2017","Sat Jan 21 16:00:01 2017"} |         -1
- _hyper_3_8_chunk  | temp    | {17.1,89.5}                                             |         -1
- _hyper_3_8_chunk  | time    | {"Thu Apr 20 16:00:01 2017","Fri Apr 21 16:00:01 2017"} |         -1
- _hyper_3_9_chunk  | temp    | {11,18.5}                                               |         -1
- _hyper_3_9_chunk  | time    | {"Tue Jun 20 16:00:01 2017","Wed Jun 21 16:00:01 2017"} |         -1
- _hyper_4_10_chunk | temp    | {17.5,19.1}                                             |         -1
- _hyper_4_10_chunk | time    | {"Fri Jan 20 16:00:01 2017","Sat Jan 21 16:00:01 2017"} |         -1
- _hyper_4_11_chunk | temp    | {17.1,89.5}                                             |         -1
- _hyper_4_11_chunk | time    | {"Thu Apr 20 16:00:01 2017","Fri Apr 21 16:00:01 2017"} |         -1
- _hyper_4_12_chunk | temp    | {11,18.5}                                               |         -1
- _hyper_4_12_chunk | time    | {"Tue Jun 20 16:00:01 2017","Wed Jun 21 16:00:01 2017"} |         -1
-(12 rows)
-
--- stats should exist on parent hypertable and normal table
-SELECT tablename, attname, histogram_bounds, n_distinct FROM pg_stats
-WHERE schemaname = 'public'
-ORDER BY tablename, attname, array_to_string(histogram_bounds, ',');
-  tablename   | attname |                                                                          histogram_bounds                                                                           | n_distinct 
---------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------
- analyze_test | temp    | {11,17.1,17.5,18.5,19.1,89.5}                                                                                                                                       |         -1
- analyze_test | time    | {"Fri Jan 20 16:00:01 2017","Sat Jan 21 16:00:01 2017","Thu Apr 20 16:00:01 2017","Fri Apr 21 16:00:01 2017","Tue Jun 20 16:00:01 2017","Wed Jun 21 16:00:01 2017"} |         -1
- vacuum_norm  | temp    | {11,17.1,17.5,18.5,19.1,89.5}                                                                                                                                       |         -1
- vacuum_norm  | time    | {"Fri Jan 20 09:00:01 2017","Sat Jan 21 09:00:01 2017","Thu Apr 20 09:00:01 2017","Fri Apr 21 09:00:01 2017","Tue Jun 20 09:00:01 2017","Wed Jun 21 09:00:01 2017"} |         -1
- vacuum_test  | temp    | {11,17.1,17.5,18.5,19.1,89.5}                                                                                                                                       |         -1
- vacuum_test  | time    | {"Fri Jan 20 16:00:01 2017","Sat Jan 21 16:00:01 2017","Thu Apr 20 16:00:01 2017","Fri Apr 21 16:00:01 2017","Tue Jun 20 16:00:01 2017","Wed Jun 21 16:00:01 2017"} |         -1
-(6 rows)
-
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/vacuum_parallel.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/vacuum_parallel.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/vacuum_parallel.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/vacuum_parallel.out	2023-11-25 05:27:49.297007649 +0000
@@ -1,39 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
--- PG13 introduced parallel VACUUM functionality. It gets invoked when a table
--- has two or more indexes on it. Read up more at
--- https://www.postgresql.org/docs/13/sql-vacuum.html#PARALLEL
-CREATE TABLE vacuum_test(time timestamp NOT NULL, temp1 float, temp2 int);
--- create hypertable
--- we create chunks in public schema cause otherwise we would need
--- elevated privileges to create indexes directly
-SELECT create_hypertable('vacuum_test', 'time', create_default_indexes => false, associated_schema_name => 'public');
-WARNING:  column type "timestamp without time zone" used for "time" does not follow best practices
-    create_hypertable     
---------------------------
- (1,public,vacuum_test,t)
-(1 row)
-
--- parallel vacuum needs the index size to be larger than min_parallel_index_scan_size to kick in
-SET min_parallel_index_scan_size TO 0;
-INSERT INTO vacuum_test SELECT TIMESTAMP 'epoch' + (i * INTERVAL '4h'),
-                i, i+1 FROM generate_series(1, 100) as T(i);
--- create indexes on the temp columns
--- we create indexes manually because otherwise vacuum verbose output
--- would be different between 13.2 and 13.3+
--- 13.2 would try to vacuum the parent table index too while 13.3+ wouldn't
-CREATE INDEX ON _hyper_1_1_chunk(time);
-CREATE INDEX ON _hyper_1_1_chunk(temp1);
-CREATE INDEX ON _hyper_1_1_chunk(temp2);
-CREATE INDEX ON _hyper_1_2_chunk(time);
-CREATE INDEX ON _hyper_1_2_chunk(temp1);
-CREATE INDEX ON _hyper_1_2_chunk(temp2);
-CREATE INDEX ON _hyper_1_3_chunk(time);
-CREATE INDEX ON _hyper_1_3_chunk(temp1);
-CREATE INDEX ON _hyper_1_3_chunk(temp2);
--- INSERT only will not trigger vacuum on indexes for PG13.3+
-UPDATE vacuum_test SET time = time + '1s'::interval, temp1 = random(), temp2 = random();
--- we should see two parallel workers for each chunk
-VACUUM (PARALLEL 3) vacuum_test;
-DROP TABLE vacuum_test;
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
diff -u /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/version.out /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/version.out
--- /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/test/expected/version.out	2023-06-28 16:40:59.000000000 +0000
+++ /home/abigalek/pgext-cli/pgextworkdir/timescaledb-2.11.1/build/test/results/version.out	2023-11-25 05:27:49.297007649 +0000
@@ -1,10 +1,2 @@
--- This file and its contents are licensed under the Apache License 2.0.
--- Please see the included NOTICE for copyright information and
--- LICENSE-APACHE for a copy of the license.
--- Test that get_os_info returns 3 x text
-select pg_typeof(sysname) AS sysname_type,pg_typeof(version) AS version_type,pg_typeof(release) AS release_type from _timescaledb_internal.get_os_info();
- sysname_type | version_type | release_type 
---------------+--------------+--------------
- text         | text         | text
-(1 row)
-
+ERROR:  source database "template1" is being accessed by other users
+DETAIL:  There is 1 other session using the database.
