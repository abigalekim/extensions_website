The files belonging to this database system will be owned by user "abigalek".
This user must also own the server process.

The database cluster will be initialized with locale "en_US.UTF-8".
The default database encoding has accordingly been set to "UTF8".
The default text search configuration will be set to "english".

Data page checksums are disabled.

creating directory pg-15-data ... ok
creating subdirectories ... ok
selecting dynamic shared memory implementation ... posix
selecting default max_connections ... 100
selecting default shared_buffers ... 128MB
selecting default time zone ... Etc/UTC
creating configuration files ... ok
running bootstrap script ... ok
performing post-bootstrap initialization ... ok
syncing data to disk ... ok

initdb: warning: enabling "trust" authentication for local connections
initdb: hint: You can change this by editing pg_hba.conf or using the option -A, or --auth-local and --auth-host, the next time you run initdb.

Success. You can now start the database server using:

    pg-15-dist/bin/pg_ctl -D pg-15-data -l logfile start

waiting for server to start.... done
server started
(using postmaster on Unix socket, default port)
============== dropping database "contrib_regression" ==============
SET
DROP DATABASE
============== creating database "contrib_regression" ==============
CREATE DATABASE
ALTER DATABASE
ALTER DATABASE
ALTER DATABASE
ALTER DATABASE
ALTER DATABASE
ALTER DATABASE
============== installing pgcrypto                    ==============
CREATE EXTENSION
============== running regression test queries        ==============
test pg_cron-test                 ... ok          769 ms

=====================
 All 1 tests passed. 
=====================

(using postmaster on Unix socket, default port)
============== dropping database "contrib_regression" ==============
SET
DROP DATABASE
============== creating database "contrib_regression" ==============
CREATE DATABASE
ALTER DATABASE
ALTER DATABASE
ALTER DATABASE
ALTER DATABASE
ALTER DATABASE
ALTER DATABASE
============== installing pg_cron                     ==============
CREATE EXTENSION
============== running regression test queries        ==============
test init                         ... ok           23 ms
test md5                          ... ok            9 ms
test sha1                         ... ok            9 ms
test hmac-md5                     ... ok           10 ms
test hmac-sha1                    ... ok           10 ms
test blowfish                     ... ok           10 ms
test rijndael                     ... ok           11 ms
test sha2                         ... ok           11 ms
test des                          ... ok            9 ms
test 3des                         ... ok           10 ms
test cast5                        ... ok           10 ms
test crypt-des                    ... ok           29 ms
test crypt-md5                    ... ok           35 ms
test crypt-blowfish               ... ok           83 ms
test crypt-xdes                   ... ok           30 ms
test pgp-armor                    ... ok           39 ms
test pgp-decrypt                  ... ok           67 ms
test pgp-encrypt                  ... ok          404 ms
test pgp-compression              ... ok           15 ms
test pgp-pubkey-decrypt           ... ok          100 ms
test pgp-pubkey-encrypt           ... ok           19 ms
test pgp-info                     ... ok            8 ms

======================
 All 22 tests passed. 
======================

ERROR:  can only create extension in database postgres
DETAIL:  Jobs must be scheduled from the database configured in cron.database_name, since the pg_cron background worker reads job descriptions from this database.
HINT:  Add cron.database_name = 'pgbench_test' in postgresql.conf to use the current database.
CONTEXT:  PL/pgSQL function inline_code_block line 4 at RAISE
CREATE EXTENSION
dropping old tables...
NOTICE:  table "pgbench_accounts" does not exist, skipping
NOTICE:  table "pgbench_branches" does not exist, skipping
NOTICE:  table "pgbench_history" does not exist, skipping
NOTICE:  table "pgbench_tellers" does not exist, skipping
creating tables...
generating data (client-side)...
100000 of 1000000 tuples (10%) done (elapsed 0.08 s, remaining 0.69 s)
200000 of 1000000 tuples (20%) done (elapsed 0.13 s, remaining 0.53 s)
300000 of 1000000 tuples (30%) done (elapsed 0.26 s, remaining 0.61 s)
400000 of 1000000 tuples (40%) done (elapsed 0.34 s, remaining 0.52 s)
500000 of 1000000 tuples (50%) done (elapsed 0.45 s, remaining 0.45 s)
600000 of 1000000 tuples (60%) done (elapsed 0.57 s, remaining 0.38 s)
700000 of 1000000 tuples (70%) done (elapsed 0.65 s, remaining 0.28 s)
800000 of 1000000 tuples (80%) done (elapsed 0.79 s, remaining 0.20 s)
900000 of 1000000 tuples (90%) done (elapsed 0.92 s, remaining 0.10 s)
1000000 of 1000000 tuples (100%) done (elapsed 0.98 s, remaining 0.00 s)
vacuuming...
creating primary keys...
done in 1.53 s (drop tables 0.00 s, create tables 0.01 s, client-side generate 1.05 s, vacuum 0.12 s, primary keys 0.35 s).
CREATE EXTENSION
ERROR:  can only create extension in database postgres
DETAIL:  Jobs must be scheduled from the database configured in cron.database_name, since the pg_cron background worker reads job descriptions from this database.
HINT:  Add cron.database_name = 'pgbench_test' in postgresql.conf to use the current database.
CONTEXT:  PL/pgSQL function inline_code_block line 4 at RAISE
dropping old tables...
NOTICE:  table "pgbench_accounts" does not exist, skipping
NOTICE:  table "pgbench_branches" does not exist, skipping
NOTICE:  table "pgbench_history" does not exist, skipping
NOTICE:  table "pgbench_tellers" does not exist, skipping
creating tables...
generating data (client-side)...
100000 of 1000000 tuples (10%) done (elapsed 0.06 s, remaining 0.57 s)
200000 of 1000000 tuples (20%) done (elapsed 0.14 s, remaining 0.56 s)
300000 of 1000000 tuples (30%) done (elapsed 0.21 s, remaining 0.49 s)
400000 of 1000000 tuples (40%) done (elapsed 0.29 s, remaining 0.44 s)
500000 of 1000000 tuples (50%) done (elapsed 0.37 s, remaining 0.37 s)
600000 of 1000000 tuples (60%) done (elapsed 0.46 s, remaining 0.31 s)
700000 of 1000000 tuples (70%) done (elapsed 0.53 s, remaining 0.23 s)
800000 of 1000000 tuples (80%) done (elapsed 0.61 s, remaining 0.15 s)
900000 of 1000000 tuples (90%) done (elapsed 0.69 s, remaining 0.08 s)
1000000 of 1000000 tuples (100%) done (elapsed 0.77 s, remaining 0.00 s)
vacuuming...
creating primary keys...
done in 1.19 s (drop tables 0.00 s, create tables 0.01 s, client-side generate 0.80 s, vacuum 0.10 s, primary keys 0.28 s).
waiting for server to shut down.... done
server stopped
