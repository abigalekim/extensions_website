2023-11-25 13:48:21.907 UTC [133093] LOG:  00000: number of prepared transactions has not been configured, overriding
2023-11-25 13:48:21.907 UTC [133093] DETAIL:  max_prepared_transactions is now set to 200
2023-11-25 13:48:21.907 UTC [133093] LOCATION:  AdjustMaxPreparedTransactions, transaction_management.c:761
2023-11-25 13:48:21.924 UTC [133093] LOG:  00000: starting PostgreSQL 15.3 on x86_64-pc-linux-gnu, compiled by gcc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0, 64-bit
2023-11-25 13:48:21.924 UTC [133093] LOCATION:  PostmasterMain, postmaster.c:1189
2023-11-25 13:48:21.924 UTC [133093] LOG:  00000: listening on IPv4 address "127.0.0.1", port 57636
2023-11-25 13:48:21.924 UTC [133093] LOCATION:  StreamServerPort, pqcomm.c:582
2023-11-25 13:48:21.924 UTC [133093] LOG:  00000: listening on Unix socket "/tmp/.s.PGSQL.57636"
2023-11-25 13:48:21.924 UTC [133093] LOCATION:  StreamServerPort, pqcomm.c:577
2023-11-25 13:48:21.925 UTC [133096] LOG:  00000: database system was shut down at 2023-11-25 13:48:21 UTC
2023-11-25 13:48:21.925 UTC [133096] LOCATION:  StartupXLOG, xlog.c:4928
2023-11-25 13:48:21.927 UTC [133093] LOG:  00000: database system is ready to accept connections
2023-11-25 13:48:21.927 UTC [133093] LOCATION:  reaper, postmaster.c:3117
2023-11-25 13:48:22.677 UTC [133163] LOG:  00000: starting maintenance daemon on database 16384 user 10
2023-11-25 13:48:22.677 UTC [133163] CONTEXT:  Citus maintenance daemon for database 16384 user 10
2023-11-25 13:48:22.677 UTC [133163] LOCATION:  CitusMaintenanceDaemonMain, maintenanced.c:373
2023-11-25 13:48:22.688 UTC [133093] LOG:  00000: received SIGHUP, reloading configuration files
2023-11-25 13:48:22.688 UTC [133093] LOCATION:  SIGHUP_handler, postmaster.c:2769
2023-11-25 13:48:22.689 UTC [133093] LOG:  00000: parameter "citus.metadata_sync_interval" changed to "3000"
2023-11-25 13:48:22.689 UTC [133093] LOCATION:  ProcessConfigFileInternal, guc-file.l:453
2023-11-25 13:48:22.689 UTC [133093] LOG:  00000: parameter "citus.metadata_sync_retry_interval" changed to "500"
2023-11-25 13:48:22.689 UTC [133093] LOCATION:  ProcessConfigFileInternal, guc-file.l:453
2023-11-25 13:48:22.889 UTC [133197] ERROR:  55000: disabling the first worker node in the metadata is not allowed
2023-11-25 13:48:22.889 UTC [133197] DETAIL:  Citus uses the first worker node in the metadata for certain internal operations when replicated tables are modified. Synchronous mode ensures that all nodes have the same view of the first worker node, which is used for certain locking operations.
2023-11-25 13:48:22.889 UTC [133197] HINT:  You can force disabling node, SELECT citus_disable_node('localhost', 57637, synchronous:=true);
2023-11-25 13:48:22.889 UTC [133197] LOCATION:  citus_disable_node, node_metadata.c:542
2023-11-25 13:48:22.889 UTC [133197] STATEMENT:  SELECT citus_disable_node('localhost', 57637);
2023-11-25 13:48:22.947 UTC [133197] ERROR:  XX000: cannot remove or disable the node localhost:57638 because because it contains the only shard placement for shard 1220001
2023-11-25 13:48:22.947 UTC [133197] DETAIL:  One of the table(s) that prevents the operation complete successfully is public.cluster_management_test
2023-11-25 13:48:22.947 UTC [133197] HINT:  To proceed, either drop the tables or use undistribute_table() function to convert them to local tables
2023-11-25 13:48:22.947 UTC [133197] LOCATION:  ErrorIfNodeContainsNonRemovablePlacements, node_metadata.c:1947
2023-11-25 13:48:22.947 UTC [133197] STATEMENT:  SELECT master_remove_node('localhost', 57638);
2023-11-25 13:48:22.955 UTC [133197] ERROR:  XX000: cannot remove or disable the node localhost:57638 because because it contains the only shard placement for shard 1220001
2023-11-25 13:48:22.955 UTC [133197] DETAIL:  One of the table(s) that prevents the operation complete successfully is public.cluster_management_test
2023-11-25 13:48:22.955 UTC [133197] HINT:  To proceed, either drop the tables or use undistribute_table() function to convert them to local tables
2023-11-25 13:48:22.955 UTC [133197] LOCATION:  ErrorIfNodeContainsNonRemovablePlacements, node_metadata.c:1947
2023-11-25 13:48:22.955 UTC [133197] STATEMENT:  SELECT citus_remove_node('localhost', 57638);
2023-11-25 13:48:22.955 UTC [133197] ERROR:  XX000: cannot remove or disable the node localhost:57638 because because it contains the only shard placement for shard 1220001
2023-11-25 13:48:22.955 UTC [133197] DETAIL:  One of the table(s) that prevents the operation complete successfully is public.cluster_management_test
2023-11-25 13:48:22.955 UTC [133197] HINT:  To proceed, either drop the tables or use undistribute_table() function to convert them to local tables
2023-11-25 13:48:22.955 UTC [133197] LOCATION:  ErrorIfNodeContainsNonRemovablePlacements, node_metadata.c:1947
2023-11-25 13:48:22.955 UTC [133197] STATEMENT:  SELECT citus_disable_node('localhost', 57638);
2023-11-25 13:48:22.955 UTC [133197] ERROR:  XX000: node at "localhost.noexist:2345" does not exist
2023-11-25 13:48:22.955 UTC [133197] LOCATION:  ModifiableWorkerNode, node_metadata.c:732
2023-11-25 13:48:22.955 UTC [133197] STATEMENT:  SELECT citus_disable_node('localhost.noexist', 2345);
2023-11-25 13:48:23.061 UTC [133197] ERROR:  42501: permission denied for function master_add_inactive_node
2023-11-25 13:48:23.061 UTC [133197] LOCATION:  aclcheck_error, aclchk.c:3650
2023-11-25 13:48:23.061 UTC [133197] STATEMENT:  SELECT 1 FROM master_add_inactive_node('localhost', 57638 + 1);
2023-11-25 13:48:23.061 UTC [133197] ERROR:  42501: permission denied for function master_activate_node
2023-11-25 13:48:23.061 UTC [133197] LOCATION:  aclcheck_error, aclchk.c:3650
2023-11-25 13:48:23.061 UTC [133197] STATEMENT:  SELECT 1 FROM master_activate_node('localhost', 57638 + 1);
2023-11-25 13:48:23.061 UTC [133197] ERROR:  42501: permission denied for function citus_disable_node
2023-11-25 13:48:23.061 UTC [133197] LOCATION:  aclcheck_error, aclchk.c:3650
2023-11-25 13:48:23.061 UTC [133197] STATEMENT:  SELECT 1 FROM citus_disable_node('localhost', 57638 + 1);
2023-11-25 13:48:23.061 UTC [133197] ERROR:  42501: permission denied for function master_remove_node
2023-11-25 13:48:23.061 UTC [133197] LOCATION:  aclcheck_error, aclchk.c:3650
2023-11-25 13:48:23.061 UTC [133197] STATEMENT:  SELECT 1 FROM master_remove_node('localhost', 57638 + 1);
2023-11-25 13:48:23.061 UTC [133197] ERROR:  42501: permission denied for function master_add_node
2023-11-25 13:48:23.061 UTC [133197] LOCATION:  aclcheck_error, aclchk.c:3650
2023-11-25 13:48:23.061 UTC [133197] STATEMENT:  SELECT 1 FROM master_add_node('localhost', 57638 + 1);
2023-11-25 13:48:23.061 UTC [133197] ERROR:  42501: permission denied for function master_add_secondary_node
2023-11-25 13:48:23.061 UTC [133197] LOCATION:  aclcheck_error, aclchk.c:3650
2023-11-25 13:48:23.061 UTC [133197] STATEMENT:  SELECT 1 FROM master_add_secondary_node('localhost', 57638 + 2, 'localhost', 57638);
2023-11-25 13:48:23.062 UTC [133197] ERROR:  42501: permission denied for function master_update_node
2023-11-25 13:48:23.062 UTC [133197] LOCATION:  aclcheck_error, aclchk.c:3650
2023-11-25 13:48:23.062 UTC [133197] STATEMENT:  SELECT master_update_node(nodeid, 'localhost', 57638 + 3) FROM pg_dist_node WHERE nodeport = 57638;
2023-11-25 13:48:23.062 UTC [133197] ERROR:  XX000: operation is not allowed
2023-11-25 13:48:23.062 UTC [133197] HINT:  Run the command with a superuser.
2023-11-25 13:48:23.062 UTC [133197] LOCATION:  EnsureSuperUser, metadata_utility.c:2299
2023-11-25 13:48:23.062 UTC [133197] STATEMENT:  SELECT 1 FROM master_add_node('localhost', 57638);
2023-11-25 13:48:23.114 UTC [133225] ERROR:  XX000: cannot remove or disable the node localhost:57638 because because it contains the only shard placement for shard 1220001
2023-11-25 13:48:23.114 UTC [133225] DETAIL:  One of the table(s) that prevents the operation complete successfully is public.cluster_management_test
2023-11-25 13:48:23.114 UTC [133225] HINT:  To proceed, either drop the tables or use undistribute_table() function to convert them to local tables
2023-11-25 13:48:23.114 UTC [133225] LOCATION:  ErrorIfNodeContainsNonRemovablePlacements, node_metadata.c:1947
2023-11-25 13:48:23.114 UTC [133225] STATEMENT:  SELECT master_remove_node('localhost', 57638);
2023-11-25 13:48:23.131 UTC [133225] ERROR:  XX000: cannot remove or disable the node localhost:57638 because because it contains the only shard placement for shard 1220001
2023-11-25 13:48:23.131 UTC [133225] DETAIL:  One of the table(s) that prevents the operation complete successfully is public.cluster_management_test
2023-11-25 13:48:23.131 UTC [133225] HINT:  To proceed, either drop the tables or use undistribute_table() function to convert them to local tables
2023-11-25 13:48:23.131 UTC [133225] LOCATION:  ErrorIfNodeContainsNonRemovablePlacements, node_metadata.c:1947
2023-11-25 13:48:23.131 UTC [133225] STATEMENT:  SELECT master_remove_node('localhost', 57638);
2023-11-25 13:48:23.148 UTC [133225] ERROR:  XX000: node group 5 does not have a primary node
2023-11-25 13:48:23.148 UTC [133225] LOCATION:  LookupNodeForGroup, metadata_cache.c:1189
2023-11-25 13:48:23.148 UTC [133225] STATEMENT:  SELECT * FROM cluster_management_test;
2023-11-25 13:48:23.161 UTC [133225] ERROR:  XX000: there is a shard placement in node group 5 but there are no nodes in that group
2023-11-25 13:48:23.161 UTC [133225] LOCATION:  LookupNodeForGroup, metadata_cache.c:1181
2023-11-25 13:48:23.161 UTC [133225] STATEMENT:  SELECT * FROM cluster_management_test;
2023-11-25 13:48:23.168 UTC [133225] WARNING:  01000: could not find any shard placements for shardId 1220001
2023-11-25 13:48:23.168 UTC [133225] LOCATION:  ShardPlacementList, metadata_cache.c:1247
2023-11-25 13:48:23.168 UTC [133225] WARNING:  01000: could not find any shard placements for shardId 1220003
2023-11-25 13:48:23.168 UTC [133225] LOCATION:  ShardPlacementList, metadata_cache.c:1247
2023-11-25 13:48:23.168 UTC [133225] WARNING:  01000: could not find any shard placements for shardId 1220005
2023-11-25 13:48:23.168 UTC [133225] LOCATION:  ShardPlacementList, metadata_cache.c:1247
2023-11-25 13:48:23.168 UTC [133225] WARNING:  01000: could not find any shard placements for shardId 1220007
2023-11-25 13:48:23.168 UTC [133225] LOCATION:  ShardPlacementList, metadata_cache.c:1247
2023-11-25 13:48:23.168 UTC [133225] WARNING:  01000: could not find any shard placements for shardId 1220009
2023-11-25 13:48:23.168 UTC [133225] LOCATION:  ShardPlacementList, metadata_cache.c:1247
2023-11-25 13:48:23.168 UTC [133225] WARNING:  01000: could not find any shard placements for shardId 1220011
2023-11-25 13:48:23.168 UTC [133225] LOCATION:  ShardPlacementList, metadata_cache.c:1247
2023-11-25 13:48:23.168 UTC [133225] WARNING:  01000: could not find any shard placements for shardId 1220013
2023-11-25 13:48:23.168 UTC [133225] LOCATION:  ShardPlacementList, metadata_cache.c:1247
2023-11-25 13:48:23.168 UTC [133225] WARNING:  01000: could not find any shard placements for shardId 1220015
2023-11-25 13:48:23.168 UTC [133225] LOCATION:  ShardPlacementList, metadata_cache.c:1247
2023-11-25 13:48:23.196 UTC [133225] ERROR:  XX000: cannot remove or disable the node localhost:57638 because because it contains the only shard placement for shard 1220001
2023-11-25 13:48:23.196 UTC [133225] DETAIL:  One of the table(s) that prevents the operation complete successfully is public.cluster_management_test
2023-11-25 13:48:23.196 UTC [133225] HINT:  To proceed, either drop the tables or use undistribute_table() function to convert them to local tables
2023-11-25 13:48:23.196 UTC [133225] LOCATION:  ErrorIfNodeContainsNonRemovablePlacements, node_metadata.c:1947
2023-11-25 13:48:23.196 UTC [133225] STATEMENT:  SELECT master_remove_node('localhost', 57638);
2023-11-25 13:48:23.419 UTC [133263] ERROR:  XX000: primaries must be added to the default cluster
2023-11-25 13:48:23.419 UTC [133263] LOCATION:  AddNodeMetadata, node_metadata.c:2141
2023-11-25 13:48:23.419 UTC [133263] STATEMENT:  SELECT master_add_node('localhost', 9999, nodecluster => 'olap');
2023-11-25 13:48:23.419 UTC [133263] ERROR:  XX000: group 14 already has a primary node
2023-11-25 13:48:23.419 UTC [133263] LOCATION:  AddNodeMetadata, node_metadata.c:2130
2023-11-25 13:48:23.419 UTC [133263] STATEMENT:  SELECT master_add_node('localhost', 9999, groupid => 14, noderole => 'primary');
2023-11-25 13:48:23.422 UTC [133263] ERROR:  P0001: there cannot be two primary nodes in a group
2023-11-25 13:48:23.422 UTC [133263] CONTEXT:  PL/pgSQL function citus_internal.pg_dist_node_trigger_func() line 10 at RAISE
2023-11-25 13:48:23.422 UTC [133263] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:23.422 UTC [133263] STATEMENT:  INSERT INTO pg_dist_node (nodename, nodeport, groupid, noderole)
	  VALUES ('localhost', 5000, 14, 'primary');
2023-11-25 13:48:23.423 UTC [133263] ERROR:  P0001: there cannot be two primary nodes in a group
2023-11-25 13:48:23.423 UTC [133263] CONTEXT:  PL/pgSQL function citus_internal.pg_dist_node_trigger_func() line 18 at RAISE
2023-11-25 13:48:23.423 UTC [133263] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:23.423 UTC [133263] STATEMENT:  UPDATE pg_dist_node SET noderole = 'primary'
	  WHERE groupid = 14 AND nodeport = 9998;
2023-11-25 13:48:23.423 UTC [133263] ERROR:  23514: new row for relation "pg_dist_node" violates check constraint "primaries_are_only_allowed_in_the_default_cluster"
2023-11-25 13:48:23.423 UTC [133263] DETAIL:  Failing row contains (24, 1000, localhost, 5000, default, f, t, primary, olap, f, t).
2023-11-25 13:48:23.423 UTC [133263] LOCATION:  ExecConstraints, execMain.c:2019
2023-11-25 13:48:23.423 UTC [133263] STATEMENT:  INSERT INTO pg_dist_node (nodename, nodeport, groupid, noderole, nodecluster)
	  VALUES ('localhost', 5000, 1000, 'primary', 'olap');
2023-11-25 13:48:23.423 UTC [133263] ERROR:  23514: new row for relation "pg_dist_node" violates check constraint "primaries_are_only_allowed_in_the_default_cluster"
2023-11-25 13:48:23.423 UTC [133263] DETAIL:  Failing row contains (16, 14, localhost, 57637, default, f, t, primary, olap, f, t).
2023-11-25 13:48:23.423 UTC [133263] LOCATION:  ExecConstraints, execMain.c:2019
2023-11-25 13:48:23.423 UTC [133263] STATEMENT:  UPDATE pg_dist_node SET nodecluster = 'olap'
	  WHERE nodeport = 57637;
2023-11-25 13:48:23.424 UTC [133263] ERROR:  XX000: node at "localhost:2000" does not exist
2023-11-25 13:48:23.424 UTC [133263] LOCATION:  GroupForNode, node_metadata.c:801
2023-11-25 13:48:23.424 UTC [133263] STATEMENT:  SELECT master_add_secondary_node('localhost', 9993, 'localhost', 2000);
2023-11-25 13:48:23.425 UTC [133263] ERROR:  P0002: node 100 not found
2023-11-25 13:48:23.425 UTC [133263] LOCATION:  citus_update_node, node_metadata.c:1216
2023-11-25 13:48:23.425 UTC [133263] STATEMENT:  SELECT master_update_node(100, 'localhost', 8000);
2023-11-25 13:48:23.426 UTC [133263] ERROR:  55000: there is already another node with the specified hostname and port
2023-11-25 13:48:23.426 UTC [133263] LOCATION:  citus_update_node, node_metadata.c:1207
2023-11-25 13:48:23.426 UTC [133263] STATEMENT:  SELECT master_update_node(16, 'localhost', 57638);
2023-11-25 13:48:23.499 UTC [133263] ERROR:  XX000: only the 'shouldhaveshards' property can be set using this function
2023-11-25 13:48:23.499 UTC [133263] LOCATION:  citus_set_node_property, node_metadata.c:695
2023-11-25 13:48:23.499 UTC [133263] STATEMENT:  SELECT * from master_set_node_property('localhost', 57638, 'bogusproperty', false);
2023-11-25 13:48:23.541 UTC [133263] ERROR:  XX000: do not sync metadata in transaction block when the sync mode is nontransactional
2023-11-25 13:48:23.541 UTC [133263] HINT:  resync after SET citus.metadata_sync_mode TO 'transactional'
2023-11-25 13:48:23.541 UTC [133263] LOCATION:  ActivateNodeList, node_metadata.c:1061
2023-11-25 13:48:23.541 UTC [133263] STATEMENT:  SELECT start_metadata_sync_to_all_nodes();
2023-11-25 13:48:23.586 UTC [133263] ERROR:  XX000: do not add node in transaction block when the sync mode is nontransactional
2023-11-25 13:48:23.586 UTC [133263] HINT:  add the node after SET citus.metadata_sync_mode TO 'transactional'
2023-11-25 13:48:23.586 UTC [133263] LOCATION:  citus_add_node, node_metadata.c:322
2023-11-25 13:48:23.586 UTC [133263] STATEMENT:  SELECT citus_add_node('localhost', 57637);
2023-11-25 13:48:23.684 UTC [133292] ERROR:  42601: syntax error at or near ""123"" at character 37
2023-11-25 13:48:23.684 UTC [133292] LOCATION:  scanner_yyerror, scan.l:1188
2023-11-25 13:48:23.684 UTC [133292] STATEMENT:  CREATE ROLE create_role_sysid SYSID "123";
2023-11-25 13:48:24.046 UTC [133349] ERROR:  42704: role "nonexisting_role_2" does not exist
2023-11-25 13:48:24.046 UTC [133349] LOCATION:  get_role_oid, acl.c:5184
2023-11-25 13:48:24.046 UTC [133349] STATEMENT:  GRANT existing_role_1, nonexisting_role_1 TO existing_role_2, nonexisting_role_2;
2023-11-25 13:48:24.054 UTC [133353] ERROR:  42704: role "nonexisting_role_1" does not exist
2023-11-25 13:48:24.054 UTC [133353] LOCATION:  DropRole, user.c:951
2023-11-25 13:48:24.054 UTC [133353] STATEMENT:  DROP ROLE existing_role_1, existing_role_2, nonexisting_role_1, nonexisting_role_2;
2023-11-25 13:48:24.737 UTC [133482] ERROR:  42P16: cannot distribute relation "data_load_test"
2023-11-25 13:48:24.737 UTC [133482] DETAIL:  Relation "data_load_test" contains data.
2023-11-25 13:48:24.737 UTC [133482] HINT:  Empty your table before distributing it.
2023-11-25 13:48:24.737 UTC [133482] LOCATION:  EnsureLocalTableEmpty, create_distributed_table.c:2032
2023-11-25 13:48:24.737 UTC [133482] STATEMENT:  SELECT create_distributed_table('data_load_test', 'col1', 'append');
2023-11-25 13:48:24.746 UTC [133482] ERROR:  42P16: cannot distribute relation "data_load_test"
2023-11-25 13:48:24.746 UTC [133482] DETAIL:  Relation "data_load_test" contains data.
2023-11-25 13:48:24.746 UTC [133482] HINT:  Empty your table before distributing it.
2023-11-25 13:48:24.746 UTC [133482] LOCATION:  EnsureLocalTableEmpty, create_distributed_table.c:2032
2023-11-25 13:48:24.746 UTC [133482] STATEMENT:  SELECT create_distributed_table('data_load_test', 'col1', 'range');
2023-11-25 13:48:24.888 UTC [133482] ERROR:  0A000: cannot create a citus table from a catalog table
2023-11-25 13:48:24.888 UTC [133482] LOCATION:  ErrorIfTableIsACatalogTable, create_distributed_table.c:1968
2023-11-25 13:48:24.888 UTC [133482] STATEMENT:  SELECT create_distributed_table('pg_class', 'relname');
2023-11-25 13:48:24.889 UTC [133482] ERROR:  0A000: cannot create a citus table from a catalog table
2023-11-25 13:48:24.889 UTC [133482] LOCATION:  ErrorIfTableIsACatalogTable, create_distributed_table.c:1968
2023-11-25 13:48:24.889 UTC [133482] STATEMENT:  SELECT create_reference_table('pg_class');
2023-11-25 13:48:24.902 UTC [133482] ERROR:  XX000: 0 is outside the valid range for parameter "shard_count" (1 .. 64000)
2023-11-25 13:48:24.902 UTC [133482] LOCATION:  create_distributed_table, create_distributed_table.c:276
2023-11-25 13:48:24.902 UTC [133482] STATEMENT:  SELECT create_distributed_table('shard_count_table_2', 'a', shard_count:=0);
2023-11-25 13:48:24.902 UTC [133482] ERROR:  XX000: -100 is outside the valid range for parameter "shard_count" (1 .. 64000)
2023-11-25 13:48:24.902 UTC [133482] LOCATION:  create_distributed_table, create_distributed_table.c:276
2023-11-25 13:48:24.902 UTC [133482] STATEMENT:  SELECT create_distributed_table('shard_count_table_2', 'a', shard_count:=-100);
2023-11-25 13:48:24.902 UTC [133482] ERROR:  XX000: 64001 is outside the valid range for parameter "shard_count" (1 .. 64000)
2023-11-25 13:48:24.902 UTC [133482] LOCATION:  create_distributed_table, create_distributed_table.c:276
2023-11-25 13:48:24.902 UTC [133482] STATEMENT:  SELECT create_distributed_table('shard_count_table_2', 'a', shard_count:=64001);
2023-11-25 13:48:24.902 UTC [133482] ERROR:  XX000: Cannot use colocate_with with a table and shard_count at the same time
2023-11-25 13:48:24.902 UTC [133482] LOCATION:  create_distributed_table, create_distributed_table.c:237
2023-11-25 13:48:24.902 UTC [133482] STATEMENT:  SELECT create_distributed_table('shard_count_table_2', 'a', shard_count:=12, colocate_with:='shard_count');
2023-11-25 13:48:24.952 UTC [133482] LOG:  00000: performing blocking isolate_tenant_to_new_shard 
2023-11-25 13:48:24.952 UTC [133482] LOCATION:  SplitShard, shard_split.c:507
2023-11-25 13:48:24.952 UTC [133482] STATEMENT:  SELECT 1 FROM isolate_tenant_to_new_shard('shard_split_table', 5, shard_transfer_mode => 'block_writes');
2023-11-25 13:48:24.952 UTC [133482] LOG:  00000: creating child shards for isolate_tenant_to_new_shard
2023-11-25 13:48:24.952 UTC [133482] LOCATION:  BlockingShardSplit, shard_split.c:571
2023-11-25 13:48:24.952 UTC [133482] STATEMENT:  SELECT 1 FROM isolate_tenant_to_new_shard('shard_split_table', 5, shard_transfer_mode => 'block_writes');
2023-11-25 13:48:24.961 UTC [133482] LOG:  00000: performing copy for isolate_tenant_to_new_shard
2023-11-25 13:48:24.961 UTC [133482] LOCATION:  BlockingShardSplit, shard_split.c:577
2023-11-25 13:48:24.961 UTC [133482] STATEMENT:  SELECT 1 FROM isolate_tenant_to_new_shard('shard_split_table', 5, shard_transfer_mode => 'block_writes');
2023-11-25 13:48:24.962 UTC [133482] LOG:  00000: creating auxillary structures (indexes, stats, replicaindentities, triggers) for isolate_tenant_to_new_shard
2023-11-25 13:48:24.962 UTC [133482] LOCATION:  BlockingShardSplit, shard_split.c:587
2023-11-25 13:48:24.962 UTC [133482] STATEMENT:  SELECT 1 FROM isolate_tenant_to_new_shard('shard_split_table', 5, shard_transfer_mode => 'block_writes');
2023-11-25 13:48:24.962 UTC [133482] LOG:  00000: marking deferred cleanup of source shard(s) for isolate_tenant_to_new_shard
2023-11-25 13:48:24.962 UTC [133482] LOCATION:  BlockingShardSplit, shard_split.c:609
2023-11-25 13:48:24.962 UTC [133482] STATEMENT:  SELECT 1 FROM isolate_tenant_to_new_shard('shard_split_table', 5, shard_transfer_mode => 'block_writes');
2023-11-25 13:48:24.963 UTC [133482] LOG:  00000: creating foreign key constraints (if any) for isolate_tenant_to_new_shard
2023-11-25 13:48:24.963 UTC [133482] LOCATION:  BlockingShardSplit, shard_split.c:625
2023-11-25 13:48:24.963 UTC [133482] STATEMENT:  SELECT 1 FROM isolate_tenant_to_new_shard('shard_split_table', 5, shard_transfer_mode => 'block_writes');
2023-11-25 13:48:25.024 UTC [133482] ERROR:  0A000: cannot distribute a temporary table
2023-11-25 13:48:25.024 UTC [133482] LOCATION:  ErrorIfTemporaryTable, create_distributed_table.c:1950
2023-11-25 13:48:25.024 UTC [133482] STATEMENT:  select create_distributed_table('temp_table', 'a');
2023-11-25 13:48:25.024 UTC [133482] ERROR:  0A000: cannot distribute a temporary table
2023-11-25 13:48:25.024 UTC [133482] LOCATION:  ErrorIfTemporaryTable, create_distributed_table.c:1950
2023-11-25 13:48:25.024 UTC [133482] STATEMENT:  select create_reference_table('temp_table');
2023-11-25 13:48:26.043 UTC [133711] WARNING:  0A000: table "uniq_cns_append_tables" has a UNIQUE or EXCLUDE constraint
2023-11-25 13:48:26.043 UTC [133711] DETAIL:  UNIQUE constraints, EXCLUDE constraints, and PRIMARY KEYs on append-partitioned tables cannot be enforced.
2023-11-25 13:48:26.043 UTC [133711] HINT:  Consider using hash partitioning.
2023-11-25 13:48:26.043 UTC [133711] LOCATION:  ErrorIfUnsupportedConstraint, table.c:2975
2023-11-25 13:48:26.044 UTC [133711] WARNING:  0A000: table "excl_cns_append_tables" has a UNIQUE or EXCLUDE constraint
2023-11-25 13:48:26.044 UTC [133711] DETAIL:  UNIQUE constraints, EXCLUDE constraints, and PRIMARY KEYs on append-partitioned tables cannot be enforced.
2023-11-25 13:48:26.044 UTC [133711] HINT:  Consider using hash partitioning.
2023-11-25 13:48:26.044 UTC [133711] LOCATION:  ErrorIfUnsupportedConstraint, table.c:2975
2023-11-25 13:48:26.051 UTC [133711] ERROR:  0A000: cannot create constraint on "pk_on_non_part_col"
2023-11-25 13:48:26.051 UTC [133711] DETAIL:  Distributed relations cannot have UNIQUE, EXCLUDE, or PRIMARY KEY constraints that do not include the partition column (with an equality operator if EXCLUDE).
2023-11-25 13:48:26.051 UTC [133711] LOCATION:  ErrorIfUnsupportedConstraint, table.c:3021
2023-11-25 13:48:26.051 UTC [133711] STATEMENT:  SELECT create_distributed_table('pk_on_non_part_col', 'partition_col', 'hash');
2023-11-25 13:48:26.102 UTC [133711] ERROR:  23505: duplicate key value violates unique constraint "pk_on_non_part_col_pkey_365000"
2023-11-25 13:48:26.102 UTC [133711] DETAIL:  Key (other_col)=(1) already exists.
2023-11-25 13:48:26.102 UTC [133711] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:26.102 UTC [133711] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:26.102 UTC [133711] STATEMENT:  INSERT INTO pk_on_non_part_col VALUES (1,1);
2023-11-25 13:48:26.111 UTC [133711] ERROR:  0A000: cannot create constraint on "uq_on_non_part_col"
2023-11-25 13:48:26.111 UTC [133711] DETAIL:  Distributed relations cannot have UNIQUE, EXCLUDE, or PRIMARY KEY constraints that do not include the partition column (with an equality operator if EXCLUDE).
2023-11-25 13:48:26.111 UTC [133711] LOCATION:  ErrorIfUnsupportedConstraint, table.c:3021
2023-11-25 13:48:26.111 UTC [133711] STATEMENT:  SELECT create_distributed_table('uq_on_non_part_col', 'partition_col', 'hash');
2023-11-25 13:48:26.113 UTC [133711] ERROR:  0A000: cannot create constraint on "ex_on_non_part_col"
2023-11-25 13:48:26.113 UTC [133711] DETAIL:  Distributed relations cannot have UNIQUE, EXCLUDE, or PRIMARY KEY constraints that do not include the partition column (with an equality operator if EXCLUDE).
2023-11-25 13:48:26.113 UTC [133711] LOCATION:  ErrorIfUnsupportedConstraint, table.c:3021
2023-11-25 13:48:26.113 UTC [133711] STATEMENT:  SELECT create_distributed_table('ex_on_non_part_col', 'partition_col', 'hash');
2023-11-25 13:48:26.140 UTC [133711] ERROR:  23P01: conflicting key value violates exclusion constraint "ex_on_non_part_col_other_col_excl_365004"
2023-11-25 13:48:26.140 UTC [133711] DETAIL:  Key (other_col)=(1) conflicts with existing key (other_col)=(1).
2023-11-25 13:48:26.140 UTC [133711] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:26.140 UTC [133711] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:26.140 UTC [133711] STATEMENT:  INSERT INTO ex_on_non_part_col VALUES (1,1);
2023-11-25 13:48:26.194 UTC [133711] ERROR:  23505: duplicate key value violates unique constraint "uq_two_columns_partition_col_other_col_key_365016"
2023-11-25 13:48:26.194 UTC [133711] DETAIL:  Key (partition_col, other_col)=(1, 1) already exists.
2023-11-25 13:48:26.194 UTC [133711] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:26.194 UTC [133711] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:26.194 UTC [133711] STATEMENT:  INSERT INTO uq_two_columns (partition_col, other_col) VALUES (1,1);
2023-11-25 13:48:26.196 UTC [133711] ERROR:  0A000: cannot create constraint on "pk_on_two_non_part_cols"
2023-11-25 13:48:26.196 UTC [133711] DETAIL:  Distributed relations cannot have UNIQUE, EXCLUDE, or PRIMARY KEY constraints that do not include the partition column (with an equality operator if EXCLUDE).
2023-11-25 13:48:26.196 UTC [133711] LOCATION:  ErrorIfUnsupportedConstraint, table.c:3021
2023-11-25 13:48:26.196 UTC [133711] STATEMENT:  SELECT create_distributed_table('pk_on_two_non_part_cols', 'partition_col', 'hash');
2023-11-25 13:48:26.233 UTC [133711] ERROR:  23505: duplicate key value violates unique constraint "pk_on_two_non_part_cols_pkey_365020"
2023-11-25 13:48:26.233 UTC [133711] DETAIL:  Key (other_col, other_col_2)=(1, 1) already exists.
2023-11-25 13:48:26.233 UTC [133711] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:26.233 UTC [133711] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:26.233 UTC [133711] STATEMENT:  INSERT INTO pk_on_two_non_part_cols VALUES (1,1,1);
2023-11-25 13:48:26.312 UTC [133711] ERROR:  23P01: conflicting key value violates exclusion constraint "ex_on_part_col_partition_col_excl_365024"
2023-11-25 13:48:26.312 UTC [133711] DETAIL:  Key (partition_col)=(1) conflicts with existing key (partition_col)=(1).
2023-11-25 13:48:26.312 UTC [133711] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:26.312 UTC [133711] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:26.312 UTC [133711] STATEMENT:  INSERT INTO ex_on_part_col (partition_col, other_col) VALUES (1,2);
2023-11-25 13:48:26.327 UTC [133711] ERROR:  23P01: conflicting key value violates exclusion constraint "ex_on_two_columns_partition_col_other_col_excl_365028"
2023-11-25 13:48:26.327 UTC [133711] DETAIL:  Key (partition_col, other_col)=(1, 1) conflicts with existing key (partition_col, other_col)=(1, 1).
2023-11-25 13:48:26.327 UTC [133711] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:26.327 UTC [133711] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:26.327 UTC [133711] STATEMENT:  INSERT INTO ex_on_two_columns (partition_col, other_col) VALUES (1,1);
2023-11-25 13:48:26.347 UTC [133711] ERROR:  23P01: conflicting key value violates exclusion constraint "ex_on_two_columns_prt_partition_col_other_col_excl_365032"
2023-11-25 13:48:26.347 UTC [133711] DETAIL:  Key (partition_col, other_col)=(1, 101) conflicts with existing key (partition_col, other_col)=(1, 101).
2023-11-25 13:48:26.347 UTC [133711] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:26.347 UTC [133711] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:26.347 UTC [133711] STATEMENT:  INSERT INTO ex_on_two_columns_prt (partition_col, other_col) VALUES (1,101);
2023-11-25 13:48:26.351 UTC [133711] ERROR:  0A000: cannot create constraint on "ex_wrong_operator"
2023-11-25 13:48:26.351 UTC [133711] DETAIL:  Distributed relations cannot have UNIQUE, EXCLUDE, or PRIMARY KEY constraints that do not include the partition column (with an equality operator if EXCLUDE).
2023-11-25 13:48:26.351 UTC [133711] LOCATION:  ErrorIfUnsupportedConstraint, table.c:3021
2023-11-25 13:48:26.351 UTC [133711] STATEMENT:  SELECT create_distributed_table('ex_wrong_operator', 'partition_col', 'hash');
2023-11-25 13:48:26.376 UTC [133711] ERROR:  23P01: conflicting key value violates exclusion constraint "ex_overlaps_other_col_partition_col_excl_365039"
2023-11-25 13:48:26.376 UTC [133711] DETAIL:  Key (other_col, partition_col)=(["2016-01-15 00:00:00","2016-02-01 00:00:00"], ["2016-01-01 00:00:00","2016-02-01 00:00:00"]) conflicts with existing key (other_col, partition_col)=(["2016-01-01 00:00:00","2016-02-01 00:00:00"], ["2016-01-01 00:00:00","2016-02-01 00:00:00"]).
2023-11-25 13:48:26.376 UTC [133711] CONTEXT:  while executing command on localhost:57638
2023-11-25 13:48:26.376 UTC [133711] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:26.376 UTC [133711] STATEMENT:  INSERT INTO ex_overlaps (partition_col, other_col) VALUES ('[2016-01-01 00:00:00, 2016-02-01 00:00:00]', '[2016-01-15 00:00:00, 2016-02-01 00:00:00]');
2023-11-25 13:48:26.417 UTC [133711] ERROR:  23505: duplicate key value violates unique constraint "uq_two_columns_named_uniq_365048"
2023-11-25 13:48:26.417 UTC [133711] DETAIL:  Key (partition_col, other_col)=(1, 1) already exists.
2023-11-25 13:48:26.417 UTC [133711] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:26.417 UTC [133711] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:26.417 UTC [133711] STATEMENT:  INSERT INTO uq_two_columns_named (partition_col, other_col) VALUES (1,1);
2023-11-25 13:48:26.427 UTC [133711] ERROR:  23P01: conflicting key value violates exclusion constraint "ex_on_part_col_named_exclude_365052"
2023-11-25 13:48:26.427 UTC [133711] DETAIL:  Key (partition_col)=(1) conflicts with existing key (partition_col)=(1).
2023-11-25 13:48:26.427 UTC [133711] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:26.427 UTC [133711] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:26.427 UTC [133711] STATEMENT:  INSERT INTO ex_on_part_col_named (partition_col, other_col) VALUES (1,2);
2023-11-25 13:48:26.439 UTC [133711] ERROR:  23P01: conflicting key value violates exclusion constraint "ex_on_two_columns_named_exclude_365056"
2023-11-25 13:48:26.439 UTC [133711] DETAIL:  Key (partition_col, other_col)=(1, 1) conflicts with existing key (partition_col, other_col)=(1, 1).
2023-11-25 13:48:26.439 UTC [133711] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:26.439 UTC [133711] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:26.439 UTC [133711] STATEMENT:  INSERT INTO ex_on_two_columns_named (partition_col, other_col) VALUES (1,1);
2023-11-25 13:48:26.455 UTC [133711] ERROR:  23P01: conflicting key value violates exclusion constraint "ex_multiple_excludes_excl1_365060"
2023-11-25 13:48:26.455 UTC [133711] DETAIL:  Key (partition_col, other_col)=(1, 1) conflicts with existing key (partition_col, other_col)=(1, 1).
2023-11-25 13:48:26.455 UTC [133711] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:26.455 UTC [133711] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:26.455 UTC [133711] STATEMENT:  INSERT INTO ex_multiple_excludes (partition_col, other_col, other_other_col) VALUES (1,1,2);
2023-11-25 13:48:26.455 UTC [133711] ERROR:  23P01: conflicting key value violates exclusion constraint "ex_multiple_excludes_excl2_365060"
2023-11-25 13:48:26.455 UTC [133711] DETAIL:  Key (partition_col, other_other_col)=(1, 1) conflicts with existing key (partition_col, other_other_col)=(1, 1).
2023-11-25 13:48:26.455 UTC [133711] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:26.455 UTC [133711] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:26.455 UTC [133711] STATEMENT:  INSERT INTO ex_multiple_excludes (partition_col, other_col, other_other_col) VALUES (1,2,1);
2023-11-25 13:48:26.457 UTC [133711] ERROR:  0A000: cannot create constraint on "ex_wrong_operator_named"
2023-11-25 13:48:26.457 UTC [133711] DETAIL:  Distributed relations cannot have UNIQUE, EXCLUDE, or PRIMARY KEY constraints that do not include the partition column (with an equality operator if EXCLUDE).
2023-11-25 13:48:26.457 UTC [133711] LOCATION:  ErrorIfUnsupportedConstraint, table.c:3021
2023-11-25 13:48:26.457 UTC [133711] STATEMENT:  SELECT create_distributed_table('ex_wrong_operator_named', 'partition_col', 'hash');
2023-11-25 13:48:26.472 UTC [133711] ERROR:  23P01: conflicting key value violates exclusion constraint "ex_overlaps_operator_named_exclude_365067"
2023-11-25 13:48:26.472 UTC [133711] DETAIL:  Key (other_col, partition_col)=(["2016-01-15 00:00:00","2016-02-01 00:00:00"], ["2016-01-01 00:00:00","2016-02-01 00:00:00"]) conflicts with existing key (other_col, partition_col)=(["2016-01-01 00:00:00","2016-02-01 00:00:00"], ["2016-01-01 00:00:00","2016-02-01 00:00:00"]).
2023-11-25 13:48:26.472 UTC [133711] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:26.472 UTC [133711] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:26.472 UTC [133711] STATEMENT:  INSERT INTO ex_overlaps_named (partition_col, other_col) VALUES ('[2016-01-01 00:00:00, 2016-02-01 00:00:00]', '[2016-01-15 00:00:00, 2016-02-01 00:00:00]');
2023-11-25 13:48:26.640 UTC [133800] ERROR:  2BP01: cannot drop table raw_table_1 because other objects depend on it
2023-11-25 13:48:26.640 UTC [133800] DETAIL:  constraint raw_table_2_user_id_fkey on table raw_table_2 depends on table raw_table_1
2023-11-25 13:48:26.640 UTC [133800] HINT:  Use DROP ... CASCADE to drop the dependent objects too.
2023-11-25 13:48:26.640 UTC [133800] LOCATION:  reportDependentObjects, dependency.c:1189
2023-11-25 13:48:26.640 UTC [133800] STATEMENT:  DROP TABLE raw_table_1;
2023-11-25 13:48:26.880 UTC [133898] ERROR:  0A000: complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
2023-11-25 13:48:26.880 UTC [133898] LOCATION:  JoinOrderList, multi_join_order.c:310
2023-11-25 13:48:26.880 UTC [133898] STATEMENT:  INSERT INTO agg_results_third (user_id, value_1_agg)
	SELECT user_id, array_length(events_table, 1)
	FROM (
	  SELECT user_id, array_agg(event ORDER BY time) AS events_table
	  FROM (
	    SELECT u.user_id, e.event_type::text AS event, e.time
	    FROM users_table AS u,
	         events_table AS e
	    WHERE u.user_id != e.user_id
	      AND u.user_id >= 10
	      AND u.user_id <= 25
	      AND e.event_type IN (100, 101, 102)
	  ) t
	  GROUP BY user_id
	) q;
2023-11-25 13:48:26.883 UTC [133898] ERROR:  0A000: complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
2023-11-25 13:48:26.883 UTC [133898] LOCATION:  JoinOrderList, multi_join_order.c:310
2023-11-25 13:48:26.883 UTC [133898] STATEMENT:  INSERT INTO agg_results_third (user_id, value_1_agg, value_2_agg )
	SELECT user_id, sum(array_length(events_table, 1)), length(hasdone_event)
	FROM (
	  SELECT
	    t1.user_id,
	    array_agg(event ORDER BY time) AS events_table,
	    COALESCE(hasdone_event, 'Has not done event') AS hasdone_event
	  FROM (
	    (
	      SELECT u.user_id, 'step=>1'::text AS event, e.time
	      FROM users_table AS u,
	          events_table AS e
	      WHERE  u.user_id != e.user_id
	      AND u.user_id >= 10
	      AND u.user_id <= 25
	      AND e.event_type IN (100, 101, 102)
	    )
	    UNION
	    (
	      SELECT u.user_id, 'step=>2'::text AS event, e.time
	      FROM users_table AS u,
	         events_table AS e
	      WHERE  u.user_id = e.user_id
	      AND u.user_id >= 10
	      AND u.user_id <= 25
	      AND e.event_type IN (103, 104, 105)
	    )
	  ) t1 LEFT JOIN (
	      SELECT DISTINCT user_id,
	        'Has done event'::TEXT AS hasdone_event
	      FROM  events_table AS e
	      WHERE  e.user_id >= 10
	      AND e.user_id <= 25
	      AND e.event_type IN (106, 107, 108)
	  ) t2 ON (t1.user_id = t2.user_id)
	  GROUP BY  t1.user_id, hasdone_event
	) t GROUP BY user_id, hasdone_event;
2023-11-25 13:48:26.886 UTC [133898] ERROR:  XX000: the query contains a join that requires repartitioning
2023-11-25 13:48:26.886 UTC [133898] HINT:  Set citus.enable_repartition_joins to on to enable repartitioning
2023-11-25 13:48:26.886 UTC [133898] LOCATION:  JobExecutorType, multi_server_executor.c:68
2023-11-25 13:48:26.886 UTC [133898] STATEMENT:  INSERT INTO agg_results_third (user_id, value_1_agg, value_2_agg )
	SELECT user_id, sum(array_length(events_table, 1)), length(hasdone_event)
	FROM (
	  SELECT
	    t1.user_id,
	    array_agg(event ORDER BY time) AS events_table,
	    COALESCE(hasdone_event, 'Has not done event') AS hasdone_event
	  FROM (
	    (
	      SELECT u.user_id, 'step=>1'::text AS event, e.time
	      FROM users_table AS u,
	          events_table AS e
	      WHERE  u.user_id = e.user_id
	      AND u.user_id >= 10
	      AND u.user_id <= 25
	      AND e.event_type IN (100, 101, 102)
	    )
	    UNION
	    (
	      SELECT u.user_id, 'step=>2'::text AS event, e.time
	      FROM users_table AS u,
	         events_table AS e
	      WHERE  u.user_id = e.event_type
	      AND u.user_id >= 10
	      AND u.user_id <= 25
	      AND e.event_type IN (103, 104, 105)
	    )
	  ) t1 LEFT JOIN (
	      SELECT DISTINCT user_id,
	        'Has done event'::TEXT AS hasdone_event
	      FROM  events_table AS e
	      WHERE  e.user_id >= 10
	      AND e.user_id <= 25
	      AND e.event_type IN (106, 107, 108)
	  ) t2 ON (t1.user_id = t2.user_id)
	  GROUP BY  t1.user_id, hasdone_event
	) t GROUP BY user_id, hasdone_event;
2023-11-25 13:48:26.913 UTC [133898] ERROR:  0A000: complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
2023-11-25 13:48:26.913 UTC [133898] LOCATION:  JoinOrderList, multi_join_order.c:310
2023-11-25 13:48:26.913 UTC [133898] STATEMENT:  INSERT INTO agg_results_third (user_id, value_1_agg, value_2_agg)
	SELECT
	  user_id,
	  avg(array_length(events_table, 1)) AS event_average,
	  count_pay
	  FROM (
	  SELECT
	  subquery_1.user_id,
	  array_agg(event ORDER BY time) AS events_table,
	  COALESCE(count_pay, 0) AS count_pay
	  FROM
	  (
	    (SELECT
	      users_table.user_id,
	      'action=>1'AS event,
	      events_table.time
	    FROM
	      users_table,
	      events_table
	    WHERE
	      users_table.user_id = events_table.user_id AND
	      users_table.user_id >= 10 AND
	      users_table.user_id <= 70 AND
	      events_table.event_type > 10 AND events_table.event_type < 12
	      )
	    UNION
	    (SELECT
	      users_table.user_id,
	      'action=>2'AS event,
	      events_table.time
	    FROM
	      users_table,
	      events_table
	    WHERE
	      users_table.user_id != events_table.user_id AND
	      users_table.user_id >= 10 AND
	      users_table.user_id <= 70 AND
	      events_table.event_type > 12 AND events_table.event_type < 14
	    )
	  ) AS subquery_1
	  LEFT JOIN
	    (SELECT
	       user_id,
	      COUNT(*) AS count_pay
	    FROM
	      users_table
	    WHERE
	      user_id >= 10 AND
	      user_id <= 70 AND
	      users_table.value_1 > 15 AND users_table.value_1 < 17
	    GROUP BY
	      user_id
	    HAVING
	      COUNT(*) > 1) AS subquery_2
	  ON
	    subquery_1.user_id = subquery_2.user_id
	  GROUP BY
	    subquery_1.user_id,
	    count_pay) AS subquery_top
	WHERE
	  array_ndims(events_table) > 0
	GROUP BY
	  count_pay, user_id
	ORDER BY
	  count_pay;
2023-11-25 13:48:26.933 UTC [133898] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:26.933 UTC [133898] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:26.933 UTC [133898] STATEMENT:  INSERT INTO agg_results_third (user_id, agg_time, value_2_agg)
	SELECT
	    user_id,
	    user_lastseen,
	    array_length(event_array, 1)
	FROM (
	    SELECT
	        user_id,
	        max(u.time) as user_lastseen,
	        array_agg(event_type ORDER BY u.time) AS event_array
	    FROM (
	        SELECT user_id, time
	        FROM users_table
	        WHERE
	        user_id >= 10 AND
	        user_id <= 70 AND
	        users_table.value_1 > 10 AND users_table.value_1 < 12
	        ) u LEFT JOIN LATERAL (
	          SELECT event_type, time
	          FROM events_table
	          WHERE user_id != u.user_id AND
	          events_table.event_type > 10 AND events_table.event_type < 12
	        ) t ON true
	        GROUP BY user_id
	) AS shard_union
	ORDER BY user_lastseen DESC;
2023-11-25 13:48:26.934 UTC [133898] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:26.934 UTC [133898] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:26.934 UTC [133898] STATEMENT:  INSERT INTO agg_results_third (user_id, agg_time, value_2_agg)
	SELECT
	    user_id,
	    user_lastseen,
	    array_length(event_array, 1)
	FROM (
	    SELECT
	        user_id,
	        max(u.time) as user_lastseen,
	        array_agg(event_type ORDER BY u.time) AS event_array
	    FROM (
	        SELECT user_id, time
	        FROM users_table
	        WHERE
	        user_id >= 10 AND
	        user_id <= 70 AND
	        users_table.value_1 > 10 AND users_table.value_1 < 12
	        ) u LEFT JOIN LATERAL (
	          SELECT event_type, time
	          FROM events_table
	          WHERE event_type = u.user_id AND
	          events_table.event_type > 10 AND events_table.event_type < 12
	        ) t ON true
	        GROUP BY user_id
	) AS shard_union
	ORDER BY user_lastseen DESC;
2023-11-25 13:48:26.936 UTC [133898] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:26.936 UTC [133898] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:26.936 UTC [133898] STATEMENT:  INSERT INTO agg_results_third (user_id, agg_time, value_2_agg)
	SELECT
	    user_id,
	    user_lastseen,
	    array_length(event_array, 1)
	FROM (
	    SELECT
	        user_id,
	        max(u.time) as user_lastseen,
	        array_agg(event_type ORDER BY u.time) AS event_array
	    FROM (
	        SELECT user_id, time, value_3 as val_3
	        FROM users_table
	        WHERE
	        user_id >= 10 AND
	        user_id <= 70 AND
	        users_table.value_1 > 10 AND users_table.value_1 < 12
	        ) u LEFT JOIN LATERAL (
	          SELECT event_type, time
	          FROM events_table
	          WHERE event_type = u.val_3 AND
	          events_table.event_type > 10 AND events_table.event_type < 12
	        ) t ON true
	        GROUP BY user_id
	) AS shard_union
	ORDER BY user_lastseen DESC;
2023-11-25 13:48:26.961 UTC [133898] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:26.961 UTC [133898] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:26.961 UTC [133898] STATEMENT:  INSERT INTO agg_results_third(user_id, value_2_agg)
	SELECT user_id, value_2 FROM users_table WHERE
	  value_1 > 101 AND value_1 < 110
	  AND value_2 >= 5
	  AND EXISTS (SELECT user_id FROM events_table WHERE event_type>101  AND event_type < 110 AND value_3 > 100 AND user_id!=users_table.user_id);
2023-11-25 13:48:26.962 UTC [133898] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:26.962 UTC [133898] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:26.962 UTC [133898] STATEMENT:  INSERT INTO agg_results_third(user_id, value_2_agg)
	SELECT user_id, value_2 FROM users_table WHERE
	  value_1 > 101 AND value_1 < 110
	  AND value_2 >= 5
	  AND EXISTS (SELECT user_id FROM events_table WHERE event_type>101  AND event_type < 110 AND value_3 > 100 AND event_type = users_table.user_id);
2023-11-25 13:48:26.963 UTC [133898] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:26.963 UTC [133898] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:26.963 UTC [133898] STATEMENT:  INSERT INTO agg_results_third(user_id, value_2_agg)
	SELECT user_id, value_2 FROM users_table WHERE
	  value_1 = 101
	  AND value_2 >= 5
	  AND NOT EXISTS (SELECT user_id FROM events_table WHERE event_type=101 AND value_3 > 100 AND user_id!=users_table.user_id);
2023-11-25 13:48:26.963 UTC [133898] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:26.963 UTC [133898] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:26.963 UTC [133898] STATEMENT:  INSERT INTO agg_results_third(user_id, value_2_agg)
	SELECT user_id, value_2 FROM users_table WHERE
	  value_1 = 101
	  AND value_2 >= 5
	  AND NOT EXISTS (SELECT user_id FROM events_table WHERE event_type=101 AND value_3 > 100 AND event_type=users_table.user_id);
2023-11-25 13:48:26.964 UTC [133898] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:26.964 UTC [133898] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:26.964 UTC [133898] STATEMENT:  INSERT INTO agg_results_third(user_id, value_2_agg)
	SELECT user_id, value_2 FROM users_table WHERE
	  value_1 > 100
	  AND value_2 >= 5
	  AND  EXISTS (SELECT user_id FROM events_table WHERE event_type!=100 AND value_3 > 100 AND user_id=users_table.user_id)
	  AND  EXISTS (SELECT user_id FROM events_table WHERE event_type=101 AND value_3 > 100 AND user_id!=users_table.user_id);
2023-11-25 13:48:26.965 UTC [133898] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:26.965 UTC [133898] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:26.965 UTC [133898] STATEMENT:  INSERT INTO agg_results_third(user_id, value_2_agg)
	SELECT user_id, value_2 FROM users_table WHERE
	  value_2 >= 5
	  AND  EXISTS (SELECT user_id FROM events_table WHERE event_type > 100 AND event_type <= 300 AND value_3 > 100 AND user_id!=users_table.user_id)
	  AND  NOT EXISTS (SELECT user_id FROM events_table WHERE event_type > 300 AND event_type <= 350  AND value_3 > 100 AND user_id=users_table.user_id);
2023-11-25 13:48:26.966 UTC [133898] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:26.966 UTC [133898] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:26.966 UTC [133898] STATEMENT:  INSERT INTO agg_results_third(user_id, value_2_agg)
	  SELECT user_id,
	         value_2
	  FROM   users_table
	  WHERE  value_1 > 100
	         AND value_1 < 124
	         AND value_2 >= 5
	         AND EXISTS (SELECT user_id
	                     FROM   events_table
	                     WHERE  event_type > 100
	                            AND event_type < 124
	                            AND value_3 > 100
	                            AND user_id != users_table.user_id
	                     GROUP  BY user_id
	                     HAVING Count(*) > 2);
2023-11-25 13:48:26.966 UTC [133898] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:26.966 UTC [133898] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:26.966 UTC [133898] STATEMENT:  INSERT INTO agg_results_third(user_id, value_2_agg)
	  SELECT user_id,
	         value_2
	  FROM   users_table
	  WHERE  value_1 > 100
	         AND value_1 < 124
	         AND value_2 >= 5
	         AND EXISTS (SELECT user_id
	                     FROM   events_table
	                     WHERE  event_type > 100
	                            AND event_type < 124
	                            AND value_3 > 100
	                            AND event_type = users_table.user_id
	                     GROUP  BY user_id
	                     HAVING Count(*) > 2);
2023-11-25 13:48:26.967 UTC [133898] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:26.967 UTC [133898] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:26.967 UTC [133898] STATEMENT:  INSERT INTO agg_results_third(user_id, value_2_agg)
	  SELECT user_id,
	         value_2
	  FROM   users_table
	  WHERE  value_1 > 100
	         AND value_1 < 124
	         AND value_2 >= 5
	         AND EXISTS (SELECT user_id
	                     FROM   events_table
	                     WHERE  event_type > 100
	                            AND event_type < 124
	                            AND value_3 > 100
	                            AND user_id = users_table.value_1
	                     GROUP  BY user_id
	                     HAVING Count(*) > 2);
2023-11-25 13:48:26.992 UTC [133898] ERROR:  0A000: complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
2023-11-25 13:48:26.992 UTC [133898] LOCATION:  JoinOrderList, multi_join_order.c:310
2023-11-25 13:48:26.992 UTC [133898] STATEMENT:  INSERT INTO agg_results_third(user_id, value_1_agg, value_3_agg)
	SELECT
	    users_table.user_id, users_table.value_1, prob
	FROM
	   users_table
	        JOIN
	   (SELECT
	      ma.user_id, (GREATEST(coalesce(ma.value_4 / 250, 0.0) + GREATEST(1.0))) / 2 AS prob
	    FROM
	      users_table AS ma, events_table as short_list
	    WHERE
	      short_list.user_id != ma.user_id and ma.value_1 < 50 and short_list.event_type < 50
	    ) temp
	  ON users_table.user_id = temp.user_id
	  WHERE users_table.value_1 < 50;
2023-11-25 13:48:26.994 UTC [133898] ERROR:  XX000: the query contains a join that requires repartitioning
2023-11-25 13:48:26.994 UTC [133898] HINT:  Set citus.enable_repartition_joins to on to enable repartitioning
2023-11-25 13:48:26.994 UTC [133898] LOCATION:  JobExecutorType, multi_server_executor.c:68
2023-11-25 13:48:26.994 UTC [133898] STATEMENT:  INSERT INTO agg_results_third(user_id, value_1_agg, value_3_agg)
	SELECT
	    users_table.user_id, users_table.value_1, prob
	FROM
	   users_table
	        JOIN
	   (SELECT
	      ma.user_id, (GREATEST(coalesce(ma.value_4 / 250, 0.0) + GREATEST(1.0))) / 2 AS prob
	    FROM
	      users_table AS ma, events_table as short_list
	    WHERE
	      short_list.user_id = ma.value_2 and ma.value_1 < 50 and short_list.event_type < 50
	    ) temp
	  ON users_table.user_id = temp.user_id
	  WHERE users_table.value_1 < 50;
2023-11-25 13:48:27.031 UTC [133897] ERROR:  23505: duplicate key value violates unique constraint "raw_events_second_user_id_value_1_key_13300004"
2023-11-25 13:48:27.031 UTC [133897] DETAIL:  Key (user_id, value_1)=(1, 10) already exists.
2023-11-25 13:48:27.031 UTC [133897] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:27.031 UTC [133897] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:27.031 UTC [133897] STATEMENT:  INSERT INTO raw_events_second  SELECT * FROM raw_events_first;
2023-11-25 13:48:27.073 UTC [133897] ERROR:  42883: function multi_insert_select.evaluate_on_master(integer) does not exist
2023-11-25 13:48:27.073 UTC [133897] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:48:27.073 UTC [133897] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:27.073 UTC [133897] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:27.073 UTC [133897] STATEMENT:  INSERT INTO raw_events_second (user_id, value_1)
	SELECT
	  user_id, evaluate_on_master(value_1)
	FROM
	  raw_events_first
	WHERE
	  user_id = 0;
2023-11-25 13:48:27.103 UTC [133897] ERROR:  23505: duplicate key value violates unique constraint "raw_events_second_user_id_value_1_key_13300007"
2023-11-25 13:48:27.103 UTC [133897] DETAIL:  Key (user_id, value_1)=(9, 90) already exists.
2023-11-25 13:48:27.103 UTC [133897] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:27.103 UTC [133897] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:27.103 UTC [133897] STATEMENT:  INSERT INTO raw_events_second (user_id, value_1, value_3)
	SELECT
	   user_id, value_1, value_3
	FROM
	   raw_events_first
	WHERE
	   user_id = 9 OR user_id = 16
	RETURNING *;
2023-11-25 13:48:27.128 UTC [133897] ERROR:  23505: duplicate key value violates unique constraint "agg_events_user_id_value_1_agg_key_13300008"
2023-11-25 13:48:27.128 UTC [133897] DETAIL:  Key (user_id, value_1_agg)=(1, 10) already exists.
2023-11-25 13:48:27.128 UTC [133897] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:27.128 UTC [133897] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:27.128 UTC [133897] STATEMENT:  INSERT INTO agg_events (value_3_agg, value_4_agg, value_1_agg, user_id)
	SELECT
	   sum(value_3), count(value_4), sum(value_1), user_id
	FROM
	   raw_events_first
	GROUP BY
	   value_2, user_id
	RETURNING *;
2023-11-25 13:48:27.132 UTC [133897] ERROR:  23505: duplicate key value violates unique constraint "agg_events_user_id_value_1_agg_key_13300008"
2023-11-25 13:48:27.132 UTC [133897] DETAIL:  Key (user_id, value_1_agg)=(1, 10) already exists.
2023-11-25 13:48:27.132 UTC [133897] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:27.132 UTC [133897] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:27.132 UTC [133897] STATEMENT:  INSERT INTO agg_events
	            (value_1_agg,
	             user_id)
	SELECT SUM(value_1),
	       id
	FROM   (SELECT raw_events_second.user_id AS id,
	               raw_events_second.value_1
	        FROM   raw_events_first,
	               raw_events_second
	        WHERE  raw_events_first.user_id = raw_events_second.user_id) AS foo
	GROUP  BY id
	ORDER  BY id;
2023-11-25 13:48:27.136 UTC [133897] ERROR:  23505: duplicate key value violates unique constraint "agg_events_user_id_value_1_agg_key_13300008"
2023-11-25 13:48:27.136 UTC [133897] DETAIL:  Key (user_id, value_1_agg)=(1, 10) already exists.
2023-11-25 13:48:27.136 UTC [133897] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:27.136 UTC [133897] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:27.136 UTC [133897] STATEMENT:  INSERT INTO agg_events
	            (value_4_agg,
	             value_1_agg,
	             user_id)
	SELECT v4,
	       v1,
	       id
	FROM   (SELECT SUM(raw_events_second.value_4) AS v4,
	               SUM(raw_events_first.value_1) AS v1,
	               raw_events_second.user_id      AS id
	        FROM   raw_events_first,
	               raw_events_second
	        WHERE  raw_events_first.user_id = raw_events_second.user_id
	        GROUP  BY raw_events_second.user_id) AS foo
	ORDER  BY id;
2023-11-25 13:48:27.537 UTC [133897] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:27.537 UTC [133897] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:27.537 UTC [133897] STATEMENT:  INSERT INTO agg_events (user_id)
	 SELECT
	   raw_events_first.user_id
	 FROM
	   raw_events_first LEFT JOIN raw_events_second ON raw_events_first.user_id = raw_events_second.value_1;
2023-11-25 13:48:27.617 UTC [133897] ERROR:  0A000: cannot perform distributed planning on this query
2023-11-25 13:48:27.617 UTC [133897] DETAIL:  Cartesian products are currently unsupported
2023-11-25 13:48:27.617 UTC [133897] LOCATION:  JoinSequenceArray, multi_physical_planner.c:3589
2023-11-25 13:48:27.617 UTC [133897] STATEMENT:  INSERT INTO agg_events
	             (user_id)
	 SELECT raw_events_second.user_id
	 FROM   raw_events_first,
	        raw_events_second
	 WHERE  raw_events_first.user_id = raw_events_first.value_1;
2023-11-25 13:48:27.617 UTC [133897] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:27.617 UTC [133897] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:27.617 UTC [133897] STATEMENT:  INSERT INTO agg_events (user_id)
	 SELECT
	   raw_events_first.user_id
	 FROM
	   raw_events_first LEFT JOIN raw_events_second ON raw_events_first.value_1 = raw_events_second.value_1;
2023-11-25 13:48:27.710 UTC [133897] ERROR:  XX000: EXPLAIN ANALYZE is currently not supported for INSERT ... SELECT commands via coordinator
2023-11-25 13:48:27.710 UTC [133897] LOCATION:  NonPushableInsertSelectExplainScan, multi_explain.c:252
2023-11-25 13:48:27.710 UTC [133897] STATEMENT:  EXPLAIN (costs off, analyze on)
	 INSERT INTO agg_events (user_id)
	 SELECT
	   raw_events_first.user_id
	 FROM
	   raw_events_first INNER JOIN raw_events_second ON raw_events_first.value_1 = raw_events_second.value_1;
2023-11-25 13:48:27.712 UTC [133897] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:27.712 UTC [133897] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:27.712 UTC [133897] STATEMENT:  INSERT INTO agg_events (user_id)
	SELECT
	  raw_events_first.user_id
	FROM
	  raw_events_first LEFT JOIN raw_events_second ON raw_events_first.user_id = raw_events_second.value_1
	WHERE
	  raw_events_first.user_id = 10;
2023-11-25 13:48:27.724 UTC [133897] ERROR:  0A000: complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
2023-11-25 13:48:27.724 UTC [133897] LOCATION:  JoinOrderList, multi_join_order.c:310
2023-11-25 13:48:27.724 UTC [133897] STATEMENT:  INSERT INTO agg_events
	            (value_4_agg,
	             value_1_agg,
	             user_id)
	SELECT v4,
	       v1,
	       id
	FROM   (SELECT SUM(raw_events_second.value_4) AS v4,
	               SUM(raw_events_first.value_1) AS v1,
	               raw_events_second.user_id      AS id
	        FROM   raw_events_first,
	               raw_events_second
	        WHERE  raw_events_first.user_id != raw_events_second.user_id
	        GROUP  BY raw_events_second.user_id) AS foo;
2023-11-25 13:48:27.728 UTC [133897] ERROR:  22004: the partition column of table multi_insert_select.agg_events cannot be NULL
2023-11-25 13:48:27.728 UTC [133897] LOCATION:  ShardIdForTuple, multi_copy.c:2592
2023-11-25 13:48:27.728 UTC [133897] STATEMENT:  INSERT INTO agg_events
	            (value_4_agg,
	             value_1_agg,
	             user_id)
	SELECT v4,
	       v1,
	       id
	FROM   (SELECT SUM(raw_events_second.value_4) AS v4,
	               SUM(raw_events_first.value_1) AS v1,
	               raw_events_second.value_3      AS id
	        FROM   raw_events_first,
	               raw_events_second
	        WHERE  raw_events_first.user_id = raw_events_second.user_id
	        GROUP  BY raw_events_second.value_3) AS foo;
2023-11-25 13:48:27.729 UTC [133897] ERROR:  22004: the partition column of table multi_insert_select.raw_events_second should have a value
2023-11-25 13:48:27.729 UTC [133897] LOCATION:  NonPushableInsertSelectExecScan, insert_select_executor.c:153
2023-11-25 13:48:27.729 UTC [133897] STATEMENT:  INSERT INTO raw_events_second
	            (value_1)
	SELECT value_1
	FROM   raw_events_first;
2023-11-25 13:48:27.729 UTC [133897] ERROR:  22004: the partition column of table multi_insert_select.raw_events_second should have a value
2023-11-25 13:48:27.729 UTC [133897] LOCATION:  NonPushableInsertSelectExecScan, insert_select_executor.c:153
2023-11-25 13:48:27.729 UTC [133897] STATEMENT:  INSERT INTO raw_events_second
	            (value_1)
	SELECT user_id
	FROM   raw_events_first;
2023-11-25 13:48:27.730 UTC [133897] ERROR:  22004: the partition column value cannot be NULL
2023-11-25 13:48:27.730 UTC [133897] CONTEXT:  while executing command on localhost:57638
2023-11-25 13:48:27.730 UTC [133897] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:27.730 UTC [133897] STATEMENT:  INSERT INTO raw_events_second
	            (user_id)
	SELECT value_1
	FROM   raw_events_first;
2023-11-25 13:48:27.758 UTC [133897] ERROR:  22004: the partition column value cannot be NULL
2023-11-25 13:48:27.758 UTC [133897] CONTEXT:  while executing command on localhost:57638
2023-11-25 13:48:27.758 UTC [133897] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:27.758 UTC [133897] STATEMENT:  INSERT INTO agg_events
	            (value_3_agg,
	             value_4_agg,
	             value_1_agg,
	             value_2_agg,
	             user_id)
	SELECT SUM(value_3),
	       Count(value_4),
	       user_id,
	       SUM(value_1),
	       Avg(value_2)
	FROM   raw_events_first
	GROUP  BY user_id;
2023-11-25 13:48:27.762 UTC [133897] ERROR:  22004: the partition column value cannot be NULL
2023-11-25 13:48:27.762 UTC [133897] CONTEXT:  while executing command on localhost:57638
2023-11-25 13:48:27.762 UTC [133897] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:27.762 UTC [133897] STATEMENT:  INSERT INTO agg_events
	            (value_3_agg,
	             value_4_agg,
	             value_1_agg,
	             value_2_agg,
	             user_id)
	SELECT SUM(value_3),
	       Count(value_4),
	       user_id,
	       SUM(value_1),
	       value_2
	FROM   raw_events_first
	GROUP  BY user_id,
	          value_2;
2023-11-25 13:48:27.859 UTC [133897] ERROR:  0A000: could not run distributed query with GROUPING SETS, CUBE, or ROLLUP
2023-11-25 13:48:27.859 UTC [133897] HINT:  Consider using an equality filter on the distributed table's partition column.
2023-11-25 13:48:27.859 UTC [133897] LOCATION:  DeferErrorIfQueryNotSupported, multi_logical_planner.c:945
2023-11-25 13:48:27.859 UTC [133897] STATEMENT:  INSERT INTO agg_events
	            (user_id,
	             value_1_agg,
	             value_2_agg)
	SELECT user_id,
	       Sum(value_1) AS sum_val1,
	       Sum(value_2) AS sum_val2
	FROM   raw_events_second
	GROUP  BY grouping sets ( ( user_id ), ( value_1 ), ( user_id, value_1 ), ( ) );
2023-11-25 13:48:28.716 UTC [133897] ERROR:  0A000: INSERT ... SELECT into an append-distributed table is not supported
2023-11-25 13:48:28.716 UTC [133897] LOCATION:  NonPushableInsertSelectSupported, insert_select_planner.c:1572
2023-11-25 13:48:28.716 UTC [133897] STATEMENT:  INSERT INTO insert_append_table (user_id, value_4)
	SELECT user_id, 1 FROM raw_events_second LIMIT 5;
2023-11-25 13:48:28.838 UTC [133897] ERROR:  XX000: value too long for type character(1)
2023-11-25 13:48:28.838 UTC [133897] LOCATION:  ReportCopyError, multi_copy.c:1193
2023-11-25 13:48:28.838 UTC [133897] STATEMENT:  INSERT INTO coerce_agg(user_id, value_1_agg)
	SELECT *
	FROM (
	  SELECT user_id, value_1
	  FROM coerce_events
	) AS ftop
	LIMIT 5;
2023-11-25 13:48:28.861 UTC [133897] ERROR:  XX000: value too long for type character(1)
2023-11-25 13:48:28.861 UTC [133897] LOCATION:  ReportCopyError, multi_copy.c:1193
2023-11-25 13:48:28.861 UTC [133897] STATEMENT:  INSERT INTO coerce_agg(user_id, value_1_agg)
	SELECT *
	FROM (
	  SELECT user_id, value_1
	  FROM coerce_events
	) AS ftop
	LIMIT 5;
2023-11-25 13:48:28.903 UTC [133897] ERROR:  23514: new row for relation "coerce_agg_13300067" violates check constraint "small_number_13300067"
2023-11-25 13:48:28.903 UTC [133897] DETAIL:  Failing row contains (10, 10).
2023-11-25 13:48:28.903 UTC [133897] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:28.903 UTC [133897] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:28.903 UTC [133897] STATEMENT:  INSERT INTO coerce_agg(user_id, value_1_agg)
	SELECT *
	FROM (
	  SELECT user_id, value_1
	  FROM coerce_events
	) AS ftop;
2023-11-25 13:48:28.970 UTC [133897] ERROR:  0A000: modifying the partition value of rows is not allowed
2023-11-25 13:48:28.970 UTC [133897] LOCATION:  ErrorIfOnConflictNotSupported, multi_router_planner.c:1255
2023-11-25 13:48:28.970 UTC [133897] STATEMENT:  INSERT INTO agg_events AS ae
	            (
	                        user_id,
	                        value_1_agg,
	                        agg_time
	            )
	SELECT user_id,
	       value_1,
	       time
	FROM   raw_events_first
	ON conflict (user_id, value_1_agg)
	DO UPDATE
	   SET    user_id = 42
	RETURNING user_id, value_1_agg;
2023-11-25 13:48:29.517 UTC [135096] ERROR:  42P01: relation "users_copy_table" does not exist at character 26
2023-11-25 13:48:29.517 UTC [135096] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 13:48:29.517 UTC [135096] STATEMENT:  SELECT SUM(value_3) FROM users_copy_table;
2023-11-25 13:48:29.860 UTC [135096] ERROR:  21000: more than one row returned by a subquery used as an expression
2023-11-25 13:48:29.860 UTC [135096] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:29.860 UTC [135096] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:29.860 UTC [135096] STATEMENT:  UPDATE users_test_table as utt
	SET    value_1 = 3
	WHERE value_2 > (SELECT value_3 FROM events_test_table as ett WHERE utt.user_id = ett.user_id);
2023-11-25 13:48:29.860 UTC [135096] ERROR:  0A000: only reference tables may be queried when targeting a reference table with multi shard UPDATE/DELETE queries with multiple tables 
2023-11-25 13:48:29.860 UTC [135096] LOCATION:  MultiShardUpdateDeleteSupported, multi_router_planner.c:1294
2023-11-25 13:48:29.860 UTC [135096] STATEMENT:  UPDATE users_reference_copy_table
	SET    value_2 = 5
	FROM   events_test_table
	WHERE  users_reference_copy_table.user_id = events_test_table.user_id;
2023-11-25 13:48:29.860 UTC [135096] ERROR:  0A000: a join with USING causes an internal naming conflict, use ON instead
2023-11-25 13:48:29.860 UTC [135096] LOCATION:  MultiShardUpdateDeleteSupported, multi_router_planner.c:1279
2023-11-25 13:48:29.860 UTC [135096] STATEMENT:  UPDATE events_test_table
	SET value_2 = users_test_table.user_id
	FROM users_test_table
	FULL OUTER JOIN events_test_table e2 USING (user_id)
	WHERE e2.user_id = events_test_table.user_id RETURNING events_test_table.value_2;
2023-11-25 13:48:29.880 UTC [135096] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:29.880 UTC [135096] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:29.880 UTC [135096] STATEMENT:  UPDATE users_test_table
	SET    value_2 = (SELECT value_3
	                  FROM   users_test_table);
2023-11-25 13:48:29.880 UTC [135096] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:29.880 UTC [135096] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:29.880 UTC [135096] STATEMENT:  UPDATE users_test_table
	SET value_2 = 2
	WHERE
	  value_2 >
	          (SELECT
	              max(value_2)
	           FROM
	              events_test_table
	           WHERE
	              users_test_table.user_id > events_test_table.user_id AND
	              users_test_table.value_1 = events_test_table.value_1
	           GROUP BY
	              user_id
	          );
2023-11-25 13:48:29.890 UTC [135096] ERROR:  0A000: functions used in the WHERE/ON/WHEN clause of modification queries on distributed tables must not be VOLATILE
2023-11-25 13:48:29.890 UTC [135096] LOCATION:  TargetlistAndFunctionsSupported, multi_router_planner.c:616
2023-11-25 13:48:29.890 UTC [135096] STATEMENT:  UPDATE users_test_table
	SET    value_2 = 5
	FROM   events_test_table
	WHERE  users_test_table.user_id = events_test_table.user_id * random();
2023-11-25 13:48:29.890 UTC [135096] ERROR:  0A000: functions used in UPDATE queries on distributed tables must not be VOLATILE
2023-11-25 13:48:29.890 UTC [135096] LOCATION:  TargetlistAndFunctionsSupported, multi_router_planner.c:575
2023-11-25 13:48:29.890 UTC [135096] STATEMENT:  UPDATE users_test_table
	SET    value_2 = 5 * random()
	FROM   events_test_table
	WHERE  users_test_table.user_id = events_test_table.user_id;
2023-11-25 13:48:29.891 UTC [135096] ERROR:  0A000: functions used in UPDATE queries on distributed tables must not be VOLATILE
2023-11-25 13:48:29.891 UTC [135096] LOCATION:  SingleShardUpdateDeleteSupported, multi_router_planner.c:1330
2023-11-25 13:48:29.891 UTC [135096] STATEMENT:  UPDATE users_test_table
	SET    value_1 = 3
	WHERE  user_id = 1 AND value_1 IN (SELECT value_1
	                                   FROM users_test_table
	                                   WHERE user_id = 1 AND value_2 > random());
2023-11-25 13:48:29.891 UTC [135096] ERROR:  0A000: functions used in UPDATE queries on distributed tables must not be VOLATILE
2023-11-25 13:48:29.891 UTC [135096] LOCATION:  SingleShardUpdateDeleteSupported, multi_router_planner.c:1330
2023-11-25 13:48:29.891 UTC [135096] STATEMENT:  UPDATE users_test_table
	SET    value_2 = subquery.random FROM (SELECT user_id, random()
	                                       FROM events_test_table) subquery
	WHERE  users_test_table.user_id = subquery.user_id;
2023-11-25 13:48:29.917 UTC [135096] ERROR:  0A000: cannot push down this subquery
2023-11-25 13:48:29.917 UTC [135096] DETAIL:  Shards of relations in subquery need to have 1-to-1 shard partitioning
2023-11-25 13:48:29.917 UTC [135096] LOCATION:  ErrorIfUnsupportedShardDistribution, multi_physical_planner.c:2432
2023-11-25 13:48:29.917 UTC [135096] STATEMENT:  UPDATE users_test_table
	SET    value_2 = 5
	FROM   events_test_table_2
	WHERE  users_test_table.user_id = events_test_table_2.user_id;
2023-11-25 13:48:29.920 UTC [135096] ERROR:  21000: more than one row returned by a subquery used as an expression
2023-11-25 13:48:29.920 UTC [135096] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:29.920 UTC [135096] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:29.920 UTC [135096] STATEMENT:  DELETE FROM users_test_table
	WHERE  users_test_table.user_id = (SELECT user_id
	                                   FROM   events_test_table);
2023-11-25 13:48:29.927 UTC [135096] ERROR:  0A000: cannot run DML queries with cursors
2023-11-25 13:48:29.927 UTC [135096] LOCATION:  TargetlistAndFunctionsSupported, multi_router_planner.c:655
2023-11-25 13:48:29.927 UTC [135096] STATEMENT:  UPDATE users_test_table SET value_2 = 5 WHERE CURRENT OF test_cursor;
2023-11-25 13:48:29.967 UTC [135096] ERROR:  0A000: functions used in UPDATE queries on distributed tables must not be VOLATILE
2023-11-25 13:48:29.967 UTC [135096] LOCATION:  TargetlistAndFunctionsSupported, multi_router_planner.c:575
2023-11-25 13:48:29.967 UTC [135096] STATEMENT:  UPDATE test_table_2 SET double_col = random();
2023-11-25 13:48:30.202 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.202 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.202 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.202 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.202 UTC [135455] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 13:48:30.202 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.202 UTC [135455] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 13:48:30.202 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.202 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.202 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.202 UTC [135455] DEBUG:  00000: push down of limit count: 5
2023-11-25 13:48:30.202 UTC [135455] LOCATION:  WorkerLimitCount, multi_logical_optimizer.c:4778
2023-11-25 13:48:30.202 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.202 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.202 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.202 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.202 UTC [135455] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 13:48:30.202 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.202 UTC [135455] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 13:48:30.202 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.202 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum, worker_column_2 AS worker_column_3 FROM (SELECT us.user_id AS worker_column_1, us.value_1 AS worker_column_2 FROM (public.users_table_1400256 us JOIN public.events_table_1400260 ev ON ((us.user_id OPERATOR(pg_catalog.=) ev.user_id)))) worker_subquery GROUP BY worker_column_1, worker_column_2 ORDER BY worker_column_1, (sum(worker_column_2) OVER (PARTITION BY worker_column_1)) LIMIT '5'::bigint
2023-11-25 13:48:30.202 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.202 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum, worker_column_2 AS worker_column_3 FROM (SELECT us.user_id AS worker_column_1, us.value_1 AS worker_column_2 FROM (public.users_table_1400257 us JOIN public.events_table_1400261 ev ON ((us.user_id OPERATOR(pg_catalog.=) ev.user_id)))) worker_subquery GROUP BY worker_column_1, worker_column_2 ORDER BY worker_column_1, (sum(worker_column_2) OVER (PARTITION BY worker_column_1)) LIMIT '5'::bigint
2023-11-25 13:48:30.202 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.202 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum, worker_column_2 AS worker_column_3 FROM (SELECT us.user_id AS worker_column_1, us.value_1 AS worker_column_2 FROM (public.users_table_1400258 us JOIN public.events_table_1400262 ev ON ((us.user_id OPERATOR(pg_catalog.=) ev.user_id)))) worker_subquery GROUP BY worker_column_1, worker_column_2 ORDER BY worker_column_1, (sum(worker_column_2) OVER (PARTITION BY worker_column_1)) LIMIT '5'::bigint
2023-11-25 13:48:30.202 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.202 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum, worker_column_2 AS worker_column_3 FROM (SELECT us.user_id AS worker_column_1, us.value_1 AS worker_column_2 FROM (public.users_table_1400259 us JOIN public.events_table_1400263 ev ON ((us.user_id OPERATOR(pg_catalog.=) ev.user_id)))) worker_subquery GROUP BY worker_column_1, worker_column_2 ORDER BY worker_column_1, (sum(worker_column_2) OVER (PARTITION BY worker_column_1)) LIMIT '5'::bigint
2023-11-25 13:48:30.202 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.202 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.202 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.202 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.202 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.202 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.202 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.202 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.202 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.202 UTC [135455] DEBUG:  00000: combine query: SELECT user_id, sum FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, sum bigint, worker_column_3 integer) ORDER BY user_id, sum LIMIT '5'::bigint
2023-11-25 13:48:30.202 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.202 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.202 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.203 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 13 in 3515 microseconds
2023-11-25 13:48:30.203 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.203 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.203 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.203 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 15 in 8559 microseconds
2023-11-25 13:48:30.203 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.204 UTC [135455] DEBUG:  00000: task execution (1) for placement (856) on anchor shard (1400260) finished in 919 microseconds on worker node localhost:57637
2023-11-25 13:48:30.204 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.204 UTC [135455] DEBUG:  00000: task execution (2) for placement (857) on anchor shard (1400261) finished in 1208 microseconds on worker node localhost:57638
2023-11-25 13:48:30.204 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.204 UTC [135455] DEBUG:  00000: task execution (3) for placement (858) on anchor shard (1400262) finished in 576 microseconds on worker node localhost:57637
2023-11-25 13:48:30.204 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.205 UTC [135455] DEBUG:  00000: task execution (4) for placement (859) on anchor shard (1400263) finished in 546 microseconds on worker node localhost:57638
2023-11-25 13:48:30.205 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.207 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 14 in 4456 microseconds
2023-11-25 13:48:30.207 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.208 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 16 in 5015 microseconds
2023-11-25 13:48:30.208 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.208 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 13: 2 to node localhost:57637
2023-11-25 13:48:30.208 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.208 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 14: 0 to node localhost:57637
2023-11-25 13:48:30.208 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.208 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 15: 2 to node localhost:57638
2023-11-25 13:48:30.208 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.208 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 16: 0 to node localhost:57638
2023-11-25 13:48:30.208 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.208 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.208 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.208 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.208 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.208 UTC [135455] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 13:48:30.208 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.208 UTC [135455] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 13:48:30.208 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.208 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.208 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.208 UTC [135455] DEBUG:  00000: push down of limit count: 5
2023-11-25 13:48:30.208 UTC [135455] LOCATION:  WorkerLimitCount, multi_logical_optimizer.c:4778
2023-11-25 13:48:30.208 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.208 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.208 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.208 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.208 UTC [135455] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 13:48:30.208 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.208 UTC [135455] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 13:48:30.208 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.209 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS value_1, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum FROM (SELECT j.user_id AS worker_column_1, j.value_1 AS worker_column_2 FROM (public.users_table_1400256 us(user_id, "time", value_1, value_2, value_3, value_4) JOIN public.events_table_1400260 ev(user_id, "time", event_type, value_2, value_3, value_4) USING (user_id)) j(user_id, "time", value_1, value_2, value_3, value_4, time_1, event_type, value_2_1, value_3_1, value_4_1)) worker_subquery GROUP BY worker_column_1, worker_column_2 ORDER BY (sum(worker_column_2) OVER (PARTITION BY worker_column_1)) DESC, worker_column_2 DESC, worker_column_1 DESC LIMIT '5'::bigint
2023-11-25 13:48:30.209 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.209 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS value_1, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum FROM (SELECT j.user_id AS worker_column_1, j.value_1 AS worker_column_2 FROM (public.users_table_1400257 us(user_id, "time", value_1, value_2, value_3, value_4) JOIN public.events_table_1400261 ev(user_id, "time", event_type, value_2, value_3, value_4) USING (user_id)) j(user_id, "time", value_1, value_2, value_3, value_4, time_1, event_type, value_2_1, value_3_1, value_4_1)) worker_subquery GROUP BY worker_column_1, worker_column_2 ORDER BY (sum(worker_column_2) OVER (PARTITION BY worker_column_1)) DESC, worker_column_2 DESC, worker_column_1 DESC LIMIT '5'::bigint
2023-11-25 13:48:30.209 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.209 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS value_1, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum FROM (SELECT j.user_id AS worker_column_1, j.value_1 AS worker_column_2 FROM (public.users_table_1400258 us(user_id, "time", value_1, value_2, value_3, value_4) JOIN public.events_table_1400262 ev(user_id, "time", event_type, value_2, value_3, value_4) USING (user_id)) j(user_id, "time", value_1, value_2, value_3, value_4, time_1, event_type, value_2_1, value_3_1, value_4_1)) worker_subquery GROUP BY worker_column_1, worker_column_2 ORDER BY (sum(worker_column_2) OVER (PARTITION BY worker_column_1)) DESC, worker_column_2 DESC, worker_column_1 DESC LIMIT '5'::bigint
2023-11-25 13:48:30.209 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.209 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS value_1, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum FROM (SELECT j.user_id AS worker_column_1, j.value_1 AS worker_column_2 FROM (public.users_table_1400259 us(user_id, "time", value_1, value_2, value_3, value_4) JOIN public.events_table_1400263 ev(user_id, "time", event_type, value_2, value_3, value_4) USING (user_id)) j(user_id, "time", value_1, value_2, value_3, value_4, time_1, event_type, value_2_1, value_3_1, value_4_1)) worker_subquery GROUP BY worker_column_1, worker_column_2 ORDER BY (sum(worker_column_2) OVER (PARTITION BY worker_column_1)) DESC, worker_column_2 DESC, worker_column_1 DESC LIMIT '5'::bigint
2023-11-25 13:48:30.209 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.209 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.209 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.209 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.209 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.209 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.209 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.209 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.209 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.209 UTC [135455] DEBUG:  00000: combine query: SELECT user_id, value_1, sum FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, value_1 integer, sum bigint) ORDER BY sum DESC, value_1 DESC, user_id DESC LIMIT '5'::bigint
2023-11-25 13:48:30.209 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.209 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.209 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.209 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 17 in 3515 microseconds
2023-11-25 13:48:30.209 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.209 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.209 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.209 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 19 in 8559 microseconds
2023-11-25 13:48:30.209 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.210 UTC [135455] DEBUG:  00000: task execution (1) for placement (856) on anchor shard (1400260) finished in 896 microseconds on worker node localhost:57637
2023-11-25 13:48:30.210 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.210 UTC [135455] DEBUG:  00000: task execution (3) for placement (858) on anchor shard (1400262) finished in 486 microseconds on worker node localhost:57637
2023-11-25 13:48:30.210 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.210 UTC [135455] DEBUG:  00000: task execution (2) for placement (857) on anchor shard (1400261) finished in 810 microseconds on worker node localhost:57638
2023-11-25 13:48:30.210 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.211 UTC [135455] DEBUG:  00000: task execution (4) for placement (859) on anchor shard (1400263) finished in 770 microseconds on worker node localhost:57638
2023-11-25 13:48:30.211 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.213 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 20 in 3876 microseconds
2023-11-25 13:48:30.213 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.214 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 18 in 5050 microseconds
2023-11-25 13:48:30.214 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.214 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 17: 2 to node localhost:57637
2023-11-25 13:48:30.214 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.214 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 18: 0 to node localhost:57637
2023-11-25 13:48:30.214 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.214 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 19: 2 to node localhost:57638
2023-11-25 13:48:30.214 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.214 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 20: 0 to node localhost:57638
2023-11-25 13:48:30.214 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.223 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.223 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.223 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.223 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.223 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.223 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.223 UTC [135455] DEBUG:  00000: push down of limit count: 10
2023-11-25 13:48:30.223 UTC [135455] LOCATION:  WorkerLimitCount, multi_logical_optimizer.c:4778
2023-11-25 13:48:30.223 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.223 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.223 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.223 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.223 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS rank FROM (SELECT window_view.user_id AS worker_column_1, window_view.rank AS worker_column_2 FROM (SELECT DISTINCT users_table.user_id, rank() OVER (PARTITION BY users_table.user_id ORDER BY users_table.value_1) AS rank FROM public.users_table_1400256 users_table GROUP BY users_table.user_id, users_table.value_1 HAVING (count(*) OPERATOR(pg_catalog.>) 1)) window_view) worker_subquery ORDER BY worker_column_2 DESC, worker_column_1 LIMIT '10'::bigint
2023-11-25 13:48:30.223 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.223 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS rank FROM (SELECT window_view.user_id AS worker_column_1, window_view.rank AS worker_column_2 FROM (SELECT DISTINCT users_table.user_id, rank() OVER (PARTITION BY users_table.user_id ORDER BY users_table.value_1) AS rank FROM public.users_table_1400257 users_table GROUP BY users_table.user_id, users_table.value_1 HAVING (count(*) OPERATOR(pg_catalog.>) 1)) window_view) worker_subquery ORDER BY worker_column_2 DESC, worker_column_1 LIMIT '10'::bigint
2023-11-25 13:48:30.223 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.223 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS rank FROM (SELECT window_view.user_id AS worker_column_1, window_view.rank AS worker_column_2 FROM (SELECT DISTINCT users_table.user_id, rank() OVER (PARTITION BY users_table.user_id ORDER BY users_table.value_1) AS rank FROM public.users_table_1400258 users_table GROUP BY users_table.user_id, users_table.value_1 HAVING (count(*) OPERATOR(pg_catalog.>) 1)) window_view) worker_subquery ORDER BY worker_column_2 DESC, worker_column_1 LIMIT '10'::bigint
2023-11-25 13:48:30.223 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.223 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS rank FROM (SELECT window_view.user_id AS worker_column_1, window_view.rank AS worker_column_2 FROM (SELECT DISTINCT users_table.user_id, rank() OVER (PARTITION BY users_table.user_id ORDER BY users_table.value_1) AS rank FROM public.users_table_1400259 users_table GROUP BY users_table.user_id, users_table.value_1 HAVING (count(*) OPERATOR(pg_catalog.>) 1)) window_view) worker_subquery ORDER BY worker_column_2 DESC, worker_column_1 LIMIT '10'::bigint
2023-11-25 13:48:30.223 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.223 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.223 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.223 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.223 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.223 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.223 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.223 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.223 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.223 UTC [135455] DEBUG:  00000: combine query: SELECT user_id, rank FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, rank bigint) ORDER BY rank DESC, user_id LIMIT '10'::bigint
2023-11-25 13:48:30.223 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.223 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.223 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.223 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 21 in 3515 microseconds
2023-11-25 13:48:30.223 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.223 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.223 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.224 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 23 in 8559 microseconds
2023-11-25 13:48:30.224 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.224 UTC [135455] DEBUG:  00000: task execution (1) for placement (852) on anchor shard (1400256) finished in 566 microseconds on worker node localhost:57637
2023-11-25 13:48:30.224 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.225 UTC [135455] DEBUG:  00000: task execution (2) for placement (853) on anchor shard (1400257) finished in 1081 microseconds on worker node localhost:57638
2023-11-25 13:48:30.225 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.225 UTC [135455] DEBUG:  00000: task execution (3) for placement (854) on anchor shard (1400258) finished in 720 microseconds on worker node localhost:57637
2023-11-25 13:48:30.225 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.225 UTC [135455] DEBUG:  00000: task execution (4) for placement (855) on anchor shard (1400259) finished in 505 microseconds on worker node localhost:57638
2023-11-25 13:48:30.225 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.227 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 22 in 3265 microseconds
2023-11-25 13:48:30.227 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.228 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 24 in 4794 microseconds
2023-11-25 13:48:30.228 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.228 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 21: 2 to node localhost:57637
2023-11-25 13:48:30.228 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.228 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 22: 0 to node localhost:57637
2023-11-25 13:48:30.228 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.228 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 23: 2 to node localhost:57638
2023-11-25 13:48:30.228 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.228 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 24: 0 to node localhost:57638
2023-11-25 13:48:30.228 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.233 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.233 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.233 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.233 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.233 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.233 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.234 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.234 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.234 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.234 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.234 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, rank() OVER (PARTITION BY worker_column_1 ORDER BY worker_column_2) AS rank, worker_column_2 AS worker_column_3 FROM (SELECT users_view.user_id AS worker_column_1, users_view.value_1 AS worker_column_2 FROM (SELECT users_table.user_id, users_table."time", users_table.value_1, users_table.value_2, users_table.value_3, users_table.value_4 FROM public.users_table_1400256 users_table) users_view) worker_subquery GROUP BY worker_column_1, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 4)
2023-11-25 13:48:30.234 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.234 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, rank() OVER (PARTITION BY worker_column_1 ORDER BY worker_column_2) AS rank, worker_column_2 AS worker_column_3 FROM (SELECT users_view.user_id AS worker_column_1, users_view.value_1 AS worker_column_2 FROM (SELECT users_table.user_id, users_table."time", users_table.value_1, users_table.value_2, users_table.value_3, users_table.value_4 FROM public.users_table_1400257 users_table) users_view) worker_subquery GROUP BY worker_column_1, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 4)
2023-11-25 13:48:30.234 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.234 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, rank() OVER (PARTITION BY worker_column_1 ORDER BY worker_column_2) AS rank, worker_column_2 AS worker_column_3 FROM (SELECT users_view.user_id AS worker_column_1, users_view.value_1 AS worker_column_2 FROM (SELECT users_table.user_id, users_table."time", users_table.value_1, users_table.value_2, users_table.value_3, users_table.value_4 FROM public.users_table_1400258 users_table) users_view) worker_subquery GROUP BY worker_column_1, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 4)
2023-11-25 13:48:30.234 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.234 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, rank() OVER (PARTITION BY worker_column_1 ORDER BY worker_column_2) AS rank, worker_column_2 AS worker_column_3 FROM (SELECT users_view.user_id AS worker_column_1, users_view.value_1 AS worker_column_2 FROM (SELECT users_table.user_id, users_table."time", users_table.value_1, users_table.value_2, users_table.value_3, users_table.value_4 FROM public.users_table_1400259 users_table) users_view) worker_subquery GROUP BY worker_column_1, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 4)
2023-11-25 13:48:30.234 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.234 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.234 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.234 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.234 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.234 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.234 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.234 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.234 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.234 UTC [135455] DEBUG:  00000: combine query: SELECT DISTINCT user_id, rank FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, rank bigint, worker_column_3 integer) ORDER BY rank DESC, user_id
2023-11-25 13:48:30.234 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.234 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.234 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.234 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 25 in 3515 microseconds
2023-11-25 13:48:30.234 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.234 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.234 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.235 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 27 in 8559 microseconds
2023-11-25 13:48:30.235 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.235 UTC [135455] DEBUG:  00000: task execution (1) for placement (852) on anchor shard (1400256) finished in 593 microseconds on worker node localhost:57637
2023-11-25 13:48:30.235 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.235 UTC [135455] DEBUG:  00000: task execution (2) for placement (853) on anchor shard (1400257) finished in 414 microseconds on worker node localhost:57638
2023-11-25 13:48:30.235 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.235 UTC [135455] DEBUG:  00000: task execution (3) for placement (854) on anchor shard (1400258) finished in 651 microseconds on worker node localhost:57637
2023-11-25 13:48:30.235 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.236 UTC [135455] DEBUG:  00000: task execution (4) for placement (855) on anchor shard (1400259) finished in 613 microseconds on worker node localhost:57638
2023-11-25 13:48:30.236 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.238 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 26 in 3860 microseconds
2023-11-25 13:48:30.238 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.240 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 28 in 5231 microseconds
2023-11-25 13:48:30.240 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.240 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 25: 2 to node localhost:57637
2023-11-25 13:48:30.240 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.240 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 26: 0 to node localhost:57637
2023-11-25 13:48:30.240 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.240 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 27: 2 to node localhost:57638
2023-11-25 13:48:30.240 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.240 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 28: 0 to node localhost:57638
2023-11-25 13:48:30.240 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.240 UTC [135455] DEBUG:  00000: switching to sequential query execution mode
2023-11-25 13:48:30.240 UTC [135455] DETAIL:  A command for a distributed view is run. To make sure subsequent commands see the view correctly we need to make sure to use only one connection for all future commands
2023-11-25 13:48:30.240 UTC [135455] LOCATION:  EnsureSequentialMode, multi_executor.c:745
2023-11-25 13:48:30.241 UTC [135455] DEBUG:  00000: drop auto-cascades to type window_view
2023-11-25 13:48:30.241 UTC [135455] LOCATION:  reportDependentObjects, dependency.c:1122
2023-11-25 13:48:30.241 UTC [135455] DEBUG:  00000: drop auto-cascades to type window_view[]
2023-11-25 13:48:30.241 UTC [135455] LOCATION:  reportDependentObjects, dependency.c:1122
2023-11-25 13:48:30.241 UTC [135455] DEBUG:  00000: drop auto-cascades to rule _RETURN on view window_view
2023-11-25 13:48:30.241 UTC [135455] LOCATION:  reportDependentObjects, dependency.c:1122
2023-11-25 13:48:30.241 UTC [135455] DEBUG:  00000: drop auto-cascades to type users_view
2023-11-25 13:48:30.241 UTC [135455] LOCATION:  reportDependentObjects, dependency.c:1122
2023-11-25 13:48:30.241 UTC [135455] DEBUG:  00000: drop auto-cascades to type users_view[]
2023-11-25 13:48:30.241 UTC [135455] LOCATION:  reportDependentObjects, dependency.c:1122
2023-11-25 13:48:30.241 UTC [135455] DEBUG:  00000: drop auto-cascades to rule _RETURN on view users_view
2023-11-25 13:48:30.241 UTC [135455] LOCATION:  reportDependentObjects, dependency.c:1122
2023-11-25 13:48:30.241 UTC [135455] DEBUG:  00000: EventTriggerInvoke 16675
2023-11-25 13:48:30.241 UTC [135455] LOCATION:  EventTriggerInvoke, event_trigger.c:900
2023-11-25 13:48:30.244 UTC [135455] DEBUG:  00000: opening 1 new connections to localhost:57637
2023-11-25 13:48:30.244 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.244 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 29 in 3515 microseconds
2023-11-25 13:48:30.244 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.244 UTC [135455] DEBUG:  00000: opening 1 new connections to localhost:57638
2023-11-25 13:48:30.244 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.244 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 30 in 8559 microseconds
2023-11-25 13:48:30.244 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.248 UTC [135455] DEBUG:  00000: task execution (0) for placement (0) on anchor shard (0) finished in 3882 microseconds on worker node localhost:57637
2023-11-25 13:48:30.248 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.250 UTC [135455] DEBUG:  00000: task execution (0) for placement (0) on anchor shard (0) finished in 4946 microseconds on worker node localhost:57638
2023-11-25 13:48:30.250 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.250 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 29: 1 to node localhost:57637
2023-11-25 13:48:30.250 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.250 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 30: 1 to node localhost:57638
2023-11-25 13:48:30.250 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.251 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.251 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.251 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.251 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.251 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.251 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.251 UTC [135455] DEBUG:  00000: Distributed planning for a fast-path router query
2023-11-25 13:48:30.251 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2322
2023-11-25 13:48:30.251 UTC [135455] DEBUG:  00000: Creating router plan
2023-11-25 13:48:30.251 UTC [135455] LOCATION:  CreateSingleTaskRouterSelectPlan, multi_router_planner.c:284
2023-11-25 13:48:30.251 UTC [135455] DEBUG:  00000: generating subplan 8_1 for subquery SELECT min(k_no) AS min FROM public.users_ref_test_table
2023-11-25 13:48:30.251 UTC [135455] LOCATION:  RecursivelyPlanSubquery, recursive_planning.c:1551
2023-11-25 13:48:30.251 UTC [135455] DEBUG:  00000: Plan 8 query after replacing subqueries and CTEs: SELECT user_id, count(user_id) OVER (PARTITION BY user_id) AS count FROM public.users_table GROUP BY user_id HAVING (avg(value_1) OPERATOR(pg_catalog.<) ((SELECT intermediate_result.min FROM read_intermediate_result('8_1'::text, 'binary'::citus_copy_format) intermediate_result(min integer)))::numeric) ORDER BY user_id DESC, (count(user_id) OVER (PARTITION BY user_id)) DESC LIMIT 1
2023-11-25 13:48:30.251 UTC [135455] LOCATION:  GenerateSubplansForSubqueriesAndCTEs, recursive_planning.c:248
2023-11-25 13:48:30.251 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.251 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.251 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.251 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.251 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.251 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.251 UTC [135455] DEBUG:  00000: push down of limit count: 1
2023-11-25 13:48:30.251 UTC [135455] LOCATION:  WorkerLimitCount, multi_logical_optimizer.c:4778
2023-11-25 13:48:30.251 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.251 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.251 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.251 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.251 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, count(worker_column_1) OVER (PARTITION BY worker_column_1) AS count FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1 HAVING (avg(worker_column_2) OPERATOR(pg_catalog.<) ((SELECT intermediate_result.min FROM read_intermediate_result('8_1'::text, 'binary'::citus_copy_format) intermediate_result(min integer)))::numeric) ORDER BY worker_column_1 DESC, (count(worker_column_1) OVER (PARTITION BY worker_column_1)) DESC LIMIT '1'::bigint
2023-11-25 13:48:30.251 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.252 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, count(worker_column_1) OVER (PARTITION BY worker_column_1) AS count FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1 HAVING (avg(worker_column_2) OPERATOR(pg_catalog.<) ((SELECT intermediate_result.min FROM read_intermediate_result('8_1'::text, 'binary'::citus_copy_format) intermediate_result(min integer)))::numeric) ORDER BY worker_column_1 DESC, (count(worker_column_1) OVER (PARTITION BY worker_column_1)) DESC LIMIT '1'::bigint
2023-11-25 13:48:30.252 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.252 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, count(worker_column_1) OVER (PARTITION BY worker_column_1) AS count FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1 HAVING (avg(worker_column_2) OPERATOR(pg_catalog.<) ((SELECT intermediate_result.min FROM read_intermediate_result('8_1'::text, 'binary'::citus_copy_format) intermediate_result(min integer)))::numeric) ORDER BY worker_column_1 DESC, (count(worker_column_1) OVER (PARTITION BY worker_column_1)) DESC LIMIT '1'::bigint
2023-11-25 13:48:30.252 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.252 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, count(worker_column_1) OVER (PARTITION BY worker_column_1) AS count FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1 HAVING (avg(worker_column_2) OPERATOR(pg_catalog.<) ((SELECT intermediate_result.min FROM read_intermediate_result('8_1'::text, 'binary'::citus_copy_format) intermediate_result(min integer)))::numeric) ORDER BY worker_column_1 DESC, (count(worker_column_1) OVER (PARTITION BY worker_column_1)) DESC LIMIT '1'::bigint
2023-11-25 13:48:30.252 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.252 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.252 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.252 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.252 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.252 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.252 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.252 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.252 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.252 UTC [135455] DEBUG:  00000: combine query: SELECT user_id, count FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, count bigint) ORDER BY user_id DESC, count DESC LIMIT '1'::bigint
2023-11-25 13:48:30.252 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.252 UTC [135455] DEBUG:  00000: Subplan 8_1 is used in 8
2023-11-25 13:48:30.252 UTC [135455] LOCATION:  RecordSubplanExecutionsOnNodes, intermediate_result_pruning.c:193
2023-11-25 13:48:30.252 UTC [135455] DEBUG:  00000: Subplan 8_1 will be sent to localhost:57637
2023-11-25 13:48:30.252 UTC [135455] LOCATION:  LogIntermediateResultMulticastSummary, intermediate_result_pruning.c:422
2023-11-25 13:48:30.252 UTC [135455] DEBUG:  00000: Subplan 8_1 will be sent to localhost:57638
2023-11-25 13:48:30.252 UTC [135455] LOCATION:  LogIntermediateResultMulticastSummary, intermediate_result_pruning.c:422
2023-11-25 13:48:30.252 UTC [135455] DEBUG:  00000: opening 1 new connections to localhost:57637
2023-11-25 13:48:30.252 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.252 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 31 in 3515 microseconds
2023-11-25 13:48:30.252 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.252 UTC [135455] DEBUG:  00000: task execution (0) for placement (896) on anchor shard (1400284) finished in 189 microseconds on worker node localhost:57637
2023-11-25 13:48:30.252 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.252 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 31: 1 to node localhost:57637
2023-11-25 13:48:30.252 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.253 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.253 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.253 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 32 in 3515 microseconds
2023-11-25 13:48:30.253 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.253 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.253 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.253 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 34 in 8559 microseconds
2023-11-25 13:48:30.253 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.254 UTC [135455] DEBUG:  00000: task execution (2) for placement (853) on anchor shard (1400257) finished in 651 microseconds on worker node localhost:57638
2023-11-25 13:48:30.254 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.254 UTC [135455] DEBUG:  00000: task execution (1) for placement (852) on anchor shard (1400256) finished in 1356 microseconds on worker node localhost:57637
2023-11-25 13:48:30.254 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.257 UTC [135455] DEBUG:  00000: task execution (3) for placement (854) on anchor shard (1400258) finished in 2742 microseconds on worker node localhost:57637
2023-11-25 13:48:30.257 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.257 UTC [135455] DEBUG:  00000: task execution (4) for placement (855) on anchor shard (1400259) finished in 3345 microseconds on worker node localhost:57638
2023-11-25 13:48:30.257 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.257 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 35 in 4084 microseconds
2023-11-25 13:48:30.257 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.257 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 33 in 4587 microseconds
2023-11-25 13:48:30.257 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.257 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 32: 2 to node localhost:57637
2023-11-25 13:48:30.257 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.257 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 33: 0 to node localhost:57637
2023-11-25 13:48:30.257 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.257 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 34: 2 to node localhost:57638
2023-11-25 13:48:30.257 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.257 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 35: 0 to node localhost:57638
2023-11-25 13:48:30.257 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.258 UTC [135455] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 13:48:30.258 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.258 UTC [135455] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 13:48:30.258 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.258 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.258 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.258 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.258 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.259 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.259 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.259 UTC [135455] DEBUG:  00000: push down of limit count: 10
2023-11-25 13:48:30.259 UTC [135455] LOCATION:  WorkerLimitCount, multi_logical_optimizer.c:4778
2023-11-25 13:48:30.259 UTC [135455] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 13:48:30.259 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.259 UTC [135455] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 13:48:30.259 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.259 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.259 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.259 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.259 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.259 UTC [135455] DEBUG:  00000: distributed statement: SELECT DISTINCT ON ((rank() OVER my_win), worker_column_1) worker_column_1 AS user_id, rank() OVER my_win AS rnk, worker_column_2 AS worker_column_3, worker_column_3 AS worker_column_4 FROM (SELECT events_table.user_id AS worker_column_1, events_table."time" AS worker_column_2, users_table.value_1 AS worker_column_3 FROM public.events_table_1400260 events_table, public.users_table_1400256 users_table WHERE (users_table.user_id OPERATOR(pg_catalog.=) events_table.user_id)) worker_subquery WINDOW my_win AS (PARTITION BY worker_column_1, worker_column_3 ORDER BY worker_column_2 DESC) ORDER BY (rank() OVER my_win) DESC, worker_column_1 DESC LIMIT '10'::bigint
2023-11-25 13:48:30.259 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.259 UTC [135455] DEBUG:  00000: distributed statement: SELECT DISTINCT ON ((rank() OVER my_win), worker_column_1) worker_column_1 AS user_id, rank() OVER my_win AS rnk, worker_column_2 AS worker_column_3, worker_column_3 AS worker_column_4 FROM (SELECT events_table.user_id AS worker_column_1, events_table."time" AS worker_column_2, users_table.value_1 AS worker_column_3 FROM public.events_table_1400261 events_table, public.users_table_1400257 users_table WHERE (users_table.user_id OPERATOR(pg_catalog.=) events_table.user_id)) worker_subquery WINDOW my_win AS (PARTITION BY worker_column_1, worker_column_3 ORDER BY worker_column_2 DESC) ORDER BY (rank() OVER my_win) DESC, worker_column_1 DESC LIMIT '10'::bigint
2023-11-25 13:48:30.259 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.259 UTC [135455] DEBUG:  00000: distributed statement: SELECT DISTINCT ON ((rank() OVER my_win), worker_column_1) worker_column_1 AS user_id, rank() OVER my_win AS rnk, worker_column_2 AS worker_column_3, worker_column_3 AS worker_column_4 FROM (SELECT events_table.user_id AS worker_column_1, events_table."time" AS worker_column_2, users_table.value_1 AS worker_column_3 FROM public.events_table_1400262 events_table, public.users_table_1400258 users_table WHERE (users_table.user_id OPERATOR(pg_catalog.=) events_table.user_id)) worker_subquery WINDOW my_win AS (PARTITION BY worker_column_1, worker_column_3 ORDER BY worker_column_2 DESC) ORDER BY (rank() OVER my_win) DESC, worker_column_1 DESC LIMIT '10'::bigint
2023-11-25 13:48:30.259 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.259 UTC [135455] DEBUG:  00000: distributed statement: SELECT DISTINCT ON ((rank() OVER my_win), worker_column_1) worker_column_1 AS user_id, rank() OVER my_win AS rnk, worker_column_2 AS worker_column_3, worker_column_3 AS worker_column_4 FROM (SELECT events_table.user_id AS worker_column_1, events_table."time" AS worker_column_2, users_table.value_1 AS worker_column_3 FROM public.events_table_1400263 events_table, public.users_table_1400259 users_table WHERE (users_table.user_id OPERATOR(pg_catalog.=) events_table.user_id)) worker_subquery WINDOW my_win AS (PARTITION BY worker_column_1, worker_column_3 ORDER BY worker_column_2 DESC) ORDER BY (rank() OVER my_win) DESC, worker_column_1 DESC LIMIT '10'::bigint
2023-11-25 13:48:30.259 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.259 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.259 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.259 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.259 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.259 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.259 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.259 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.259 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.259 UTC [135455] DEBUG:  00000: combine query: SELECT DISTINCT ON (rnk, user_id) user_id, rnk FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, rnk bigint, worker_column_3 timestamp without time zone, worker_column_4 integer) ORDER BY rnk DESC, user_id DESC LIMIT '10'::bigint
2023-11-25 13:48:30.259 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.259 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.259 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.259 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 36 in 3515 microseconds
2023-11-25 13:48:30.259 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.259 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.259 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.260 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 38 in 8559 microseconds
2023-11-25 13:48:30.260 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.261 UTC [135455] DEBUG:  00000: task execution (1) for placement (852) on anchor shard (1400256) finished in 2022 microseconds on worker node localhost:57637
2023-11-25 13:48:30.261 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.262 UTC [135455] DEBUG:  00000: task execution (3) for placement (854) on anchor shard (1400258) finished in 710 microseconds on worker node localhost:57637
2023-11-25 13:48:30.262 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.262 UTC [135455] DEBUG:  00000: task execution (2) for placement (853) on anchor shard (1400257) finished in 2752 microseconds on worker node localhost:57638
2023-11-25 13:48:30.262 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.263 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 39 in 3105 microseconds
2023-11-25 13:48:30.263 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.264 UTC [135455] DEBUG:  00000: task execution (4) for placement (855) on anchor shard (1400259) finished in 1673 microseconds on worker node localhost:57638
2023-11-25 13:48:30.264 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.266 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 37 in 6471 microseconds
2023-11-25 13:48:30.266 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.266 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 36: 2 to node localhost:57637
2023-11-25 13:48:30.266 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.266 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 37: 0 to node localhost:57637
2023-11-25 13:48:30.266 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.266 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 38: 2 to node localhost:57638
2023-11-25 13:48:30.266 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.266 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 39: 0 to node localhost:57638
2023-11-25 13:48:30.266 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.266 UTC [135455] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 13:48:30.266 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.266 UTC [135455] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 13:48:30.266 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.266 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.266 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.266 UTC [135455] DEBUG:  00000: push down of limit count: 10
2023-11-25 13:48:30.266 UTC [135455] LOCATION:  WorkerLimitCount, multi_logical_optimizer.c:4778
2023-11-25 13:48:30.266 UTC [135455] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 13:48:30.266 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.266 UTC [135455] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 13:48:30.266 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.267 UTC [135455] DEBUG:  00000: distributed statement: SELECT DISTINCT ON ((rank() OVER my_win), worker_column_1) worker_column_1 AS user_id, rank() OVER my_win AS rnk, worker_column_2 AS worker_column_3, worker_column_3 AS worker_column_4 FROM (SELECT events_table.user_id AS worker_column_1, events_table."time" AS worker_column_2, uref.k_no AS worker_column_3 FROM public.events_table_1400260 events_table, public.users_ref_test_table_1400284 uref WHERE (uref.id OPERATOR(pg_catalog.=) events_table.user_id)) worker_subquery WINDOW my_win AS (PARTITION BY worker_column_1, worker_column_3 ORDER BY worker_column_2 DESC) ORDER BY (rank() OVER my_win) DESC, worker_column_1 DESC LIMIT '10'::bigint
2023-11-25 13:48:30.267 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.267 UTC [135455] DEBUG:  00000: distributed statement: SELECT DISTINCT ON ((rank() OVER my_win), worker_column_1) worker_column_1 AS user_id, rank() OVER my_win AS rnk, worker_column_2 AS worker_column_3, worker_column_3 AS worker_column_4 FROM (SELECT events_table.user_id AS worker_column_1, events_table."time" AS worker_column_2, uref.k_no AS worker_column_3 FROM public.events_table_1400261 events_table, public.users_ref_test_table_1400284 uref WHERE (uref.id OPERATOR(pg_catalog.=) events_table.user_id)) worker_subquery WINDOW my_win AS (PARTITION BY worker_column_1, worker_column_3 ORDER BY worker_column_2 DESC) ORDER BY (rank() OVER my_win) DESC, worker_column_1 DESC LIMIT '10'::bigint
2023-11-25 13:48:30.267 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.267 UTC [135455] DEBUG:  00000: distributed statement: SELECT DISTINCT ON ((rank() OVER my_win), worker_column_1) worker_column_1 AS user_id, rank() OVER my_win AS rnk, worker_column_2 AS worker_column_3, worker_column_3 AS worker_column_4 FROM (SELECT events_table.user_id AS worker_column_1, events_table."time" AS worker_column_2, uref.k_no AS worker_column_3 FROM public.events_table_1400262 events_table, public.users_ref_test_table_1400284 uref WHERE (uref.id OPERATOR(pg_catalog.=) events_table.user_id)) worker_subquery WINDOW my_win AS (PARTITION BY worker_column_1, worker_column_3 ORDER BY worker_column_2 DESC) ORDER BY (rank() OVER my_win) DESC, worker_column_1 DESC LIMIT '10'::bigint
2023-11-25 13:48:30.267 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.267 UTC [135455] DEBUG:  00000: distributed statement: SELECT DISTINCT ON ((rank() OVER my_win), worker_column_1) worker_column_1 AS user_id, rank() OVER my_win AS rnk, worker_column_2 AS worker_column_3, worker_column_3 AS worker_column_4 FROM (SELECT events_table.user_id AS worker_column_1, events_table."time" AS worker_column_2, uref.k_no AS worker_column_3 FROM public.events_table_1400263 events_table, public.users_ref_test_table_1400284 uref WHERE (uref.id OPERATOR(pg_catalog.=) events_table.user_id)) worker_subquery WINDOW my_win AS (PARTITION BY worker_column_1, worker_column_3 ORDER BY worker_column_2 DESC) ORDER BY (rank() OVER my_win) DESC, worker_column_1 DESC LIMIT '10'::bigint
2023-11-25 13:48:30.267 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.267 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.267 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.267 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.267 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.267 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.267 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.267 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.267 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.267 UTC [135455] DEBUG:  00000: combine query: SELECT DISTINCT ON (rnk, user_id) user_id, rnk FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, rnk bigint, worker_column_3 timestamp without time zone, worker_column_4 integer) ORDER BY rnk DESC, user_id DESC LIMIT '10'::bigint
2023-11-25 13:48:30.267 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.267 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.267 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.267 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 40 in 3515 microseconds
2023-11-25 13:48:30.267 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.267 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.267 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.267 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 42 in 8559 microseconds
2023-11-25 13:48:30.267 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.268 UTC [135455] DEBUG:  00000: task execution (2) for placement (857) on anchor shard (1400261) finished in 868 microseconds on worker node localhost:57638
2023-11-25 13:48:30.268 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.268 UTC [135455] DEBUG:  00000: task execution (1) for placement (856) on anchor shard (1400260) finished in 1219 microseconds on worker node localhost:57637
2023-11-25 13:48:30.268 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.269 UTC [135455] DEBUG:  00000: task execution (4) for placement (859) on anchor shard (1400263) finished in 298 microseconds on worker node localhost:57638
2023-11-25 13:48:30.269 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.269 UTC [135455] DEBUG:  00000: task execution (3) for placement (858) on anchor shard (1400262) finished in 713 microseconds on worker node localhost:57637
2023-11-25 13:48:30.269 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.270 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 41 in 3308 microseconds
2023-11-25 13:48:30.270 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.272 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 43 in 4514 microseconds
2023-11-25 13:48:30.272 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.272 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 40: 2 to node localhost:57637
2023-11-25 13:48:30.272 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.272 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 41: 0 to node localhost:57637
2023-11-25 13:48:30.272 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.272 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 42: 2 to node localhost:57638
2023-11-25 13:48:30.272 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.272 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 43: 0 to node localhost:57638
2023-11-25 13:48:30.272 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.272 UTC [135455] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 13:48:30.272 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.272 UTC [135455] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 13:48:30.272 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.272 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.272 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.272 UTC [135455] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 13:48:30.272 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.272 UTC [135455] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 13:48:30.272 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.273 UTC [135455] DEBUG:  00000: generated sql query for task 1
2023-11-25 13:48:30.273 UTC [135455] DETAIL:  query string: "SELECT events_table.user_id, events_table."time" AS worker_column_2, events_table.value_2 AS worker_column_3, uref.k_no AS worker_column_4 FROM (public.events_table_1400260 events_table JOIN public.users_ref_test_table_1400284 uref ON ((uref.id OPERATOR(pg_catalog.=) events_table.user_id))) WHERE true"
2023-11-25 13:48:30.273 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.273 UTC [135455] DEBUG:  00000: generated sql query for task 2
2023-11-25 13:48:30.273 UTC [135455] DETAIL:  query string: "SELECT events_table.user_id, events_table."time" AS worker_column_2, events_table.value_2 AS worker_column_3, uref.k_no AS worker_column_4 FROM (public.events_table_1400261 events_table JOIN public.users_ref_test_table_1400284 uref ON ((uref.id OPERATOR(pg_catalog.=) events_table.user_id))) WHERE true"
2023-11-25 13:48:30.273 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.273 UTC [135455] DEBUG:  00000: generated sql query for task 3
2023-11-25 13:48:30.273 UTC [135455] DETAIL:  query string: "SELECT events_table.user_id, events_table."time" AS worker_column_2, events_table.value_2 AS worker_column_3, uref.k_no AS worker_column_4 FROM (public.events_table_1400262 events_table JOIN public.users_ref_test_table_1400284 uref ON ((uref.id OPERATOR(pg_catalog.=) events_table.user_id))) WHERE true"
2023-11-25 13:48:30.273 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.273 UTC [135455] DEBUG:  00000: generated sql query for task 4
2023-11-25 13:48:30.273 UTC [135455] DETAIL:  query string: "SELECT events_table.user_id, events_table."time" AS worker_column_2, events_table.value_2 AS worker_column_3, uref.k_no AS worker_column_4 FROM (public.events_table_1400263 events_table JOIN public.users_ref_test_table_1400284 uref ON ((uref.id OPERATOR(pg_catalog.=) events_table.user_id))) WHERE true"
2023-11-25 13:48:30.273 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.273 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.273 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.273 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.273 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.273 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.273 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.273 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.273 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.273 UTC [135455] DEBUG:  00000: combine query: SELECT DISTINCT ON ((rank() OVER my_win), user_id) user_id, rank() OVER my_win AS rnk FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), '(i 1 2)'::cstring(0)) remote_scan(user_id integer, worker_column_2 timestamp without time zone, worker_column_3 integer, worker_column_4 integer) WINDOW my_win AS (PARTITION BY worker_column_3, worker_column_4 ORDER BY worker_column_2 DESC) ORDER BY (rank() OVER my_win) DESC, user_id DESC LIMIT '10'::bigint
2023-11-25 13:48:30.273 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.273 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.273 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.273 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 44 in 3515 microseconds
2023-11-25 13:48:30.273 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.273 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.273 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.273 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 46 in 8559 microseconds
2023-11-25 13:48:30.273 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.274 UTC [135455] DEBUG:  00000: task execution (1) for placement (856) on anchor shard (1400260) finished in 708 microseconds on worker node localhost:57637
2023-11-25 13:48:30.274 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.274 UTC [135455] DEBUG:  00000: task execution (2) for placement (857) on anchor shard (1400261) finished in 1014 microseconds on worker node localhost:57638
2023-11-25 13:48:30.274 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.274 UTC [135455] DEBUG:  00000: task execution (3) for placement (858) on anchor shard (1400262) finished in 600 microseconds on worker node localhost:57637
2023-11-25 13:48:30.274 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.275 UTC [135455] DEBUG:  00000: task execution (4) for placement (859) on anchor shard (1400263) finished in 338 microseconds on worker node localhost:57638
2023-11-25 13:48:30.275 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.277 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 47 in 3621 microseconds
2023-11-25 13:48:30.277 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.278 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 45 in 4916 microseconds
2023-11-25 13:48:30.278 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.278 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 44: 2 to node localhost:57637
2023-11-25 13:48:30.278 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.278 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 45: 0 to node localhost:57637
2023-11-25 13:48:30.278 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.278 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 46: 2 to node localhost:57638
2023-11-25 13:48:30.278 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.278 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 47: 0 to node localhost:57638
2023-11-25 13:48:30.278 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.279 UTC [135455] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 13:48:30.279 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.279 UTC [135455] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 13:48:30.279 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.279 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.279 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.279 UTC [135455] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 13:48:30.279 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.279 UTC [135455] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 13:48:30.279 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.279 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, rank() OVER my_win AS rnk, avg(worker_column_2) AS avg_val_2, date_trunc('day'::text, worker_column_3) AS worker_column_4, avg(worker_column_4) AS worker_column_5 FROM (SELECT events_table.user_id AS worker_column_1, events_table.value_2 AS worker_column_2, events_table."time" AS worker_column_3, events_table.event_type AS worker_column_4 FROM public.events_table_1400260 events_table) worker_subquery GROUP BY worker_column_1, (date_trunc('day'::text, worker_column_3)) WINDOW my_win AS (PARTITION BY worker_column_1 ORDER BY (avg(worker_column_4)) DESC)
2023-11-25 13:48:30.279 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.279 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, rank() OVER my_win AS rnk, avg(worker_column_2) AS avg_val_2, date_trunc('day'::text, worker_column_3) AS worker_column_4, avg(worker_column_4) AS worker_column_5 FROM (SELECT events_table.user_id AS worker_column_1, events_table.value_2 AS worker_column_2, events_table."time" AS worker_column_3, events_table.event_type AS worker_column_4 FROM public.events_table_1400261 events_table) worker_subquery GROUP BY worker_column_1, (date_trunc('day'::text, worker_column_3)) WINDOW my_win AS (PARTITION BY worker_column_1 ORDER BY (avg(worker_column_4)) DESC)
2023-11-25 13:48:30.279 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.279 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, rank() OVER my_win AS rnk, avg(worker_column_2) AS avg_val_2, date_trunc('day'::text, worker_column_3) AS worker_column_4, avg(worker_column_4) AS worker_column_5 FROM (SELECT events_table.user_id AS worker_column_1, events_table.value_2 AS worker_column_2, events_table."time" AS worker_column_3, events_table.event_type AS worker_column_4 FROM public.events_table_1400262 events_table) worker_subquery GROUP BY worker_column_1, (date_trunc('day'::text, worker_column_3)) WINDOW my_win AS (PARTITION BY worker_column_1 ORDER BY (avg(worker_column_4)) DESC)
2023-11-25 13:48:30.279 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.279 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, rank() OVER my_win AS rnk, avg(worker_column_2) AS avg_val_2, date_trunc('day'::text, worker_column_3) AS worker_column_4, avg(worker_column_4) AS worker_column_5 FROM (SELECT events_table.user_id AS worker_column_1, events_table.value_2 AS worker_column_2, events_table."time" AS worker_column_3, events_table.event_type AS worker_column_4 FROM public.events_table_1400263 events_table) worker_subquery GROUP BY worker_column_1, (date_trunc('day'::text, worker_column_3)) WINDOW my_win AS (PARTITION BY worker_column_1 ORDER BY (avg(worker_column_4)) DESC)
2023-11-25 13:48:30.279 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.279 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.279 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.279 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.279 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.279 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.279 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.279 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.279 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.279 UTC [135455] DEBUG:  00000: combine query: SELECT user_id, rnk, avg_val_2 FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, rnk bigint, avg_val_2 numeric, worker_column_4 timestamp without time zone, worker_column_5 numeric) ORDER BY avg_val_2 DESC, rnk DESC, user_id DESC
2023-11-25 13:48:30.279 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.280 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.280 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.280 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 48 in 3515 microseconds
2023-11-25 13:48:30.280 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.280 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.280 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.280 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 50 in 8559 microseconds
2023-11-25 13:48:30.280 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.281 UTC [135455] DEBUG:  00000: task execution (1) for placement (856) on anchor shard (1400260) finished in 729 microseconds on worker node localhost:57637
2023-11-25 13:48:30.281 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.281 UTC [135455] DEBUG:  00000: task execution (2) for placement (857) on anchor shard (1400261) finished in 738 microseconds on worker node localhost:57638
2023-11-25 13:48:30.281 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.281 UTC [135455] DEBUG:  00000: task execution (3) for placement (858) on anchor shard (1400262) finished in 399 microseconds on worker node localhost:57637
2023-11-25 13:48:30.281 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.281 UTC [135455] DEBUG:  00000: task execution (4) for placement (859) on anchor shard (1400263) finished in 342 microseconds on worker node localhost:57638
2023-11-25 13:48:30.281 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.283 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 51 in 3352 microseconds
2023-11-25 13:48:30.283 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.284 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 49 in 3807 microseconds
2023-11-25 13:48:30.284 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.284 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 48: 2 to node localhost:57637
2023-11-25 13:48:30.284 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.284 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 49: 0 to node localhost:57637
2023-11-25 13:48:30.284 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.284 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 50: 2 to node localhost:57638
2023-11-25 13:48:30.284 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.284 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 51: 0 to node localhost:57638
2023-11-25 13:48:30.284 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.285 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.285 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.285 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.285 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.285 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.285 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.285 UTC [135455] DEBUG:  00000: push down of limit count: 5
2023-11-25 13:48:30.285 UTC [135455] LOCATION:  WorkerLimitCount, multi_logical_optimizer.c:4778
2023-11-25 13:48:30.285 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.285 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.285 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.285 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.286 UTC [135455] DEBUG:  00000: distributed statement: SELECT count(*) OVER (PARTITION BY worker_column_2, (worker_column_2 OPERATOR(pg_catalog.+) 1)) AS count, rank() OVER (PARTITION BY worker_column_2) AS cnt1, count(*) OVER (PARTITION BY worker_column_2, (abs((worker_column_3 OPERATOR(pg_catalog.-) worker_column_4)))) AS cnt2, date_trunc('min'::text, lag(worker_column_1) OVER (PARTITION BY worker_column_2 ORDER BY worker_column_1)) AS datee, rank() OVER my_win AS rnnk, avg(CASE WHEN (worker_column_2 OPERATOR(pg_catalog.>) 4) THEN worker_column_3 ELSE worker_column_4 END) FILTER (WHERE (worker_column_2 OPERATOR(pg_catalog.>) 2)) OVER my_win_2 AS filtered_count, sum(((((worker_column_2)::numeric OPERATOR(pg_catalog.*) (5.0 OPERATOR(pg_catalog./) (((worker_column_3 OPERATOR(pg_catalog.+) worker_column_4))::numeric OPERATOR(pg_catalog.+) 0.1))))::double precision OPERATOR(pg_catalog.*) worker_column_5)) FILTER (WHERE ((worker_column_3)::text OPERATOR(pg_catalog.~~) '%1%'::text)) OVER my_win_4 AS cnt_with_filter_2, worker_column_2 AS worker_column_8, worker_column_1 AS worker_column_9, (worker_column_3 OPERATOR(pg_catalog.%) 3) AS worker_column_10, worker_column_3 AS worker_column_11, date_trunc('min'::text, worker_column_1) AS worker_column_12, worker_column_4 AS worker_column_13, worker_column_5 AS worker_column_14, (worker_column_2 OPERATOR(pg_catalog.+) 1) AS worker_column_15, abs((worker_column_3 OPERATOR(pg_catalog.-) worker_column_4)) AS worker_column_16 FROM (SELECT users_table."time" AS worker_column_1, users_table.user_id AS worker_column_2, users_table.value_1 AS worker_column_3, users_table.value_2 AS worker_column_4, users_table.value_3 AS worker_column_5 FROM public.users_table_1400256 users_table) worker_subquery WINDOW my_win AS (PARTITION BY worker_column_2, (worker_column_3 OPERATOR(pg_catalog.%) 3) ORDER BY worker_column_1 DESC), my_win_2 AS (PARTITION BY worker_column_2, worker_column_3 ORDER BY worker_column_1 DESC), my_win_3 AS (PARTITION BY worker_column_2, (date_trunc('min'::text, worker_column_1))), my_win_4 AS (my_win_3 ORDER BY worker_column_4, worker_column_5) ORDER BY (sum(((((worker_column_2)::numeric OPERATOR(pg_catalog.*) (5.0 OPERATOR(pg_catalog./) (((worker_column_3 OPERATOR(pg_catalog.+) worker_column_4))::numeric OPERATOR(pg_catalog.+) 0.1))))::double precision OPERATOR(pg_catalog.*) worker_column_5)) FILTER (WHERE ((worker_column_3)::text OPERATOR(pg_catalog.~~) '%1%'::text)) OVER my_win_4) DESC NULLS LAST, (avg(CASE WHEN (worker_column_2 OPERATOR(pg_catalog.>) 4) THEN worker_column_3 ELSE worker_column_4 END) FILTER (WHERE (worker_column_2 OPERATOR(pg_catalog.>) 2)) OVER my_win_2) DESC NULLS LAST, (date_trunc('min'::text, lag(worker_column_1) OVER (PARTITION BY worker_column_2 ORDER BY worker_column_1))) DESC NULLS LAST, (rank() OVER my_win) DESC, (count(*) OVER (PARTITION BY worker_column_2, (abs((worker_column_3 OPERATOR(pg_catalog.-) worker_column_4))))) DESC, (rank() OVER (PARTITION BY worker_column_2)) DESC, worker_column_2 DESC LIMIT '5'::bigint
2023-11-25 13:48:30.286 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.286 UTC [135455] DEBUG:  00000: distributed statement: SELECT count(*) OVER (PARTITION BY worker_column_2, (worker_column_2 OPERATOR(pg_catalog.+) 1)) AS count, rank() OVER (PARTITION BY worker_column_2) AS cnt1, count(*) OVER (PARTITION BY worker_column_2, (abs((worker_column_3 OPERATOR(pg_catalog.-) worker_column_4)))) AS cnt2, date_trunc('min'::text, lag(worker_column_1) OVER (PARTITION BY worker_column_2 ORDER BY worker_column_1)) AS datee, rank() OVER my_win AS rnnk, avg(CASE WHEN (worker_column_2 OPERATOR(pg_catalog.>) 4) THEN worker_column_3 ELSE worker_column_4 END) FILTER (WHERE (worker_column_2 OPERATOR(pg_catalog.>) 2)) OVER my_win_2 AS filtered_count, sum(((((worker_column_2)::numeric OPERATOR(pg_catalog.*) (5.0 OPERATOR(pg_catalog./) (((worker_column_3 OPERATOR(pg_catalog.+) worker_column_4))::numeric OPERATOR(pg_catalog.+) 0.1))))::double precision OPERATOR(pg_catalog.*) worker_column_5)) FILTER (WHERE ((worker_column_3)::text OPERATOR(pg_catalog.~~) '%1%'::text)) OVER my_win_4 AS cnt_with_filter_2, worker_column_2 AS worker_column_8, worker_column_1 AS worker_column_9, (worker_column_3 OPERATOR(pg_catalog.%) 3) AS worker_column_10, worker_column_3 AS worker_column_11, date_trunc('min'::text, worker_column_1) AS worker_column_12, worker_column_4 AS worker_column_13, worker_column_5 AS worker_column_14, (worker_column_2 OPERATOR(pg_catalog.+) 1) AS worker_column_15, abs((worker_column_3 OPERATOR(pg_catalog.-) worker_column_4)) AS worker_column_16 FROM (SELECT users_table."time" AS worker_column_1, users_table.user_id AS worker_column_2, users_table.value_1 AS worker_column_3, users_table.value_2 AS worker_column_4, users_table.value_3 AS worker_column_5 FROM public.users_table_1400257 users_table) worker_subquery WINDOW my_win AS (PARTITION BY worker_column_2, (worker_column_3 OPERATOR(pg_catalog.%) 3) ORDER BY worker_column_1 DESC), my_win_2 AS (PARTITION BY worker_column_2, worker_column_3 ORDER BY worker_column_1 DESC), my_win_3 AS (PARTITION BY worker_column_2, (date_trunc('min'::text, worker_column_1))), my_win_4 AS (my_win_3 ORDER BY worker_column_4, worker_column_5) ORDER BY (sum(((((worker_column_2)::numeric OPERATOR(pg_catalog.*) (5.0 OPERATOR(pg_catalog./) (((worker_column_3 OPERATOR(pg_catalog.+) worker_column_4))::numeric OPERATOR(pg_catalog.+) 0.1))))::double precision OPERATOR(pg_catalog.*) worker_column_5)) FILTER (WHERE ((worker_column_3)::text OPERATOR(pg_catalog.~~) '%1%'::text)) OVER my_win_4) DESC NULLS LAST, (avg(CASE WHEN (worker_column_2 OPERATOR(pg_catalog.>) 4) THEN worker_column_3 ELSE worker_column_4 END) FILTER (WHERE (worker_column_2 OPERATOR(pg_catalog.>) 2)) OVER my_win_2) DESC NULLS LAST, (date_trunc('min'::text, lag(worker_column_1) OVER (PARTITION BY worker_column_2 ORDER BY worker_column_1))) DESC NULLS LAST, (rank() OVER my_win) DESC, (count(*) OVER (PARTITION BY worker_column_2, (abs((worker_column_3 OPERATOR(pg_catalog.-) worker_column_4))))) DESC, (rank() OVER (PARTITION BY worker_column_2)) DESC, worker_column_2 DESC LIMIT '5'::bigint
2023-11-25 13:48:30.286 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.286 UTC [135455] DEBUG:  00000: distributed statement: SELECT count(*) OVER (PARTITION BY worker_column_2, (worker_column_2 OPERATOR(pg_catalog.+) 1)) AS count, rank() OVER (PARTITION BY worker_column_2) AS cnt1, count(*) OVER (PARTITION BY worker_column_2, (abs((worker_column_3 OPERATOR(pg_catalog.-) worker_column_4)))) AS cnt2, date_trunc('min'::text, lag(worker_column_1) OVER (PARTITION BY worker_column_2 ORDER BY worker_column_1)) AS datee, rank() OVER my_win AS rnnk, avg(CASE WHEN (worker_column_2 OPERATOR(pg_catalog.>) 4) THEN worker_column_3 ELSE worker_column_4 END) FILTER (WHERE (worker_column_2 OPERATOR(pg_catalog.>) 2)) OVER my_win_2 AS filtered_count, sum(((((worker_column_2)::numeric OPERATOR(pg_catalog.*) (5.0 OPERATOR(pg_catalog./) (((worker_column_3 OPERATOR(pg_catalog.+) worker_column_4))::numeric OPERATOR(pg_catalog.+) 0.1))))::double precision OPERATOR(pg_catalog.*) worker_column_5)) FILTER (WHERE ((worker_column_3)::text OPERATOR(pg_catalog.~~) '%1%'::text)) OVER my_win_4 AS cnt_with_filter_2, worker_column_2 AS worker_column_8, worker_column_1 AS worker_column_9, (worker_column_3 OPERATOR(pg_catalog.%) 3) AS worker_column_10, worker_column_3 AS worker_column_11, date_trunc('min'::text, worker_column_1) AS worker_column_12, worker_column_4 AS worker_column_13, worker_column_5 AS worker_column_14, (worker_column_2 OPERATOR(pg_catalog.+) 1) AS worker_column_15, abs((worker_column_3 OPERATOR(pg_catalog.-) worker_column_4)) AS worker_column_16 FROM (SELECT users_table."time" AS worker_column_1, users_table.user_id AS worker_column_2, users_table.value_1 AS worker_column_3, users_table.value_2 AS worker_column_4, users_table.value_3 AS worker_column_5 FROM public.users_table_1400258 users_table) worker_subquery WINDOW my_win AS (PARTITION BY worker_column_2, (worker_column_3 OPERATOR(pg_catalog.%) 3) ORDER BY worker_column_1 DESC), my_win_2 AS (PARTITION BY worker_column_2, worker_column_3 ORDER BY worker_column_1 DESC), my_win_3 AS (PARTITION BY worker_column_2, (date_trunc('min'::text, worker_column_1))), my_win_4 AS (my_win_3 ORDER BY worker_column_4, worker_column_5) ORDER BY (sum(((((worker_column_2)::numeric OPERATOR(pg_catalog.*) (5.0 OPERATOR(pg_catalog./) (((worker_column_3 OPERATOR(pg_catalog.+) worker_column_4))::numeric OPERATOR(pg_catalog.+) 0.1))))::double precision OPERATOR(pg_catalog.*) worker_column_5)) FILTER (WHERE ((worker_column_3)::text OPERATOR(pg_catalog.~~) '%1%'::text)) OVER my_win_4) DESC NULLS LAST, (avg(CASE WHEN (worker_column_2 OPERATOR(pg_catalog.>) 4) THEN worker_column_3 ELSE worker_column_4 END) FILTER (WHERE (worker_column_2 OPERATOR(pg_catalog.>) 2)) OVER my_win_2) DESC NULLS LAST, (date_trunc('min'::text, lag(worker_column_1) OVER (PARTITION BY worker_column_2 ORDER BY worker_column_1))) DESC NULLS LAST, (rank() OVER my_win) DESC, (count(*) OVER (PARTITION BY worker_column_2, (abs((worker_column_3 OPERATOR(pg_catalog.-) worker_column_4))))) DESC, (rank() OVER (PARTITION BY worker_column_2)) DESC, worker_column_2 DESC LIMIT '5'::bigint
2023-11-25 13:48:30.286 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.286 UTC [135455] DEBUG:  00000: distributed statement: SELECT count(*) OVER (PARTITION BY worker_column_2, (worker_column_2 OPERATOR(pg_catalog.+) 1)) AS count, rank() OVER (PARTITION BY worker_column_2) AS cnt1, count(*) OVER (PARTITION BY worker_column_2, (abs((worker_column_3 OPERATOR(pg_catalog.-) worker_column_4)))) AS cnt2, date_trunc('min'::text, lag(worker_column_1) OVER (PARTITION BY worker_column_2 ORDER BY worker_column_1)) AS datee, rank() OVER my_win AS rnnk, avg(CASE WHEN (worker_column_2 OPERATOR(pg_catalog.>) 4) THEN worker_column_3 ELSE worker_column_4 END) FILTER (WHERE (worker_column_2 OPERATOR(pg_catalog.>) 2)) OVER my_win_2 AS filtered_count, sum(((((worker_column_2)::numeric OPERATOR(pg_catalog.*) (5.0 OPERATOR(pg_catalog./) (((worker_column_3 OPERATOR(pg_catalog.+) worker_column_4))::numeric OPERATOR(pg_catalog.+) 0.1))))::double precision OPERATOR(pg_catalog.*) worker_column_5)) FILTER (WHERE ((worker_column_3)::text OPERATOR(pg_catalog.~~) '%1%'::text)) OVER my_win_4 AS cnt_with_filter_2, worker_column_2 AS worker_column_8, worker_column_1 AS worker_column_9, (worker_column_3 OPERATOR(pg_catalog.%) 3) AS worker_column_10, worker_column_3 AS worker_column_11, date_trunc('min'::text, worker_column_1) AS worker_column_12, worker_column_4 AS worker_column_13, worker_column_5 AS worker_column_14, (worker_column_2 OPERATOR(pg_catalog.+) 1) AS worker_column_15, abs((worker_column_3 OPERATOR(pg_catalog.-) worker_column_4)) AS worker_column_16 FROM (SELECT users_table."time" AS worker_column_1, users_table.user_id AS worker_column_2, users_table.value_1 AS worker_column_3, users_table.value_2 AS worker_column_4, users_table.value_3 AS worker_column_5 FROM public.users_table_1400259 users_table) worker_subquery WINDOW my_win AS (PARTITION BY worker_column_2, (worker_column_3 OPERATOR(pg_catalog.%) 3) ORDER BY worker_column_1 DESC), my_win_2 AS (PARTITION BY worker_column_2, worker_column_3 ORDER BY worker_column_1 DESC), my_win_3 AS (PARTITION BY worker_column_2, (date_trunc('min'::text, worker_column_1))), my_win_4 AS (my_win_3 ORDER BY worker_column_4, worker_column_5) ORDER BY (sum(((((worker_column_2)::numeric OPERATOR(pg_catalog.*) (5.0 OPERATOR(pg_catalog./) (((worker_column_3 OPERATOR(pg_catalog.+) worker_column_4))::numeric OPERATOR(pg_catalog.+) 0.1))))::double precision OPERATOR(pg_catalog.*) worker_column_5)) FILTER (WHERE ((worker_column_3)::text OPERATOR(pg_catalog.~~) '%1%'::text)) OVER my_win_4) DESC NULLS LAST, (avg(CASE WHEN (worker_column_2 OPERATOR(pg_catalog.>) 4) THEN worker_column_3 ELSE worker_column_4 END) FILTER (WHERE (worker_column_2 OPERATOR(pg_catalog.>) 2)) OVER my_win_2) DESC NULLS LAST, (date_trunc('min'::text, lag(worker_column_1) OVER (PARTITION BY worker_column_2 ORDER BY worker_column_1))) DESC NULLS LAST, (rank() OVER my_win) DESC, (count(*) OVER (PARTITION BY worker_column_2, (abs((worker_column_3 OPERATOR(pg_catalog.-) worker_column_4))))) DESC, (rank() OVER (PARTITION BY worker_column_2)) DESC, worker_column_2 DESC LIMIT '5'::bigint
2023-11-25 13:48:30.286 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.286 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.286 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.286 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.286 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.286 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.286 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.286 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.286 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.286 UTC [135455] DEBUG:  00000: combine query: SELECT count, cnt1, cnt2, datee, rnnk, filtered_count, cnt_with_filter_2 FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(count bigint, cnt1 bigint, cnt2 bigint, datee timestamp without time zone, rnnk bigint, filtered_count numeric, cnt_with_filter_2 double precision, worker_column_8 integer, worker_column_9 timestamp without time zone, worker_column_10 integer, worker_column_11 integer, worker_column_12 timestamp without time zone, worker_column_13 integer, worker_column_14 double precision, worker_column_15 integer, worker_column_16 integer) ORDER BY cnt_with_filter_2 DESC NULLS LAST, filtered_count DESC NULLS LAST, datee DESC NULLS LAST, rnnk DESC, cnt2 DESC, cnt1 DESC, worker_column_8 DESC LIMIT '5'::bigint
2023-11-25 13:48:30.286 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.286 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.286 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.287 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 52 in 3515 microseconds
2023-11-25 13:48:30.287 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.287 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.287 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.287 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 54 in 8559 microseconds
2023-11-25 13:48:30.287 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.288 UTC [135455] DEBUG:  00000: task execution (1) for placement (852) on anchor shard (1400256) finished in 1700 microseconds on worker node localhost:57637
2023-11-25 13:48:30.288 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.289 UTC [135455] DEBUG:  00000: task execution (3) for placement (854) on anchor shard (1400258) finished in 1052 microseconds on worker node localhost:57637
2023-11-25 13:48:30.289 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.289 UTC [135455] DEBUG:  00000: task execution (2) for placement (853) on anchor shard (1400257) finished in 1887 microseconds on worker node localhost:57638
2023-11-25 13:48:30.289 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.290 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 53 in 3593 microseconds
2023-11-25 13:48:30.290 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.291 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 55 in 3064 microseconds
2023-11-25 13:48:30.291 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.291 UTC [135455] DEBUG:  00000: task execution (4) for placement (855) on anchor shard (1400259) finished in 1219 microseconds on worker node localhost:57638
2023-11-25 13:48:30.291 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.291 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 52: 2 to node localhost:57637
2023-11-25 13:48:30.291 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.291 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 53: 0 to node localhost:57637
2023-11-25 13:48:30.291 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.291 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 54: 2 to node localhost:57638
2023-11-25 13:48:30.291 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.291 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 55: 0 to node localhost:57638
2023-11-25 13:48:30.291 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.291 UTC [135455] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 13:48:30.291 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.291 UTC [135455] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 13:48:30.291 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.291 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.291 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.292 UTC [135455] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 13:48:30.292 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.292 UTC [135455] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 13:48:30.292 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.292 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, rank() OVER my_win AS my_rank, avg(avg(worker_column_2)) OVER my_win_2 AS avg, max(worker_column_3) AS mx_time, worker_column_4 AS worker_column_5, count(*) AS worker_column_6, max(worker_column_2) AS worker_column_7, avg(worker_column_1) AS worker_column_8 FROM (SELECT events_table.user_id AS worker_column_1, events_table.event_type AS worker_column_2, events_table."time" AS worker_column_3, events_table.value_2 AS worker_column_4 FROM public.events_table_1400260 events_table) worker_subquery GROUP BY worker_column_1, worker_column_4 WINDOW my_win AS (PARTITION BY worker_column_1, (max(worker_column_2)) ORDER BY (count(*)) DESC), my_win_2 AS (PARTITION BY worker_column_1, (avg(worker_column_1)) ORDER BY (count(*)) DESC)
2023-11-25 13:48:30.292 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.292 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, rank() OVER my_win AS my_rank, avg(avg(worker_column_2)) OVER my_win_2 AS avg, max(worker_column_3) AS mx_time, worker_column_4 AS worker_column_5, count(*) AS worker_column_6, max(worker_column_2) AS worker_column_7, avg(worker_column_1) AS worker_column_8 FROM (SELECT events_table.user_id AS worker_column_1, events_table.event_type AS worker_column_2, events_table."time" AS worker_column_3, events_table.value_2 AS worker_column_4 FROM public.events_table_1400261 events_table) worker_subquery GROUP BY worker_column_1, worker_column_4 WINDOW my_win AS (PARTITION BY worker_column_1, (max(worker_column_2)) ORDER BY (count(*)) DESC), my_win_2 AS (PARTITION BY worker_column_1, (avg(worker_column_1)) ORDER BY (count(*)) DESC)
2023-11-25 13:48:30.292 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.292 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, rank() OVER my_win AS my_rank, avg(avg(worker_column_2)) OVER my_win_2 AS avg, max(worker_column_3) AS mx_time, worker_column_4 AS worker_column_5, count(*) AS worker_column_6, max(worker_column_2) AS worker_column_7, avg(worker_column_1) AS worker_column_8 FROM (SELECT events_table.user_id AS worker_column_1, events_table.event_type AS worker_column_2, events_table."time" AS worker_column_3, events_table.value_2 AS worker_column_4 FROM public.events_table_1400262 events_table) worker_subquery GROUP BY worker_column_1, worker_column_4 WINDOW my_win AS (PARTITION BY worker_column_1, (max(worker_column_2)) ORDER BY (count(*)) DESC), my_win_2 AS (PARTITION BY worker_column_1, (avg(worker_column_1)) ORDER BY (count(*)) DESC)
2023-11-25 13:48:30.292 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.292 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, rank() OVER my_win AS my_rank, avg(avg(worker_column_2)) OVER my_win_2 AS avg, max(worker_column_3) AS mx_time, worker_column_4 AS worker_column_5, count(*) AS worker_column_6, max(worker_column_2) AS worker_column_7, avg(worker_column_1) AS worker_column_8 FROM (SELECT events_table.user_id AS worker_column_1, events_table.event_type AS worker_column_2, events_table."time" AS worker_column_3, events_table.value_2 AS worker_column_4 FROM public.events_table_1400263 events_table) worker_subquery GROUP BY worker_column_1, worker_column_4 WINDOW my_win AS (PARTITION BY worker_column_1, (max(worker_column_2)) ORDER BY (count(*)) DESC), my_win_2 AS (PARTITION BY worker_column_1, (avg(worker_column_1)) ORDER BY (count(*)) DESC)
2023-11-25 13:48:30.292 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.292 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.292 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.292 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.292 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.292 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.292 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.292 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.292 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.292 UTC [135455] DEBUG:  00000: combine query: SELECT user_id, my_rank, avg, mx_time FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, my_rank bigint, avg numeric, mx_time timestamp without time zone, worker_column_5 integer, worker_column_6 bigint, worker_column_7 integer, worker_column_8 numeric) ORDER BY avg DESC, mx_time DESC, my_rank DESC, user_id DESC
2023-11-25 13:48:30.292 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.292 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.292 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.293 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 56 in 3515 microseconds
2023-11-25 13:48:30.293 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.293 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.293 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.293 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 58 in 8559 microseconds
2023-11-25 13:48:30.293 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.293 UTC [135455] DEBUG:  00000: task execution (1) for placement (856) on anchor shard (1400260) finished in 651 microseconds on worker node localhost:57637
2023-11-25 13:48:30.293 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.294 UTC [135455] DEBUG:  00000: task execution (2) for placement (857) on anchor shard (1400261) finished in 776 microseconds on worker node localhost:57638
2023-11-25 13:48:30.294 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.295 UTC [135455] DEBUG:  00000: task execution (3) for placement (858) on anchor shard (1400262) finished in 1137 microseconds on worker node localhost:57637
2023-11-25 13:48:30.295 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.295 UTC [135455] DEBUG:  00000: task execution (4) for placement (859) on anchor shard (1400263) finished in 534 microseconds on worker node localhost:57638
2023-11-25 13:48:30.295 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.296 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 57 in 2990 microseconds
2023-11-25 13:48:30.296 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.296 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 59 in 2673 microseconds
2023-11-25 13:48:30.296 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.296 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 56: 2 to node localhost:57637
2023-11-25 13:48:30.296 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.296 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 57: 0 to node localhost:57637
2023-11-25 13:48:30.296 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.296 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 58: 2 to node localhost:57638
2023-11-25 13:48:30.296 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.296 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 59: 0 to node localhost:57638
2023-11-25 13:48:30.296 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.297 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.297 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.297 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.297 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.297 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.297 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.297 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.297 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.297 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.297 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.298 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, rank() OVER (PARTITION BY worker_column_1 ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS rank, dense_rank() OVER (PARTITION BY worker_column_1 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS dense_rank, cume_dist() OVER (PARTITION BY worker_column_1 RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS cume_dist, percent_rank() OVER (PARTITION BY worker_column_1 ORDER BY (avg(worker_column_2)) RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS percent_rank, avg(worker_column_2) AS worker_column_6 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1
2023-11-25 13:48:30.298 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.298 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, rank() OVER (PARTITION BY worker_column_1 ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS rank, dense_rank() OVER (PARTITION BY worker_column_1 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS dense_rank, cume_dist() OVER (PARTITION BY worker_column_1 RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS cume_dist, percent_rank() OVER (PARTITION BY worker_column_1 ORDER BY (avg(worker_column_2)) RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS percent_rank, avg(worker_column_2) AS worker_column_6 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1
2023-11-25 13:48:30.298 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.298 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, rank() OVER (PARTITION BY worker_column_1 ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS rank, dense_rank() OVER (PARTITION BY worker_column_1 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS dense_rank, cume_dist() OVER (PARTITION BY worker_column_1 RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS cume_dist, percent_rank() OVER (PARTITION BY worker_column_1 ORDER BY (avg(worker_column_2)) RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS percent_rank, avg(worker_column_2) AS worker_column_6 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1
2023-11-25 13:48:30.298 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.298 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, rank() OVER (PARTITION BY worker_column_1 ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS rank, dense_rank() OVER (PARTITION BY worker_column_1 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS dense_rank, cume_dist() OVER (PARTITION BY worker_column_1 RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS cume_dist, percent_rank() OVER (PARTITION BY worker_column_1 ORDER BY (avg(worker_column_2)) RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS percent_rank, avg(worker_column_2) AS worker_column_6 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1
2023-11-25 13:48:30.298 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.298 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.298 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.298 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.298 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.298 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.298 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.298 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.298 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.298 UTC [135455] DEBUG:  00000: combine query: SELECT user_id, rank, dense_rank, cume_dist, percent_rank FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, rank bigint, dense_rank bigint, cume_dist double precision, percent_rank double precision, worker_column_6 numeric) ORDER BY cume_dist DESC, dense_rank DESC, rank DESC, user_id DESC
2023-11-25 13:48:30.298 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.298 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.298 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.298 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 60 in 3515 microseconds
2023-11-25 13:48:30.298 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.298 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.298 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.298 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 62 in 8559 microseconds
2023-11-25 13:48:30.298 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.299 UTC [135455] DEBUG:  00000: task execution (1) for placement (852) on anchor shard (1400256) finished in 613 microseconds on worker node localhost:57637
2023-11-25 13:48:30.299 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.299 UTC [135455] DEBUG:  00000: task execution (2) for placement (853) on anchor shard (1400257) finished in 609 microseconds on worker node localhost:57638
2023-11-25 13:48:30.299 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.299 UTC [135455] DEBUG:  00000: task execution (3) for placement (854) on anchor shard (1400258) finished in 449 microseconds on worker node localhost:57637
2023-11-25 13:48:30.299 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.300 UTC [135455] DEBUG:  00000: task execution (4) for placement (855) on anchor shard (1400259) finished in 601 microseconds on worker node localhost:57638
2023-11-25 13:48:30.300 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.301 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 61 in 3236 microseconds
2023-11-25 13:48:30.301 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.301 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 63 in 3141 microseconds
2023-11-25 13:48:30.301 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.301 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 60: 2 to node localhost:57637
2023-11-25 13:48:30.301 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.301 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 61: 0 to node localhost:57637
2023-11-25 13:48:30.301 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.301 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 62: 2 to node localhost:57638
2023-11-25 13:48:30.301 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.301 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 63: 0 to node localhost:57638
2023-11-25 13:48:30.301 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.302 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.302 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.302 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.302 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.302 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.302 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.302 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.302 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.302 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.302 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.302 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS value_1, array_agg(worker_column_2) OVER (PARTITION BY worker_column_1 ORDER BY worker_column_2 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS array_agg, array_agg(worker_column_2) OVER (PARTITION BY worker_column_1 ORDER BY worker_column_2 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW EXCLUDE CURRENT ROW) AS array_agg FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400256 users_table WHERE ((users_table.user_id OPERATOR(pg_catalog.>) 2) AND (users_table.user_id OPERATOR(pg_catalog.<) 6))) worker_subquery
2023-11-25 13:48:30.302 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.302 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS value_1, array_agg(worker_column_2) OVER (PARTITION BY worker_column_1 ORDER BY worker_column_2 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS array_agg, array_agg(worker_column_2) OVER (PARTITION BY worker_column_1 ORDER BY worker_column_2 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW EXCLUDE CURRENT ROW) AS array_agg FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400257 users_table WHERE ((users_table.user_id OPERATOR(pg_catalog.>) 2) AND (users_table.user_id OPERATOR(pg_catalog.<) 6))) worker_subquery
2023-11-25 13:48:30.302 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.303 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS value_1, array_agg(worker_column_2) OVER (PARTITION BY worker_column_1 ORDER BY worker_column_2 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS array_agg, array_agg(worker_column_2) OVER (PARTITION BY worker_column_1 ORDER BY worker_column_2 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW EXCLUDE CURRENT ROW) AS array_agg FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400258 users_table WHERE ((users_table.user_id OPERATOR(pg_catalog.>) 2) AND (users_table.user_id OPERATOR(pg_catalog.<) 6))) worker_subquery
2023-11-25 13:48:30.303 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.303 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS value_1, array_agg(worker_column_2) OVER (PARTITION BY worker_column_1 ORDER BY worker_column_2 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS array_agg, array_agg(worker_column_2) OVER (PARTITION BY worker_column_1 ORDER BY worker_column_2 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW EXCLUDE CURRENT ROW) AS array_agg FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400259 users_table WHERE ((users_table.user_id OPERATOR(pg_catalog.>) 2) AND (users_table.user_id OPERATOR(pg_catalog.<) 6))) worker_subquery
2023-11-25 13:48:30.303 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.303 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.303 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.303 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.303 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.303 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.303 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.303 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.303 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.303 UTC [135455] DEBUG:  00000: combine query: SELECT user_id, value_1, array_agg, array_agg_1 AS array_agg FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, value_1 integer, array_agg integer[], array_agg_1 integer[]) ORDER BY user_id, value_1, array_agg, array_agg_1
2023-11-25 13:48:30.303 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.303 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.303 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.303 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 64 in 3515 microseconds
2023-11-25 13:48:30.303 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.303 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.303 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.303 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 66 in 8559 microseconds
2023-11-25 13:48:30.303 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.304 UTC [135455] DEBUG:  00000: task execution (1) for placement (852) on anchor shard (1400256) finished in 1227 microseconds on worker node localhost:57637
2023-11-25 13:48:30.304 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.305 UTC [135455] DEBUG:  00000: task execution (2) for placement (853) on anchor shard (1400257) finished in 1571 microseconds on worker node localhost:57638
2023-11-25 13:48:30.305 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.305 UTC [135455] DEBUG:  00000: task execution (4) for placement (855) on anchor shard (1400259) finished in 290 microseconds on worker node localhost:57638
2023-11-25 13:48:30.305 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.305 UTC [135455] DEBUG:  00000: task execution (3) for placement (854) on anchor shard (1400258) finished in 1056 microseconds on worker node localhost:57637
2023-11-25 13:48:30.305 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.306 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 65 in 3271 microseconds
2023-11-25 13:48:30.306 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.307 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 67 in 4290 microseconds
2023-11-25 13:48:30.307 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.307 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 64: 2 to node localhost:57637
2023-11-25 13:48:30.307 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.307 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 65: 0 to node localhost:57637
2023-11-25 13:48:30.307 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.307 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 66: 2 to node localhost:57638
2023-11-25 13:48:30.307 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.307 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 67: 0 to node localhost:57638
2023-11-25 13:48:30.307 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.309 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.309 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.309 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.309 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.309 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.309 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.309 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.309 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.309 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.309 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.309 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS value_1, array_agg(worker_column_2) OVER range_window AS array_agg, array_agg(worker_column_2) OVER range_window_exclude AS array_agg FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400256 users_table WHERE ((users_table.user_id OPERATOR(pg_catalog.>) 2) AND (users_table.user_id OPERATOR(pg_catalog.<) 6))) worker_subquery WINDOW range_window AS (PARTITION BY worker_column_1 ORDER BY worker_column_2 RANGE BETWEEN 1 PRECEDING AND 1 FOLLOWING), range_window_exclude AS (PARTITION BY worker_column_1 ORDER BY worker_column_2 RANGE BETWEEN 1 PRECEDING AND 1 FOLLOWING EXCLUDE CURRENT ROW)
2023-11-25 13:48:30.309 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.309 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS value_1, array_agg(worker_column_2) OVER range_window AS array_agg, array_agg(worker_column_2) OVER range_window_exclude AS array_agg FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400257 users_table WHERE ((users_table.user_id OPERATOR(pg_catalog.>) 2) AND (users_table.user_id OPERATOR(pg_catalog.<) 6))) worker_subquery WINDOW range_window AS (PARTITION BY worker_column_1 ORDER BY worker_column_2 RANGE BETWEEN 1 PRECEDING AND 1 FOLLOWING), range_window_exclude AS (PARTITION BY worker_column_1 ORDER BY worker_column_2 RANGE BETWEEN 1 PRECEDING AND 1 FOLLOWING EXCLUDE CURRENT ROW)
2023-11-25 13:48:30.309 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.309 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS value_1, array_agg(worker_column_2) OVER range_window AS array_agg, array_agg(worker_column_2) OVER range_window_exclude AS array_agg FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400258 users_table WHERE ((users_table.user_id OPERATOR(pg_catalog.>) 2) AND (users_table.user_id OPERATOR(pg_catalog.<) 6))) worker_subquery WINDOW range_window AS (PARTITION BY worker_column_1 ORDER BY worker_column_2 RANGE BETWEEN 1 PRECEDING AND 1 FOLLOWING), range_window_exclude AS (PARTITION BY worker_column_1 ORDER BY worker_column_2 RANGE BETWEEN 1 PRECEDING AND 1 FOLLOWING EXCLUDE CURRENT ROW)
2023-11-25 13:48:30.309 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.309 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS value_1, array_agg(worker_column_2) OVER range_window AS array_agg, array_agg(worker_column_2) OVER range_window_exclude AS array_agg FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400259 users_table WHERE ((users_table.user_id OPERATOR(pg_catalog.>) 2) AND (users_table.user_id OPERATOR(pg_catalog.<) 6))) worker_subquery WINDOW range_window AS (PARTITION BY worker_column_1 ORDER BY worker_column_2 RANGE BETWEEN 1 PRECEDING AND 1 FOLLOWING), range_window_exclude AS (PARTITION BY worker_column_1 ORDER BY worker_column_2 RANGE BETWEEN 1 PRECEDING AND 1 FOLLOWING EXCLUDE CURRENT ROW)
2023-11-25 13:48:30.309 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.309 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.309 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.309 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.309 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.309 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.309 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.309 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.309 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.309 UTC [135455] DEBUG:  00000: combine query: SELECT user_id, value_1, array_agg, array_agg_1 AS array_agg FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, value_1 integer, array_agg integer[], array_agg_1 integer[]) ORDER BY user_id, value_1, array_agg, array_agg_1
2023-11-25 13:48:30.309 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.310 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.310 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.310 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 68 in 3515 microseconds
2023-11-25 13:48:30.310 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.310 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.310 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.310 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 70 in 8559 microseconds
2023-11-25 13:48:30.310 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.311 UTC [135455] DEBUG:  00000: task execution (1) for placement (852) on anchor shard (1400256) finished in 1478 microseconds on worker node localhost:57637
2023-11-25 13:48:30.311 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.311 UTC [135455] DEBUG:  00000: task execution (2) for placement (853) on anchor shard (1400257) finished in 1507 microseconds on worker node localhost:57638
2023-11-25 13:48:30.311 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.312 UTC [135455] DEBUG:  00000: task execution (3) for placement (854) on anchor shard (1400258) finished in 382 microseconds on worker node localhost:57637
2023-11-25 13:48:30.312 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.312 UTC [135455] DEBUG:  00000: task execution (4) for placement (855) on anchor shard (1400259) finished in 334 microseconds on worker node localhost:57638
2023-11-25 13:48:30.312 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.314 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 69 in 4116 microseconds
2023-11-25 13:48:30.314 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.316 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 71 in 5835 microseconds
2023-11-25 13:48:30.316 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.316 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 68: 2 to node localhost:57637
2023-11-25 13:48:30.316 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.316 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 69: 0 to node localhost:57637
2023-11-25 13:48:30.316 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.316 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 70: 2 to node localhost:57638
2023-11-25 13:48:30.316 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.316 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 71: 0 to node localhost:57638
2023-11-25 13:48:30.316 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.317 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.317 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.317 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.317 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.317 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.317 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.317 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.317 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.317 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.317 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.317 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS value_1, array_agg(worker_column_2) OVER row_window AS array_agg, array_agg(worker_column_2) OVER row_window_exclude AS array_agg FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400256 users_table WHERE ((users_table.user_id OPERATOR(pg_catalog.>) 2) AND (users_table.user_id OPERATOR(pg_catalog.<) 6))) worker_subquery WINDOW row_window AS (PARTITION BY worker_column_1 ORDER BY worker_column_2 ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING), row_window_exclude AS (PARTITION BY worker_column_1 ORDER BY worker_column_2 ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING EXCLUDE CURRENT ROW)
2023-11-25 13:48:30.317 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.317 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS value_1, array_agg(worker_column_2) OVER row_window AS array_agg, array_agg(worker_column_2) OVER row_window_exclude AS array_agg FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400257 users_table WHERE ((users_table.user_id OPERATOR(pg_catalog.>) 2) AND (users_table.user_id OPERATOR(pg_catalog.<) 6))) worker_subquery WINDOW row_window AS (PARTITION BY worker_column_1 ORDER BY worker_column_2 ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING), row_window_exclude AS (PARTITION BY worker_column_1 ORDER BY worker_column_2 ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING EXCLUDE CURRENT ROW)
2023-11-25 13:48:30.317 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.317 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS value_1, array_agg(worker_column_2) OVER row_window AS array_agg, array_agg(worker_column_2) OVER row_window_exclude AS array_agg FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400258 users_table WHERE ((users_table.user_id OPERATOR(pg_catalog.>) 2) AND (users_table.user_id OPERATOR(pg_catalog.<) 6))) worker_subquery WINDOW row_window AS (PARTITION BY worker_column_1 ORDER BY worker_column_2 ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING), row_window_exclude AS (PARTITION BY worker_column_1 ORDER BY worker_column_2 ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING EXCLUDE CURRENT ROW)
2023-11-25 13:48:30.317 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.317 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, worker_column_2 AS value_1, array_agg(worker_column_2) OVER row_window AS array_agg, array_agg(worker_column_2) OVER row_window_exclude AS array_agg FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2 FROM public.users_table_1400259 users_table WHERE ((users_table.user_id OPERATOR(pg_catalog.>) 2) AND (users_table.user_id OPERATOR(pg_catalog.<) 6))) worker_subquery WINDOW row_window AS (PARTITION BY worker_column_1 ORDER BY worker_column_2 ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING), row_window_exclude AS (PARTITION BY worker_column_1 ORDER BY worker_column_2 ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING EXCLUDE CURRENT ROW)
2023-11-25 13:48:30.317 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.317 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.317 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.317 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.317 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.317 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.317 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.317 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.317 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.318 UTC [135455] DEBUG:  00000: combine query: SELECT user_id, value_1, array_agg, array_agg_1 AS array_agg FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, value_1 integer, array_agg integer[], array_agg_1 integer[]) ORDER BY user_id, value_1, array_agg, array_agg_1
2023-11-25 13:48:30.318 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.318 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.318 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.318 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 72 in 3515 microseconds
2023-11-25 13:48:30.318 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.318 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.318 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.318 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 74 in 8559 microseconds
2023-11-25 13:48:30.318 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.319 UTC [135455] DEBUG:  00000: task execution (2) for placement (853) on anchor shard (1400257) finished in 1004 microseconds on worker node localhost:57638
2023-11-25 13:48:30.319 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.319 UTC [135455] DEBUG:  00000: task execution (1) for placement (852) on anchor shard (1400256) finished in 1299 microseconds on worker node localhost:57637
2023-11-25 13:48:30.319 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.319 UTC [135455] DEBUG:  00000: task execution (4) for placement (855) on anchor shard (1400259) finished in 372 microseconds on worker node localhost:57638
2023-11-25 13:48:30.319 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.320 UTC [135455] DEBUG:  00000: task execution (3) for placement (854) on anchor shard (1400258) finished in 617 microseconds on worker node localhost:57637
2023-11-25 13:48:30.320 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.322 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 73 in 3720 microseconds
2023-11-25 13:48:30.322 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.322 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 75 in 3590 microseconds
2023-11-25 13:48:30.322 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.322 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 72: 2 to node localhost:57637
2023-11-25 13:48:30.322 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.322 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 73: 0 to node localhost:57637
2023-11-25 13:48:30.322 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.322 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 74: 2 to node localhost:57638
2023-11-25 13:48:30.322 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.322 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 75: 0 to node localhost:57638
2023-11-25 13:48:30.322 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.322 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.322 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.322 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.322 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.322 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.322 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.322 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.322 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.322 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.322 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.323 UTC [135455] DEBUG:  00000: generated sql query for task 1
2023-11-25 13:48:30.323 UTC [135455] DETAIL:  query string: "SELECT value_2, sum(value_1) AS worker_column_2, count(value_1) AS worker_column_3 FROM public.users_table_1400256 users_table WHERE true GROUP BY value_2"
2023-11-25 13:48:30.323 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.323 UTC [135455] DEBUG:  00000: generated sql query for task 2
2023-11-25 13:48:30.323 UTC [135455] DETAIL:  query string: "SELECT value_2, sum(value_1) AS worker_column_2, count(value_1) AS worker_column_3 FROM public.users_table_1400257 users_table WHERE true GROUP BY value_2"
2023-11-25 13:48:30.323 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.323 UTC [135455] DEBUG:  00000: generated sql query for task 3
2023-11-25 13:48:30.323 UTC [135455] DETAIL:  query string: "SELECT value_2, sum(value_1) AS worker_column_2, count(value_1) AS worker_column_3 FROM public.users_table_1400258 users_table WHERE true GROUP BY value_2"
2023-11-25 13:48:30.323 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.323 UTC [135455] DEBUG:  00000: generated sql query for task 4
2023-11-25 13:48:30.323 UTC [135455] DETAIL:  query string: "SELECT value_2, sum(value_1) AS worker_column_2, count(value_1) AS worker_column_3 FROM public.users_table_1400259 users_table WHERE true GROUP BY value_2"
2023-11-25 13:48:30.323 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.323 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.323 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.323 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.323 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.323 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.323 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.323 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.323 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.323 UTC [135455] DEBUG:  00000: combine query: SELECT value_2, rank() OVER (PARTITION BY value_2 ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS rank, dense_rank() OVER (PARTITION BY value_2 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS dense_rank, cume_dist() OVER (PARTITION BY value_2 RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS cume_dist, percent_rank() OVER (PARTITION BY value_2 ORDER BY (pg_catalog.sum(worker_column_2) OPERATOR(pg_catalog./) pg_catalog.sum(worker_column_3)) RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS percent_rank FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), '(i 1)'::cstring(0)) remote_scan(value_2 integer, worker_column_2 bigint, worker_column_3 bigint) GROUP BY value_2 ORDER BY (cume_dist() OVER (PARTITION BY value_2 RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)) DESC, (dense_rank() OVER (PARTITION BY value_2 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)) DESC, (rank() OVER (PARTITION BY value_2 ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)) DESC, value_2 DESC
2023-11-25 13:48:30.323 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.323 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.323 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.323 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 76 in 3515 microseconds
2023-11-25 13:48:30.323 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.323 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.323 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.323 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 78 in 8559 microseconds
2023-11-25 13:48:30.323 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.325 UTC [135455] DEBUG:  00000: task execution (2) for placement (853) on anchor shard (1400257) finished in 1940 microseconds on worker node localhost:57638
2023-11-25 13:48:30.325 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.327 UTC [135455] DEBUG:  00000: task execution (4) for placement (855) on anchor shard (1400259) finished in 1445 microseconds on worker node localhost:57638
2023-11-25 13:48:30.327 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.327 UTC [135455] DEBUG:  00000: task execution (1) for placement (852) on anchor shard (1400256) finished in 3646 microseconds on worker node localhost:57637
2023-11-25 13:48:30.327 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.327 UTC [135455] DEBUG:  00000: task execution (3) for placement (854) on anchor shard (1400258) finished in 318 microseconds on worker node localhost:57637
2023-11-25 13:48:30.327 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.329 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 79 in 5357 microseconds
2023-11-25 13:48:30.329 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.329 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 77 in 6040 microseconds
2023-11-25 13:48:30.329 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.329 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 76: 2 to node localhost:57637
2023-11-25 13:48:30.329 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.329 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 77: 0 to node localhost:57637
2023-11-25 13:48:30.329 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.329 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 78: 2 to node localhost:57638
2023-11-25 13:48:30.329 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.329 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 79: 0 to node localhost:57638
2023-11-25 13:48:30.329 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.331 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.331 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.331 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.331 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.331 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.331 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.331 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.331 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.331 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.331 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.331 UTC [135455] DEBUG:  00000: generated sql query for task 1
2023-11-25 13:48:30.331 UTC [135455] DETAIL:  query string: "SELECT value_2, value_1, value_1 AS array_agg, value_1 AS array_agg FROM public.users_table_1400256 users_table WHERE ((value_2 OPERATOR(pg_catalog.>) 2) AND (value_2 OPERATOR(pg_catalog.<) 6))"
2023-11-25 13:48:30.331 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.331 UTC [135455] DEBUG:  00000: generated sql query for task 2
2023-11-25 13:48:30.331 UTC [135455] DETAIL:  query string: "SELECT value_2, value_1, value_1 AS array_agg, value_1 AS array_agg FROM public.users_table_1400257 users_table WHERE ((value_2 OPERATOR(pg_catalog.>) 2) AND (value_2 OPERATOR(pg_catalog.<) 6))"
2023-11-25 13:48:30.331 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.331 UTC [135455] DEBUG:  00000: generated sql query for task 3
2023-11-25 13:48:30.331 UTC [135455] DETAIL:  query string: "SELECT value_2, value_1, value_1 AS array_agg, value_1 AS array_agg FROM public.users_table_1400258 users_table WHERE ((value_2 OPERATOR(pg_catalog.>) 2) AND (value_2 OPERATOR(pg_catalog.<) 6))"
2023-11-25 13:48:30.331 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.331 UTC [135455] DEBUG:  00000: generated sql query for task 4
2023-11-25 13:48:30.331 UTC [135455] DETAIL:  query string: "SELECT value_2, value_1, value_1 AS array_agg, value_1 AS array_agg FROM public.users_table_1400259 users_table WHERE ((value_2 OPERATOR(pg_catalog.>) 2) AND (value_2 OPERATOR(pg_catalog.<) 6))"
2023-11-25 13:48:30.331 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.331 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.331 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.331 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.331 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.331 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.331 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.331 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.331 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.331 UTC [135455] DEBUG:  00000: combine query: SELECT value_2, value_1, array_agg(array_agg) OVER (PARTITION BY value_2 ORDER BY value_1 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS array_agg, array_agg(array_agg_1) OVER (PARTITION BY value_2 ORDER BY value_1 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW EXCLUDE CURRENT ROW) AS array_agg FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), '(i 1)'::cstring(0)) remote_scan(value_2 integer, value_1 integer, array_agg integer, array_agg_1 integer) ORDER BY value_2, value_1, (array_agg(array_agg) OVER (PARTITION BY value_2 ORDER BY value_1 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)), (array_agg(array_agg_1) OVER (PARTITION BY value_2 ORDER BY value_1 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW EXCLUDE CURRENT ROW))
2023-11-25 13:48:30.331 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.331 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.331 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.331 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 80 in 3515 microseconds
2023-11-25 13:48:30.331 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.331 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.331 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.331 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 82 in 8559 microseconds
2023-11-25 13:48:30.331 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.332 UTC [135455] DEBUG:  00000: task execution (1) for placement (852) on anchor shard (1400256) finished in 432 microseconds on worker node localhost:57637
2023-11-25 13:48:30.332 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.332 UTC [135455] DEBUG:  00000: task execution (2) for placement (853) on anchor shard (1400257) finished in 615 microseconds on worker node localhost:57638
2023-11-25 13:48:30.332 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.332 UTC [135455] DEBUG:  00000: task execution (3) for placement (854) on anchor shard (1400258) finished in 506 microseconds on worker node localhost:57637
2023-11-25 13:48:30.332 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.333 UTC [135455] DEBUG:  00000: task execution (4) for placement (855) on anchor shard (1400259) finished in 566 microseconds on worker node localhost:57638
2023-11-25 13:48:30.333 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.335 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 83 in 3370 microseconds
2023-11-25 13:48:30.335 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.335 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 81 in 3608 microseconds
2023-11-25 13:48:30.335 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.335 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 80: 2 to node localhost:57637
2023-11-25 13:48:30.335 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.335 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 81: 0 to node localhost:57637
2023-11-25 13:48:30.335 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.335 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 82: 2 to node localhost:57638
2023-11-25 13:48:30.335 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.335 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 83: 0 to node localhost:57638
2023-11-25 13:48:30.335 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.336 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.336 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.336 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.336 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.336 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.336 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.336 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.336 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.336 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.336 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.336 UTC [135455] DEBUG:  00000: generated sql query for task 1
2023-11-25 13:48:30.336 UTC [135455] DETAIL:  query string: "SELECT value_2, value_1, value_1 AS array_agg, value_1 AS array_agg FROM public.users_table_1400256 users_table WHERE ((value_2 OPERATOR(pg_catalog.>) 2) AND (value_2 OPERATOR(pg_catalog.<) 6))"
2023-11-25 13:48:30.336 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.336 UTC [135455] DEBUG:  00000: generated sql query for task 2
2023-11-25 13:48:30.336 UTC [135455] DETAIL:  query string: "SELECT value_2, value_1, value_1 AS array_agg, value_1 AS array_agg FROM public.users_table_1400257 users_table WHERE ((value_2 OPERATOR(pg_catalog.>) 2) AND (value_2 OPERATOR(pg_catalog.<) 6))"
2023-11-25 13:48:30.336 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.336 UTC [135455] DEBUG:  00000: generated sql query for task 3
2023-11-25 13:48:30.336 UTC [135455] DETAIL:  query string: "SELECT value_2, value_1, value_1 AS array_agg, value_1 AS array_agg FROM public.users_table_1400258 users_table WHERE ((value_2 OPERATOR(pg_catalog.>) 2) AND (value_2 OPERATOR(pg_catalog.<) 6))"
2023-11-25 13:48:30.336 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.336 UTC [135455] DEBUG:  00000: generated sql query for task 4
2023-11-25 13:48:30.336 UTC [135455] DETAIL:  query string: "SELECT value_2, value_1, value_1 AS array_agg, value_1 AS array_agg FROM public.users_table_1400259 users_table WHERE ((value_2 OPERATOR(pg_catalog.>) 2) AND (value_2 OPERATOR(pg_catalog.<) 6))"
2023-11-25 13:48:30.336 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.336 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.336 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.336 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.336 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.336 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.336 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.336 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.336 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.337 UTC [135455] DEBUG:  00000: combine query: SELECT value_2, value_1, array_agg(array_agg) OVER range_window AS array_agg, array_agg(array_agg_1) OVER range_window_exclude AS array_agg FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), '(i 1)'::cstring(0)) remote_scan(value_2 integer, value_1 integer, array_agg integer, array_agg_1 integer) WINDOW range_window AS (PARTITION BY value_2 ORDER BY value_1 RANGE BETWEEN 1 PRECEDING AND 1 FOLLOWING), range_window_exclude AS (PARTITION BY value_2 ORDER BY value_1 RANGE BETWEEN 1 PRECEDING AND 1 FOLLOWING EXCLUDE CURRENT ROW) ORDER BY value_2, value_1, (array_agg(array_agg) OVER range_window), (array_agg(array_agg_1) OVER range_window_exclude)
2023-11-25 13:48:30.337 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.337 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.337 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.337 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 84 in 3515 microseconds
2023-11-25 13:48:30.337 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.337 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.337 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.337 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 86 in 8559 microseconds
2023-11-25 13:48:30.337 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.337 UTC [135455] DEBUG:  00000: task execution (1) for placement (852) on anchor shard (1400256) finished in 613 microseconds on worker node localhost:57637
2023-11-25 13:48:30.337 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.338 UTC [135455] DEBUG:  00000: task execution (2) for placement (853) on anchor shard (1400257) finished in 797 microseconds on worker node localhost:57638
2023-11-25 13:48:30.338 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.338 UTC [135455] DEBUG:  00000: task execution (4) for placement (855) on anchor shard (1400259) finished in 474 microseconds on worker node localhost:57638
2023-11-25 13:48:30.338 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.339 UTC [135455] DEBUG:  00000: task execution (3) for placement (854) on anchor shard (1400258) finished in 1417 microseconds on worker node localhost:57637
2023-11-25 13:48:30.339 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.340 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 87 in 3188 microseconds
2023-11-25 13:48:30.340 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.344 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 85 in 6919 microseconds
2023-11-25 13:48:30.344 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.344 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 84: 2 to node localhost:57637
2023-11-25 13:48:30.344 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.344 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 85: 0 to node localhost:57637
2023-11-25 13:48:30.344 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.344 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 86: 2 to node localhost:57638
2023-11-25 13:48:30.344 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.344 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 87: 0 to node localhost:57638
2023-11-25 13:48:30.344 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.345 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.345 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.345 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.345 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.345 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.345 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.345 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.345 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.345 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.345 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.346 UTC [135455] DEBUG:  00000: generated sql query for task 1
2023-11-25 13:48:30.346 UTC [135455] DETAIL:  query string: "SELECT value_2, value_1, value_1 AS array_agg, value_1 AS array_agg FROM public.users_table_1400256 users_table WHERE ((value_2 OPERATOR(pg_catalog.>) 2) AND (value_2 OPERATOR(pg_catalog.<) 6))"
2023-11-25 13:48:30.346 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.346 UTC [135455] DEBUG:  00000: generated sql query for task 2
2023-11-25 13:48:30.346 UTC [135455] DETAIL:  query string: "SELECT value_2, value_1, value_1 AS array_agg, value_1 AS array_agg FROM public.users_table_1400257 users_table WHERE ((value_2 OPERATOR(pg_catalog.>) 2) AND (value_2 OPERATOR(pg_catalog.<) 6))"
2023-11-25 13:48:30.346 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.346 UTC [135455] DEBUG:  00000: generated sql query for task 3
2023-11-25 13:48:30.346 UTC [135455] DETAIL:  query string: "SELECT value_2, value_1, value_1 AS array_agg, value_1 AS array_agg FROM public.users_table_1400258 users_table WHERE ((value_2 OPERATOR(pg_catalog.>) 2) AND (value_2 OPERATOR(pg_catalog.<) 6))"
2023-11-25 13:48:30.346 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.346 UTC [135455] DEBUG:  00000: generated sql query for task 4
2023-11-25 13:48:30.346 UTC [135455] DETAIL:  query string: "SELECT value_2, value_1, value_1 AS array_agg, value_1 AS array_agg FROM public.users_table_1400259 users_table WHERE ((value_2 OPERATOR(pg_catalog.>) 2) AND (value_2 OPERATOR(pg_catalog.<) 6))"
2023-11-25 13:48:30.346 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.346 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.346 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.346 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.346 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.346 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.346 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.346 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.346 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.346 UTC [135455] DEBUG:  00000: combine query: SELECT value_2, value_1, array_agg(array_agg) OVER row_window AS array_agg, array_agg(array_agg_1) OVER row_window_exclude AS array_agg FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), '(i 1)'::cstring(0)) remote_scan(value_2 integer, value_1 integer, array_agg integer, array_agg_1 integer) WINDOW row_window AS (PARTITION BY value_2 ORDER BY value_1 ROWS BETWEEN '1'::bigint PRECEDING AND '1'::bigint FOLLOWING), row_window_exclude AS (PARTITION BY value_2 ORDER BY value_1 ROWS BETWEEN '1'::bigint PRECEDING AND '1'::bigint FOLLOWING EXCLUDE CURRENT ROW) ORDER BY value_2, value_1, (array_agg(array_agg) OVER row_window), (array_agg(array_agg_1) OVER row_window_exclude)
2023-11-25 13:48:30.346 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.346 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.346 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.346 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 88 in 3515 microseconds
2023-11-25 13:48:30.346 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.346 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.346 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.346 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 90 in 8559 microseconds
2023-11-25 13:48:30.346 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.346 UTC [135455] DEBUG:  00000: task execution (1) for placement (852) on anchor shard (1400256) finished in 456 microseconds on worker node localhost:57637
2023-11-25 13:48:30.346 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.347 UTC [135455] DEBUG:  00000: task execution (2) for placement (853) on anchor shard (1400257) finished in 416 microseconds on worker node localhost:57638
2023-11-25 13:48:30.347 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.347 UTC [135455] DEBUG:  00000: task execution (3) for placement (854) on anchor shard (1400258) finished in 492 microseconds on worker node localhost:57637
2023-11-25 13:48:30.347 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.347 UTC [135455] DEBUG:  00000: task execution (4) for placement (855) on anchor shard (1400259) finished in 414 microseconds on worker node localhost:57638
2023-11-25 13:48:30.347 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.350 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 89 in 3772 microseconds
2023-11-25 13:48:30.350 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.350 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 91 in 4158 microseconds
2023-11-25 13:48:30.350 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.350 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 88: 2 to node localhost:57637
2023-11-25 13:48:30.350 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.350 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 89: 0 to node localhost:57637
2023-11-25 13:48:30.350 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.350 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 90: 2 to node localhost:57638
2023-11-25 13:48:30.350 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.350 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 91: 0 to node localhost:57638
2023-11-25 13:48:30.350 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.351 UTC [135455] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 13:48:30.351 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.351 UTC [135455] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 13:48:30.351 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.351 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.351 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.352 UTC [135455] DEBUG:  00000: push down of limit count: 5
2023-11-25 13:48:30.352 UTC [135455] LOCATION:  WorkerLimitCount, multi_logical_optimizer.c:4778
2023-11-25 13:48:30.352 UTC [135455] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 13:48:30.352 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.352 UTC [135455] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 13:48:30.352 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.352 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER my_win AS sum, worker_column_2 AS event_type, count(*) AS worker_column_4, max(worker_column_2) AS worker_column_5 FROM (SELECT events_table.user_id AS worker_column_1, events_table.event_type AS worker_column_2 FROM public.events_table_1400260 events_table) worker_subquery GROUP BY worker_column_1, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 2) WINDOW my_win AS (PARTITION BY worker_column_1, (max(worker_column_2)) ORDER BY (count(*)) DESC) ORDER BY (sum(worker_column_2) OVER my_win) DESC, worker_column_2 DESC, worker_column_1 DESC LIMIT '5'::bigint
2023-11-25 13:48:30.352 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.352 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER my_win AS sum, worker_column_2 AS event_type, count(*) AS worker_column_4, max(worker_column_2) AS worker_column_5 FROM (SELECT events_table.user_id AS worker_column_1, events_table.event_type AS worker_column_2 FROM public.events_table_1400261 events_table) worker_subquery GROUP BY worker_column_1, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 2) WINDOW my_win AS (PARTITION BY worker_column_1, (max(worker_column_2)) ORDER BY (count(*)) DESC) ORDER BY (sum(worker_column_2) OVER my_win) DESC, worker_column_2 DESC, worker_column_1 DESC LIMIT '5'::bigint
2023-11-25 13:48:30.352 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.352 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER my_win AS sum, worker_column_2 AS event_type, count(*) AS worker_column_4, max(worker_column_2) AS worker_column_5 FROM (SELECT events_table.user_id AS worker_column_1, events_table.event_type AS worker_column_2 FROM public.events_table_1400262 events_table) worker_subquery GROUP BY worker_column_1, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 2) WINDOW my_win AS (PARTITION BY worker_column_1, (max(worker_column_2)) ORDER BY (count(*)) DESC) ORDER BY (sum(worker_column_2) OVER my_win) DESC, worker_column_2 DESC, worker_column_1 DESC LIMIT '5'::bigint
2023-11-25 13:48:30.352 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.352 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER my_win AS sum, worker_column_2 AS event_type, count(*) AS worker_column_4, max(worker_column_2) AS worker_column_5 FROM (SELECT events_table.user_id AS worker_column_1, events_table.event_type AS worker_column_2 FROM public.events_table_1400263 events_table) worker_subquery GROUP BY worker_column_1, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 2) WINDOW my_win AS (PARTITION BY worker_column_1, (max(worker_column_2)) ORDER BY (count(*)) DESC) ORDER BY (sum(worker_column_2) OVER my_win) DESC, worker_column_2 DESC, worker_column_1 DESC LIMIT '5'::bigint
2023-11-25 13:48:30.352 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.352 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.352 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.352 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.352 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.352 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.352 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.352 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.352 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.352 UTC [135455] DEBUG:  00000: combine query: SELECT user_id, sum, event_type FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, sum bigint, event_type integer, worker_column_4 bigint, worker_column_5 integer) ORDER BY sum DESC, event_type DESC, user_id DESC LIMIT '5'::bigint
2023-11-25 13:48:30.352 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.352 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.352 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.352 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 92 in 3515 microseconds
2023-11-25 13:48:30.352 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.352 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.352 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.352 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 94 in 8559 microseconds
2023-11-25 13:48:30.352 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.353 UTC [135455] DEBUG:  00000: task execution (1) for placement (856) on anchor shard (1400260) finished in 567 microseconds on worker node localhost:57637
2023-11-25 13:48:30.353 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.353 UTC [135455] DEBUG:  00000: task execution (2) for placement (857) on anchor shard (1400261) finished in 562 microseconds on worker node localhost:57638
2023-11-25 13:48:30.353 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.354 UTC [135455] DEBUG:  00000: task execution (3) for placement (858) on anchor shard (1400262) finished in 637 microseconds on worker node localhost:57637
2023-11-25 13:48:30.354 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.354 UTC [135455] DEBUG:  00000: task execution (4) for placement (859) on anchor shard (1400263) finished in 501 microseconds on worker node localhost:57638
2023-11-25 13:48:30.354 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.357 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 95 in 4497 microseconds
2023-11-25 13:48:30.357 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.361 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 93 in 9163 microseconds
2023-11-25 13:48:30.361 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.361 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 92: 2 to node localhost:57637
2023-11-25 13:48:30.361 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.361 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 93: 0 to node localhost:57637
2023-11-25 13:48:30.361 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.361 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 94: 2 to node localhost:57638
2023-11-25 13:48:30.361 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.362 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 95: 0 to node localhost:57638
2023-11-25 13:48:30.362 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.362 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.362 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.362 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.362 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.362 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.362 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.362 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.362 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.362 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.362 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.362 UTC [135455] DEBUG:  00000: generated sql query for task 1
2023-11-25 13:48:30.362 UTC [135455] DETAIL:  query string: "SELECT value_1, sum(value_3) AS avg, count(value_3) AS avg, sum(value_2) AS worker_column_4, count(value_2) AS worker_column_5 FROM public.users_table_1400256 users_table WHERE true GROUP BY value_1"
2023-11-25 13:48:30.362 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.362 UTC [135455] DEBUG:  00000: generated sql query for task 2
2023-11-25 13:48:30.362 UTC [135455] DETAIL:  query string: "SELECT value_1, sum(value_3) AS avg, count(value_3) AS avg, sum(value_2) AS worker_column_4, count(value_2) AS worker_column_5 FROM public.users_table_1400257 users_table WHERE true GROUP BY value_1"
2023-11-25 13:48:30.362 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.362 UTC [135455] DEBUG:  00000: generated sql query for task 3
2023-11-25 13:48:30.362 UTC [135455] DETAIL:  query string: "SELECT value_1, sum(value_3) AS avg, count(value_3) AS avg, sum(value_2) AS worker_column_4, count(value_2) AS worker_column_5 FROM public.users_table_1400258 users_table WHERE true GROUP BY value_1"
2023-11-25 13:48:30.362 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.362 UTC [135455] DEBUG:  00000: generated sql query for task 4
2023-11-25 13:48:30.362 UTC [135455] DETAIL:  query string: "SELECT value_1, sum(value_3) AS avg, count(value_3) AS avg, sum(value_2) AS worker_column_4, count(value_2) AS worker_column_5 FROM public.users_table_1400259 users_table WHERE true GROUP BY value_1"
2023-11-25 13:48:30.362 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.362 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.362 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.363 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.363 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.363 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.363 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.363 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.363 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.363 UTC [135455] DEBUG:  00000: combine query: SELECT value_1, (sum(avg) OPERATOR(pg_catalog./) (pg_catalog.sum(avg_1))::double precision) AS avg, dense_rank() OVER (PARTITION BY (sum(avg) OPERATOR(pg_catalog./) (pg_catalog.sum(avg_1))::double precision) ORDER BY (pg_catalog.sum(worker_column_4) OPERATOR(pg_catalog./) pg_catalog.sum(worker_column_5))) AS dense_rank FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), '(i 1)'::cstring(0)) remote_scan(value_1 integer, avg double precision, avg_1 bigint, worker_column_4 bigint, worker_column_5 bigint) GROUP BY value_1 ORDER BY value_1
2023-11-25 13:48:30.363 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.363 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.363 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.363 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 96 in 3515 microseconds
2023-11-25 13:48:30.363 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.363 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.363 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.363 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 98 in 8559 microseconds
2023-11-25 13:48:30.363 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.363 UTC [135455] DEBUG:  00000: task execution (1) for placement (852) on anchor shard (1400256) finished in 463 microseconds on worker node localhost:57637
2023-11-25 13:48:30.363 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.364 UTC [135455] DEBUG:  00000: task execution (2) for placement (853) on anchor shard (1400257) finished in 655 microseconds on worker node localhost:57638
2023-11-25 13:48:30.364 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.364 UTC [135455] DEBUG:  00000: task execution (3) for placement (854) on anchor shard (1400258) finished in 568 microseconds on worker node localhost:57637
2023-11-25 13:48:30.364 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.364 UTC [135455] DEBUG:  00000: task execution (4) for placement (855) on anchor shard (1400259) finished in 408 microseconds on worker node localhost:57638
2023-11-25 13:48:30.364 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.367 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 99 in 3485 microseconds
2023-11-25 13:48:30.367 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.368 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 97 in 5151 microseconds
2023-11-25 13:48:30.368 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.368 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 96: 2 to node localhost:57637
2023-11-25 13:48:30.368 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.368 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 97: 0 to node localhost:57637
2023-11-25 13:48:30.368 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.368 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 98: 2 to node localhost:57638
2023-11-25 13:48:30.368 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.368 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 99: 0 to node localhost:57638
2023-11-25 13:48:30.368 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.369 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.369 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.369 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.369 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.369 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.369 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.369 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.369 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.369 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.369 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.369 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum, worker_column_3, worker_column_2 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_2 AS worker_column_2, users_table.value_1 AS worker_column_3 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 2)
2023-11-25 13:48:30.369 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.369 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum, worker_column_3, worker_column_2 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_2 AS worker_column_2, users_table.value_1 AS worker_column_3 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 2)
2023-11-25 13:48:30.369 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.369 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum, worker_column_3, worker_column_2 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_2 AS worker_column_2, users_table.value_1 AS worker_column_3 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 2)
2023-11-25 13:48:30.369 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.369 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum, worker_column_3, worker_column_2 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_2 AS worker_column_2, users_table.value_1 AS worker_column_3 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 2)
2023-11-25 13:48:30.369 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.369 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.369 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.369 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.369 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.369 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.369 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.369 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.369 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.369 UTC [135455] DEBUG:  00000: combine query: SELECT DISTINCT user_id, sum FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, sum bigint, worker_column_3 integer, worker_column_4 integer) ORDER BY sum DESC, user_id LIMIT '10'::bigint
2023-11-25 13:48:30.369 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.369 UTC [135456] ERROR:  22004: the partition column of table insert_select_repartition.target_table should have a value
2023-11-25 13:48:30.369 UTC [135456] LOCATION:  NonPushableInsertSelectExecScan, insert_select_executor.c:153
2023-11-25 13:48:30.369 UTC [135456] STATEMENT:  INSERT INTO target_table(value) SELECT value FROM source_table;
2023-11-25 13:48:30.369 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.369 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.369 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 100 in 3515 microseconds
2023-11-25 13:48:30.369 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.369 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.369 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.370 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 102 in 8559 microseconds
2023-11-25 13:48:30.370 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.370 UTC [135455] DEBUG:  00000: task execution (2) for placement (853) on anchor shard (1400257) finished in 533 microseconds on worker node localhost:57638
2023-11-25 13:48:30.370 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.371 UTC [135455] DEBUG:  00000: task execution (4) for placement (855) on anchor shard (1400259) finished in 391 microseconds on worker node localhost:57638
2023-11-25 13:48:30.371 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.371 UTC [135455] DEBUG:  00000: task execution (1) for placement (852) on anchor shard (1400256) finished in 1485 microseconds on worker node localhost:57637
2023-11-25 13:48:30.371 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.371 UTC [135455] DEBUG:  00000: task execution (3) for placement (854) on anchor shard (1400258) finished in 568 microseconds on worker node localhost:57637
2023-11-25 13:48:30.371 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.374 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 103 in 4256 microseconds
2023-11-25 13:48:30.374 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.374 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 101 in 4691 microseconds
2023-11-25 13:48:30.374 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.374 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 100: 2 to node localhost:57637
2023-11-25 13:48:30.374 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.374 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 101: 0 to node localhost:57637
2023-11-25 13:48:30.374 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.374 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 102: 2 to node localhost:57638
2023-11-25 13:48:30.374 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.374 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 103: 0 to node localhost:57638
2023-11-25 13:48:30.374 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.375 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.375 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.375 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.375 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.375 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.375 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.375 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.375 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.375 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.375 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.375 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum, worker_column_3, worker_column_2 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_2 AS worker_column_2, users_table.value_1 AS worker_column_3 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 2)
2023-11-25 13:48:30.375 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.375 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum, worker_column_3, worker_column_2 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_2 AS worker_column_2, users_table.value_1 AS worker_column_3 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 2)
2023-11-25 13:48:30.375 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.375 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum, worker_column_3, worker_column_2 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_2 AS worker_column_2, users_table.value_1 AS worker_column_3 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 2)
2023-11-25 13:48:30.375 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.375 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum, worker_column_3, worker_column_2 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_2 AS worker_column_2, users_table.value_1 AS worker_column_3 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 2)
2023-11-25 13:48:30.375 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.375 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.375 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.375 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.375 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.375 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.375 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.375 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.375 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.375 UTC [135455] DEBUG:  00000: combine query: SELECT DISTINCT ON (user_id) user_id, sum FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, sum bigint, worker_column_3 integer, worker_column_4 integer) ORDER BY user_id, sum DESC LIMIT '10'::bigint
2023-11-25 13:48:30.375 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.375 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.375 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.375 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 104 in 3515 microseconds
2023-11-25 13:48:30.375 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.375 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.375 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.376 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 106 in 8559 microseconds
2023-11-25 13:48:30.376 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.376 UTC [135455] DEBUG:  00000: task execution (1) for placement (852) on anchor shard (1400256) finished in 431 microseconds on worker node localhost:57637
2023-11-25 13:48:30.376 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.377 UTC [135455] DEBUG:  00000: task execution (3) for placement (854) on anchor shard (1400258) finished in 1119 microseconds on worker node localhost:57637
2023-11-25 13:48:30.377 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.377 UTC [135455] DEBUG:  00000: task execution (2) for placement (853) on anchor shard (1400257) finished in 1429 microseconds on worker node localhost:57638
2023-11-25 13:48:30.377 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.377 UTC [135455] DEBUG:  00000: task execution (4) for placement (855) on anchor shard (1400259) finished in 330 microseconds on worker node localhost:57638
2023-11-25 13:48:30.377 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.380 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 105 in 4243 microseconds
2023-11-25 13:48:30.380 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.381 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 107 in 5325 microseconds
2023-11-25 13:48:30.381 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.381 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 104: 2 to node localhost:57637
2023-11-25 13:48:30.381 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.381 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 105: 0 to node localhost:57637
2023-11-25 13:48:30.381 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.381 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 106: 2 to node localhost:57638
2023-11-25 13:48:30.381 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.381 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 107: 0 to node localhost:57638
2023-11-25 13:48:30.381 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.381 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.381 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.381 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.381 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.381 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.381 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.382 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.382 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.382 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.382 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.382 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum, sum(worker_column_3) OVER (PARTITION BY worker_column_1) AS worker_column_3, worker_column_3 AS worker_column_4, worker_column_2 AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_2 AS worker_column_2, users_table.value_1 AS worker_column_3 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 2)
2023-11-25 13:48:30.382 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.382 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum, sum(worker_column_3) OVER (PARTITION BY worker_column_1) AS worker_column_3, worker_column_3 AS worker_column_4, worker_column_2 AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_2 AS worker_column_2, users_table.value_1 AS worker_column_3 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 2)
2023-11-25 13:48:30.382 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.382 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum, sum(worker_column_3) OVER (PARTITION BY worker_column_1) AS worker_column_3, worker_column_3 AS worker_column_4, worker_column_2 AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_2 AS worker_column_2, users_table.value_1 AS worker_column_3 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 2)
2023-11-25 13:48:30.382 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.382 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) OVER (PARTITION BY worker_column_1) AS sum, sum(worker_column_3) OVER (PARTITION BY worker_column_1) AS worker_column_3, worker_column_3 AS worker_column_4, worker_column_2 AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_2 AS worker_column_2, users_table.value_1 AS worker_column_3 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3, worker_column_2 HAVING (count(*) OPERATOR(pg_catalog.>) 2)
2023-11-25 13:48:30.382 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.382 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.382 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.382 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.382 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.382 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.382 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.382 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.382 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.382 UTC [135455] DEBUG:  00000: combine query: SELECT DISTINCT ON (worker_column_3) user_id, sum FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, sum bigint, worker_column_3 bigint, worker_column_4 integer, worker_column_5 integer) ORDER BY worker_column_3, sum DESC, user_id LIMIT '10'::bigint
2023-11-25 13:48:30.382 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.382 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.382 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.382 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 108 in 3515 microseconds
2023-11-25 13:48:30.382 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.382 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.382 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.382 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 110 in 8559 microseconds
2023-11-25 13:48:30.382 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.383 UTC [135455] DEBUG:  00000: task execution (1) for placement (852) on anchor shard (1400256) finished in 507 microseconds on worker node localhost:57637
2023-11-25 13:48:30.383 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.383 UTC [135455] DEBUG:  00000: task execution (2) for placement (853) on anchor shard (1400257) finished in 575 microseconds on worker node localhost:57638
2023-11-25 13:48:30.383 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.383 UTC [135455] DEBUG:  00000: task execution (3) for placement (854) on anchor shard (1400258) finished in 355 microseconds on worker node localhost:57637
2023-11-25 13:48:30.383 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.383 UTC [135455] DEBUG:  00000: task execution (4) for placement (855) on anchor shard (1400259) finished in 341 microseconds on worker node localhost:57638
2023-11-25 13:48:30.383 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.388 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 109 in 5760 microseconds
2023-11-25 13:48:30.388 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.388 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 111 in 6140 microseconds
2023-11-25 13:48:30.388 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.388 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 108: 2 to node localhost:57637
2023-11-25 13:48:30.388 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.388 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 109: 0 to node localhost:57637
2023-11-25 13:48:30.388 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.388 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 110: 2 to node localhost:57638
2023-11-25 13:48:30.388 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.388 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 111: 0 to node localhost:57638
2023-11-25 13:48:30.388 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.389 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.389 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.389 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.389 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.389 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.389 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.389 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.389 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.389 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.389 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.389 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(avg(worker_column_2)) OVER (PARTITION BY worker_column_1, (max(worker_column_1)), (min(worker_column_3))) AS avg, avg(avg(worker_column_1)) OVER (PARTITION BY worker_column_1, (min(worker_column_1)), (avg(worker_column_2))) AS avg, max(worker_column_1) AS worker_column_4, min(worker_column_3) AS worker_column_5, min(worker_column_1) AS worker_column_6, avg(worker_column_2) AS worker_column_7 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1
2023-11-25 13:48:30.389 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.389 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(avg(worker_column_2)) OVER (PARTITION BY worker_column_1, (max(worker_column_1)), (min(worker_column_3))) AS avg, avg(avg(worker_column_1)) OVER (PARTITION BY worker_column_1, (min(worker_column_1)), (avg(worker_column_2))) AS avg, max(worker_column_1) AS worker_column_4, min(worker_column_3) AS worker_column_5, min(worker_column_1) AS worker_column_6, avg(worker_column_2) AS worker_column_7 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1
2023-11-25 13:48:30.389 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.389 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(avg(worker_column_2)) OVER (PARTITION BY worker_column_1, (max(worker_column_1)), (min(worker_column_3))) AS avg, avg(avg(worker_column_1)) OVER (PARTITION BY worker_column_1, (min(worker_column_1)), (avg(worker_column_2))) AS avg, max(worker_column_1) AS worker_column_4, min(worker_column_3) AS worker_column_5, min(worker_column_1) AS worker_column_6, avg(worker_column_2) AS worker_column_7 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1
2023-11-25 13:48:30.389 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.389 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(avg(worker_column_2)) OVER (PARTITION BY worker_column_1, (max(worker_column_1)), (min(worker_column_3))) AS avg, avg(avg(worker_column_1)) OVER (PARTITION BY worker_column_1, (min(worker_column_1)), (avg(worker_column_2))) AS avg, max(worker_column_1) AS worker_column_4, min(worker_column_3) AS worker_column_5, min(worker_column_1) AS worker_column_6, avg(worker_column_2) AS worker_column_7 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1
2023-11-25 13:48:30.389 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.389 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.389 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.389 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.389 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.389 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.389 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.389 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.389 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.389 UTC [135455] DEBUG:  00000: combine query: SELECT user_id, avg, avg_1 AS avg FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, avg numeric, avg_1 numeric, worker_column_4 integer, worker_column_5 integer, worker_column_6 integer, worker_column_7 numeric) ORDER BY avg_1 DESC, avg DESC, user_id DESC
2023-11-25 13:48:30.389 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.390 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.390 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.390 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 112 in 3515 microseconds
2023-11-25 13:48:30.390 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.390 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.390 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.390 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 114 in 8559 microseconds
2023-11-25 13:48:30.390 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.391 UTC [135455] DEBUG:  00000: task execution (1) for placement (852) on anchor shard (1400256) finished in 1267 microseconds on worker node localhost:57637
2023-11-25 13:48:30.391 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.391 UTC [135455] DEBUG:  00000: task execution (2) for placement (853) on anchor shard (1400257) finished in 1150 microseconds on worker node localhost:57638
2023-11-25 13:48:30.391 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.392 UTC [135455] DEBUG:  00000: task execution (3) for placement (854) on anchor shard (1400258) finished in 652 microseconds on worker node localhost:57637
2023-11-25 13:48:30.392 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.392 UTC [135455] DEBUG:  00000: task execution (4) for placement (855) on anchor shard (1400259) finished in 622 microseconds on worker node localhost:57638
2023-11-25 13:48:30.392 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.395 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 115 in 5141 microseconds
2023-11-25 13:48:30.395 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.396 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 113 in 5813 microseconds
2023-11-25 13:48:30.396 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.396 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 112: 2 to node localhost:57637
2023-11-25 13:48:30.396 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.396 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 113: 0 to node localhost:57637
2023-11-25 13:48:30.396 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.396 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 114: 2 to node localhost:57638
2023-11-25 13:48:30.396 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.396 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 115: 0 to node localhost:57638
2023-11-25 13:48:30.396 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.397 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.397 UTC [135455] CONTEXT:  PL/pgSQL function coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 13:48:30.397 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.397 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.397 UTC [135455] CONTEXT:  PL/pgSQL function coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 13:48:30.397 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.397 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.397 UTC [135455] CONTEXT:  PL/pgSQL function coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 13:48:30.397 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.397 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.397 UTC [135455] CONTEXT:  PL/pgSQL function coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 13:48:30.397 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.397 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.397 UTC [135455] CONTEXT:  PL/pgSQL function coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 13:48:30.397 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.397 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(avg(worker_column_2)) OVER (PARTITION BY worker_column_1, (max(worker_column_1)), (min(worker_column_3))) AS avg, avg(avg(worker_column_1)) OVER (PARTITION BY worker_column_1, (min(worker_column_1)), (avg(worker_column_2))) AS avg, max(worker_column_1) AS worker_column_4, min(worker_column_3) AS worker_column_5, min(worker_column_1) AS worker_column_6, avg(worker_column_2) AS worker_column_7 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1
2023-11-25 13:48:30.397 UTC [135455] CONTEXT:  PL/pgSQL function coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 13:48:30.397 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.397 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(avg(worker_column_2)) OVER (PARTITION BY worker_column_1, (max(worker_column_1)), (min(worker_column_3))) AS avg, avg(avg(worker_column_1)) OVER (PARTITION BY worker_column_1, (min(worker_column_1)), (avg(worker_column_2))) AS avg, max(worker_column_1) AS worker_column_4, min(worker_column_3) AS worker_column_5, min(worker_column_1) AS worker_column_6, avg(worker_column_2) AS worker_column_7 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1
2023-11-25 13:48:30.397 UTC [135455] CONTEXT:  PL/pgSQL function coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 13:48:30.397 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.397 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(avg(worker_column_2)) OVER (PARTITION BY worker_column_1, (max(worker_column_1)), (min(worker_column_3))) AS avg, avg(avg(worker_column_1)) OVER (PARTITION BY worker_column_1, (min(worker_column_1)), (avg(worker_column_2))) AS avg, max(worker_column_1) AS worker_column_4, min(worker_column_3) AS worker_column_5, min(worker_column_1) AS worker_column_6, avg(worker_column_2) AS worker_column_7 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1
2023-11-25 13:48:30.397 UTC [135455] CONTEXT:  PL/pgSQL function coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 13:48:30.397 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.397 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(avg(worker_column_2)) OVER (PARTITION BY worker_column_1, (max(worker_column_1)), (min(worker_column_3))) AS avg, avg(avg(worker_column_1)) OVER (PARTITION BY worker_column_1, (min(worker_column_1)), (avg(worker_column_2))) AS avg, max(worker_column_1) AS worker_column_4, min(worker_column_3) AS worker_column_5, min(worker_column_1) AS worker_column_6, avg(worker_column_2) AS worker_column_7 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1
2023-11-25 13:48:30.397 UTC [135455] CONTEXT:  PL/pgSQL function coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 13:48:30.397 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.397 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.397 UTC [135455] CONTEXT:  PL/pgSQL function coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 13:48:30.397 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.397 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.397 UTC [135455] CONTEXT:  PL/pgSQL function coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 13:48:30.397 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.397 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.397 UTC [135455] CONTEXT:  PL/pgSQL function coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 13:48:30.397 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.397 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.397 UTC [135455] CONTEXT:  PL/pgSQL function coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 13:48:30.397 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.397 UTC [135455] DEBUG:  00000: combine query: SELECT user_id, avg, avg_1 AS avg FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, avg numeric, avg_1 numeric, worker_column_4 integer, worker_column_5 integer, worker_column_6 integer, worker_column_7 numeric) ORDER BY avg_1 DESC, avg DESC, user_id DESC
2023-11-25 13:48:30.397 UTC [135455] CONTEXT:  PL/pgSQL function coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 13:48:30.397 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.399 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.399 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.399 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.399 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.399 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.399 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.399 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.399 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.399 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.399 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.399 UTC [135455] DEBUG:  00000: generated sql query for task 1
2023-11-25 13:48:30.399 UTC [135455] DETAIL:  query string: "SELECT value_2, sum(value_1) AS avg, count(value_1) AS avg, sum(value_2) AS avg, count(value_2) AS avg, max(value_2) AS worker_column_6, min(value_2) AS worker_column_7, sum(value_1) AS worker_column_8, count(value_1) AS worker_column_9 FROM public.users_table_1400256 users_table WHERE true GROUP BY value_2"
2023-11-25 13:48:30.399 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.399 UTC [135455] DEBUG:  00000: generated sql query for task 2
2023-11-25 13:48:30.399 UTC [135455] DETAIL:  query string: "SELECT value_2, sum(value_1) AS avg, count(value_1) AS avg, sum(value_2) AS avg, count(value_2) AS avg, max(value_2) AS worker_column_6, min(value_2) AS worker_column_7, sum(value_1) AS worker_column_8, count(value_1) AS worker_column_9 FROM public.users_table_1400257 users_table WHERE true GROUP BY value_2"
2023-11-25 13:48:30.399 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.399 UTC [135455] DEBUG:  00000: generated sql query for task 3
2023-11-25 13:48:30.399 UTC [135455] DETAIL:  query string: "SELECT value_2, sum(value_1) AS avg, count(value_1) AS avg, sum(value_2) AS avg, count(value_2) AS avg, max(value_2) AS worker_column_6, min(value_2) AS worker_column_7, sum(value_1) AS worker_column_8, count(value_1) AS worker_column_9 FROM public.users_table_1400258 users_table WHERE true GROUP BY value_2"
2023-11-25 13:48:30.399 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.399 UTC [135455] DEBUG:  00000: generated sql query for task 4
2023-11-25 13:48:30.399 UTC [135455] DETAIL:  query string: "SELECT value_2, sum(value_1) AS avg, count(value_1) AS avg, sum(value_2) AS avg, count(value_2) AS avg, max(value_2) AS worker_column_6, min(value_2) AS worker_column_7, sum(value_1) AS worker_column_8, count(value_1) AS worker_column_9 FROM public.users_table_1400259 users_table WHERE true GROUP BY value_2"
2023-11-25 13:48:30.399 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.399 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.399 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.399 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.399 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.399 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.399 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.399 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.399 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.399 UTC [135455] DEBUG:  00000: combine query: SELECT value_2, avg((pg_catalog.sum(avg) OPERATOR(pg_catalog./) pg_catalog.sum(avg_1))) OVER (PARTITION BY value_2, (max(worker_column_6)), (min(worker_column_7))) AS avg, avg((pg_catalog.sum(avg_2) OPERATOR(pg_catalog./) pg_catalog.sum(avg_3))) OVER (PARTITION BY value_2, (min(worker_column_7)), (pg_catalog.sum(worker_column_8) OPERATOR(pg_catalog./) pg_catalog.sum(worker_column_9))) AS avg FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), '(i 1)'::cstring(0)) remote_scan(value_2 integer, avg bigint, avg_1 bigint, avg_2 bigint, avg_3 bigint, worker_column_6 integer, worker_column_7 integer, worker_column_8 bigint, worker_column_9 bigint) GROUP BY value_2 ORDER BY (avg((pg_catalog.sum(avg_2) OPERATOR(pg_catalog./) pg_catalog.sum(avg_3))) OVER (PARTITION BY value_2, (min(worker_column_7)), (pg_catalog.sum(worker_column_8) OPERATOR(pg_catalog./) pg_catalog.sum(worker_column_9)))) DESC, (avg((pg_catalog.sum(avg) OPERATOR(pg_catalog./) pg_catalog.sum(avg_1))) OVER (PARTITION BY value_2, (max(worker_column_6)), (min(worker_column_7)))) DESC, value_2 DESC
2023-11-25 13:48:30.399 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.400 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.400 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.400 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 116 in 3515 microseconds
2023-11-25 13:48:30.400 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.400 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.400 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.400 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 118 in 8559 microseconds
2023-11-25 13:48:30.400 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.401 UTC [135455] DEBUG:  00000: task execution (1) for placement (852) on anchor shard (1400256) finished in 909 microseconds on worker node localhost:57637
2023-11-25 13:48:30.401 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.401 UTC [135455] DEBUG:  00000: task execution (3) for placement (854) on anchor shard (1400258) finished in 434 microseconds on worker node localhost:57637
2023-11-25 13:48:30.401 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.401 UTC [135455] DEBUG:  00000: task execution (2) for placement (853) on anchor shard (1400257) finished in 1326 microseconds on worker node localhost:57638
2023-11-25 13:48:30.401 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.402 UTC [135455] DEBUG:  00000: task execution (4) for placement (855) on anchor shard (1400259) finished in 391 microseconds on worker node localhost:57638
2023-11-25 13:48:30.402 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.405 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 117 in 5405 microseconds
2023-11-25 13:48:30.405 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.406 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 119 in 6379 microseconds
2023-11-25 13:48:30.406 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.406 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 116: 2 to node localhost:57637
2023-11-25 13:48:30.406 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.406 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 117: 0 to node localhost:57637
2023-11-25 13:48:30.406 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.406 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 118: 2 to node localhost:57638
2023-11-25 13:48:30.406 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.406 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 119: 0 to node localhost:57638
2023-11-25 13:48:30.406 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.407 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.407 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.407 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.407 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.407 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.407 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.407 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.407 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.407 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.407 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.407 UTC [135455] DEBUG:  00000: generated sql query for task 1
2023-11-25 13:48:30.407 UTC [135455] DETAIL:  query string: "SELECT value_2, user_id, avg(value_1) AS avg, avg(value_2) AS avg, max(value_2) AS worker_column_5, min(value_2) AS worker_column_6, avg(value_1) AS worker_column_7 FROM public.users_table_1400256 users_table WHERE true GROUP BY value_2, user_id"
2023-11-25 13:48:30.407 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.407 UTC [135455] DEBUG:  00000: generated sql query for task 2
2023-11-25 13:48:30.407 UTC [135455] DETAIL:  query string: "SELECT value_2, user_id, avg(value_1) AS avg, avg(value_2) AS avg, max(value_2) AS worker_column_5, min(value_2) AS worker_column_6, avg(value_1) AS worker_column_7 FROM public.users_table_1400257 users_table WHERE true GROUP BY value_2, user_id"
2023-11-25 13:48:30.407 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.407 UTC [135455] DEBUG:  00000: generated sql query for task 3
2023-11-25 13:48:30.407 UTC [135455] DETAIL:  query string: "SELECT value_2, user_id, avg(value_1) AS avg, avg(value_2) AS avg, max(value_2) AS worker_column_5, min(value_2) AS worker_column_6, avg(value_1) AS worker_column_7 FROM public.users_table_1400258 users_table WHERE true GROUP BY value_2, user_id"
2023-11-25 13:48:30.407 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.407 UTC [135455] DEBUG:  00000: generated sql query for task 4
2023-11-25 13:48:30.407 UTC [135455] DETAIL:  query string: "SELECT value_2, user_id, avg(value_1) AS avg, avg(value_2) AS avg, max(value_2) AS worker_column_5, min(value_2) AS worker_column_6, avg(value_1) AS worker_column_7 FROM public.users_table_1400259 users_table WHERE true GROUP BY value_2, user_id"
2023-11-25 13:48:30.407 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.407 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.407 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.407 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.407 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.408 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.408 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.408 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.408 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.408 UTC [135455] DEBUG:  00000: combine query: SELECT value_2, user_id, avg(avg) OVER (PARTITION BY value_2, worker_column_5, worker_column_6) AS avg, avg(avg_1) OVER (PARTITION BY user_id, worker_column_6, worker_column_7) AS avg FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), '(i 1)'::cstring(0)) remote_scan(value_2 integer, user_id integer, avg numeric, avg_1 numeric, worker_column_5 integer, worker_column_6 integer, worker_column_7 numeric) ORDER BY (avg(avg) OVER (PARTITION BY value_2, worker_column_5, worker_column_6)) DESC, user_id DESC, value_2 DESC
2023-11-25 13:48:30.408 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.408 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.408 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.408 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 120 in 3515 microseconds
2023-11-25 13:48:30.408 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.408 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.408 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.408 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 122 in 8559 microseconds
2023-11-25 13:48:30.408 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.408 UTC [135455] DEBUG:  00000: task execution (1) for placement (852) on anchor shard (1400256) finished in 454 microseconds on worker node localhost:57637
2023-11-25 13:48:30.408 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.409 UTC [135455] DEBUG:  00000: task execution (2) for placement (853) on anchor shard (1400257) finished in 552 microseconds on worker node localhost:57638
2023-11-25 13:48:30.409 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.409 UTC [135455] DEBUG:  00000: task execution (3) for placement (854) on anchor shard (1400258) finished in 647 microseconds on worker node localhost:57637
2023-11-25 13:48:30.409 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.409 UTC [135455] DEBUG:  00000: task execution (4) for placement (855) on anchor shard (1400259) finished in 420 microseconds on worker node localhost:57638
2023-11-25 13:48:30.409 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.412 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 121 in 3608 microseconds
2023-11-25 13:48:30.412 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.413 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 123 in 4658 microseconds
2023-11-25 13:48:30.413 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.413 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 120: 2 to node localhost:57637
2023-11-25 13:48:30.413 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.413 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 121: 0 to node localhost:57637
2023-11-25 13:48:30.413 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.413 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 122: 2 to node localhost:57638
2023-11-25 13:48:30.413 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.413 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 123: 0 to node localhost:57638
2023-11-25 13:48:30.413 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.414 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.414 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.414 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.414 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.414 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.414 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.414 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.414 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.414 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.414 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.414 UTC [135455] DEBUG:  00000: generated sql query for task 1
2023-11-25 13:48:30.414 UTC [135455] DETAIL:  query string: "SELECT user_id, avg(user_id) AS sum FROM public.users_table_1400256 users_table WHERE true GROUP BY user_id"
2023-11-25 13:48:30.414 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.414 UTC [135455] DEBUG:  00000: generated sql query for task 2
2023-11-25 13:48:30.414 UTC [135455] DETAIL:  query string: "SELECT user_id, avg(user_id) AS sum FROM public.users_table_1400257 users_table WHERE true GROUP BY user_id"
2023-11-25 13:48:30.414 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.414 UTC [135455] DEBUG:  00000: generated sql query for task 3
2023-11-25 13:48:30.414 UTC [135455] DETAIL:  query string: "SELECT user_id, avg(user_id) AS sum FROM public.users_table_1400258 users_table WHERE true GROUP BY user_id"
2023-11-25 13:48:30.414 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.414 UTC [135455] DEBUG:  00000: generated sql query for task 4
2023-11-25 13:48:30.414 UTC [135455] DETAIL:  query string: "SELECT user_id, avg(user_id) AS sum FROM public.users_table_1400259 users_table WHERE true GROUP BY user_id"
2023-11-25 13:48:30.414 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.414 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.414 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.414 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.414 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.414 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.414 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.414 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.414 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.414 UTC [135455] DEBUG:  00000: combine query: SELECT user_id, sum(sum) OVER () AS sum FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), '(i 1)'::cstring(0)) remote_scan(user_id integer, sum numeric) ORDER BY user_id LIMIT '10'::bigint
2023-11-25 13:48:30.414 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.414 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.414 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.414 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 124 in 3515 microseconds
2023-11-25 13:48:30.414 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.414 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.414 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.415 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 126 in 8559 microseconds
2023-11-25 13:48:30.415 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.415 UTC [135455] DEBUG:  00000: task execution (1) for placement (852) on anchor shard (1400256) finished in 859 microseconds on worker node localhost:57637
2023-11-25 13:48:30.415 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.416 UTC [135455] DEBUG:  00000: task execution (2) for placement (853) on anchor shard (1400257) finished in 936 microseconds on worker node localhost:57638
2023-11-25 13:48:30.416 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.416 UTC [135455] DEBUG:  00000: task execution (3) for placement (854) on anchor shard (1400258) finished in 303 microseconds on worker node localhost:57637
2023-11-25 13:48:30.416 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.416 UTC [135455] DEBUG:  00000: task execution (4) for placement (855) on anchor shard (1400259) finished in 341 microseconds on worker node localhost:57638
2023-11-25 13:48:30.416 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.419 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 125 in 4404 microseconds
2023-11-25 13:48:30.419 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.420 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 127 in 5241 microseconds
2023-11-25 13:48:30.420 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.420 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 124: 2 to node localhost:57637
2023-11-25 13:48:30.420 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.420 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 125: 0 to node localhost:57637
2023-11-25 13:48:30.420 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.420 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 126: 2 to node localhost:57638
2023-11-25 13:48:30.420 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.420 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 127: 0 to node localhost:57638
2023-11-25 13:48:30.420 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.420 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.420 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.420 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.420 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.420 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.420 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.420 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.420 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.420 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.420 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.420 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, (1 OPERATOR(pg_catalog.+) sum(worker_column_2)), ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_3) OVER (PARTITION BY worker_column_1)), worker_column_3 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 13:48:30.420 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.420 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, (1 OPERATOR(pg_catalog.+) sum(worker_column_2)), ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_3) OVER (PARTITION BY worker_column_1)), worker_column_3 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 13:48:30.420 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.421 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, (1 OPERATOR(pg_catalog.+) sum(worker_column_2)), ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_3) OVER (PARTITION BY worker_column_1)), worker_column_3 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 13:48:30.421 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.421 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, (1 OPERATOR(pg_catalog.+) sum(worker_column_2)), ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_3) OVER (PARTITION BY worker_column_1)), worker_column_3 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 13:48:30.421 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.421 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.421 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.421 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.421 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.421 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.421 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.421 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.421 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.421 UTC [135455] DEBUG:  00000: combine query: SELECT user_id, "?column?", "?column?_1" AS "?column?" FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, "?column?" bigint, "?column?_1" numeric, worker_column_4 integer) ORDER BY user_id, worker_column_4
2023-11-25 13:48:30.421 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.421 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.421 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.421 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 128 in 3515 microseconds
2023-11-25 13:48:30.421 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.421 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.421 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.422 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 130 in 8559 microseconds
2023-11-25 13:48:30.422 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.422 UTC [135455] DEBUG:  00000: task execution (1) for placement (852) on anchor shard (1400256) finished in 1253 microseconds on worker node localhost:57637
2023-11-25 13:48:30.422 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.423 UTC [135455] DEBUG:  00000: task execution (2) for placement (853) on anchor shard (1400257) finished in 805 microseconds on worker node localhost:57638
2023-11-25 13:48:30.423 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.423 UTC [135455] DEBUG:  00000: task execution (3) for placement (854) on anchor shard (1400258) finished in 843 microseconds on worker node localhost:57637
2023-11-25 13:48:30.423 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.423 UTC [135455] DEBUG:  00000: task execution (4) for placement (855) on anchor shard (1400259) finished in 445 microseconds on worker node localhost:57638
2023-11-25 13:48:30.423 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.425 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 131 in 3116 microseconds
2023-11-25 13:48:30.425 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.426 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 129 in 5161 microseconds
2023-11-25 13:48:30.426 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.426 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 128: 2 to node localhost:57637
2023-11-25 13:48:30.426 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.426 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 129: 0 to node localhost:57637
2023-11-25 13:48:30.426 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.426 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 130: 2 to node localhost:57638
2023-11-25 13:48:30.426 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.426 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 131: 0 to node localhost:57638
2023-11-25 13:48:30.426 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.427 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.427 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.427 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.427 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.427 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.427 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.427 UTC [135455] DEBUG:  00000: push down of limit count: 5
2023-11-25 13:48:30.427 UTC [135455] LOCATION:  WorkerLimitCount, multi_logical_optimizer.c:4778
2023-11-25 13:48:30.427 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.427 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.427 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.427 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.428 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, (1 OPERATOR(pg_catalog.+) sum(worker_column_2)), ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_3) OVER (PARTITION BY worker_column_1)), worker_column_3 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY (1 OPERATOR(pg_catalog.+) sum(worker_column_2)) DESC, worker_column_1 LIMIT '5'::bigint
2023-11-25 13:48:30.428 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.428 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, (1 OPERATOR(pg_catalog.+) sum(worker_column_2)), ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_3) OVER (PARTITION BY worker_column_1)), worker_column_3 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY (1 OPERATOR(pg_catalog.+) sum(worker_column_2)) DESC, worker_column_1 LIMIT '5'::bigint
2023-11-25 13:48:30.428 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.429 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, (1 OPERATOR(pg_catalog.+) sum(worker_column_2)), ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_3) OVER (PARTITION BY worker_column_1)), worker_column_3 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY (1 OPERATOR(pg_catalog.+) sum(worker_column_2)) DESC, worker_column_1 LIMIT '5'::bigint
2023-11-25 13:48:30.429 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.429 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, (1 OPERATOR(pg_catalog.+) sum(worker_column_2)), ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_3) OVER (PARTITION BY worker_column_1)), worker_column_3 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY (1 OPERATOR(pg_catalog.+) sum(worker_column_2)) DESC, worker_column_1 LIMIT '5'::bigint
2023-11-25 13:48:30.429 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.429 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.429 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.429 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.429 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.429 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.429 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.429 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.429 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.429 UTC [135455] DEBUG:  00000: combine query: SELECT user_id, "?column?", "?column?_1" AS "?column?" FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, "?column?" bigint, "?column?_1" numeric, worker_column_4 integer) ORDER BY "?column?" DESC, user_id LIMIT '5'::bigint
2023-11-25 13:48:30.429 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.429 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.429 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.430 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 132 in 3515 microseconds
2023-11-25 13:48:30.430 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.430 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.430 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.431 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 134 in 8559 microseconds
2023-11-25 13:48:30.431 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.431 UTC [135455] DEBUG:  00000: task execution (1) for placement (852) on anchor shard (1400256) finished in 927 microseconds on worker node localhost:57637
2023-11-25 13:48:30.431 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.431 UTC [135455] DEBUG:  00000: task execution (3) for placement (854) on anchor shard (1400258) finished in 458 microseconds on worker node localhost:57637
2023-11-25 13:48:30.431 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.431 UTC [135455] DEBUG:  00000: task execution (2) for placement (853) on anchor shard (1400257) finished in 573 microseconds on worker node localhost:57638
2023-11-25 13:48:30.431 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.432 UTC [135455] DEBUG:  00000: task execution (4) for placement (855) on anchor shard (1400259) finished in 420 microseconds on worker node localhost:57638
2023-11-25 13:48:30.432 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.435 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 133 in 5119 microseconds
2023-11-25 13:48:30.435 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.438 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 135 in 7298 microseconds
2023-11-25 13:48:30.438 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.438 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 132: 2 to node localhost:57637
2023-11-25 13:48:30.438 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.438 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 133: 0 to node localhost:57637
2023-11-25 13:48:30.438 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.438 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 134: 2 to node localhost:57638
2023-11-25 13:48:30.438 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.438 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 135: 0 to node localhost:57638
2023-11-25 13:48:30.438 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.438 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.438 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.438 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.438 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.438 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.438 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.439 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.439 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.439 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.439 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.439 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY worker_column_3) AS rank, worker_column_3 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 13:48:30.439 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.439 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY worker_column_3) AS rank, worker_column_3 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 13:48:30.439 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.439 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY worker_column_3) AS rank, worker_column_3 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 13:48:30.439 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.439 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY worker_column_3) AS rank, worker_column_3 AS worker_column_4 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 13:48:30.439 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.439 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.439 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.439 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.439 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.439 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.439 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.439 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.439 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.439 UTC [135455] DEBUG:  00000: combine query: SELECT user_id, avg, rank FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, avg numeric, rank bigint, worker_column_4 integer) ORDER BY user_id, worker_column_4 DESC
2023-11-25 13:48:30.439 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.439 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.439 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.439 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 136 in 3515 microseconds
2023-11-25 13:48:30.439 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.439 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.439 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.439 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 138 in 8559 microseconds
2023-11-25 13:48:30.439 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.442 UTC [135455] DEBUG:  00000: task execution (1) for placement (852) on anchor shard (1400256) finished in 2457 microseconds on worker node localhost:57637
2023-11-25 13:48:30.442 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.442 UTC [135455] DEBUG:  00000: task execution (2) for placement (853) on anchor shard (1400257) finished in 2375 microseconds on worker node localhost:57638
2023-11-25 13:48:30.442 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.442 UTC [135455] DEBUG:  00000: task execution (3) for placement (854) on anchor shard (1400258) finished in 893 microseconds on worker node localhost:57637
2023-11-25 13:48:30.442 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.443 UTC [135455] DEBUG:  00000: task execution (4) for placement (855) on anchor shard (1400259) finished in 875 microseconds on worker node localhost:57638
2023-11-25 13:48:30.443 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.449 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 137 in 9705 microseconds
2023-11-25 13:48:30.449 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.449 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 139 in 9627 microseconds
2023-11-25 13:48:30.449 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.449 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 136: 2 to node localhost:57637
2023-11-25 13:48:30.449 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.449 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 137: 0 to node localhost:57637
2023-11-25 13:48:30.449 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.449 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 138: 2 to node localhost:57638
2023-11-25 13:48:30.449 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.449 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 139: 0 to node localhost:57638
2023-11-25 13:48:30.449 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.450 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.450 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.450 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.450 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.450 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.450 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.450 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.450 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.450 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.450 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.450 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 13:48:30.450 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.450 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 13:48:30.450 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.450 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 13:48:30.450 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.450 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 13:48:30.450 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.450 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.450 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.450 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.450 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.450 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.450 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.450 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.450 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.450 UTC [135455] DEBUG:  00000: combine query: SELECT user_id, avg, rank FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, avg numeric, rank bigint, worker_column_4 integer, worker_column_5 numeric) ORDER BY user_id, avg DESC
2023-11-25 13:48:30.450 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.450 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.450 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.450 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 140 in 3515 microseconds
2023-11-25 13:48:30.450 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.451 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.451 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.451 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 142 in 8559 microseconds
2023-11-25 13:48:30.451 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.453 UTC [135455] DEBUG:  00000: task execution (1) for placement (852) on anchor shard (1400256) finished in 2086 microseconds on worker node localhost:57637
2023-11-25 13:48:30.453 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.453 UTC [135455] DEBUG:  00000: task execution (2) for placement (853) on anchor shard (1400257) finished in 1986 microseconds on worker node localhost:57638
2023-11-25 13:48:30.453 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.454 UTC [135455] DEBUG:  00000: task execution (3) for placement (854) on anchor shard (1400258) finished in 1394 microseconds on worker node localhost:57637
2023-11-25 13:48:30.454 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.454 UTC [135455] DEBUG:  00000: task execution (4) for placement (855) on anchor shard (1400259) finished in 1349 microseconds on worker node localhost:57638
2023-11-25 13:48:30.454 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.456 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 143 in 5507 microseconds
2023-11-25 13:48:30.456 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.460 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 141 in 9248 microseconds
2023-11-25 13:48:30.460 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.460 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 140: 2 to node localhost:57637
2023-11-25 13:48:30.460 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.460 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 141: 0 to node localhost:57637
2023-11-25 13:48:30.460 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.460 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 142: 2 to node localhost:57638
2023-11-25 13:48:30.460 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.460 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 143: 0 to node localhost:57638
2023-11-25 13:48:30.460 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.461 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.461 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.461 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.461 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.461 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.461 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.461 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.461 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.461 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.461 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.461 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 13:48:30.461 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.461 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 13:48:30.461 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.461 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 13:48:30.461 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.461 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 13:48:30.461 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.461 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.461 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.461 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.461 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.461 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.461 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.461 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.461 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.461 UTC [135455] DEBUG:  00000: combine query: SELECT user_id, avg, rank FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, avg numeric, rank bigint, worker_column_4 integer, worker_column_5 numeric) ORDER BY user_id, avg DESC
2023-11-25 13:48:30.461 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.463 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.463 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.463 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.463 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.463 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.463 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.463 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.463 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.463 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.463 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.463 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 13:48:30.463 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.463 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 13:48:30.463 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.463 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 13:48:30.463 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.463 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3
2023-11-25 13:48:30.463 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.463 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.463 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.463 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.463 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.463 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.463 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.463 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.463 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.463 UTC [135455] DEBUG:  00000: combine query: SELECT user_id, avg, rank FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, avg numeric, rank bigint, worker_column_4 integer, worker_column_5 numeric) ORDER BY user_id, avg DESC
2023-11-25 13:48:30.463 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.463 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.463 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.463 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 144 in 3515 microseconds
2023-11-25 13:48:30.463 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.464 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.464 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.464 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 146 in 8559 microseconds
2023-11-25 13:48:30.464 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.465 UTC [135455] DEBUG:  00000: task execution (1) for placement (852) on anchor shard (1400256) finished in 1081 microseconds on worker node localhost:57637
2023-11-25 13:48:30.465 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.465 UTC [135455] DEBUG:  00000: task execution (2) for placement (853) on anchor shard (1400257) finished in 994 microseconds on worker node localhost:57638
2023-11-25 13:48:30.465 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.466 UTC [135455] DEBUG:  00000: task execution (3) for placement (854) on anchor shard (1400258) finished in 1000 microseconds on worker node localhost:57637
2023-11-25 13:48:30.466 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.466 UTC [135455] DEBUG:  00000: task execution (4) for placement (855) on anchor shard (1400259) finished in 978 microseconds on worker node localhost:57638
2023-11-25 13:48:30.466 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.472 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 145 in 9030 microseconds
2023-11-25 13:48:30.472 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.480 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 147 in 16686 microseconds
2023-11-25 13:48:30.480 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.480 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 144: 2 to node localhost:57637
2023-11-25 13:48:30.480 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.480 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 145: 0 to node localhost:57637
2023-11-25 13:48:30.480 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.480 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 146: 2 to node localhost:57638
2023-11-25 13:48:30.480 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.480 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 147: 0 to node localhost:57638
2023-11-25 13:48:30.480 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.481 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.481 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.481 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.481 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.481 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.481 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.481 UTC [135455] DEBUG:  00000: push down of limit count: 5
2023-11-25 13:48:30.481 UTC [135455] LOCATION:  WorkerLimitCount, multi_logical_optimizer.c:4778
2023-11-25 13:48:30.481 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.481 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.481 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.481 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.481 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY worker_column_1, (avg(worker_column_2)) DESC LIMIT '5'::bigint
2023-11-25 13:48:30.481 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.481 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY worker_column_1, (avg(worker_column_2)) DESC LIMIT '5'::bigint
2023-11-25 13:48:30.481 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.482 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY worker_column_1, (avg(worker_column_2)) DESC LIMIT '5'::bigint
2023-11-25 13:48:30.482 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.482 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY worker_column_1, (avg(worker_column_2)) DESC LIMIT '5'::bigint
2023-11-25 13:48:30.482 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.482 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.482 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.482 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.482 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.482 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.482 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.482 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.482 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.482 UTC [135455] DEBUG:  00000: combine query: SELECT user_id, avg, rank FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, avg numeric, rank bigint, worker_column_4 integer, worker_column_5 numeric) ORDER BY user_id, avg DESC LIMIT '5'::bigint
2023-11-25 13:48:30.482 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.484 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.484 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.484 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.484 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.484 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.484 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.484 UTC [135455] DEBUG:  00000: push down of limit count: 5
2023-11-25 13:48:30.484 UTC [135455] LOCATION:  WorkerLimitCount, multi_logical_optimizer.c:4778
2023-11-25 13:48:30.484 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.484 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.484 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.484 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.484 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY worker_column_1, (avg(worker_column_2)) DESC LIMIT '5'::bigint
2023-11-25 13:48:30.484 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.484 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY worker_column_1, (avg(worker_column_2)) DESC LIMIT '5'::bigint
2023-11-25 13:48:30.484 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.484 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY worker_column_1, (avg(worker_column_2)) DESC LIMIT '5'::bigint
2023-11-25 13:48:30.484 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.484 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2)))) AS rank, worker_column_3 AS worker_column_4, ((1)::numeric OPERATOR(pg_catalog./) ((1)::numeric OPERATOR(pg_catalog.+) avg(worker_column_2))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY worker_column_1, (avg(worker_column_2)) DESC LIMIT '5'::bigint
2023-11-25 13:48:30.484 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.484 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.484 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.484 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.484 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.484 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.484 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.484 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.484 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.484 UTC [135455] DEBUG:  00000: combine query: SELECT user_id, avg, rank FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, avg numeric, rank bigint, worker_column_4 integer, worker_column_5 numeric) ORDER BY user_id, avg DESC LIMIT '5'::bigint
2023-11-25 13:48:30.484 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.486 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.486 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.486 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.486 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.486 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.486 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.486 UTC [135455] DEBUG:  00000: push down of limit count: 5
2023-11-25 13:48:30.486 UTC [135455] LOCATION:  WorkerLimitCount, multi_logical_optimizer.c:4778
2023-11-25 13:48:30.486 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.486 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.486 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.486 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.486 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY (1 OPERATOR(pg_catalog./) (1 OPERATOR(pg_catalog.+) sum(worker_column_3)))) AS rank, worker_column_3 AS worker_column_4, (1 OPERATOR(pg_catalog./) (1 OPERATOR(pg_catalog.+) sum(worker_column_3))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY worker_column_1, (avg(worker_column_2)) DESC LIMIT '5'::bigint
2023-11-25 13:48:30.486 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.486 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY (1 OPERATOR(pg_catalog./) (1 OPERATOR(pg_catalog.+) sum(worker_column_3)))) AS rank, worker_column_3 AS worker_column_4, (1 OPERATOR(pg_catalog./) (1 OPERATOR(pg_catalog.+) sum(worker_column_3))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY worker_column_1, (avg(worker_column_2)) DESC LIMIT '5'::bigint
2023-11-25 13:48:30.486 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.487 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY (1 OPERATOR(pg_catalog./) (1 OPERATOR(pg_catalog.+) sum(worker_column_3)))) AS rank, worker_column_3 AS worker_column_4, (1 OPERATOR(pg_catalog./) (1 OPERATOR(pg_catalog.+) sum(worker_column_3))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY worker_column_1, (avg(worker_column_2)) DESC LIMIT '5'::bigint
2023-11-25 13:48:30.487 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.487 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY (1 OPERATOR(pg_catalog./) (1 OPERATOR(pg_catalog.+) sum(worker_column_3)))) AS rank, worker_column_3 AS worker_column_4, (1 OPERATOR(pg_catalog./) (1 OPERATOR(pg_catalog.+) sum(worker_column_3))) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY worker_column_1, (avg(worker_column_2)) DESC LIMIT '5'::bigint
2023-11-25 13:48:30.487 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.487 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.487 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.487 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.487 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.487 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.487 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.487 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.487 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.487 UTC [135455] DEBUG:  00000: combine query: SELECT user_id, avg, rank FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, avg numeric, rank bigint, worker_column_4 integer, worker_column_5 bigint) ORDER BY user_id, avg DESC LIMIT '5'::bigint
2023-11-25 13:48:30.487 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.494 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.494 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.494 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.494 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.494 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.494 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.494 UTC [135455] DEBUG:  00000: push down of limit count: 5
2023-11-25 13:48:30.494 UTC [135455] LOCATION:  WorkerLimitCount, multi_logical_optimizer.c:4778
2023-11-25 13:48:30.494 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.494 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.494 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.494 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.494 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY (sum(worker_column_3))) AS rank, worker_column_3 AS worker_column_4, sum(worker_column_3) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400256 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY worker_column_1, (avg(worker_column_2)) DESC LIMIT '5'::bigint
2023-11-25 13:48:30.494 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.494 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY (sum(worker_column_3))) AS rank, worker_column_3 AS worker_column_4, sum(worker_column_3) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400257 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY worker_column_1, (avg(worker_column_2)) DESC LIMIT '5'::bigint
2023-11-25 13:48:30.494 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.494 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY (sum(worker_column_3))) AS rank, worker_column_3 AS worker_column_4, sum(worker_column_3) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400258 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY worker_column_1, (avg(worker_column_2)) DESC LIMIT '5'::bigint
2023-11-25 13:48:30.494 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.494 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, avg(worker_column_2) AS avg, rank() OVER (PARTITION BY worker_column_1 ORDER BY (sum(worker_column_3))) AS rank, worker_column_3 AS worker_column_4, sum(worker_column_3) AS worker_column_5 FROM (SELECT users_table.user_id AS worker_column_1, users_table.value_1 AS worker_column_2, users_table.value_2 AS worker_column_3 FROM public.users_table_1400259 users_table) worker_subquery GROUP BY worker_column_1, worker_column_3 ORDER BY worker_column_1, (avg(worker_column_2)) DESC LIMIT '5'::bigint
2023-11-25 13:48:30.494 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.494 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.494 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.494 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.494 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.494 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.494 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.494 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.494 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.494 UTC [135455] DEBUG:  00000: combine query: SELECT user_id, avg, rank FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, avg numeric, rank bigint, worker_column_4 integer, worker_column_5 bigint) ORDER BY user_id, avg DESC LIMIT '5'::bigint
2023-11-25 13:48:30.494 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.500 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.500 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.500 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.500 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.500 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.500 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.500 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.500 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.500 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.500 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.500 UTC [135455] DEBUG:  00000: generated sql query for task 1
2023-11-25 13:48:30.500 UTC [135455] DETAIL:  query string: "SELECT user_id, count(value_1) AS count, stddev(value_1) AS stddev, user_id AS count, random() AS worker_column_5 FROM public.users_table_1400256 users_table WHERE true GROUP BY user_id HAVING (avg(value_1) OPERATOR(pg_catalog.>) '2'::numeric)"
2023-11-25 13:48:30.500 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.500 UTC [135455] DEBUG:  00000: generated sql query for task 2
2023-11-25 13:48:30.500 UTC [135455] DETAIL:  query string: "SELECT user_id, count(value_1) AS count, stddev(value_1) AS stddev, user_id AS count, random() AS worker_column_5 FROM public.users_table_1400257 users_table WHERE true GROUP BY user_id HAVING (avg(value_1) OPERATOR(pg_catalog.>) '2'::numeric)"
2023-11-25 13:48:30.500 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.500 UTC [135455] DEBUG:  00000: generated sql query for task 3
2023-11-25 13:48:30.500 UTC [135455] DETAIL:  query string: "SELECT user_id, count(value_1) AS count, stddev(value_1) AS stddev, user_id AS count, random() AS worker_column_5 FROM public.users_table_1400258 users_table WHERE true GROUP BY user_id HAVING (avg(value_1) OPERATOR(pg_catalog.>) '2'::numeric)"
2023-11-25 13:48:30.500 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.500 UTC [135455] DEBUG:  00000: generated sql query for task 4
2023-11-25 13:48:30.500 UTC [135455] DETAIL:  query string: "SELECT user_id, count(value_1) AS count, stddev(value_1) AS stddev, user_id AS count, random() AS worker_column_5 FROM public.users_table_1400259 users_table WHERE true GROUP BY user_id HAVING (avg(value_1) OPERATOR(pg_catalog.>) '2'::numeric)"
2023-11-25 13:48:30.500 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.500 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.500 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.500 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.500 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.500 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.500 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.500 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.500 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.500 UTC [135455] DEBUG:  00000: combine query: SELECT user_id, count, stddev, count(count_1) OVER (PARTITION BY worker_column_5) AS count FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), '(i 1)'::cstring(0)) remote_scan(user_id integer, count bigint, stddev numeric, count_1 integer, worker_column_5 double precision) LIMIT '1'::bigint
2023-11-25 13:48:30.500 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.503 UTC [135455] DEBUG:  00000: CTE cte is going to be inlined via distributed planning
2023-11-25 13:48:30.503 UTC [135455] LOCATION:  InlineCTEsInQueryTree, cte_inline.c:117
2023-11-25 13:48:30.503 UTC [135455] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 13:48:30.503 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.503 UTC [135455] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 13:48:30.503 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.503 UTC [135455] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 13:48:30.503 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.503 UTC [135455] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 13:48:30.503 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.503 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.503 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.503 UTC [135455] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 13:48:30.503 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.503 UTC [135455] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 13:48:30.503 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.503 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.503 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.503 UTC [135455] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 13:48:30.503 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.503 UTC [135455] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 13:48:30.503 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.503 UTC [135455] DEBUG:  00000: generated sql query for task 1
2023-11-25 13:48:30.503 UTC [135455] DETAIL:  query string: "SELECT uref.id AS user_id, events_table.value_2, count(*) AS c FROM (public.events_table_1400260 events_table JOIN public.users_ref_test_table_1400284 uref ON ((uref.id OPERATOR(pg_catalog.=) events_table.user_id))) WHERE true GROUP BY uref.id, events_table.value_2"
2023-11-25 13:48:30.503 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.503 UTC [135455] DEBUG:  00000: generated sql query for task 2
2023-11-25 13:48:30.503 UTC [135455] DETAIL:  query string: "SELECT uref.id AS user_id, events_table.value_2, count(*) AS c FROM (public.events_table_1400261 events_table JOIN public.users_ref_test_table_1400284 uref ON ((uref.id OPERATOR(pg_catalog.=) events_table.user_id))) WHERE true GROUP BY uref.id, events_table.value_2"
2023-11-25 13:48:30.503 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.503 UTC [135455] DEBUG:  00000: generated sql query for task 3
2023-11-25 13:48:30.503 UTC [135455] DETAIL:  query string: "SELECT uref.id AS user_id, events_table.value_2, count(*) AS c FROM (public.events_table_1400262 events_table JOIN public.users_ref_test_table_1400284 uref ON ((uref.id OPERATOR(pg_catalog.=) events_table.user_id))) WHERE true GROUP BY uref.id, events_table.value_2"
2023-11-25 13:48:30.503 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.503 UTC [135455] DEBUG:  00000: generated sql query for task 4
2023-11-25 13:48:30.503 UTC [135455] DETAIL:  query string: "SELECT uref.id AS user_id, events_table.value_2, count(*) AS c FROM (public.events_table_1400263 events_table JOIN public.users_ref_test_table_1400284 uref ON ((uref.id OPERATOR(pg_catalog.=) events_table.user_id))) WHERE true GROUP BY uref.id, events_table.value_2"
2023-11-25 13:48:30.503 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.503 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.503 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.503 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.503 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.503 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.503 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.503 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.503 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.503 UTC [135455] DEBUG:  00000: combine query: SELECT user_id, value_2, COALESCE((pg_catalog.sum(c))::bigint, '0'::bigint) AS c FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), '(i 1 2)'::cstring(0)) remote_scan(user_id integer, value_2 integer, c bigint) GROUP BY user_id, value_2
2023-11-25 13:48:30.503 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.504 UTC [135455] DEBUG:  00000: generating subplan 46_1 for subquery SELECT uref.id AS user_id, events_table.value_2, count(*) AS c FROM (public.events_table JOIN public.users_ref_test_table uref ON ((uref.id OPERATOR(pg_catalog.=) events_table.user_id))) GROUP BY uref.id, events_table.value_2
2023-11-25 13:48:30.504 UTC [135455] LOCATION:  RecursivelyPlanSubquery, recursive_planning.c:1551
2023-11-25 13:48:30.504 UTC [135455] DEBUG:  00000: Plan 46 query after replacing subqueries and CTEs: SELECT DISTINCT cte.value_2, cte.c, sum(cte.value_2) OVER (PARTITION BY cte.c) AS sum FROM ((SELECT intermediate_result.user_id, intermediate_result.value_2, intermediate_result.c FROM read_intermediate_result('46_1'::text, 'binary'::citus_copy_format) intermediate_result(user_id integer, value_2 integer, c bigint)) cte JOIN public.events_table et ON (((et.value_2 OPERATOR(pg_catalog.=) cte.value_2) AND (et.value_2 OPERATOR(pg_catalog.=) cte.c)))) ORDER BY cte.value_2
2023-11-25 13:48:30.504 UTC [135455] LOCATION:  GenerateSubplansForSubqueriesAndCTEs, recursive_planning.c:248
2023-11-25 13:48:30.504 UTC [135455] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 13:48:30.504 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.504 UTC [135455] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 13:48:30.504 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.504 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.504 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.504 UTC [135455] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 13:48:30.504 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.504 UTC [135455] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 13:48:30.504 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.504 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS value_2, worker_column_2 AS c, worker_column_1 AS sum FROM (SELECT cte.value_2 AS worker_column_1, cte.c AS worker_column_2 FROM ((SELECT intermediate_result.user_id, intermediate_result.value_2, intermediate_result.c FROM read_intermediate_result('46_1'::text, 'binary'::citus_copy_format) intermediate_result(user_id integer, value_2 integer, c bigint)) cte JOIN public.events_table_1400260 et ON (((et.value_2 OPERATOR(pg_catalog.=) cte.value_2) AND (et.value_2 OPERATOR(pg_catalog.=) cte.c))))) worker_subquery
2023-11-25 13:48:30.504 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.504 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS value_2, worker_column_2 AS c, worker_column_1 AS sum FROM (SELECT cte.value_2 AS worker_column_1, cte.c AS worker_column_2 FROM ((SELECT intermediate_result.user_id, intermediate_result.value_2, intermediate_result.c FROM read_intermediate_result('46_1'::text, 'binary'::citus_copy_format) intermediate_result(user_id integer, value_2 integer, c bigint)) cte JOIN public.events_table_1400261 et ON (((et.value_2 OPERATOR(pg_catalog.=) cte.value_2) AND (et.value_2 OPERATOR(pg_catalog.=) cte.c))))) worker_subquery
2023-11-25 13:48:30.504 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.504 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS value_2, worker_column_2 AS c, worker_column_1 AS sum FROM (SELECT cte.value_2 AS worker_column_1, cte.c AS worker_column_2 FROM ((SELECT intermediate_result.user_id, intermediate_result.value_2, intermediate_result.c FROM read_intermediate_result('46_1'::text, 'binary'::citus_copy_format) intermediate_result(user_id integer, value_2 integer, c bigint)) cte JOIN public.events_table_1400262 et ON (((et.value_2 OPERATOR(pg_catalog.=) cte.value_2) AND (et.value_2 OPERATOR(pg_catalog.=) cte.c))))) worker_subquery
2023-11-25 13:48:30.504 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.504 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS value_2, worker_column_2 AS c, worker_column_1 AS sum FROM (SELECT cte.value_2 AS worker_column_1, cte.c AS worker_column_2 FROM ((SELECT intermediate_result.user_id, intermediate_result.value_2, intermediate_result.c FROM read_intermediate_result('46_1'::text, 'binary'::citus_copy_format) intermediate_result(user_id integer, value_2 integer, c bigint)) cte JOIN public.events_table_1400263 et ON (((et.value_2 OPERATOR(pg_catalog.=) cte.value_2) AND (et.value_2 OPERATOR(pg_catalog.=) cte.c))))) worker_subquery
2023-11-25 13:48:30.504 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.505 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.505 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.505 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.505 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.505 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.505 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.505 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.505 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.505 UTC [135455] DEBUG:  00000: combine query: SELECT DISTINCT value_2, c, sum(sum) OVER (PARTITION BY c) AS sum FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(value_2 integer, c bigint, sum integer) ORDER BY value_2
2023-11-25 13:48:30.505 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.505 UTC [135455] DEBUG:  00000: Subplan 46_1 is used in 46
2023-11-25 13:48:30.505 UTC [135455] LOCATION:  RecordSubplanExecutionsOnNodes, intermediate_result_pruning.c:193
2023-11-25 13:48:30.505 UTC [135455] DEBUG:  00000: Subplan 46_1 will be sent to localhost:57637
2023-11-25 13:48:30.505 UTC [135455] LOCATION:  LogIntermediateResultMulticastSummary, intermediate_result_pruning.c:422
2023-11-25 13:48:30.505 UTC [135455] DEBUG:  00000: Subplan 46_1 will be sent to localhost:57638
2023-11-25 13:48:30.505 UTC [135455] LOCATION:  LogIntermediateResultMulticastSummary, intermediate_result_pruning.c:422
2023-11-25 13:48:30.505 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.505 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.505 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 148 in 3515 microseconds
2023-11-25 13:48:30.505 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.505 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.505 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.505 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 150 in 8559 microseconds
2023-11-25 13:48:30.505 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.507 UTC [135455] DEBUG:  00000: task execution (1) for placement (856) on anchor shard (1400260) finished in 1246 microseconds on worker node localhost:57637
2023-11-25 13:48:30.507 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.508 UTC [135455] DEBUG:  00000: task execution (2) for placement (857) on anchor shard (1400261) finished in 1022 microseconds on worker node localhost:57638
2023-11-25 13:48:30.508 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.508 UTC [135455] DEBUG:  00000: task execution (4) for placement (859) on anchor shard (1400263) finished in 384 microseconds on worker node localhost:57638
2023-11-25 13:48:30.508 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.508 UTC [135455] DEBUG:  00000: task execution (3) for placement (858) on anchor shard (1400262) finished in 1665 microseconds on worker node localhost:57637
2023-11-25 13:48:30.508 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.515 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 151 in 9402 microseconds
2023-11-25 13:48:30.515 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.516 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 149 in 10880 microseconds
2023-11-25 13:48:30.516 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.516 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 148: 2 to node localhost:57637
2023-11-25 13:48:30.516 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.516 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 149: 0 to node localhost:57637
2023-11-25 13:48:30.516 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.516 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 150: 2 to node localhost:57638
2023-11-25 13:48:30.516 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.516 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 151: 0 to node localhost:57638
2023-11-25 13:48:30.516 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.517 UTC [135455] DEBUG:  00000: Session 152 (localhost:57637) has an assigned task
2023-11-25 13:48:30.517 UTC [135455] LOCATION:  AssignTasksToConnectionsOrWorkerPool, adaptive_executor.c:1468
2023-11-25 13:48:30.517 UTC [135455] DEBUG:  00000: Session 153 (localhost:57638) has an assigned task
2023-11-25 13:48:30.517 UTC [135455] LOCATION:  AssignTasksToConnectionsOrWorkerPool, adaptive_executor.c:1468
2023-11-25 13:48:30.517 UTC [135455] DEBUG:  00000: Session 152 (localhost:57637) has an assigned task
2023-11-25 13:48:30.517 UTC [135455] LOCATION:  AssignTasksToConnectionsOrWorkerPool, adaptive_executor.c:1468
2023-11-25 13:48:30.517 UTC [135455] DEBUG:  00000: Session 153 (localhost:57638) has an assigned task
2023-11-25 13:48:30.517 UTC [135455] LOCATION:  AssignTasksToConnectionsOrWorkerPool, adaptive_executor.c:1468
2023-11-25 13:48:30.518 UTC [135455] DEBUG:  00000: task execution (1) for placement (856) on anchor shard (1400260) finished in 671 microseconds on worker node localhost:57637
2023-11-25 13:48:30.518 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.518 UTC [135455] DEBUG:  00000: task execution (2) for placement (857) on anchor shard (1400261) finished in 731 microseconds on worker node localhost:57638
2023-11-25 13:48:30.518 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.518 UTC [135455] DEBUG:  00000: task execution (4) for placement (859) on anchor shard (1400263) finished in 524 microseconds on worker node localhost:57638
2023-11-25 13:48:30.518 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.518 UTC [135455] DEBUG:  00000: task execution (3) for placement (858) on anchor shard (1400262) finished in 741 microseconds on worker node localhost:57637
2023-11-25 13:48:30.518 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.518 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 152: 2 to node localhost:57637
2023-11-25 13:48:30.518 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.518 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 153: 2 to node localhost:57638
2023-11-25 13:48:30.518 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.522 UTC [135455] DEBUG:  00000: FromList item is not empty
2023-11-25 13:48:30.522 UTC [135455] CONTEXT:  SQL statement "SELECT TRUE FROM public.daily_uniques LIMIT 1"
2023-11-25 13:48:30.522 UTC [135455] LOCATION:  TryToDelegateFunctionCall, function_call_delegation.c:196
2023-11-25 13:48:30.523 UTC [135455] DEBUG:  00000: opening 1 new connections to localhost:57637
2023-11-25 13:48:30.523 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.524 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 154 in 3515 microseconds
2023-11-25 13:48:30.524 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.524 UTC [135455] DEBUG:  00000: opening 1 new connections to localhost:57638
2023-11-25 13:48:30.524 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.524 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 155 in 8559 microseconds
2023-11-25 13:48:30.524 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.526 UTC [135455] DEBUG:  00000: task execution (1) for placement (1067) on anchor shard (360164) finished in 1872 microseconds on worker node localhost:57637
2023-11-25 13:48:30.526 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.526 UTC [135455] DEBUG:  00000: task execution (2) for placement (1068) on anchor shard (360164) finished in 1887 microseconds on worker node localhost:57638
2023-11-25 13:48:30.526 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.527 UTC [135455] DEBUG:  00000: task execution (3) for placement (1069) on anchor shard (360165) finished in 1293 microseconds on worker node localhost:57637
2023-11-25 13:48:30.527 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.527 UTC [135455] DEBUG:  00000: task execution (4) for placement (1070) on anchor shard (360165) finished in 1298 microseconds on worker node localhost:57638
2023-11-25 13:48:30.527 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.528 UTC [135455] DEBUG:  00000: task execution (5) for placement (1071) on anchor shard (360166) finished in 829 microseconds on worker node localhost:57637
2023-11-25 13:48:30.528 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.528 UTC [135455] DEBUG:  00000: task execution (6) for placement (1072) on anchor shard (360166) finished in 830 microseconds on worker node localhost:57638
2023-11-25 13:48:30.528 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.530 UTC [135455] DEBUG:  00000: task execution (8) for placement (1074) on anchor shard (360167) finished in 2034 microseconds on worker node localhost:57638
2023-11-25 13:48:30.530 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.530 UTC [135455] DEBUG:  00000: task execution (7) for placement (1073) on anchor shard (360167) finished in 2314 microseconds on worker node localhost:57637
2023-11-25 13:48:30.530 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.530 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 154: 4 to node localhost:57637
2023-11-25 13:48:30.530 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.530 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 155: 4 to node localhost:57638
2023-11-25 13:48:30.530 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.546 UTC [135455] DEBUG:  00000: no shard pruning constraints on daily_uniques found
2023-11-25 13:48:30.546 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.546 UTC [135455] DEBUG:  00000: shard count after pruning for daily_uniques: 4
2023-11-25 13:48:30.546 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.546 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.546 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.546 UTC [135455] DEBUG:  00000: push down of limit count: 10
2023-11-25 13:48:30.546 UTC [135455] LOCATION:  WorkerLimitCount, multi_logical_optimizer.c:4778
2023-11-25 13:48:30.546 UTC [135455] DEBUG:  00000: no shard pruning constraints on daily_uniques found
2023-11-25 13:48:30.546 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.546 UTC [135455] DEBUG:  00000: shard count after pruning for daily_uniques: 4
2023-11-25 13:48:30.546 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.546 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) AS commits, rank() OVER (PARTITION BY worker_column_1 ORDER BY (sum(worker_column_2)) DESC) AS rank FROM (SELECT daily_uniques.user_id AS worker_column_1, daily_uniques.value_2 AS worker_column_2 FROM public.daily_uniques_360164 daily_uniques) worker_subquery GROUP BY worker_column_1 HAVING (sum(worker_column_2) OPERATOR(pg_catalog.>) (0)::double precision) ORDER BY (sum(worker_column_2)) DESC LIMIT '10'::bigint
2023-11-25 13:48:30.546 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.546 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) AS commits, rank() OVER (PARTITION BY worker_column_1 ORDER BY (sum(worker_column_2)) DESC) AS rank FROM (SELECT daily_uniques.user_id AS worker_column_1, daily_uniques.value_2 AS worker_column_2 FROM public.daily_uniques_360165 daily_uniques) worker_subquery GROUP BY worker_column_1 HAVING (sum(worker_column_2) OPERATOR(pg_catalog.>) (0)::double precision) ORDER BY (sum(worker_column_2)) DESC LIMIT '10'::bigint
2023-11-25 13:48:30.546 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.546 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) AS commits, rank() OVER (PARTITION BY worker_column_1 ORDER BY (sum(worker_column_2)) DESC) AS rank FROM (SELECT daily_uniques.user_id AS worker_column_1, daily_uniques.value_2 AS worker_column_2 FROM public.daily_uniques_360166 daily_uniques) worker_subquery GROUP BY worker_column_1 HAVING (sum(worker_column_2) OPERATOR(pg_catalog.>) (0)::double precision) ORDER BY (sum(worker_column_2)) DESC LIMIT '10'::bigint
2023-11-25 13:48:30.546 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.547 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, sum(worker_column_2) AS commits, rank() OVER (PARTITION BY worker_column_1 ORDER BY (sum(worker_column_2)) DESC) AS rank FROM (SELECT daily_uniques.user_id AS worker_column_1, daily_uniques.value_2 AS worker_column_2 FROM public.daily_uniques_360167 daily_uniques) worker_subquery GROUP BY worker_column_1 HAVING (sum(worker_column_2) OPERATOR(pg_catalog.>) (0)::double precision) ORDER BY (sum(worker_column_2)) DESC LIMIT '10'::bigint
2023-11-25 13:48:30.547 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.547 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.547 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.547 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.547 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.547 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.547 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.547 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.547 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.547 UTC [135455] DEBUG:  00000: combine query: SELECT user_id, commits, rank FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id bigint, commits double precision, rank bigint) ORDER BY commits DESC LIMIT '10'::bigint
2023-11-25 13:48:30.547 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.548 UTC [135455] DEBUG:  00000: drop auto-cascades to type daily_uniques
2023-11-25 13:48:30.548 UTC [135455] LOCATION:  reportDependentObjects, dependency.c:1122
2023-11-25 13:48:30.548 UTC [135455] DEBUG:  00000: drop auto-cascades to type daily_uniques[]
2023-11-25 13:48:30.548 UTC [135455] LOCATION:  reportDependentObjects, dependency.c:1122
2023-11-25 13:48:30.548 UTC [135455] DEBUG:  00000: drop auto-cascades to trigger truncate_trigger_18431 on table daily_uniques
2023-11-25 13:48:30.548 UTC [135455] LOCATION:  reportDependentObjects, dependency.c:1122
2023-11-25 13:48:30.549 UTC [135455] DEBUG:  00000: EventTriggerInvoke 16675
2023-11-25 13:48:30.549 UTC [135455] LOCATION:  EventTriggerInvoke, event_trigger.c:900
2023-11-25 13:48:30.565 UTC [135455] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 13:48:30.565 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.565 UTC [135455] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 13:48:30.565 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.565 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.565 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.565 UTC [135455] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 13:48:30.565 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.565 UTC [135455] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 13:48:30.565 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.565 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.565 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.565 UTC [135455] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 13:48:30.565 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.565 UTC [135455] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 13:48:30.565 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.565 UTC [135455] DEBUG:  00000: generated sql query for task 1
2023-11-25 13:48:30.565 UTC [135455] DETAIL:  query string: "SELECT events_table.value_2, uref.k_no AS rnk, uref.id AS worker_column_3 FROM (public.events_table_1400260 events_table JOIN public.users_ref_test_table_1400284 uref ON ((uref.id OPERATOR(pg_catalog.=) events_table.user_id))) WHERE true"
2023-11-25 13:48:30.565 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.565 UTC [135455] DEBUG:  00000: generated sql query for task 2
2023-11-25 13:48:30.565 UTC [135455] DETAIL:  query string: "SELECT events_table.value_2, uref.k_no AS rnk, uref.id AS worker_column_3 FROM (public.events_table_1400261 events_table JOIN public.users_ref_test_table_1400284 uref ON ((uref.id OPERATOR(pg_catalog.=) events_table.user_id))) WHERE true"
2023-11-25 13:48:30.565 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.565 UTC [135455] DEBUG:  00000: generated sql query for task 3
2023-11-25 13:48:30.565 UTC [135455] DETAIL:  query string: "SELECT events_table.value_2, uref.k_no AS rnk, uref.id AS worker_column_3 FROM (public.events_table_1400262 events_table JOIN public.users_ref_test_table_1400284 uref ON ((uref.id OPERATOR(pg_catalog.=) events_table.user_id))) WHERE true"
2023-11-25 13:48:30.565 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.565 UTC [135455] DEBUG:  00000: generated sql query for task 4
2023-11-25 13:48:30.565 UTC [135455] DETAIL:  query string: "SELECT events_table.value_2, uref.k_no AS rnk, uref.id AS worker_column_3 FROM (public.events_table_1400263 events_table JOIN public.users_ref_test_table_1400284 uref ON ((uref.id OPERATOR(pg_catalog.=) events_table.user_id))) WHERE true"
2023-11-25 13:48:30.565 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.565 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.565 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.565 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.565 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.565 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.565 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.565 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.565 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.565 UTC [135455] DEBUG:  00000: combine query: SELECT value_2, sum(rnk) OVER (PARTITION BY worker_column_3) AS rnk FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), '(i 1 2)'::cstring(0)) remote_scan(value_2 integer, rnk integer, worker_column_3 integer)
2023-11-25 13:48:30.565 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.565 UTC [135455] DEBUG:  00000: generating subplan 49_1 for subquery SELECT events_table.value_2, sum(uref.k_no) OVER (PARTITION BY uref.id) AS rnk FROM (public.events_table JOIN public.users_ref_test_table uref ON ((uref.id OPERATOR(pg_catalog.=) events_table.user_id)))
2023-11-25 13:48:30.565 UTC [135455] LOCATION:  RecursivelyPlanSubquery, recursive_planning.c:1551
2023-11-25 13:48:30.565 UTC [135455] DEBUG:  00000: Plan 49 query after replacing subqueries and CTEs: SELECT DISTINCT value_2, array_agg(rnk ORDER BY rnk) AS array_agg FROM (SELECT intermediate_result.value_2, intermediate_result.rnk FROM read_intermediate_result('49_1'::text, 'binary'::citus_copy_format) intermediate_result(value_2 integer, rnk bigint)) sq GROUP BY value_2 ORDER BY value_2
2023-11-25 13:48:30.565 UTC [135455] LOCATION:  GenerateSubplansForSubqueriesAndCTEs, recursive_planning.c:248
2023-11-25 13:48:30.565 UTC [135455] DEBUG:  00000: Creating router plan
2023-11-25 13:48:30.565 UTC [135455] LOCATION:  CreateSingleTaskRouterSelectPlan, multi_router_planner.c:284
2023-11-25 13:48:30.565 UTC [135455] DEBUG:  00000: Subplan 49_1 is used in 49
2023-11-25 13:48:30.565 UTC [135455] LOCATION:  RecordSubplanExecutionsOnNodes, intermediate_result_pruning.c:193
2023-11-25 13:48:30.566 UTC [135455] DEBUG:  00000: Subplan 49_1 will be written to local file
2023-11-25 13:48:30.566 UTC [135455] LOCATION:  LogIntermediateResultMulticastSummary, intermediate_result_pruning.c:416
2023-11-25 13:48:30.566 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.566 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.566 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 156 in 3515 microseconds
2023-11-25 13:48:30.566 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.566 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.566 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.566 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 158 in 8559 microseconds
2023-11-25 13:48:30.566 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.567 UTC [135455] DEBUG:  00000: task execution (1) for placement (856) on anchor shard (1400260) finished in 692 microseconds on worker node localhost:57637
2023-11-25 13:48:30.567 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.568 UTC [135455] DEBUG:  00000: task execution (3) for placement (858) on anchor shard (1400262) finished in 766 microseconds on worker node localhost:57637
2023-11-25 13:48:30.568 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.568 UTC [135455] DEBUG:  00000: task execution (2) for placement (857) on anchor shard (1400261) finished in 1297 microseconds on worker node localhost:57638
2023-11-25 13:48:30.568 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.569 UTC [135455] DEBUG:  00000: task execution (4) for placement (859) on anchor shard (1400263) finished in 448 microseconds on worker node localhost:57638
2023-11-25 13:48:30.569 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.573 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 157 in 7472 microseconds
2023-11-25 13:48:30.573 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.577 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 159 in 10825 microseconds
2023-11-25 13:48:30.577 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.577 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 156: 2 to node localhost:57637
2023-11-25 13:48:30.577 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.577 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 157: 0 to node localhost:57637
2023-11-25 13:48:30.577 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.577 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 158: 2 to node localhost:57638
2023-11-25 13:48:30.577 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.577 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 159: 0 to node localhost:57638
2023-11-25 13:48:30.577 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.579 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.579 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.579 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.579 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.579 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.579 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.579 UTC [135455] DEBUG:  00000: push down of limit count: 1
2023-11-25 13:48:30.579 UTC [135455] LOCATION:  WorkerLimitCount, multi_logical_optimizer.c:4778
2023-11-25 13:48:30.579 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.579 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.579 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.579 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.579 UTC [135455] DEBUG:  00000: generated sql query for task 1
2023-11-25 13:48:30.579 UTC [135455] DETAIL:  query string: "SELECT NULL::boolean FROM public.users_table_1400256 ut WHERE true LIMIT '1'::bigint"
2023-11-25 13:48:30.579 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.579 UTC [135455] DEBUG:  00000: generated sql query for task 2
2023-11-25 13:48:30.579 UTC [135455] DETAIL:  query string: "SELECT NULL::boolean FROM public.users_table_1400257 ut WHERE true LIMIT '1'::bigint"
2023-11-25 13:48:30.579 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.579 UTC [135455] DEBUG:  00000: generated sql query for task 3
2023-11-25 13:48:30.579 UTC [135455] DETAIL:  query string: "SELECT NULL::boolean FROM public.users_table_1400258 ut WHERE true LIMIT '1'::bigint"
2023-11-25 13:48:30.579 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.579 UTC [135455] DEBUG:  00000: generated sql query for task 4
2023-11-25 13:48:30.579 UTC [135455] DETAIL:  query string: "SELECT NULL::boolean FROM public.users_table_1400259 ut WHERE true LIMIT '1'::bigint"
2023-11-25 13:48:30.579 UTC [135455] LOCATION:  SqlTaskList, multi_physical_planner.c:2713
2023-11-25 13:48:30.579 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.579 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.579 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.579 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.579 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.579 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.579 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.579 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.579 UTC [135455] DEBUG:  00000: combine query: SELECT "?column?" FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), '(i 1)'::cstring(0)) remote_scan("?column?" boolean) LIMIT '1'::bigint
2023-11-25 13:48:30.579 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.579 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.579 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.579 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 160 in 3515 microseconds
2023-11-25 13:48:30.579 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.579 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.579 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.580 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 162 in 8559 microseconds
2023-11-25 13:48:30.580 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.580 UTC [135455] DEBUG:  00000: task execution (1) for placement (852) on anchor shard (1400256) finished in 235 microseconds on worker node localhost:57637
2023-11-25 13:48:30.580 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.582 UTC [135455] DEBUG:  00000: task execution (3) for placement (854) on anchor shard (1400258) finished in 2550 microseconds on worker node localhost:57637
2023-11-25 13:48:30.582 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.582 UTC [135455] DEBUG:  00000: task execution (2) for placement (853) on anchor shard (1400257) finished in 2649 microseconds on worker node localhost:57638
2023-11-25 13:48:30.582 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.583 UTC [135455] DEBUG:  00000: task execution (4) for placement (855) on anchor shard (1400259) finished in 267 microseconds on worker node localhost:57638
2023-11-25 13:48:30.583 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.587 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 163 in 7083 microseconds
2023-11-25 13:48:30.587 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.590 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 161 in 10682 microseconds
2023-11-25 13:48:30.590 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.590 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 160: 2 to node localhost:57637
2023-11-25 13:48:30.590 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.590 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 161: 0 to node localhost:57637
2023-11-25 13:48:30.590 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.590 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 162: 2 to node localhost:57638
2023-11-25 13:48:30.590 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.590 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 163: 0 to node localhost:57638
2023-11-25 13:48:30.590 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.591 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.591 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.591 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.591 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.591 UTC [135455] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 13:48:30.591 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.591 UTC [135455] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 13:48:30.591 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.591 UTC [135455] DEBUG:  0A000: Router planner cannot handle multi-shard select queries
2023-11-25 13:48:30.591 UTC [135455] LOCATION:  PlanRouterQuery, multi_router_planner.c:2344
2023-11-25 13:48:30.591 UTC [135455] DEBUG:  00000: no shard pruning constraints on users_table found
2023-11-25 13:48:30.591 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.591 UTC [135455] DEBUG:  00000: shard count after pruning for users_table: 4
2023-11-25 13:48:30.591 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.591 UTC [135455] DEBUG:  00000: no shard pruning constraints on events_table found
2023-11-25 13:48:30.591 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:495
2023-11-25 13:48:30.591 UTC [135455] DEBUG:  00000: shard count after pruning for events_table: 4
2023-11-25 13:48:30.591 UTC [135455] LOCATION:  PruneShards, shard_pruning.c:499
2023-11-25 13:48:30.591 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, max(worker_column_2) OVER (PARTITION BY worker_column_1, (min(worker_column_3))) AS max, worker_column_2 AS worker_column_3, min(worker_column_3) AS worker_column_4 FROM (SELECT s.user_id AS worker_column_1, s.value_1 AS worker_column_2, s.value_2 AS worker_column_3 FROM (SELECT DISTINCT us.user_id, us.value_2, us.value_1, random() AS r1 FROM public.users_table_1400256 us, public.events_table_1400260 events_table WHERE ((us.user_id OPERATOR(pg_catalog.=) events_table.user_id) AND (events_table.event_type OPERATOR(pg_catalog.=) ANY (ARRAY[1, 2]))) ORDER BY us.user_id, us.value_2) s) worker_subquery GROUP BY worker_column_1, worker_column_2
2023-11-25 13:48:30.591 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.591 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, max(worker_column_2) OVER (PARTITION BY worker_column_1, (min(worker_column_3))) AS max, worker_column_2 AS worker_column_3, min(worker_column_3) AS worker_column_4 FROM (SELECT s.user_id AS worker_column_1, s.value_1 AS worker_column_2, s.value_2 AS worker_column_3 FROM (SELECT DISTINCT us.user_id, us.value_2, us.value_1, random() AS r1 FROM public.users_table_1400257 us, public.events_table_1400261 events_table WHERE ((us.user_id OPERATOR(pg_catalog.=) events_table.user_id) AND (events_table.event_type OPERATOR(pg_catalog.=) ANY (ARRAY[1, 2]))) ORDER BY us.user_id, us.value_2) s) worker_subquery GROUP BY worker_column_1, worker_column_2
2023-11-25 13:48:30.591 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.591 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, max(worker_column_2) OVER (PARTITION BY worker_column_1, (min(worker_column_3))) AS max, worker_column_2 AS worker_column_3, min(worker_column_3) AS worker_column_4 FROM (SELECT s.user_id AS worker_column_1, s.value_1 AS worker_column_2, s.value_2 AS worker_column_3 FROM (SELECT DISTINCT us.user_id, us.value_2, us.value_1, random() AS r1 FROM public.users_table_1400258 us, public.events_table_1400262 events_table WHERE ((us.user_id OPERATOR(pg_catalog.=) events_table.user_id) AND (events_table.event_type OPERATOR(pg_catalog.=) ANY (ARRAY[1, 2]))) ORDER BY us.user_id, us.value_2) s) worker_subquery GROUP BY worker_column_1, worker_column_2
2023-11-25 13:48:30.591 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.592 UTC [135455] DEBUG:  00000: distributed statement: SELECT worker_column_1 AS user_id, max(worker_column_2) OVER (PARTITION BY worker_column_1, (min(worker_column_3))) AS max, worker_column_2 AS worker_column_3, min(worker_column_3) AS worker_column_4 FROM (SELECT s.user_id AS worker_column_1, s.value_1 AS worker_column_2, s.value_2 AS worker_column_3 FROM (SELECT DISTINCT us.user_id, us.value_2, us.value_1, random() AS r1 FROM public.users_table_1400259 us, public.events_table_1400263 events_table WHERE ((us.user_id OPERATOR(pg_catalog.=) events_table.user_id) AND (events_table.event_type OPERATOR(pg_catalog.=) ANY (ARRAY[1, 2]))) ORDER BY us.user_id, us.value_2) s) worker_subquery GROUP BY worker_column_1, worker_column_2
2023-11-25 13:48:30.592 UTC [135455] LOCATION:  QueryPushdownTaskCreate, multi_physical_planner.c:2559
2023-11-25 13:48:30.592 UTC [135455] DEBUG:  00000: assigned task 1 to node localhost:57637
2023-11-25 13:48:30.592 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.592 UTC [135455] DEBUG:  00000: assigned task 2 to node localhost:57638
2023-11-25 13:48:30.592 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.592 UTC [135455] DEBUG:  00000: assigned task 3 to node localhost:57637
2023-11-25 13:48:30.592 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.592 UTC [135455] DEBUG:  00000: assigned task 4 to node localhost:57638
2023-11-25 13:48:30.592 UTC [135455] LOCATION:  GreedyAssignTask, multi_physical_planner.c:5140
2023-11-25 13:48:30.592 UTC [135455] DEBUG:  00000: combine query: SELECT user_id, max FROM pg_catalog.citus_extradata_container(10, NULL::cstring(0), NULL::cstring(0), NULL::cstring(0)) remote_scan(user_id integer, max integer, worker_column_3 integer, worker_column_4 integer) ORDER BY max DESC, user_id
2023-11-25 13:48:30.592 UTC [135455] LOCATION:  BuildSelectStatementViaStdPlanner, combine_query_planner.c:291
2023-11-25 13:48:30.592 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57637
2023-11-25 13:48:30.592 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.592 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 164 in 3515 microseconds
2023-11-25 13:48:30.592 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.592 UTC [135455] DEBUG:  00000: opening 2 new connections to localhost:57638
2023-11-25 13:48:30.592 UTC [135455] LOCATION:  OpenNewConnections, adaptive_executor.c:2572
2023-11-25 13:48:30.592 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 166 in 8559 microseconds
2023-11-25 13:48:30.592 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.593 UTC [135455] DEBUG:  00000: task execution (1) for placement (856) on anchor shard (1400260) finished in 1022 microseconds on worker node localhost:57637
2023-11-25 13:48:30.593 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.594 UTC [135455] DEBUG:  00000: task execution (2) for placement (857) on anchor shard (1400261) finished in 1704 microseconds on worker node localhost:57638
2023-11-25 13:48:30.594 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.594 UTC [135455] DEBUG:  00000: task execution (3) for placement (858) on anchor shard (1400262) finished in 898 microseconds on worker node localhost:57637
2023-11-25 13:48:30.594 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.595 UTC [135455] DEBUG:  00000: task execution (4) for placement (859) on anchor shard (1400263) finished in 1005 microseconds on worker node localhost:57638
2023-11-25 13:48:30.595 UTC [135455] LOCATION:  PlacementExecutionDone, adaptive_executor.c:4326
2023-11-25 13:48:30.596 UTC [135455] DEBUG:  00000: established connection to localhost:57638 for session 167 in 3539 microseconds
2023-11-25 13:48:30.596 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.597 UTC [135455] DEBUG:  00000: established connection to localhost:57637 for session 165 in 4813 microseconds
2023-11-25 13:48:30.597 UTC [135455] LOCATION:  HandleMultiConnectionSuccess, adaptive_executor.c:3275
2023-11-25 13:48:30.597 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 164: 2 to node localhost:57637
2023-11-25 13:48:30.597 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.597 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 165: 0 to node localhost:57637
2023-11-25 13:48:30.597 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.597 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 166: 2 to node localhost:57638
2023-11-25 13:48:30.597 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.597 UTC [135455] DEBUG:  00000: Total number of commands sent over the session 167: 0 to node localhost:57638
2023-11-25 13:48:30.597 UTC [135455] LOCATION:  CleanUpSessions, adaptive_executor.c:4863
2023-11-25 13:48:30.597 UTC [135455] DEBUG:  00000: shmem_exit(0): 6 before_shmem_exit callbacks to make
2023-11-25 13:48:30.597 UTC [135455] LOCATION:  shmem_exit, ipc.c:236
2023-11-25 13:48:30.598 UTC [135455] DEBUG:  00000: shmem_exit(0): 6 on_shmem_exit callbacks to make
2023-11-25 13:48:30.598 UTC [135455] LOCATION:  shmem_exit, ipc.c:269
2023-11-25 13:48:30.598 UTC [135455] DEBUG:  00000: proc_exit(0): 2 callbacks to make
2023-11-25 13:48:30.598 UTC [135455] LOCATION:  proc_exit_prepare, ipc.c:196
2023-11-25 13:48:30.598 UTC [135455] DEBUG:  00000: exit(0)
2023-11-25 13:48:30.598 UTC [135455] LOCATION:  proc_exit, ipc.c:150
2023-11-25 13:48:30.598 UTC [135455] DEBUG:  00000: shmem_exit(-1): 0 before_shmem_exit callbacks to make
2023-11-25 13:48:30.598 UTC [135455] LOCATION:  shmem_exit, ipc.c:236
2023-11-25 13:48:30.598 UTC [135455] DEBUG:  00000: shmem_exit(-1): 0 on_shmem_exit callbacks to make
2023-11-25 13:48:30.598 UTC [135455] LOCATION:  shmem_exit, ipc.c:269
2023-11-25 13:48:30.598 UTC [135455] DEBUG:  00000: proc_exit(-1): 0 callbacks to make
2023-11-25 13:48:30.598 UTC [135455] LOCATION:  proc_exit_prepare, ipc.c:196
2023-11-25 13:48:30.679 UTC [135457] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:30.679 UTC [135457] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:30.679 UTC [135457] STATEMENT:  UPDATE
		second_distributed_table
	SET
		dept = foo.tenant_id::int / 4
	FROM
	(
		SELECT DISTINCT foo_inner_1.tenant_id FROM
		(
			SELECT
				second_distributed_table.dept, second_distributed_table.tenant_id
			FROM
				second_distributed_table, distributed_table
			WHERE
				distributed_table.tenant_id = second_distributed_table.tenant_id
			AND
				second_distributed_table.dept IN (select dept from second_distributed_table))
		foo_inner_1 JOIN LATERAL
		(
			SELECT
				second_distributed_table.tenant_id
			FROM
				second_distributed_table, distributed_table
			WHERE
				distributed_table.tenant_id = second_distributed_table.tenant_id
				AND foo_inner_1.dept = second_distributed_table.dept
			AND
				second_distributed_table.dept IN (4,5)
		) foo_inner_2
		ON (foo_inner_2.tenant_id != foo_inner_1.tenant_id)
		) as foo
	RETURNING *;
2023-11-25 13:48:30.680 UTC [135457] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:30.680 UTC [135457] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:30.680 UTC [135457] STATEMENT:  UPDATE
		second_distributed_table
	SET
		dept = foo.tenant_id::int / 4
	FROM
	(
		SELECT baz.tenant_id FROM
		(
			SELECT
				second_distributed_table.dept, second_distributed_table.tenant_id
			FROM
				second_distributed_table, distributed_table as d1
			WHERE
				d1.tenant_id = second_distributed_table.tenant_id
			AND
				second_distributed_table.dept IN (3,4)
				AND
				second_distributed_table.tenant_id IN
				(
						SELECT s2.tenant_id
						FROM second_distributed_table as s2
						GROUP BY d1.tenant_id, s2.tenant_id
				)
		) as baz
		) as foo WHERE second_distributed_table.tenant_id = foo.tenant_id
	RETURNING *;
2023-11-25 13:48:30.680 UTC [135457] ERROR:  0A000: subqueries are not supported within INSERT queries
2023-11-25 13:48:30.680 UTC [135457] HINT:  Try rewriting your queries with 'INSERT INTO ... SELECT' syntax.
2023-11-25 13:48:30.680 UTC [135457] LOCATION:  ModifyPartialQuerySupported, multi_router_planner.c:696
2023-11-25 13:48:30.680 UTC [135457] STATEMENT:  INSERT INTO
		second_distributed_table (tenant_id, dept)
	VALUES ('3', (WITH  vals AS (SELECT 3) select * from vals));
2023-11-25 13:48:30.680 UTC [135457] ERROR:  0A000: subqueries are not supported within INSERT queries
2023-11-25 13:48:30.680 UTC [135457] HINT:  Try rewriting your queries with 'INSERT INTO ... SELECT' syntax.
2023-11-25 13:48:30.680 UTC [135457] LOCATION:  ModifyPartialQuerySupported, multi_router_planner.c:696
2023-11-25 13:48:30.680 UTC [135457] STATEMENT:  INSERT INTO
		second_distributed_table (tenant_id, dept)
	VALUES ('3', (SELECT 3));
2023-11-25 13:48:30.891 UTC [135456] ERROR:  XX000: EXPLAIN ANALYZE is currently not supported for INSERT ... SELECT commands with repartitioning
2023-11-25 13:48:30.891 UTC [135456] LOCATION:  NonPushableInsertSelectExplainScan, multi_explain.c:252
2023-11-25 13:48:30.891 UTC [135456] STATEMENT:  EXPLAIN ANALYZE INSERT INTO target_table SELECT a, max(b) FROM source_table GROUP BY a;
2023-11-25 13:48:31.155 UTC [135456] ERROR:  23502: null value in column "b" of relation "target_table_4213617" violates not-null constraint
2023-11-25 13:48:31.155 UTC [135456] DETAIL:  Failing row contains (2, null).
2023-11-25 13:48:31.155 UTC [135456] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:31.155 UTC [135456] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:31.155 UTC [135456] STATEMENT:  INSERT INTO target_table SELECT * FROM source_table;
2023-11-25 13:48:31.224 UTC [135456] ERROR:  55000: could not find shard for partition column value
2023-11-25 13:48:31.224 UTC [135456] CONTEXT:  while executing command on localhost:57638
2023-11-25 13:48:31.224 UTC [135456] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:31.224 UTC [135456] STATEMENT:  INSERT INTO target_table SELECT a * 10, b FROM source_table WHERE b IS NOT NULL;
2023-11-25 13:48:31.884 UTC [136261] ERROR:  XX000: triggers are not supported on distributed tables
2023-11-25 13:48:31.884 UTC [136261] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:726
2023-11-25 13:48:31.884 UTC [136261] STATEMENT:  CREATE TRIGGER update_value_dist
	AFTER INSERT ON distributed_table
	FOR EACH ROW EXECUTE FUNCTION update_value();
2023-11-25 13:48:31.884 UTC [136261] ERROR:  XX000: triggers are not supported on reference tables
2023-11-25 13:48:31.884 UTC [136261] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:722
2023-11-25 13:48:31.884 UTC [136261] STATEMENT:  CREATE TRIGGER update_value_ref
	AFTER INSERT ON reference_table
	FOR EACH ROW EXECUTE FUNCTION update_value();
2023-11-25 13:48:31.904 UTC [136261] ERROR:  XX000: triggers are not supported on distributed tables
2023-11-25 13:48:31.904 UTC [136261] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:726
2023-11-25 13:48:31.904 UTC [136261] STATEMENT:  ALTER TRIGGER update_value_dist ON distributed_table RENAME TO update_value_dist1;
2023-11-25 13:48:31.905 UTC [136261] ERROR:  XX000: trigger "update_value_dist" depends on an extension and this is not supported for distributed tables and local tables added to metadata
2023-11-25 13:48:31.905 UTC [136261] DETAIL:  Triggers from extensions are expected to be created on the workers by the extension they depend on.
2023-11-25 13:48:31.905 UTC [136261] LOCATION:  PreprocessAlterTriggerDependsStmt, trigger.c:552
2023-11-25 13:48:31.905 UTC [136261] STATEMENT:  ALTER TRIGGER update_value_dist ON distributed_table DEPENDS ON EXTENSION seg;
2023-11-25 13:48:31.905 UTC [136261] ERROR:  XX000: triggers are not supported on distributed tables
2023-11-25 13:48:31.905 UTC [136261] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:726
2023-11-25 13:48:31.905 UTC [136261] STATEMENT:  DROP TRIGGER update_value_dist ON distributed_table;
2023-11-25 13:48:31.905 UTC [136261] ERROR:  XX000: triggers are not supported on distributed tables
2023-11-25 13:48:31.905 UTC [136261] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:726
2023-11-25 13:48:31.905 UTC [136261] STATEMENT:  ALTER TABLE distributed_table DISABLE TRIGGER ALL;
2023-11-25 13:48:31.905 UTC [136261] ERROR:  XX000: triggers are not supported on distributed tables
2023-11-25 13:48:31.905 UTC [136261] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:726
2023-11-25 13:48:31.905 UTC [136261] STATEMENT:  ALTER TABLE distributed_table DISABLE TRIGGER USER;
2023-11-25 13:48:31.905 UTC [136261] ERROR:  XX000: triggers are not supported on distributed tables
2023-11-25 13:48:31.905 UTC [136261] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:726
2023-11-25 13:48:31.905 UTC [136261] STATEMENT:  ALTER TABLE distributed_table DISABLE TRIGGER update_value_dist;
2023-11-25 13:48:31.905 UTC [136261] ERROR:  XX000: triggers are not supported on distributed tables
2023-11-25 13:48:31.905 UTC [136261] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:726
2023-11-25 13:48:31.905 UTC [136261] STATEMENT:  ALTER TABLE distributed_table ENABLE TRIGGER ALL;
2023-11-25 13:48:31.905 UTC [136261] ERROR:  XX000: triggers are not supported on distributed tables
2023-11-25 13:48:31.905 UTC [136261] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:726
2023-11-25 13:48:31.905 UTC [136261] STATEMENT:  ALTER TABLE distributed_table ENABLE TRIGGER USER;
2023-11-25 13:48:31.905 UTC [136261] ERROR:  XX000: triggers are not supported on distributed tables
2023-11-25 13:48:31.905 UTC [136261] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:726
2023-11-25 13:48:31.905 UTC [136261] STATEMENT:  ALTER TABLE distributed_table ENABLE TRIGGER update_value_dist;
2023-11-25 13:48:31.905 UTC [136261] ERROR:  XX000: triggers are not supported on reference tables
2023-11-25 13:48:31.905 UTC [136261] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:722
2023-11-25 13:48:31.905 UTC [136261] STATEMENT:  ALTER TRIGGER update_value_ref ON reference_table RENAME TO update_value_ref1;
2023-11-25 13:48:31.905 UTC [136261] ERROR:  XX000: trigger "update_value_ref" depends on an extension and this is not supported for distributed tables and local tables added to metadata
2023-11-25 13:48:31.905 UTC [136261] DETAIL:  Triggers from extensions are expected to be created on the workers by the extension they depend on.
2023-11-25 13:48:31.905 UTC [136261] LOCATION:  PreprocessAlterTriggerDependsStmt, trigger.c:552
2023-11-25 13:48:31.905 UTC [136261] STATEMENT:  ALTER TRIGGER update_value_ref ON reference_table DEPENDS ON EXTENSION seg;
2023-11-25 13:48:31.906 UTC [136261] ERROR:  XX000: triggers are not supported on reference tables
2023-11-25 13:48:31.906 UTC [136261] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:722
2023-11-25 13:48:31.906 UTC [136261] STATEMENT:  DROP TRIGGER update_value_ref ON reference_table;
2023-11-25 13:48:31.906 UTC [136261] ERROR:  XX000: triggers are not supported on reference tables
2023-11-25 13:48:31.906 UTC [136261] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:722
2023-11-25 13:48:31.906 UTC [136261] STATEMENT:  ALTER TABLE reference_table DISABLE TRIGGER ALL;
2023-11-25 13:48:31.906 UTC [136261] ERROR:  XX000: triggers are not supported on reference tables
2023-11-25 13:48:31.906 UTC [136261] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:722
2023-11-25 13:48:31.906 UTC [136261] STATEMENT:  ALTER TABLE reference_table DISABLE TRIGGER USER;
2023-11-25 13:48:31.906 UTC [136261] ERROR:  XX000: triggers are not supported on reference tables
2023-11-25 13:48:31.906 UTC [136261] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:722
2023-11-25 13:48:31.906 UTC [136261] STATEMENT:  ALTER TABLE reference_table DISABLE TRIGGER update_value_ref;
2023-11-25 13:48:31.906 UTC [136261] ERROR:  XX000: triggers are not supported on reference tables
2023-11-25 13:48:31.906 UTC [136261] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:722
2023-11-25 13:48:31.906 UTC [136261] STATEMENT:  ALTER TABLE reference_table ENABLE TRIGGER ALL;
2023-11-25 13:48:31.906 UTC [136261] ERROR:  XX000: triggers are not supported on reference tables
2023-11-25 13:48:31.906 UTC [136261] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:722
2023-11-25 13:48:31.906 UTC [136261] STATEMENT:  ALTER TABLE reference_table ENABLE TRIGGER USER;
2023-11-25 13:48:31.906 UTC [136261] ERROR:  XX000: triggers are not supported on reference tables
2023-11-25 13:48:31.906 UTC [136261] LOCATION:  ErrorOutForTriggerIfNotSupported, trigger.c:722
2023-11-25 13:48:31.906 UTC [136261] STATEMENT:  ALTER TABLE reference_table ENABLE TRIGGER update_value_ref;
2023-11-25 13:48:31.908 UTC [136261] ERROR:  XX000: cannot distribute relation "distributed_table_1" because it has triggers
2023-11-25 13:48:31.908 UTC [136261] HINT:  Consider dropping all the triggers on "distributed_table_1" and retry.
2023-11-25 13:48:31.908 UTC [136261] LOCATION:  EnsureRelationHasNoTriggers, create_distributed_table.c:2095
2023-11-25 13:48:31.908 UTC [136261] STATEMENT:  SELECT create_distributed_table('distributed_table_1', 'value');
2023-11-25 13:48:31.908 UTC [136261] ERROR:  XX000: cannot distribute relation "reference_table_1" because it has triggers
2023-11-25 13:48:31.908 UTC [136261] HINT:  Consider dropping all the triggers on "reference_table_1" and retry.
2023-11-25 13:48:31.908 UTC [136261] LOCATION:  EnsureRelationHasNoTriggers, create_distributed_table.c:2095
2023-11-25 13:48:31.908 UTC [136261] STATEMENT:  SELECT create_reference_table('reference_table_1');
2023-11-25 13:48:31.998 UTC [136262] ERROR:  23503: insert or update on table "target_table_1900000" violates foreign key constraint "fkey_1900000"
2023-11-25 13:48:31.998 UTC [136262] DETAIL:  Key (col_1)=(1) is not present in table "test_ref_table_1900012".
2023-11-25 13:48:31.998 UTC [136262] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:31.998 UTC [136262] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:31.998 UTC [136262] STATEMENT:  INSERT INTO
			target_table
		SELECT
			col_2,
			col_1
		FROM source_table_1 ON CONFLICT (col_1) DO UPDATE SET col_2 = 55 RETURNING *;
2023-11-25 13:48:32.685 UTC [136382] ERROR:  XX000: cannot undistribute table because the table is not distributed
2023-11-25 13:48:32.685 UTC [136382] LOCATION:  UndistributeTable, alter_table.c:379
2023-11-25 13:48:32.685 UTC [136382] STATEMENT:  SELECT undistribute_table('local_source_table_1');
2023-11-25 13:48:32.840 UTC [136536] ERROR:  42883: function hll_hash_bigint(bigint) does not exist at character 49
2023-11-25 13:48:32.840 UTC [136536] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:48:32.840 UTC [136536] QUERY:  
	 EXPLAIN SELECT symbol_id,
	        HLL_ADD_AGG(HLL_HASH_BIGINT(event_id)) AS event_hll_hash,
	        HLL_CARDINALITY(HLL_ADD_AGG(HLL_HASH_BIGINT(event_id))) AS event_n_users
	 FROM (
	    SELECT event_time, composite_id, event_id, 4640476 symbol_id FROM "events"
	 UNION ALL
	    SELECT event_time, composite_id, event_id, 4640477 symbol_id FROM "events"
	 ) pushdown_events
	 GROUP BY symbol_id;
	 
2023-11-25 13:48:32.840 UTC [136536] CONTEXT:  PL/pgSQL function explain_has_distributed_subplan(text) line 5 at FOR over EXECUTE statement
2023-11-25 13:48:32.840 UTC [136536] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 13:48:32.840 UTC [136536] STATEMENT:  SELECT public.explain_has_distributed_subplan($$
	 EXPLAIN SELECT symbol_id,
	        HLL_ADD_AGG(HLL_HASH_BIGINT(event_id)) AS event_hll_hash,
	        HLL_CARDINALITY(HLL_ADD_AGG(HLL_HASH_BIGINT(event_id))) AS event_n_users
	 FROM (
	    SELECT event_time, composite_id, event_id, 4640476 symbol_id FROM "events"
	 UNION ALL
	    SELECT event_time, composite_id, event_id, 4640477 symbol_id FROM "events"
	 ) pushdown_events
	 GROUP BY symbol_id;
	 $$);
2023-11-25 13:48:32.846 UTC [136536] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 13:48:32.846 UTC [136536] HINT:  You need to have the hll extension loaded.
2023-11-25 13:48:32.846 UTC [136536] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 13:48:32.846 UTC [136536] STATEMENT:  SELECT count(distinct l_orderkey) FROM lineitem;
2023-11-25 13:48:32.847 UTC [136536] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 13:48:32.847 UTC [136536] HINT:  You need to have the hll extension loaded.
2023-11-25 13:48:32.847 UTC [136536] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 13:48:32.847 UTC [136536] STATEMENT:  SELECT count(distinct l_orderkey) FROM lineitem;
2023-11-25 13:48:32.847 UTC [136536] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 13:48:32.847 UTC [136536] HINT:  You need to have the hll extension loaded.
2023-11-25 13:48:32.847 UTC [136536] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 13:48:32.847 UTC [136536] STATEMENT:  SELECT count(distinct l_partkey) FROM lineitem;
2023-11-25 13:48:32.847 UTC [136536] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 13:48:32.847 UTC [136536] HINT:  You need to have the hll extension loaded.
2023-11-25 13:48:32.847 UTC [136536] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 13:48:32.847 UTC [136536] STATEMENT:  SELECT count(distinct l_extendedprice) FROM lineitem;
2023-11-25 13:48:32.847 UTC [136536] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 13:48:32.847 UTC [136536] HINT:  You need to have the hll extension loaded.
2023-11-25 13:48:32.847 UTC [136536] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 13:48:32.847 UTC [136536] STATEMENT:  SELECT count(distinct l_shipdate) FROM lineitem;
2023-11-25 13:48:32.847 UTC [136536] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 13:48:32.847 UTC [136536] HINT:  You need to have the hll extension loaded.
2023-11-25 13:48:32.847 UTC [136536] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 13:48:32.847 UTC [136536] STATEMENT:  SELECT count(distinct l_comment) FROM lineitem;
2023-11-25 13:48:32.848 UTC [136536] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 13:48:32.848 UTC [136536] HINT:  You need to have the hll extension loaded.
2023-11-25 13:48:32.848 UTC [136536] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 13:48:32.848 UTC [136536] STATEMENT:  SELECT count(distinct (l_orderkey * 2 + 1)) FROM lineitem;
2023-11-25 13:48:32.848 UTC [136536] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 13:48:32.848 UTC [136536] HINT:  You need to have the hll extension loaded.
2023-11-25 13:48:32.848 UTC [136536] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 13:48:32.848 UTC [136536] STATEMENT:  SELECT count(distinct extract(month from l_shipdate)) AS my_month FROM lineitem;
2023-11-25 13:48:32.848 UTC [136536] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 13:48:32.848 UTC [136536] HINT:  You need to have the hll extension loaded.
2023-11-25 13:48:32.848 UTC [136536] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 13:48:32.848 UTC [136536] STATEMENT:  SELECT count(distinct l_partkey) / count(distinct l_orderkey) FROM lineitem;
2023-11-25 13:48:32.848 UTC [136536] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 13:48:32.848 UTC [136536] HINT:  You need to have the hll extension loaded.
2023-11-25 13:48:32.848 UTC [136536] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 13:48:32.848 UTC [136536] STATEMENT:  SELECT count(distinct l_orderkey) FROM lineitem
		WHERE octet_length(l_comment) + octet_length('randomtext'::text) > 40;
2023-11-25 13:48:32.848 UTC [136536] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 13:48:32.848 UTC [136536] HINT:  You need to have the hll extension loaded.
2023-11-25 13:48:32.848 UTC [136536] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 13:48:32.848 UTC [136536] STATEMENT:  SELECT count(DISTINCT l_orderkey) FROM lineitem, orders
		WHERE l_orderkey = o_orderkey AND l_quantity < 5;
2023-11-25 13:48:32.849 UTC [136536] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 13:48:32.849 UTC [136536] HINT:  You need to have the hll extension loaded.
2023-11-25 13:48:32.849 UTC [136536] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 13:48:32.849 UTC [136536] STATEMENT:  SELECT count(DISTINCT l_orderkey) as distinct_order_count, l_quantity FROM lineitem
		WHERE l_quantity < 32.0
		GROUP BY l_quantity
		ORDER BY distinct_order_count ASC, l_quantity ASC
		LIMIT 10;
2023-11-25 13:48:32.874 UTC [136536] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 13:48:32.874 UTC [136536] HINT:  You need to have the hll extension loaded.
2023-11-25 13:48:32.874 UTC [136536] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 13:48:32.874 UTC [136536] STATEMENT:  SELECT COUNT (DISTINCT n_regionkey) FROM test_count_distinct_schema.nation_hash;
2023-11-25 13:48:32.874 UTC [136536] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 13:48:32.874 UTC [136536] HINT:  You need to have the hll extension loaded.
2023-11-25 13:48:32.874 UTC [136536] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 13:48:32.874 UTC [136536] STATEMENT:  SELECT COUNT (DISTINCT n_regionkey) FROM nation_hash;
2023-11-25 13:48:32.875 UTC [136536] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 13:48:32.875 UTC [136536] HINT:  You need to have the hll extension loaded.
2023-11-25 13:48:32.875 UTC [136536] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 13:48:32.875 UTC [136536] STATEMENT:  SELECT l_returnflag, count(DISTINCT l_shipdate) as count_distinct, count(*) as total
		FROM lineitem
		GROUP BY l_returnflag
		ORDER BY count_distinct
		LIMIT 10;
2023-11-25 13:48:32.875 UTC [136536] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 13:48:32.875 UTC [136536] HINT:  You need to have the hll extension loaded.
2023-11-25 13:48:32.875 UTC [136536] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 13:48:32.875 UTC [136536] STATEMENT:  SELECT l_returnflag, count(DISTINCT l_shipdate) as count_distinct, count(*) as total
		FROM lineitem
		GROUP BY l_returnflag
		ORDER BY total
		LIMIT 10;
2023-11-25 13:48:32.876 UTC [136536] ERROR:  0A000: cannot compute count (distinct) approximation
2023-11-25 13:48:32.876 UTC [136536] HINT:  You need to have the hll extension loaded.
2023-11-25 13:48:32.876 UTC [136536] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4133
2023-11-25 13:48:32.876 UTC [136536] STATEMENT:  SELECT
		l_partkey,
		count(l_partkey) FILTER (WHERE l_shipmode = 'AIR'),
		count(DISTINCT l_partkey) FILTER (WHERE l_shipmode = 'AIR'),
		count(DISTINCT CASE WHEN l_shipmode = 'AIR' THEN l_partkey ELSE NULL END)
		FROM lineitem
		GROUP BY l_partkey
		ORDER BY 2 DESC, 1 DESC
		LIMIT 10;
2023-11-25 13:48:33.602 UTC [136585] ERROR:  23502: null value in column "b" of relation "target_table_4213646" violates not-null constraint
2023-11-25 13:48:33.602 UTC [136585] DETAIL:  Failing row contains (84, null).
2023-11-25 13:48:33.602 UTC [136585] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:33.602 UTC [136585] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:33.602 UTC [136585] STATEMENT:  INSERT INTO target_table SELECT a, CASE WHEN a < 50 THEN b ELSE null END  FROM source_table;
2023-11-25 13:48:34.031 UTC [136709] ERROR:  XX000: "parent_table" is not a partition
2023-11-25 13:48:34.031 UTC [136709] LOCATION:  GenerateAlterTableAttachPartitionCommand, multi_partitioning_utils.c:1221
2023-11-25 13:48:34.031 UTC [136709] STATEMENT:  SELECT public.generate_alter_table_attach_partition_command('parent_table');
2023-11-25 13:48:34.031 UTC [136709] ERROR:  XX000: "child_1" is not a parent table
2023-11-25 13:48:34.031 UTC [136709] LOCATION:  GeneratePartitioningInformation, multi_partitioning_utils.c:1155
2023-11-25 13:48:34.031 UTC [136709] STATEMENT:  SELECT public.generate_partition_information('partition_child_1_schema.child_1');
2023-11-25 13:48:34.031 UTC [136709] ERROR:  XX000: "child_1" is not a parent table
2023-11-25 13:48:34.031 UTC [136709] LOCATION:  PartitionList, multi_partitioning_utils.c:1075
2023-11-25 13:48:34.031 UTC [136709] STATEMENT:  SELECT public.print_partitions('partition_child_1_schema.child_1');
2023-11-25 13:48:34.056 UTC [136709] ERROR:  42809: capitals is not a regular, foreign or partitioned table
2023-11-25 13:48:34.056 UTC [136709] LOCATION:  EnsureRelationKindSupported, citus_ruleutils.c:635
2023-11-25 13:48:34.056 UTC [136709] STATEMENT:  SELECT master_get_table_ddl_events('capitals');
2023-11-25 13:48:34.056 UTC [136709] ERROR:  42809: cities is not a regular, foreign or partitioned table
2023-11-25 13:48:34.056 UTC [136709] LOCATION:  EnsureRelationKindSupported, citus_ruleutils.c:635
2023-11-25 13:48:34.056 UTC [136709] STATEMENT:  SELECT master_get_table_ddl_events('cities');
2023-11-25 13:48:34.116 UTC [136711] ERROR:  23514: no partition of relation "partitioning_hash_test_1660012" found for row
2023-11-25 13:48:34.116 UTC [136711] DETAIL:  Partition key of the failing row contains (subid) = (5).
2023-11-25 13:48:34.116 UTC [136711] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:34.116 UTC [136711] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:34.116 UTC [136711] STATEMENT:  INSERT INTO partitioning_hash_test VALUES (8, 5);
2023-11-25 13:48:34.118 UTC [136711] ERROR:  23514: no partition of relation "partitioning_hash_test_1660015" found for row
2023-11-25 13:48:34.118 UTC [136711] DETAIL:  Partition key of the failing row contains (subid) = (12).
2023-11-25 13:48:34.118 UTC [136711] CONTEXT:  while executing command on localhost:57638
2023-11-25 13:48:34.118 UTC [136711] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:34.118 UTC [136711] STATEMENT:  INSERT INTO partitioning_hash_test VALUES (9, 12);
2023-11-25 13:48:34.124 UTC [136710] ERROR:  0A000: modifications on partitions when replication factor is greater than 1 is not supported
2023-11-25 13:48:34.124 UTC [136710] HINT:  Run the query on the parent table "collections" instead.
2023-11-25 13:48:34.124 UTC [136710] LOCATION:  DeferErrorIfPartitionTableNotSingleReplicated, distributed_planner.c:1224
2023-11-25 13:48:34.124 UTC [136710] STATEMENT:  INSERT INTO collections_4 (key, ts, collection_id, value) VALUES (4, '2009-01-01', 2, 2);
2023-11-25 13:48:34.124 UTC [136710] ERROR:  0A000: modifications on partitions when replication factor is greater than 1 is not supported
2023-11-25 13:48:34.124 UTC [136710] HINT:  Run the query on the parent table "collections" instead.
2023-11-25 13:48:34.124 UTC [136710] LOCATION:  DeferErrorIfPartitionTableNotSingleReplicated, distributed_planner.c:1224
2023-11-25 13:48:34.124 UTC [136710] STATEMENT:  UPDATE collections_1 SET ts = now() WHERE key = 1;
2023-11-25 13:48:34.124 UTC [136710] ERROR:  0A000: modifications on partitions when replication factor is greater than 1 is not supported
2023-11-25 13:48:34.124 UTC [136710] HINT:  Run the query on the parent table "collections" instead.
2023-11-25 13:48:34.124 UTC [136710] LOCATION:  DeferErrorIfPartitionTableNotSingleReplicated, distributed_planner.c:1224
2023-11-25 13:48:34.124 UTC [136710] STATEMENT:  DELETE FROM collections_1 WHERE ts = now() AND key = 1;
2023-11-25 13:48:34.124 UTC [136710] ERROR:  0A000: modifications on partitions when replication factor is greater than 1 is not supported
2023-11-25 13:48:34.124 UTC [136710] HINT:  Run the query on the parent table "collections" instead.
2023-11-25 13:48:34.124 UTC [136710] LOCATION:  DeferErrorIfPartitionTableNotSingleReplicated, distributed_planner.c:1224
2023-11-25 13:48:34.124 UTC [136710] STATEMENT:  UPDATE collections_1 SET ts = now();
2023-11-25 13:48:34.124 UTC [136710] ERROR:  0A000: modifications on partitions when replication factor is greater than 1 is not supported
2023-11-25 13:48:34.124 UTC [136710] HINT:  Run the query on the parent table "collections" instead.
2023-11-25 13:48:34.124 UTC [136710] LOCATION:  DeferErrorIfPartitionTableNotSingleReplicated, distributed_planner.c:1224
2023-11-25 13:48:34.124 UTC [136710] STATEMENT:  DELETE FROM collections_1 WHERE ts = now();
2023-11-25 13:48:34.125 UTC [136710] ERROR:  0A000: modifications on partitions when replication factor is greater than 1 is not supported
2023-11-25 13:48:34.125 UTC [136710] HINT:  Run the query on the parent table "collections" instead.
2023-11-25 13:48:34.125 UTC [136710] LOCATION:  DeferErrorIfPartitionTableNotSingleReplicated, distributed_planner.c:1224
2023-11-25 13:48:34.125 UTC [136710] STATEMENT:  INSERT INTO collections_1 SELECT * FROM collections_1;
2023-11-25 13:48:34.125 UTC [136710] ERROR:  0A000: modifications on partitions when replication factor is greater than 1 is not supported
2023-11-25 13:48:34.125 UTC [136710] HINT:  Run the query on the parent table "collections" instead.
2023-11-25 13:48:34.125 UTC [136710] LOCATION:  DeferErrorIfPartitionTableNotSingleReplicated, distributed_planner.c:1224
2023-11-25 13:48:34.125 UTC [136710] STATEMENT:  INSERT INTO collections_1 SELECT * FROM collections_1 OFFSET 0;
2023-11-25 13:48:34.125 UTC [136710] ERROR:  0A000: modifications on partitions when replication factor is greater than 1 is not supported
2023-11-25 13:48:34.125 UTC [136710] HINT:  Run the query on the parent table "collections" instead.
2023-11-25 13:48:34.125 UTC [136710] LOCATION:  DeferErrorIfPartitionTableNotSingleReplicated, distributed_planner.c:1224
2023-11-25 13:48:34.125 UTC [136710] STATEMENT:  COPY collections_1 FROM STDIN;
2023-11-25 13:48:34.126 UTC [136710] ERROR:  0A000: modifications on partitions when replication factor is greater than 1 is not supported
2023-11-25 13:48:34.126 UTC [136710] HINT:  Run the query on the parent table "collections" instead.
2023-11-25 13:48:34.126 UTC [136710] LOCATION:  DeferErrorIfPartitionTableNotSingleReplicated, distributed_planner.c:1224
2023-11-25 13:48:34.126 UTC [136710] STATEMENT:  CREATE INDEX index_on_partition ON collections_1(key);
2023-11-25 13:48:34.126 UTC [136710] ERROR:  0A000: modifications on partitions when replication factor is greater than 1 is not supported
2023-11-25 13:48:34.126 UTC [136710] HINT:  Run the query on the parent table "collections" instead.
2023-11-25 13:48:34.126 UTC [136710] LOCATION:  DeferErrorIfPartitionTableNotSingleReplicated, distributed_planner.c:1224
2023-11-25 13:48:34.126 UTC [136710] STATEMENT:  UPDATE collections_1 SET ts = now() WHERE key = 1;
2023-11-25 13:48:34.127 UTC [136710] ERROR:  0A000: modifications on partitions when replication factor is greater than 1 is not supported
2023-11-25 13:48:34.127 UTC [136710] HINT:  Run the query on the parent table "collections" instead.
2023-11-25 13:48:34.127 UTC [136710] LOCATION:  DeferErrorIfPartitionTableNotSingleReplicated, distributed_planner.c:1224
2023-11-25 13:48:34.127 UTC [136710] STATEMENT:  TRUNCATE collections_1;
2023-11-25 13:48:34.127 UTC [136710] ERROR:  0A000: modifications on partitions when replication factor is greater than 1 is not supported
2023-11-25 13:48:34.127 UTC [136710] HINT:  Run the query on the parent table "collections" instead.
2023-11-25 13:48:34.127 UTC [136710] LOCATION:  DeferErrorIfPartitionTableNotSingleReplicated, distributed_planner.c:1224
2023-11-25 13:48:34.127 UTC [136710] STATEMENT:  TRUNCATE collections, collections_1;
2023-11-25 13:48:34.127 UTC [136710] ERROR:  0A000: modifications on partitions when replication factor is greater than 1 is not supported
2023-11-25 13:48:34.127 UTC [136710] HINT:  Run the query on the parent table "collections" instead.
2023-11-25 13:48:34.127 UTC [136710] LOCATION:  DeferErrorIfPartitionTableNotSingleReplicated, distributed_planner.c:1224
2023-11-25 13:48:34.127 UTC [136710] STATEMENT:  WITH collections_5_cte AS
	(
		DELETE FROM collections_5 RETURNING *
	)
	SELECT * FROM collections_5_cte;
2023-11-25 13:48:34.138 UTC [136710] ERROR:  0A000: cannot create foreign key constraint
2023-11-25 13:48:34.138 UTC [136710] DETAIL:  Citus currently supports foreign key constraints only for "citus.shard_replication_factor = 1".
2023-11-25 13:48:34.138 UTC [136710] HINT:  Please change "citus.shard_replication_factor to 1". To learn more about using foreign keys with other replication factors, please contact us at https://citusdata.com/about/contact_us.
2023-11-25 13:48:34.138 UTC [136710] LOCATION:  EnsureReferencingTableNotReplicated, foreign_constraint.c:599
2023-11-25 13:48:34.138 UTC [136710] STATEMENT:  ALTER TABLE
		collections_5
	ADD CONSTRAINT
		fkey_delete FOREIGN KEY(key)
	REFERENCES
		fkey_test(key) ON DELETE CASCADE;
2023-11-25 13:48:34.169 UTC [136711] ERROR:  XX000: cannot distribute relation "partitioning_test_failure_2009" which is partition of "partitioning_test_failure"
2023-11-25 13:48:34.169 UTC [136711] DETAIL:  Citus does not support distributing partitions if their parent is not distributed table.
2023-11-25 13:48:34.169 UTC [136711] HINT:  Distribute the partitioned table "partitioning_test_failure" instead.
2023-11-25 13:48:34.169 UTC [136711] LOCATION:  EnsureRelationCanBeDistributed, create_distributed_table.c:1895
2023-11-25 13:48:34.169 UTC [136711] STATEMENT:  SELECT create_distributed_table('partitioning_test_failure_2009', 'id');
2023-11-25 13:48:34.169 UTC [136711] ERROR:  0A000: distributing partitioned tables in only supported for hash-distributed tables
2023-11-25 13:48:34.169 UTC [136711] LOCATION:  EnsureRelationCanBeDistributed, create_distributed_table.c:1914
2023-11-25 13:48:34.169 UTC [136711] STATEMENT:  SELECT create_distributed_table('partitioning_test_failure', 'id', 'append');
2023-11-25 13:48:34.170 UTC [136711] ERROR:  0A000: distributing partitioned tables in only supported for hash-distributed tables
2023-11-25 13:48:34.170 UTC [136711] LOCATION:  EnsureRelationCanBeDistributed, create_distributed_table.c:1914
2023-11-25 13:48:34.170 UTC [136711] STATEMENT:  SELECT create_distributed_table('partitioning_test_failure', 'id', 'range');
2023-11-25 13:48:34.170 UTC [136711] ERROR:  0A000: distributing partitioned tables in only supported for hash-distributed tables
2023-11-25 13:48:34.170 UTC [136711] LOCATION:  EnsureRelationCanBeDistributed, create_distributed_table.c:1914
2023-11-25 13:48:34.170 UTC [136711] STATEMENT:  SELECT create_reference_table('partitioning_test_failure');
2023-11-25 13:48:34.178 UTC [136711] ERROR:  XX000: non-citus partitioned tables cannot have citus table partitions
2023-11-25 13:48:34.178 UTC [136711] HINT:  Distribute the partitioned table "partitioning_test_failure" instead, or add it to metadata
2023-11-25 13:48:34.178 UTC [136711] LOCATION:  ErrorIfAttachCitusTableToPgLocalTable, table.c:651
2023-11-25 13:48:34.178 UTC [136711] STATEMENT:  ALTER TABLE partitioning_test_failure ATTACH PARTITION partitioning_test_failure_2009 FOR VALUES FROM ('2009-01-01') TO ('2010-01-01');
2023-11-25 13:48:34.193 UTC [136711] ERROR:  0A000: distributing multi-level partitioned tables is not supported
2023-11-25 13:48:34.193 UTC [136711] DETAIL:  Relation "partitioning_test_failure_2009" is partitioned table itself and it is also partition of relation "partitioning_test_failure".
2023-11-25 13:48:34.193 UTC [136711] LOCATION:  EnsureRelationCanBeDistributed, create_distributed_table.c:1924
2023-11-25 13:48:34.193 UTC [136711] STATEMENT:  SELECT create_distributed_table('partitioning_test_failure', 'id');
2023-11-25 13:48:34.199 UTC [136711] ERROR:  0A000: distributing multi-level partitioned tables is not supported
2023-11-25 13:48:34.199 UTC [136711] DETAIL:  Relation "partitioning_test_failure_2009" is partitioned table itself and it is also partition of relation "partitioning_test_failure".
2023-11-25 13:48:34.199 UTC [136711] LOCATION:  EnsureRelationCanBeDistributed, create_distributed_table.c:1924
2023-11-25 13:48:34.199 UTC [136711] STATEMENT:  CREATE TABLE partitioning_test_failure_2009 PARTITION OF partitioning_test_failure FOR VALUES FROM ('2009-01-01') TO ('2010-01-01') PARTITION BY RANGE (time);
2023-11-25 13:48:34.225 UTC [136711] ERROR:  23514: no partition of relation "partitioning_test_1660001" found for row
2023-11-25 13:48:34.225 UTC [136711] DETAIL:  Partition key of the failing row contains ("time") = (2020-07-07).
2023-11-25 13:48:34.225 UTC [136711] CONTEXT:  while executing command on localhost:57638
2023-11-25 13:48:34.225 UTC [136711] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:34.225 UTC [136711] STATEMENT:  UPDATE partitioning_test SET time = '2020-07-07' WHERE id = 7;
2023-11-25 13:48:34.232 UTC [136710] ERROR:  25001: cannot open new connections after the first modification command within a transaction
2023-11-25 13:48:34.232 UTC [136710] LOCATION:  EnsureNoModificationsHaveBeenDone, worker_transaction.c:320
2023-11-25 13:48:34.232 UTC [136710] STATEMENT:  SELECT citus_copy_shard_placement(1760036, 'localhost', 57637, 'localhost', 57638, transfer_mode := 'block_writes');
2023-11-25 13:48:34.242 UTC [136840] LOG:  00000: deferred drop of orphaned resource public.shard_split_table_360047 on localhost:57638 completed
2023-11-25 13:48:34.242 UTC [136840] LOCATION:  DropOrphanedResourcesForCleanup, shard_cleaner.c:298
2023-11-25 13:48:34.242 UTC [136840] STATEMENT:  CALL citus_cleanup_orphaned_resources()
2023-11-25 13:48:34.258 UTC [136711] ERROR:  23514: updated partition constraint for default partition "partitioning_test_default_1660054" would be violated by some row
2023-11-25 13:48:34.258 UTC [136711] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:34.258 UTC [136711] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:34.258 UTC [136711] STATEMENT:  CREATE TABLE partitioning_test_2014 PARTITION OF partitioning_test FOR VALUES FROM ('2014-01-01') TO ('2015-01-01');
2023-11-25 13:48:34.302 UTC [136711] ERROR:  23514: new row for relation "partitioning_test_2009_1660005" violates partition constraint
2023-11-25 13:48:34.302 UTC [136711] DETAIL:  Failing row contains (3, 2010-03-11).
2023-11-25 13:48:34.302 UTC [136711] CONTEXT:  while executing command on localhost:57638
2023-11-25 13:48:34.302 UTC [136711] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:34.302 UTC [136711] STATEMENT:  UPDATE partitioning_test_2009 SET time = time + INTERVAL '6 month';
2023-11-25 13:48:34.339 UTC [136884] LOG:  00000: cleaned up orphaned resource partitioned_table_replicated.customer_engagements_1760036 on localhost:57638 which was left behind after a failed operation
2023-11-25 13:48:34.339 UTC [136884] LOCATION:  DropOrphanedResourcesForCleanup, shard_cleaner.c:305
2023-11-25 13:48:34.339 UTC [136884] STATEMENT:  CALL citus_cleanup_orphaned_resources()
2023-11-25 13:48:34.340 UTC [136884] LOG:  00000: cleaned up orphaned resource partitioned_table_replicated.customer_engagements_1_1760037 on localhost:57638 which was left behind after a failed operation
2023-11-25 13:48:34.340 UTC [136884] LOCATION:  DropOrphanedResourcesForCleanup, shard_cleaner.c:305
2023-11-25 13:48:34.340 UTC [136884] STATEMENT:  CALL citus_cleanup_orphaned_resources()
2023-11-25 13:48:34.340 UTC [136884] LOG:  00000: cleaned up orphaned resource partitioned_table_replicated.customer_engagements_2_1760038 on localhost:57638 which was left behind after a failed operation
2023-11-25 13:48:34.340 UTC [136884] LOCATION:  DropOrphanedResourcesForCleanup, shard_cleaner.c:305
2023-11-25 13:48:34.340 UTC [136884] STATEMENT:  CALL citus_cleanup_orphaned_resources()
2023-11-25 13:48:34.429 UTC [136711] ERROR:  2BP01: cannot drop index partitioning_test_2009_id_idx because index partitioning_index requires it
2023-11-25 13:48:34.429 UTC [136711] HINT:  You can drop index partitioning_index instead.
2023-11-25 13:48:34.429 UTC [136711] LOCATION:  reportDependentObjects, dependency.c:1055
2023-11-25 13:48:34.429 UTC [136711] STATEMENT:  DROP INDEX partitioning_test_2009_id_idx;
2023-11-25 13:48:34.452 UTC [136711] ERROR:  42809: cannot add column to a partition
2023-11-25 13:48:34.452 UTC [136711] LOCATION:  ATExecAddColumn, tablecmds.c:6753
2023-11-25 13:48:34.452 UTC [136711] STATEMENT:  ALTER TABLE partitioning_test_2010 ADD new_column_2 int;
2023-11-25 13:48:34.460 UTC [136711] ERROR:  0A000: unique constraint on partitioned table must include all partitioning columns
2023-11-25 13:48:34.460 UTC [136711] DETAIL:  PRIMARY KEY constraint on table "partitioning_test" lacks column "time" which is part of the partition key.
2023-11-25 13:48:34.460 UTC [136711] LOCATION:  DefineIndex, indexcmds.c:1035
2023-11-25 13:48:34.460 UTC [136711] STATEMENT:  ALTER TABLE partitioning_test ADD CONSTRAINT partitioning_primary PRIMARY KEY (id);
2023-11-25 13:48:34.482 UTC [136711] ERROR:  55006: cannot ALTER TABLE "partitioning_test_2009" because it is being used by active queries in this session
2023-11-25 13:48:34.482 UTC [136711] LOCATION:  CheckTableNotInUse, tablecmds.c:4005
2023-11-25 13:48:34.482 UTC [136711] STATEMENT:  ALTER TABLE partitioning_test ADD CONSTRAINT partitioning_foreign FOREIGN KEY (id) REFERENCES partitioning_test_2009 (id);
2023-11-25 13:48:34.822 UTC [136711] ERROR:  23514: no partition of relation "multi_column_partitioning_1660133" found for row
2023-11-25 13:48:34.822 UTC [136711] DETAIL:  Partition key of the failing row contains (c1, c2) = (10, 1).
2023-11-25 13:48:34.822 UTC [136711] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:34.822 UTC [136711] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:34.822 UTC [136711] STATEMENT:  INSERT INTO multi_column_partitioning VALUES(10, 1);
2023-11-25 13:48:34.835 UTC [136711] ERROR:  23514: no partition of relation "multi_column_partitioning_1660133" found for row
2023-11-25 13:48:34.835 UTC [136711] DETAIL:  Partition key of the failing row contains (c1, c2) = (20, -20).
2023-11-25 13:48:34.835 UTC [136711] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:34.835 UTC [136711] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:34.835 UTC [136711] STATEMENT:  INSERT INTO multi_column_partitioning VALUES(20, -20);
2023-11-25 13:48:35.108 UTC [136711] ERROR:  23503: insert or update on table "partitioning_test_2010_1660314" violates foreign key constraint "partitioning_reference_fkey_1660302"
2023-11-25 13:48:35.108 UTC [136711] DETAIL:  Key (id)=(1) is not present in table "reference_table_1660300".
2023-11-25 13:48:35.108 UTC [136711] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:35.108 UTC [136711] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:35.108 UTC [136711] STATEMENT:  ALTER TABLE partitioning_test ATTACH PARTITION partitioning_test_2010
	      FOR VALUES FROM ('2010-01-01') TO ('2011-01-01');
2023-11-25 13:48:35.125 UTC [136711] ERROR:  42P07: relation "partitioning_test_2011" already exists
2023-11-25 13:48:35.125 UTC [136711] LOCATION:  heap_create_with_catalog, heap.c:1146
2023-11-25 13:48:35.125 UTC [136711] STATEMENT:  CREATE TABLE partitioning_test_2011 PARTITION OF partitioning_test FOR VALUES FROM ('2011-01-01') TO ('2012-01-01');
2023-11-25 13:48:35.131 UTC [136711] ERROR:  42P07: relation "not_partition" already exists
2023-11-25 13:48:35.131 UTC [136711] LOCATION:  heap_create_with_catalog, heap.c:1146
2023-11-25 13:48:35.131 UTC [136711] STATEMENT:  CREATE TABLE not_partition PARTITION OF partitioning_test FOR VALUES FROM ('2011-01-01') TO ('2012-01-01');
2023-11-25 13:48:35.133 UTC [136711] ERROR:  42P07: relation "partition_of_other_table" already exists
2023-11-25 13:48:35.133 UTC [136711] LOCATION:  heap_create_with_catalog, heap.c:1146
2023-11-25 13:48:35.133 UTC [136711] STATEMENT:  CREATE TABLE partition_of_other_table PARTITION OF partitioning_test FOR VALUES FROM ('2014-01-01') TO ('2015-01-01');
2023-11-25 13:48:35.135 UTC [136711] ERROR:  XX000: fix_pre_citus10_partitioned_table_constraint_names can only be called for distributed partitioned tables
2023-11-25 13:48:35.135 UTC [136711] LOCATION:  fix_pre_citus10_partitioned_table_constraint_names, multi_partitioning_utils.c:105
2023-11-25 13:48:35.135 UTC [136711] STATEMENT:  SELECT fix_pre_citus10_partitioned_table_constraint_names('public.non_distributed_partitioned_table');
2023-11-25 13:48:35.135 UTC [136711] ERROR:  XX000: could not fix partition constraints: relation does not exist or is not partitioned
2023-11-25 13:48:35.135 UTC [136711] LOCATION:  fix_pre_citus10_partitioned_table_constraint_names, multi_partitioning_utils.c:100
2023-11-25 13:48:35.135 UTC [136711] STATEMENT:  SELECT fix_pre_citus10_partitioned_table_constraint_names('reference_table');
2023-11-25 13:48:35.176 UTC [136711] ERROR:  XX000: relation "multi_column_partitioned_p1" is a partition with multiple partition columns
2023-11-25 13:48:35.176 UTC [136711] DETAIL:  time_partition_range can only be used for partitions of range-partitioned tables with a single partition column
2023-11-25 13:48:35.176 UTC [136711] LOCATION:  time_partition_range, partitioning.c:102
2023-11-25 13:48:35.176 UTC [136711] STATEMENT:  SELECT * FROM time_partition_range('multi_column_partitioned_p1');
2023-11-25 13:48:35.178 UTC [136711] ERROR:  XX000: relation "list_partitioned_p1" is not a range partition
2023-11-25 13:48:35.178 UTC [136711] DETAIL:  time_partition_range can only be used for partitions of range-partitioned tables with a single partition column
2023-11-25 13:48:35.178 UTC [136711] LOCATION:  time_partition_range, partitioning.c:78
2023-11-25 13:48:35.178 UTC [136711] STATEMENT:  SELECT * FROM time_partition_range('list_partitioned_p1');
2023-11-25 13:48:35.183 UTC [136711] ERROR:  XX000: non-distributed tables cannot inherit distributed tables
2023-11-25 13:48:35.183 UTC [136711] LOCATION:  PostprocessCreateTableStmt, table.c:253
2023-11-25 13:48:35.183 UTC [136711] STATEMENT:  CREATE TABLE local_inheritance (k int) INHERITS (test_inheritance);
2023-11-25 13:48:35.289 UTC [137040] ERROR:  P0001: partition interval of date partitioned table must be day or multiple days
2023-11-25 13:48:35.289 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 53 at RAISE
2023-11-25 13:48:35.289 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:35.289 UTC [137040] STATEMENT:  SELECT * FROM get_missing_time_partition_ranges('date_partitioned_table', INTERVAL '6 hours', '2022-01-01', '2021-01-01');
2023-11-25 13:48:35.290 UTC [137040] ERROR:  P0001: partition interval of date partitioned table must be day or multiple days
2023-11-25 13:48:35.290 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 53 at RAISE
2023-11-25 13:48:35.290 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:35.290 UTC [137040] STATEMENT:  SELECT * FROM get_missing_time_partition_ranges('date_partitioned_table', INTERVAL '1 week 1 day 1 hour', '2022-01-01', '2021-01-01');
2023-11-25 13:48:35.330 UTC [137040] ERROR:  P0001: partition date_partitioned_table_2021_01_01 with the range from 01-01-2021 to 01-02-2021 does not align with the initial partition given the partition interval
2023-11-25 13:48:35.330 UTC [137040] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 13:48:35.330 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
2023-11-25 13:48:35.330 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:35.330 UTC [137040] STATEMENT:  SELECT * FROM get_missing_time_partition_ranges('date_partitioned_table', INTERVAL '2 days', '2021-01-05', '2020-12-30');
2023-11-25 13:48:35.349 UTC [137040] ERROR:  P0001: partition date_partitioned_table_2021_01_01 with the range from 01-01-2021 to 01-02-2021 does not align with the initial partition given the partition interval
2023-11-25 13:48:35.349 UTC [137040] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 13:48:35.349 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
2023-11-25 13:48:35.349 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:35.349 UTC [137040] STATEMENT:  SELECT * FROM get_missing_time_partition_ranges('date_partitioned_table', INTERVAL '2 days', '2021-01-05', '2020-12-30');
2023-11-25 13:48:35.387 UTC [137040] ERROR:  P0001: partition date_partitioned_table_2021_01_01 with the range from 01-01-2021 to 01-02-2021 does not align with the initial partition given the partition interval
2023-11-25 13:48:35.387 UTC [137040] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 13:48:35.387 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
2023-11-25 13:48:35.387 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:35.387 UTC [137040] STATEMENT:  SELECT * FROM get_missing_time_partition_ranges('date_partitioned_table', INTERVAL '2 days', '2021-01-05', '2020-12-30');
2023-11-25 13:48:35.405 UTC [137040] ERROR:  P0001: partition date_partitioned_table_2021_01_02 with the range from 01-04-2021 to 01-06-2021 does not align with the initial partition given the partition interval
2023-11-25 13:48:35.405 UTC [137040] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 13:48:35.405 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
2023-11-25 13:48:35.405 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:35.405 UTC [137040] STATEMENT:  SELECT * FROM get_missing_time_partition_ranges('date_partitioned_table', INTERVAL '2 days', '2021-01-15', '2020-12-30');
2023-11-25 13:48:35.436 UTC [137040] ERROR:  P0001: partition tstz_partitioned_table_2021_01_01 with the range from Fri Jan 01 00:00:00 2021 PST to Sat Jan 02 00:00:00 2021 PST does not align with the initial partition given the partition interval
2023-11-25 13:48:35.436 UTC [137040] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 13:48:35.436 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
2023-11-25 13:48:35.436 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:35.436 UTC [137040] STATEMENT:  SELECT * FROM get_missing_time_partition_ranges('tstz_partitioned_table', INTERVAL '2 days', '2021-01-05 00:00:00', '2020-12-30 00:00:00');
2023-11-25 13:48:35.457 UTC [137040] ERROR:  P0001: partition tstz_partitioned_table_2021_01_01 with the range from Fri Jan 01 00:00:00 2021 PST to Sat Jan 02 00:00:00 2021 PST does not align with the initial partition given the partition interval
2023-11-25 13:48:35.457 UTC [137040] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 13:48:35.457 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
2023-11-25 13:48:35.457 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:35.457 UTC [137040] STATEMENT:  SELECT * FROM get_missing_time_partition_ranges('tstz_partitioned_table', INTERVAL '2 days', '2021-01-05 00:00:00', '2020-12-30 00:00:00');
2023-11-25 13:48:35.495 UTC [137040] ERROR:  P0001: partition tstz_partitioned_table_2021_01_01 with the range from Fri Jan 01 00:00:00 2021 PST to Sat Jan 02 00:00:00 2021 PST does not align with the initial partition given the partition interval
2023-11-25 13:48:35.495 UTC [137040] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 13:48:35.495 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
2023-11-25 13:48:35.495 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:35.495 UTC [137040] STATEMENT:  SELECT * FROM get_missing_time_partition_ranges('tstz_partitioned_table', INTERVAL '2 days', '2021-01-05 00:00:00', '2020-12-30 00:00:00');
2023-11-25 13:48:35.514 UTC [137040] ERROR:  P0001: partition tstz_partitioned_table_2021_01_02 with the range from Mon Jan 04 00:00:00 2021 PST to Wed Jan 06 00:00:00 2021 PST does not align with the initial partition given the partition interval
2023-11-25 13:48:35.514 UTC [137040] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 13:48:35.514 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
2023-11-25 13:48:35.514 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:35.514 UTC [137040] STATEMENT:  SELECT * FROM get_missing_time_partition_ranges('tstz_partitioned_table', INTERVAL '2 days', '2021-01-15 00:00:00', '2020-12-30 00:00:00');
2023-11-25 13:48:35.553 UTC [137040] ERROR:  P0001: partition tswtz_partitioned_table_2021_01_01 with the range from Fri Jan 01 00:00:00 2021 to Sat Jan 02 00:00:00 2021 does not align with the initial partition given the partition interval
2023-11-25 13:48:35.553 UTC [137040] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 13:48:35.553 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
2023-11-25 13:48:35.553 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:35.553 UTC [137040] STATEMENT:  SELECT * FROM get_missing_time_partition_ranges('tswtz_partitioned_table', INTERVAL '2 days', '2021-01-05 00:00:00', '2020-12-30 00:00:00');
2023-11-25 13:48:35.572 UTC [137040] ERROR:  P0001: partition tswtz_partitioned_table_2021_01_01 with the range from Fri Jan 01 00:00:00 2021 to Sat Jan 02 00:00:00 2021 does not align with the initial partition given the partition interval
2023-11-25 13:48:35.572 UTC [137040] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 13:48:35.572 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
2023-11-25 13:48:35.572 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:35.572 UTC [137040] STATEMENT:  SELECT * FROM get_missing_time_partition_ranges('tswtz_partitioned_table', INTERVAL '2 days', '2021-01-05 00:00:00', '2020-12-30 00:00:00');
2023-11-25 13:48:35.609 UTC [137040] ERROR:  P0001: partition tswtz_partitioned_table_2021_01_01 with the range from Fri Jan 01 00:00:00 2021 to Sat Jan 02 00:00:00 2021 does not align with the initial partition given the partition interval
2023-11-25 13:48:35.609 UTC [137040] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 13:48:35.609 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
2023-11-25 13:48:35.609 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:35.609 UTC [137040] STATEMENT:  SELECT * FROM get_missing_time_partition_ranges('tswtz_partitioned_table', INTERVAL '2 days', '2021-01-05 00:00:00', '2020-12-30 00:00:00');
2023-11-25 13:48:35.628 UTC [137040] ERROR:  P0001: partition tswtz_partitioned_table_2021_01_02 with the range from Mon Jan 04 00:00:00 2021 to Wed Jan 06 00:00:00 2021 does not align with the initial partition given the partition interval
2023-11-25 13:48:35.628 UTC [137040] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 13:48:35.628 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
2023-11-25 13:48:35.628 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:35.628 UTC [137040] STATEMENT:  SELECT * FROM get_missing_time_partition_ranges('tswtz_partitioned_table', INTERVAL '2 days', '2021-01-15 00:00:00', '2020-12-30 00:00:00');
2023-11-25 13:48:35.648 UTC [137040] ERROR:  P0001: partition interval of date partitioned table must be day or multiple days
2023-11-25 13:48:35.648 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 53 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 13:48:35.648 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:35.648 UTC [137040] STATEMENT:  SELECT create_time_partitions('date_partitioned_table', INTERVAL '6 hours', '2022-01-01', '2021-01-01');
2023-11-25 13:48:35.648 UTC [137040] ERROR:  P0001: partition interval of date partitioned table must be day or multiple days
2023-11-25 13:48:35.648 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 53 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 13:48:35.648 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:35.648 UTC [137040] STATEMENT:  SELECT create_time_partitions('date_partitioned_table', INTERVAL '1 week 1 day 1 hour', '2022-01-01', '2021-01-01');
2023-11-25 13:48:35.731 UTC [137040] ERROR:  P0001: start_from (Mon Feb 01 00:00:00 2021 PST) must be older than end_at (Fri Jan 01 00:00:00 2021 PST)
2023-11-25 13:48:35.731 UTC [137040] CONTEXT:  PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 12 at RAISE
2023-11-25 13:48:35.731 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:35.731 UTC [137040] STATEMENT:  SELECT * FROM create_time_partitions('date_partitioned_table', INTERVAL '1 day', '2021-01-01', '2021-02-01');
2023-11-25 13:48:35.738 UTC [137040] ERROR:  P0001: partition date_partitioned_table_p2020_12_30 with the range from 12-30-2020 to 12-31-2020 does not align with the initial partition given the partition interval
2023-11-25 13:48:35.738 UTC [137040] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 13:48:35.738 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 13:48:35.738 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:35.738 UTC [137040] STATEMENT:  SELECT create_time_partitions('date_partitioned_table', INTERVAL '2 days', '2021-01-15', '2020-12-25');
2023-11-25 13:48:35.745 UTC [137040] ERROR:  P0001: partition date_partitioned_table_p2020_12_30 with the range from 12-30-2020 to 12-31-2020 does not align with the initial partition given the partition interval
2023-11-25 13:48:35.745 UTC [137040] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 13:48:35.745 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 13:48:35.745 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:35.745 UTC [137040] STATEMENT:  SELECT create_time_partitions('date_partitioned_table', INTERVAL '2 days', '2021-01-05', '2020-12-30');
2023-11-25 13:48:35.755 UTC [137040] ERROR:  P0001: partition date_partitioned_table_2020_01_02 with the range from 01-02-2021 to 01-04-2021 does not align with the initial partition given the partition interval
2023-11-25 13:48:35.755 UTC [137040] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 13:48:35.755 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 13:48:35.755 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:35.755 UTC [137040] STATEMENT:  SELECT create_time_partitions('date_partitioned_table', INTERVAL '1 day', '2021-01-05', '2020-12-30');
2023-11-25 13:48:35.758 UTC [137040] ERROR:  P0001: partition date_partitioned_table_2021_01_02 with the range from 01-04-2021 to 01-06-2021 does not align with the initial partition given the partition interval
2023-11-25 13:48:35.758 UTC [137040] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 13:48:35.758 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 13:48:35.758 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:35.758 UTC [137040] STATEMENT:  SELECT create_time_partitions('date_partitioned_table', INTERVAL '2 days', '2021-01-15', '2020-12-30');
2023-11-25 13:48:35.812 UTC [137040] ERROR:  P0001: start_from (Tue Jan 05 00:00:00 2021 PST) must be older than end_at (Fri Jan 01 00:00:00 2021 PST)
2023-11-25 13:48:35.812 UTC [137040] CONTEXT:  PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 12 at RAISE
2023-11-25 13:48:35.812 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:35.812 UTC [137040] STATEMENT:  SELECT create_time_partitions('tstz_partitioned_table', INTERVAL '1 day', '2021-01-01 00:00:00', '2021-01-05 00:00:00');
2023-11-25 13:48:35.818 UTC [137040] ERROR:  P0001: partition tstz_partitioned_table_p2020_12_30 with the range from Wed Dec 30 00:00:00 2020 PST to Thu Dec 31 00:00:00 2020 PST does not align with the initial partition given the partition interval
2023-11-25 13:48:35.818 UTC [137040] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 13:48:35.818 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
2023-11-25 13:48:35.818 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:35.818 UTC [137040] STATEMENT:  SELECT * FROM get_missing_time_partition_ranges('tstz_partitioned_table', INTERVAL '2 days', '2021-01-05 00:00:00', '2020-12-30 00:00:00');
2023-11-25 13:48:35.825 UTC [137040] ERROR:  P0001: partition tstz_partitioned_table_p2020_12_30 with the range from Wed Dec 30 00:00:00 2020 PST to Thu Dec 31 00:00:00 2020 PST does not align with the initial partition given the partition interval
2023-11-25 13:48:35.825 UTC [137040] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 13:48:35.825 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 13:48:35.825 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:35.825 UTC [137040] STATEMENT:  SELECT create_time_partitions('tstz_partitioned_table', INTERVAL '2 days', '2021-01-05 00:00:00', '2020-12-30 00:00:00');
2023-11-25 13:48:35.835 UTC [137040] ERROR:  P0001: partition tstz_partitioned_table_2021_01_01 with the range from Fri Jan 01 00:00:00 2021 PST to Sat Jan 02 00:00:00 2021 PST does not align with the initial partition given the partition interval
2023-11-25 13:48:35.835 UTC [137040] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 13:48:35.835 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 13:48:35.835 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:35.835 UTC [137040] STATEMENT:  SELECT create_time_partitions('tstz_partitioned_table', INTERVAL '2 days', '2021-01-05 00:00:00', '2020-12-30 00:00:00');
2023-11-25 13:48:35.837 UTC [137040] ERROR:  P0001: partition tstz_partitioned_table_2021_01_02 with the range from Mon Jan 04 00:00:00 2021 PST to Wed Jan 06 00:00:00 2021 PST does not align with the initial partition given the partition interval
2023-11-25 13:48:35.837 UTC [137040] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 13:48:35.837 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 13:48:35.837 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:35.837 UTC [137040] STATEMENT:  SELECT create_time_partitions('tstz_partitioned_table', INTERVAL '2 days', '2021-01-15 00:00:00', '2020-12-30 00:00:00');
2023-11-25 13:48:36.297 UTC [137040] ERROR:  P0001: start_from (Tue Jan 05 00:00:00 2021 PST) must be older than end_at (Fri Jan 01 00:00:00 2021 PST)
2023-11-25 13:48:36.297 UTC [137040] CONTEXT:  PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 12 at RAISE
2023-11-25 13:48:36.297 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:36.297 UTC [137040] STATEMENT:  SELECT create_time_partitions('tswtz_partitioned_table', INTERVAL '1 day', '2021-01-01 00:00:00', '2021-01-05 00:00:00');
2023-11-25 13:48:36.350 UTC [137040] ERROR:  P0001: partition tswtz_partitioned_table_p2020_12_30 with the range from Wed Dec 30 00:00:00 2020 to Thu Dec 31 00:00:00 2020 does not align with the initial partition given the partition interval
2023-11-25 13:48:36.350 UTC [137040] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 13:48:36.350 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 13:48:36.350 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:36.350 UTC [137040] STATEMENT:  SELECT create_time_partitions('tswtz_partitioned_table', INTERVAL '2 days', '2021-01-05 00:00:00', '2020-12-30 00:00:00');
2023-11-25 13:48:36.397 UTC [137040] ERROR:  P0001: partition tswtz_partitioned_table_p2020_12_30 with the range from Wed Dec 30 00:00:00 2020 to Thu Dec 31 00:00:00 2020 does not align with the initial partition given the partition interval
2023-11-25 13:48:36.397 UTC [137040] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 13:48:36.397 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 13:48:36.397 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:36.397 UTC [137040] STATEMENT:  SELECT create_time_partitions('tswtz_partitioned_table', INTERVAL '2 days', '2021-01-05 00:00:00', '2020-12-30 00:00:00');
2023-11-25 13:48:36.478 UTC [137040] ERROR:  P0001: partition tswtz_partitioned_table_2020_01_01 with the range from Fri Jan 01 00:00:00 2021 to Sat Jan 02 00:00:00 2021 does not align with the initial partition given the partition interval
2023-11-25 13:48:36.478 UTC [137040] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 13:48:36.478 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 13:48:36.478 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:36.478 UTC [137040] STATEMENT:  SELECT create_time_partitions('tswtz_partitioned_table', INTERVAL '2 days', '2021-01-05 00:00:00', '2020-12-30 00:00:00');
2023-11-25 13:48:36.498 UTC [137040] ERROR:  P0001: partition tswtz_partitioned_table_2020_01_02 with the range from Mon Jan 04 00:00:00 2021 to Wed Jan 06 00:00:00 2021 does not align with the initial partition given the partition interval
2023-11-25 13:48:36.498 UTC [137040] HINT:  Only use partitions of the same size, without gaps between partitions.
2023-11-25 13:48:36.498 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 150 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 13:48:36.498 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:36.498 UTC [137040] STATEMENT:  SELECT create_time_partitions('tswtz_partitioned_table', INTERVAL '2 days', '2021-01-15 00:00:00', '2020-12-30 00:00:00');
2023-11-25 13:48:36.740 UTC [137040] ERROR:  P0001: partition interval of date partitioned table must be day or multiple days
2023-11-25 13:48:36.740 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 53 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 13:48:36.740 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:36.740 UTC [137040] STATEMENT:  SELECT create_time_partitions('date_distributed_partitioned_table', INTERVAL '6 hours', '2022-01-01', '2021-01-01');
2023-11-25 13:48:36.741 UTC [137040] ERROR:  P0001: partition interval of date partitioned table must be day or multiple days
2023-11-25 13:48:36.741 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 53 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 13:48:36.741 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:36.741 UTC [137040] STATEMENT:  SELECT create_time_partitions('date_distributed_partitioned_table', INTERVAL '1 week 1 day 1 hour', '2022-01-01', '2021-01-01');
2023-11-25 13:48:37.386 UTC [137110] LOG:  00000: cleaned up orphaned resource partitioned_table_replicated.customer_engagements_1760036 on localhost:57638 which was left behind after a failed operation
2023-11-25 13:48:37.386 UTC [137110] LOCATION:  DropOrphanedResourcesForCleanup, shard_cleaner.c:305
2023-11-25 13:48:37.386 UTC [137110] STATEMENT:  CALL citus_cleanup_orphaned_resources()
2023-11-25 13:48:37.386 UTC [137110] LOG:  00000: cleaned up orphaned resource partitioned_table_replicated.customer_engagements_1_1760037 on localhost:57638 which was left behind after a failed operation
2023-11-25 13:48:37.386 UTC [137110] LOCATION:  DropOrphanedResourcesForCleanup, shard_cleaner.c:305
2023-11-25 13:48:37.386 UTC [137110] STATEMENT:  CALL citus_cleanup_orphaned_resources()
2023-11-25 13:48:37.386 UTC [137110] LOG:  00000: cleaned up orphaned resource partitioned_table_replicated.customer_engagements_2_1760038 on localhost:57638 which was left behind after a failed operation
2023-11-25 13:48:37.386 UTC [137110] LOCATION:  DropOrphanedResourcesForCleanup, shard_cleaner.c:305
2023-11-25 13:48:37.386 UTC [137110] STATEMENT:  CALL citus_cleanup_orphaned_resources()
2023-11-25 13:48:37.518 UTC [137040] ERROR:  P0001: partition interval of date partitioned table must be day or multiple days
2023-11-25 13:48:37.518 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 53 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 13:48:37.518 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:37.518 UTC [137040] STATEMENT:  SELECT create_time_partitions('date_partitioned_citus_local_table', INTERVAL '6 hours', '2022-01-01', '2021-01-01');
2023-11-25 13:48:37.519 UTC [137040] ERROR:  P0001: partition interval of date partitioned table must be day or multiple days
2023-11-25 13:48:37.519 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 53 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 13:48:37.519 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:37.519 UTC [137040] STATEMENT:  SELECT create_time_partitions('date_partitioned_citus_local_table', INTERVAL '1 week 1 day 1 hour', '2022-01-01', '2021-01-01');
2023-11-25 13:48:39.500 UTC [137040] ERROR:  P0001: partitioned tables with multiple partition columns are not supported
2023-11-25 13:48:39.500 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 33 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 13:48:39.500 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:39.500 UTC [137040] STATEMENT:  SELECT create_time_partitions('multiple_partition_column_table', INTERVAL '1 month', now() + INTERVAL '1 year');
2023-11-25 13:48:39.500 UTC [137040] ERROR:  P0001: partitioned tables with multiple partition columns are not supported
2023-11-25 13:48:39.500 UTC [137040] CONTEXT:  PL/pgSQL function drop_old_time_partitions(regclass,timestamp with time zone) line 17 at RAISE
2023-11-25 13:48:39.500 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:39.500 UTC [137040] STATEMENT:  CALL drop_old_time_partitions('multiple_partition_column_table', now());
2023-11-25 13:48:39.502 UTC [137040] ERROR:  P0001: type of the partition column of the table invalid_partition_column_table must be date, timestamp or timestamptz
2023-11-25 13:48:39.502 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 47 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 13:48:39.502 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:39.502 UTC [137040] STATEMENT:  SELECT create_time_partitions('invalid_partition_column_table', INTERVAL '1 month', now() + INTERVAL '1 year');
2023-11-25 13:48:39.502 UTC [137040] ERROR:  P0001: type of the partition column of the table invalid_partition_column_table must be date, timestamp or timestamptz
2023-11-25 13:48:39.502 UTC [137040] CONTEXT:  PL/pgSQL function drop_old_time_partitions(regclass,timestamp with time zone) line 29 at RAISE
2023-11-25 13:48:39.502 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:39.502 UTC [137040] STATEMENT:  CALL drop_old_time_partitions('invalid_partition_column_table', now());
2023-11-25 13:48:39.504 UTC [137040] ERROR:  P0001: non_partitioned_table is not partitioned
2023-11-25 13:48:39.504 UTC [137040] CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 31 at RAISE
	PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
2023-11-25 13:48:39.504 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:39.504 UTC [137040] STATEMENT:  SELECT create_time_partitions('non_partitioned_table', INTERVAL '1 month', now() + INTERVAL '1 year');
2023-11-25 13:48:39.504 UTC [137040] ERROR:  P0001: non_partitioned_table is not partitioned
2023-11-25 13:48:39.504 UTC [137040] CONTEXT:  PL/pgSQL function drop_old_time_partitions(regclass,timestamp with time zone) line 15 at RAISE
2023-11-25 13:48:39.504 UTC [137040] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:39.504 UTC [137040] STATEMENT:  CALL drop_old_time_partitions('non_partitioned_table', now());
2023-11-25 13:48:40.237 UTC [137207] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a reference table
2023-11-25 13:48:40.237 UTC [137207] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:671
2023-11-25 13:48:40.237 UTC [137207] STATEMENT:  SELECT (SELECT id FROM dist WHERE dist.id > d1.id GROUP BY id) FROM ref FULL JOIN dist d1 USING (id);
2023-11-25 13:48:41.249 UTC [137474] ERROR:  0A000: recursive CTEs are not supported in distributed queries
2023-11-25 13:48:41.249 UTC [137474] LOCATION:  RecursivelyPlanCTEs, recursive_planning.c:1089
2023-11-25 13:48:41.249 UTC [137474] STATEMENT:  SELECT
	   bar.user_id
	FROM
	    (
		     WITH RECURSIVE cte AS MATERIALIZED (
		    SELECT
		    	DISTINCT users_table.user_id
		     FROM
		     	users_table, events_table
		     WHERE
		     	users_table.user_id = events_table.user_id AND
		     event_type IN (1,2,3,4)
		     ) SELECT * FROM cte ORDER BY 1 DESC
	     ) as foo,
	    (
		    SELECT
		    	DISTINCT users_table.user_id
		     FROM
		     	users_table, events_table
		     WHERE
		     	users_table.user_id = events_table.user_id AND
		     event_type IN (1,2,3,4)
	     ) as bar
	WHERE foo.user_id = bar.user_id
	ORDER BY 1 DESC;
2023-11-25 13:48:41.340 UTC [137474] ERROR:  P0001: (3/3) failed to execute one of the tasks
2023-11-25 13:48:41.340 UTC [137474] CONTEXT:  PL/pgSQL function inline_code_block line 31 at RAISE
2023-11-25 13:48:41.340 UTC [137474] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:48:41.340 UTC [137474] STATEMENT:  DO $$
	DECLARE
		errors_received INTEGER;
	BEGIN
	errors_received := 0;
	FOR i IN 1..3 LOOP
		BEGIN
			WITH cte as (
				SELECT
					user_id, value_2
				from
					events_table
			)
			SELECT * FROM users_table where value_2 < (
				SELECT
					min(cte.value_2)
				FROM
					cte
				WHERE
					users_table.user_id=cte.user_id
				GROUP BY
					user_id, cte.value_2);
		EXCEPTION WHEN OTHERS THEN
			IF SQLERRM LIKE 'more than one row returned by a subquery%%' THEN
				errors_received := errors_received + 1;
			ELSIF SQLERRM LIKE 'failed to execute task%' THEN
				errors_received := errors_received + 1;
			END IF;
		END;
	END LOOP;
	RAISE '(%/3) failed to execute one of the tasks', errors_received;
	END;
	$$;
2023-11-25 13:48:41.723 UTC [137603] ERROR:  0A000: cannot pushdown the subquery since not all subqueries in the UNION have the partition column in the same position
2023-11-25 13:48:41.723 UTC [137603] DETAIL:  Each leaf query of the UNION should return the partition column in the same position and all joins must be on the partition column
2023-11-25 13:48:41.723 UTC [137603] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:582
2023-11-25 13:48:41.723 UTC [137603] STATEMENT:  SELECT * FROM ((SELECT * FROM test) UNION (SELECT * FROM test)) foo WHERE x IN (SELECT y FROM test);
2023-11-25 13:48:41.772 UTC [137603] ERROR:  0A000: cannot push down this subquery
2023-11-25 13:48:41.772 UTC [137603] DETAIL:  Complex subqueries and CTEs are not supported within a UNION
2023-11-25 13:48:41.772 UTC [137603] LOCATION:  DeferErrorIfUnsupportedUnionQuery, query_pushdown_planning.c:1362
2023-11-25 13:48:41.772 UTC [137603] STATEMENT:  SELECT * FROM test a WHERE x IN (SELECT x FROM test b UNION SELECT y FROM test c WHERE a.x = c.x) ORDER BY 1,2;
2023-11-25 13:48:41.789 UTC [137603] ERROR:  0A000: cannot compute aggregate (distinct)
2023-11-25 13:48:41.789 UTC [137603] DETAIL:  table partitioning is unsuitable for aggregate (distinct)
2023-11-25 13:48:41.789 UTC [137603] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4205
2023-11-25 13:48:41.789 UTC [137603] STATEMENT:  select count(DISTINCT t.x) FROM ((SELECT avg(DISTINCT y) FROM test GROUP BY y) UNION (SELECT avg(DISTINCT y) FROM test GROUP BY y)) as t(x) ORDER BY 1;
2023-11-25 13:48:42.369 UTC [137808] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:42.369 UTC [137808] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:42.369 UTC [137808] STATEMENT:  EXPLAIN
	   SELECT
	    (SELECT user_id FROM users_table_part WHERE user_id = e.value_1
	        UNION ALL
	     SELECT user_id FROM users_table_part WHERE user_id = e.value_1)
	  FROM
	    (SELECT * FROM users_table_part) as e;
2023-11-25 13:48:42.398 UTC [137808] WARNING:  0A000: "view v2" has dependency to "table range_dist_table_2" that is not in Citus' metadata
2023-11-25 13:48:42.398 UTC [137808] DETAIL:  "view v2" will be created only locally
2023-11-25 13:48:42.398 UTC [137808] HINT:  Distribute "table range_dist_table_2" first to distribute "view v2"
2023-11-25 13:48:42.398 UTC [137808] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:950
2023-11-25 13:48:42.484 UTC [137808] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:42.484 UTC [137808] CONTEXT:  PL/pgSQL function public.explain_has_distributed_subplan(text) line 5 at FOR over EXECUTE statement
2023-11-25 13:48:42.484 UTC [137808] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:42.484 UTC [137808] STATEMENT:  SELECT public.explain_has_distributed_subplan($$
	EXPLAIN SELECT * FROM users_table_part u1 WHERE (value_1, user_id) IN
	(
	SELECT u1.user_id, user_id FROM users_table_part
	UNION
	SELECT u1.user_id, user_id FROM users_table_part
	);
	$$);
2023-11-25 13:48:42.672 UTC [137870] ERROR:  22012: division by zero
2023-11-25 13:48:42.672 UTC [137870] LOCATION:  int4div, int.c:840
2023-11-25 13:48:42.672 UTC [137870] STATEMENT:  (SELECT x FROM test) INTERSECT (SELECT i/0 FROM generate_series(0, 100) i) ORDER BY 1 DESC;
2023-11-25 13:48:42.921 UTC [138001] ERROR:  42P01: relation "events_table_local" does not exist at character 130
2023-11-25 13:48:42.921 UTC [138001] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 13:48:42.921 UTC [138001] STATEMENT:  SELECT COUNT(user_id) FROM users_table WHERE user_id IN
		(SELECT
			user_id
		 FROM
		 	users_table_local JOIN (SELECT user_id FROM events_table_local) as foo
		 USING (user_id)
		 );
2023-11-25 13:48:42.923 UTC [138001] ERROR:  0A000: cannot compute aggregate (distinct)
2023-11-25 13:48:42.923 UTC [138001] DETAIL:  table partitioning is unsuitable for aggregate (distinct)
2023-11-25 13:48:42.923 UTC [138001] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4205
2023-11-25 13:48:42.923 UTC [138001] STATEMENT:  SELECT
		*
	FROM
	(
		SELECT avg(DISTINCT value_1), random() FROM users_table GROUP BY user_id OFFSET 3
	) as baz,
	(
		SELECT count(DISTINCT value_1), random() FROM users_table GROUP BY value_2 OFFSET 3
	) as bar,
	(
		SELECT avg(DISTINCT value_1), random() FROM users_table GROUP BY value_2 OFFSET 3
	) as foo;
2023-11-25 13:48:42.924 UTC [138001] ERROR:  0A000: array_agg with order by is unsupported
2023-11-25 13:48:42.924 UTC [138001] LOCATION:  DeferErrorIfUnsupportedArrayAggregate, multi_logical_optimizer.c:4008
2023-11-25 13:48:42.924 UTC [138001] STATEMENT:  SELECT
		*
	FROM
		(
			SELECT
				array_agg(users_table.value_2 ORDER BY users_table.time)
			FROM
				users_table, (SELECT user_id FROM events_table) as evs
			WHERE users_table.user_id = evs.user_id
			GROUP BY users_table.value_2
			LIMIT 5
		) as foo;
2023-11-25 13:48:42.926 UTC [138001] ERROR:  0A000: cannot handle complex subqueries when the router executor is disabled
2023-11-25 13:48:42.926 UTC [138001] LOCATION:  QueryPushdownSqlTaskList, multi_physical_planner.c:2182
2023-11-25 13:48:42.926 UTC [138001] STATEMENT:  SELECT
	   user_id
	FROM
	    (SELECT
	    	DISTINCT users_table.user_id
	     FROM
	     	users_table, events_table
	     WHERE
	     	users_table.user_id = events_table.user_id AND
	     event_type IN (1,2,3,4)
	     ORDER BY 1 DESC LIMIT 5
	     ) as foo
	    ORDER BY 1 DESC;
2023-11-25 13:48:42.939 UTC [138001] ERROR:  0A000: could not run distributed query with GROUPING SETS, CUBE, or ROLLUP
2023-11-25 13:48:42.939 UTC [138001] HINT:  Consider using an equality filter on the distributed table's partition column.
2023-11-25 13:48:42.939 UTC [138001] LOCATION:  DeferErrorIfQueryNotSupported, multi_logical_planner.c:945
2023-11-25 13:48:42.939 UTC [138001] STATEMENT:  SELECT * FROM (SELECT user_id, value_1 FROM users_table GROUP BY GROUPING SETS ((user_id), (value_1))) s;
2023-11-25 13:48:42.939 UTC [138001] ERROR:  0A000: could not run distributed query with GROUPING SETS, CUBE, or ROLLUP
2023-11-25 13:48:42.939 UTC [138001] HINT:  Consider using an equality filter on the distributed table's partition column.
2023-11-25 13:48:42.939 UTC [138001] LOCATION:  DeferErrorIfQueryNotSupported, multi_logical_planner.c:945
2023-11-25 13:48:42.939 UTC [138001] STATEMENT:  SELECT * FROM (SELECT user_id, value_1 FROM users_table GROUP BY ROLLUP (user_id, value_1)) s;
2023-11-25 13:48:42.939 UTC [138001] ERROR:  0A000: could not run distributed query with GROUPING SETS, CUBE, or ROLLUP
2023-11-25 13:48:42.939 UTC [138001] HINT:  Consider using an equality filter on the distributed table's partition column.
2023-11-25 13:48:42.939 UTC [138001] LOCATION:  DeferErrorIfQueryNotSupported, multi_logical_planner.c:945
2023-11-25 13:48:42.939 UTC [138001] STATEMENT:  SELECT * FROM (SELECT user_id, value_1 FROM users_table GROUP BY CUBE (user_id, value_1)) s;
2023-11-25 13:48:43.049 UTC [137999] WARNING:  0A000: "view subquery_from_from_where_local_table" has dependency to "table events_table_local" that is not in Citus' metadata
2023-11-25 13:48:43.049 UTC [137999] DETAIL:  "view subquery_from_from_where_local_table" will be created only locally
2023-11-25 13:48:43.049 UTC [137999] HINT:  Distribute "table events_table_local" first to distribute "view subquery_from_from_where_local_table"
2023-11-25 13:48:43.049 UTC [137999] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:950
2023-11-25 13:48:43.059 UTC [138002] WARNING:  0A000: "view subquery_and_ctes" has dependency to "table users_table_local" that is not in Citus' metadata
2023-11-25 13:48:43.059 UTC [138002] DETAIL:  "view subquery_and_ctes" will be created only locally
2023-11-25 13:48:43.059 UTC [138002] HINT:  Distribute "table users_table_local" first to distribute "view subquery_and_ctes"
2023-11-25 13:48:43.059 UTC [138002] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:950
2023-11-25 13:48:43.172 UTC [137999] WARNING:  0A000: "view all_executors_view" has dependency to "table users_table_local" that is not in Citus' metadata
2023-11-25 13:48:43.172 UTC [137999] DETAIL:  "view all_executors_view" will be created only locally
2023-11-25 13:48:43.172 UTC [137999] HINT:  Distribute "table users_table_local" first to distribute "view all_executors_view"
2023-11-25 13:48:43.172 UTC [137999] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:950
2023-11-25 13:48:43.261 UTC [137999] WARNING:  0A000: "view subquery_and_ctes" has dependency to "table users_table_local" that is not in Citus' metadata
2023-11-25 13:48:43.261 UTC [137999] DETAIL:  "view subquery_and_ctes" will be created only locally
2023-11-25 13:48:43.261 UTC [137999] HINT:  Distribute "table users_table_local" first to distribute "view subquery_and_ctes"
2023-11-25 13:48:43.261 UTC [137999] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:950
2023-11-25 13:48:43.277 UTC [137999] WARNING:  0A000: "view subquery_and_ctes_second" has dependency to "table users_table_local" that is not in Citus' metadata
2023-11-25 13:48:43.277 UTC [137999] DETAIL:  "view subquery_and_ctes_second" will be created only locally
2023-11-25 13:48:43.277 UTC [137999] HINT:  Distribute "table users_table_local" first to distribute "view subquery_and_ctes_second"
2023-11-25 13:48:43.277 UTC [137999] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:950
2023-11-25 13:48:43.689 UTC [138312] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a CTE or subquery
2023-11-25 13:48:43.689 UTC [138312] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:684
2023-11-25 13:48:43.689 UTC [138312] STATEMENT:  WITH event_id
	     AS(SELECT user_id AS events_user_id,
	                time    AS events_time,
	                event_type
	         FROM   events_table)
	SELECT Count(*)
	FROM   event_id
	WHERE  events_user_id IN (SELECT user_id
	                          FROM   users_table
	                          WHERE  users_table.time = events_time);
2023-11-25 13:48:43.770 UTC [138311] ERROR:  XX000: the query contains a join that requires repartitioning
2023-11-25 13:48:43.770 UTC [138311] HINT:  Set citus.enable_repartition_joins to on to enable repartitioning
2023-11-25 13:48:43.770 UTC [138311] LOCATION:  JobExecutorType, multi_server_executor.c:68
2023-11-25 13:48:43.770 UTC [138311] STATEMENT:  SELECT key, count(*) FROM (SELECT *, random() FROM append_table a JOIN append_table b USING (key)) u GROUP BY key ORDER BY 1,2 LIMIT 3;
2023-11-25 13:48:43.776 UTC [138310] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:43.776 UTC [138310] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:43.776 UTC [138310] STATEMENT:  SELECT event_type, (SELECT max(time) FROM users_table WHERE user_id = e.user_id GROUP BY user_id)
	FROM users_table u JOIN events_table e USING (value_2)
	ORDER BY 1,2 LIMIT 1;
2023-11-25 13:48:43.776 UTC [138310] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:43.776 UTC [138310] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:43.776 UTC [138310] STATEMENT:  SELECT event_type, (SELECT max(time) FROM users_table WHERE user_id = e.value_2)
	FROM events_table e
	ORDER BY 1,2 LIMIT 1;
2023-11-25 13:48:43.776 UTC [138310] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:43.776 UTC [138310] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:43.776 UTC [138310] STATEMENT:  SELECT event_type, (SELECT max(time) FROM users_table WHERE user_id = e.value_2 GROUP BY user_id)
	FROM events_table e
	ORDER BY 1,2 LIMIT 1;
2023-11-25 13:48:43.780 UTC [138311] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:43.780 UTC [138311] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:43.780 UTC [138311] STATEMENT:  SELECT key, value FROM append_table a WHERE key IN (SELECT key FROM append_table WHERE value > 100) ORDER BY 1,2;
2023-11-25 13:48:43.805 UTC [138310] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a reference table
2023-11-25 13:48:43.805 UTC [138310] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:671
2023-11-25 13:48:43.805 UTC [138310] STATEMENT:  SELECT (SELECT max(u1.time) FROM users_table u1 JOIN users_reference_table u2 USING (user_id) WHERE u2.user_id = e.user_id GROUP BY user_id), 5
	FROM events_reference_table e
	GROUP BY 1
	ORDER BY 1,2 LIMIT 1;
2023-11-25 13:48:43.806 UTC [138310] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:43.806 UTC [138310] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:43.806 UTC [138310] STATEMENT:  SELECT event_type, (SELECT max(time) FROM users_table WHERE user_id = e.user_id GROUP BY user_id)
	FROM users_table u JOIN events_table e USING (value_2)
	ORDER BY 1,2 LIMIT 1;
2023-11-25 13:48:43.816 UTC [138311] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:43.816 UTC [138311] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:43.816 UTC [138311] STATEMENT:  DELETE FROM append_table a USING append_table b WHERE a.key = b.key;
2023-11-25 13:48:43.835 UTC [138311] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:43.835 UTC [138311] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:43.835 UTC [138311] STATEMENT:  UPDATE append_table a sET extra = 1 FROM append_table b WHERE a.key = b.key;
2023-11-25 13:48:43.840 UTC [138310] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a CTE or subquery
2023-11-25 13:48:43.840 UTC [138310] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:684
2023-11-25 13:48:43.840 UTC [138310] STATEMENT:  WITH cte_1 AS (SELECT min(user_id) u, max(time) m FROM users_table)
	SELECT count(*), (SELECT max(time) FROM users_table WHERE user_id = cte_1.u GROUP BY user_id)
	FROM cte_1
	GROUP BY 2
	ORDER BY 1,2 LIMIT 1;
2023-11-25 13:48:43.851 UTC [138310] ERROR:  0A000: cannot push down subquery on the target list
2023-11-25 13:48:43.851 UTC [138310] DETAIL:  Subqueries in the SELECT part of the query can only be pushed down if they happen before aggregates and window functions
2023-11-25 13:48:43.851 UTC [138310] LOCATION:  MultiLogicalPlanOptimize, multi_logical_optimizer.c:479
2023-11-25 13:48:43.851 UTC [138310] STATEMENT:  SELECT sum(e.user_id) + (SELECT max(value_3) FROM users_reference_table WHERE value_2 = e.value_2 GROUP BY user_id)
	FROM events_table e
	GROUP BY e.value_2
	ORDER BY 1 LIMIT 3;
2023-11-25 13:48:43.852 UTC [138310] ERROR:  0A000: cannot push down subquery on the target list
2023-11-25 13:48:43.852 UTC [138310] DETAIL:  Subqueries in the SELECT part of the query can only be pushed down if they happen before aggregates and window functions
2023-11-25 13:48:43.852 UTC [138310] LOCATION:  MultiLogicalPlanOptimize, multi_logical_optimizer.c:479
2023-11-25 13:48:43.852 UTC [138310] STATEMENT:  SELECT sum(e.user_id) + (SELECT user_id FROM users_reference_table WHERE user_id = 1 AND value_1 = 1)
	FROM events_table e;
2023-11-25 13:48:43.859 UTC [138310] ERROR:  0A000: cannot push down subquery on the target list
2023-11-25 13:48:43.859 UTC [138310] DETAIL:  Subqueries in the SELECT part of the query can only be pushed down if they happen before aggregates and window functions
2023-11-25 13:48:43.859 UTC [138310] LOCATION:  MultiLogicalPlanOptimize, multi_logical_optimizer.c:479
2023-11-25 13:48:43.859 UTC [138310] STATEMENT:  SELECT e.value_2, sum((SELECT any_value(value_3) FROM users_reference_table WHERE user_id = e.user_id GROUP BY user_id)) OVER (PARTITION BY e.value_2)
	FROM events_table e
	ORDER BY 1, 2 LIMIT 3;
2023-11-25 13:48:43.886 UTC [138310] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a reference table
2023-11-25 13:48:43.886 UTC [138310] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:671
2023-11-25 13:48:43.886 UTC [138310] STATEMENT:  SELECT (SELECT (SELECT e.user_id + user_id) FROM users_reference_table WHERE user_id = e.user_id GROUP BY user_id)
	FROM events_table e
	GROUP BY 1
	ORDER BY 1 LIMIT 3;
2023-11-25 13:48:43.887 UTC [138310] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a CTE or subquery
2023-11-25 13:48:43.887 UTC [138310] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:684
2023-11-25 13:48:43.887 UTC [138310] STATEMENT:  WITH cte_1 AS (SELECT user_id FROM users_table ORDER BY 1 LIMIT 1)
	SELECT (SELECT (SELECT e.user_id + user_id) FROM cte_1 WHERE user_id = e.user_id GROUP BY user_id)
	FROM events_table e
	GROUP BY 1
	ORDER BY 1 LIMIT 3;
2023-11-25 13:48:43.887 UTC [138310] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a subquery without FROM
2023-11-25 13:48:43.887 UTC [138310] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:690
2023-11-25 13:48:43.887 UTC [138310] STATEMENT:  SELECT (SELECT (SELECT e.user_id + user_id) FROM (SELECT 1 AS user_id) s WHERE user_id = e.user_id GROUP BY user_id)
	FROM events_table e
	GROUP BY 1
	ORDER BY 1 LIMIT 3;
2023-11-25 13:48:43.889 UTC [138310] WARNING:  0A000: "view view_1" has dependency on unsupported object "schema pg_temp_3"
2023-11-25 13:48:43.889 UTC [138310] DETAIL:  "view view_1" will be created only locally
2023-11-25 13:48:43.889 UTC [138310] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:958
2023-11-25 13:48:43.907 UTC [138310] WARNING:  0A000: "view view_2" has dependency on unsupported object "schema pg_temp_3"
2023-11-25 13:48:43.907 UTC [138310] DETAIL:  "view view_2" will be created only locally
2023-11-25 13:48:43.907 UTC [138310] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:958
2023-11-25 13:48:43.916 UTC [138310] ERROR:  42704: type "view_1" does not exist
2023-11-25 13:48:43.916 UTC [138310] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:43.916 UTC [138310] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:43.916 UTC [138310] STATEMENT:  SELECT (SELECT view_1)
	FROM view_1
	ORDER BY 1 LIMIT 1;
2023-11-25 13:48:43.918 UTC [138310] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a subquery without FROM
2023-11-25 13:48:43.918 UTC [138310] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:690
2023-11-25 13:48:43.918 UTC [138310] STATEMENT:  SELECT (SELECT (SELECT user_id))
	FROM events_table e
	ORDER BY 1 LIMIT 1;
2023-11-25 13:48:43.935 UTC [138310] ERROR:  0A000: input of anonymous composite types is not implemented
2023-11-25 13:48:43.935 UTC [138310] LOCATION:  record_in, rowtypes.c:103
2023-11-25 13:48:43.935 UTC [138310] STATEMENT:  SELECT (SELECT (user_id,value_1) FROM users_table u WHERE u.user_id = e.user_id AND time = 'Thu Nov 23 09:26:42.145043 2017')
	FROM events_table e
	WHERE user_id < 3
	GROUP BY 1
	ORDER BY 1 LIMIT 3;
2023-11-25 13:48:43.967 UTC [138313] ERROR:  0A000: input of anonymous composite types is not implemented
2023-11-25 13:48:43.967 UTC [138313] LOCATION:  record_in, rowtypes.c:103
2023-11-25 13:48:43.967 UTC [138313] STATEMENT:  SELECT b FROM (SELECT a FROM items a GROUP BY key) b ORDER BY b;
2023-11-25 13:48:43.967 UTC [138312] ERROR:  0A000: Subqueries in HAVING cannot refer to outer query
2023-11-25 13:48:43.967 UTC [138312] LOCATION:  RecursivelyPlanSubqueriesAndCTEs, recursive_planning.c:323
2023-11-25 13:48:43.967 UTC [138312] STATEMENT:  SELECT
	  count(*)
	FROM
	  events_table e
	WHERE
	  value_3 IN (SELECT min(value_3) v FROM users_table WHERE user_id = e.user_id GROUP BY e.value_2 HAVING min(value_3) > (SELECT e.value_3));
2023-11-25 13:48:43.980 UTC [138312] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:43.980 UTC [138312] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:43.980 UTC [138312] STATEMENT:  SELECT
	  count(*)
	FROM
	  events_table e
	WHERE
	  value_3 IN (
	    SELECT min(r.value_3) v FROM users_reference_table r JOIN (SELECT * FROM users_table WHERE value_2 = e.user_id) u USING (user_id)
	    WHERE u.value_2 > 3
	    GROUP BY e.value_2 HAVING min(r.value_3) > e.value_3);
2023-11-25 13:48:43.981 UTC [138312] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:43.981 UTC [138312] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:43.981 UTC [138312] STATEMENT:  SELECT
	  count(*)
	FROM
	  events_table e
	WHERE
	  value_3 IN (
	    SELECT min(r.value_3) v FROM users_reference_table r JOIN users_table u USING (user_id)
	    WHERE u.value_2 > 3
	    GROUP BY e.value_2 HAVING min(r.value_3) > e.value_3);
2023-11-25 13:48:43.981 UTC [138312] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:43.981 UTC [138312] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:43.981 UTC [138312] STATEMENT:  SELECT
	  count(*)
	FROM
	  events_table e
	WHERE
	  value_3 IN (
	    SELECT min(r.value_3) v FROM users_reference_table r JOIN users_table u USING (user_id)
	    WHERE u.value_2 > 3
	    GROUP BY u.value_2 HAVING min(r.value_3) > e.value_3);
2023-11-25 13:48:43.986 UTC [138310] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a subquery without FROM
2023-11-25 13:48:43.986 UTC [138310] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:690
2023-11-25 13:48:43.986 UTC [138310] STATEMENT:  SELECT (SELECT DISTINCT user_id FROM users_table WHERE user_id = (SELECT max(user_id) FROM users_table) AND value_2 = a)
	FROM (SELECT 1 AS a) r;
2023-11-25 13:48:43.987 UTC [138310] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a reference table
2023-11-25 13:48:43.987 UTC [138310] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:671
2023-11-25 13:48:43.987 UTC [138310] STATEMENT:  SELECT (SELECT DISTINCT user_id FROM users_table WHERE user_id = (SELECT max(user_id) FROM users_table) AND value_2 = r.user_id)
	FROM users_reference_table r;
2023-11-25 13:48:43.987 UTC [138310] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:43.987 UTC [138310] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:43.987 UTC [138310] STATEMENT:  SELECT (SELECT DISTINCT user_id FROM users_table WHERE user_id = (SELECT max(user_id) FROM users_table WHERE user_id = a))
	FROM (SELECT 1 AS a) r;
2023-11-25 13:48:43.997 UTC [138310] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a reference table
2023-11-25 13:48:43.997 UTC [138310] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:671
2023-11-25 13:48:43.997 UTC [138310] STATEMENT:  SELECT (SELECT DISTINCT user_id FROM users_table WHERE user_id = (SELECT max(user_id) FROM users_table ))
	FROM (SELECT * FROM users_reference_table WHERE value_2 IN (SELECT value_2 FROM events_table WHERE events_table.user_id = users_reference_table.user_id)) r
	ORDER BY 1 LIMIT 3;
2023-11-25 13:48:44.014 UTC [138310] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a reference table
2023-11-25 13:48:44.014 UTC [138310] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:671
2023-11-25 13:48:44.014 UTC [138310] STATEMENT:  SELECT (SELECT (SELECT user_id FROM users_table WHERE user_id = users_reference_table.user_id GROUP BY user_id)
	        FROM users_reference_table WHERE user_id < 2 GROUP BY user_id)
	FROM users_reference_table r
	ORDER BY 1 LIMIT 3;
2023-11-25 13:48:44.018 UTC [138310] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:44.018 UTC [138310] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:44.018 UTC [138310] STATEMENT:  SELECT (SELECT DISTINCT user_id FROM users_table WHERE user_id = (SELECT max(user_id) FROM users_table WHERE user_id = r.user_id))
	FROM (SELECT user_id FROM users_table ORDER BY 1 LIMIT 3) r;
2023-11-25 13:48:44.019 UTC [138310] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a CTE or subquery
2023-11-25 13:48:44.019 UTC [138310] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:684
2023-11-25 13:48:44.019 UTC [138310] STATEMENT:  SELECT (SELECT (SELECT max(user_id) FROM users_table) FROM users_table WHERE user_id = r.user_id)
	FROM (SELECT user_id FROM users_table ORDER BY 1 LIMIT 3) r;
2023-11-25 13:48:44.020 UTC [138310] ERROR:  0A000: cannot push down this subquery
2023-11-25 13:48:44.020 UTC [138310] DETAIL:  For Update/Share commands are currently unsupported
2023-11-25 13:48:44.020 UTC [138310] LOCATION:  DeferErrorIfCannotPushdownSubquery, query_pushdown_planning.c:1048
2023-11-25 13:48:44.020 UTC [138310] STATEMENT:  SELECT count(*) FROM (SELECT
	  (SELECT user_id FROM users_table WHERE user_id = u1.user_id FOR UPDATE)
	FROM users_table u1
	GROUP BY user_id) as foo;
2023-11-25 13:48:44.031 UTC [138312] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:44.031 UTC [138312] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:44.031 UTC [138312] STATEMENT:  SELECT
		u1.user_id, u2.user_id
	FROM
		users_table u1, users_table u2
	WHERE
		u1.value_1 < u2.value_1 AND
		(SELECT
			count(*)
		FROM
			events_table e1
		WHERE
			e1.user_id = u2.user_id AND
			u1.user_id = u2.user_id) > 10
	ORDER BY 1,2;
2023-11-25 13:48:44.033 UTC [138312] WARNING:  0A000: "view correlated_subquery_view" has dependency on unsupported object "schema pg_temp_8"
2023-11-25 13:48:44.033 UTC [138312] DETAIL:  "view correlated_subquery_view" will be created only locally
2023-11-25 13:48:44.033 UTC [138312] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:958
2023-11-25 13:48:44.044 UTC [138312] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:44.044 UTC [138312] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:44.044 UTC [138312] STATEMENT:  SELECT sum(value_1)
	FROM users_table u1
	WHERE (SELECT COUNT(DISTINCT e1.value_2)
	     FROM events_table e1
	     WHERE e1.user_id = u1.user_id AND false
	          ) > 115;
2023-11-25 13:48:45.035 UTC [138679] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:45.035 UTC [138679] CONTEXT:  SQL statement "EXPLAIN (FORMAT JSON) 
	
	    SELECT
	        count(*)
	    FROM
	        (users_table u1 JOIN users_table u2 using(value_1)) a JOIN (SELECT value_1, random() FROM users_table) as u3 USING (value_1);
	"
	PL/pgSQL function explain_json_2(text) line 5 at EXECUTE
2023-11-25 13:48:45.035 UTC [138679] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:45.035 UTC [138679] STATEMENT:  SELECT true AS valid FROM explain_json_2($$
	
	    SELECT
	        count(*)
	    FROM
	        (users_table u1 JOIN users_table u2 using(value_1)) a JOIN (SELECT value_1, random() FROM users_table) as u3 USING (value_1);
	$$);
2023-11-25 13:48:45.063 UTC [138679] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:45.063 UTC [138679] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:45.063 UTC [138679] STATEMENT:  SELECT *
	FROM
	  (SELECT *
	   FROM users_table
	   OFFSET 0) AS users_table
	JOIN LATERAL
	  (SELECT *
	   FROM
	     (SELECT *
	      FROM events_table
	      WHERE user_id = users_table.user_id) AS bar
	   LEFT JOIN users_table u2 ON u2.user_id = bar.value_2) AS foo ON TRUE;
2023-11-25 13:48:45.063 UTC [138679] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:45.063 UTC [138679] CONTEXT:  SQL statement "EXPLAIN (FORMAT JSON) 
	SELECT *
	FROM
	  (SELECT 1 AS user_id) AS users_table
	JOIN LATERAL
	  (SELECT *
	   FROM
	     (SELECT *
	      FROM events_table
	      WHERE user_id = users_table.user_id) AS bar
	   LEFT JOIN users_table u2 ON u2.user_id = bar.value_2) AS foo ON TRUE
	"
	PL/pgSQL function explain_json_2(text) line 5 at EXECUTE
2023-11-25 13:48:45.063 UTC [138679] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:45.063 UTC [138679] STATEMENT:  SELECT true AS valid FROM explain_json_2($$
	SELECT *
	FROM
	  (SELECT 1 AS user_id) AS users_table
	JOIN LATERAL
	  (SELECT *
	   FROM
	     (SELECT *
	      FROM events_table
	      WHERE user_id = users_table.user_id) AS bar
	   LEFT JOIN users_table u2 ON u2.user_id = bar.value_2) AS foo ON TRUE
	$$);
2023-11-25 13:48:45.277 UTC [138733] WARNING:  0A000: "view recursive_defined_non_recursive_view" has dependency to "table local_table" that is not in Citus' metadata
2023-11-25 13:48:45.277 UTC [138733] DETAIL:  "view recursive_defined_non_recursive_view" will be created only locally
2023-11-25 13:48:45.277 UTC [138733] HINT:  Distribute "table local_table" first to distribute "view recursive_defined_non_recursive_view"
2023-11-25 13:48:45.277 UTC [138733] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:950
2023-11-25 13:48:45.313 UTC [138733] ERROR:  0A000: direct joins between distributed and local tables are not supported
2023-11-25 13:48:45.313 UTC [138733] HINT:  Use CTE's or subqueries to select from local tables and use them in joins
2023-11-25 13:48:45.313 UTC [138733] LOCATION:  DeferErrorIfQueryNotSupported, multi_logical_planner.c:945
2023-11-25 13:48:45.313 UTC [138733] STATEMENT:  SELECT ref_table.* FROM ref_table WHERE EXISTS (SELECT * FROM local_table l WHERE l.a = ref_table.a);
2023-11-25 13:48:45.349 UTC [138735] ERROR:  0A000: recursive complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:45.349 UTC [138735] LOCATION:  CreateDistributedPlan, distributed_planner.c:1092
2023-11-25 13:48:45.349 UTC [138735] STATEMENT:  SELECT t1.id
	FROM (
	    SELECT t2.id
	    FROM (
	        SELECT t0.id
	        FROM tbl_dist1 t0
	        LIMIT 5
	    ) AS t2
	    INNER JOIN tbl_dist1 AS t3 USING (id)
	) AS t1
	FULL JOIN tbl_dist1 t4 USING (id);
2023-11-25 13:48:45.367 UTC [138734] ERROR:  0A000: could not run distributed query with FOR UPDATE/SHARE commands
2023-11-25 13:48:45.367 UTC [138734] HINT:  Consider using an equality filter on the distributed table's partition column.
2023-11-25 13:48:45.367 UTC [138734] LOCATION:  DeferErrorIfQueryNotSupported, multi_logical_planner.c:945
2023-11-25 13:48:45.367 UTC [138734] STATEMENT:  WITH cte_1 AS (SELECT * FROM test_table)
	SELECT
		count(*)
	FROM
		cte_1
	WHERE
		key IN (
				SELECT
					key
				FROM
					test_table
	  			 	FOR UPDATE
				);
2023-11-25 13:48:45.474 UTC [138734] ERROR:  0A000: CTEs that refer to other subqueries are not supported in multi-shard queries
2023-11-25 13:48:45.474 UTC [138734] LOCATION:  RecursivelyPlanCTEs, recursive_planning.c:1110
2023-11-25 13:48:45.474 UTC [138734] STATEMENT:  SELECT count(*)
		FROM
		  (SELECT *
		   FROM test_table) AS test_table_cte
		JOIN LATERAL
		  (WITH bar AS  (SELECT *
		      FROM test_table
		      WHERE key = test_table_cte.key)
		  	SELECT *
		   FROM
		      bar
		   LEFT JOIN test_table u2 ON u2.key = bar.value::int) AS foo ON TRUE;
2023-11-25 13:48:45.501 UTC [138731] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains VALUES
2023-11-25 13:48:45.501 UTC [138731] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:697
2023-11-25 13:48:45.501 UTC [138731] STATEMENT:  SELECT
		*
	FROM
		test_values as t1
			JOIN LATERAL (
				SELECT
					t1.key
				FROM
					(VALUES (1, 'one'), (2, 'two'), (3, 'three')) as t(num, v)
					  WHERE num > (SELECT max(key) FROM test_values)) as foo
		ON (true);
2023-11-25 13:48:45.519 UTC [138734] ERROR:  21000: more than one row returned by a subquery used as an expression
2023-11-25 13:48:45.519 UTC [138734] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:45.519 UTC [138734] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:45.519 UTC [138734] STATEMENT:  WITH cte_1 AS (SELECT * FROM test_table),
		 cte_2 AS (SELECT * FROM test_table)
	(SELECT *, (SELECT key FROM cte_1) FROM test_table)
	UNION
	(SELECT *, 1 FROM cte_2);
2023-11-25 13:48:45.519 UTC [138731] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains VALUES
2023-11-25 13:48:45.519 UTC [138731] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:697
2023-11-25 13:48:45.519 UTC [138731] STATEMENT:  SELECT
	  count(*)
	FROM
	 (SELECT a, b FROM (VALUES (1, 'one'), (2, 'two'), (3, 'three')) as t(a,b)) as values_data(a,b)
	WHERE
	  NOT EXISTS
	      (SELECT
	          value
	       FROM
	          test_values
	       WHERE
	          test_values.key = values_data.a
	      );
2023-11-25 13:48:45.662 UTC [138735] ERROR:  0A000: recursive complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:45.662 UTC [138735] LOCATION:  CreateDistributedPlan, distributed_planner.c:1092
2023-11-25 13:48:45.662 UTC [138735] STATEMENT:  SELECT avg(avgsub.id) FROM (
	    SELECT table_0.id FROM (
	        SELECT table_1.id FROM (
	            SELECT table_2.id FROM (
	                SELECT table_3.id FROM (
	                    SELECT table_4.id FROM dist0 AS table_4
	                    LEFT JOIN dist1 AS table_5 USING (id)
	                ) AS table_3 INNER JOIN dist0 AS table_6 USING (id)
	            ) AS table_2 WHERE table_2.id < 10 ORDER BY id LIMIT 47
	        ) AS table_1 RIGHT JOIN dist0 AS table_7 USING (id)
	    ) AS table_0 RIGHT JOIN dist1 AS table_8 USING (id)
	) AS avgsub;
2023-11-25 13:48:45.678 UTC [138735] ERROR:  0A000: recursive complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:45.678 UTC [138735] LOCATION:  CreateDistributedPlan, distributed_planner.c:1092
2023-11-25 13:48:45.678 UTC [138735] STATEMENT:  WITH cte_0 AS (
	    SELECT table_0.id FROM dist1 AS table_0 FULL JOIN dist1 AS table_1 USING (id)
	)
	SELECT avg(table_5.id) FROM (
	    SELECT table_6.id FROM (
	        SELECT table_7.id FROM dist0 AS table_7 ORDER BY id LIMIT 87
	    ) AS table_6 INNER JOIN dist0 AS table_8 USING (id) WHERE table_8.id < 0 ORDER BY id
	) AS table_5 INNER JOIN dist0 AS table_9 USING (id);
2023-11-25 13:48:46.450 UTC [139144] ERROR:  42601: parallel workers for vacuum must be between 0 and 1024 at character 9
2023-11-25 13:48:46.450 UTC [139144] LOCATION:  ExecVacuum, vacuum.c:188
2023-11-25 13:48:46.450 UTC [139144] STATEMENT:  VACUUM (PARALLEL -5) dist_table;
2023-11-25 13:48:46.450 UTC [139144] ERROR:  42601: parallel option requires a value between 0 and 1024 at character 9
2023-11-25 13:48:46.450 UTC [139144] LOCATION:  ExecVacuum, vacuum.c:176
2023-11-25 13:48:46.450 UTC [139144] STATEMENT:  VACUUM (PARALLEL) dist_table;
2023-11-25 13:48:46.456 UTC [139144] ERROR:  0A000: alter table command is currently unsupported
2023-11-25 13:48:46.456 UTC [139144] DETAIL:  Only ADD|DROP COLUMN, SET|DROP NOT NULL, SET|DROP DEFAULT, ADD|DROP|VALIDATE CONSTRAINT, SET (), RESET (), ENABLE|DISABLE|NO FORCE|FORCE ROW LEVEL SECURITY, ATTACH|DETACH PARTITION and TYPE subcommands are supported.
2023-11-25 13:48:46.456 UTC [139144] LOCATION:  ErrorIfUnsupportedAlterTableStmt, table.c:3506
2023-11-25 13:48:46.456 UTC [139144] STATEMENT:  ALTER TABLE generated_col_table ALTER COLUMN b DROP EXPRESSION;
2023-11-25 13:48:46.459 UTC [139143] ERROR:  0A000: cannot distribute relation: gen2
2023-11-25 13:48:46.459 UTC [139143] DETAIL:  Distribution column must not use GENERATED ALWAYS AS (...) STORED.
2023-11-25 13:48:46.459 UTC [139143] LOCATION:  EnsureRelationCanBeDistributed, create_distributed_table.c:1825
2023-11-25 13:48:46.459 UTC [139143] STATEMENT:  select create_distributed_table('gen2', 'val2');
2023-11-25 13:48:46.479 UTC [139144] WARNING:  0A000: "function myvarcharin(cstring,oid,integer)" has dependency on unsupported object "type myvarchar"
2023-11-25 13:48:46.479 UTC [139144] DETAIL:  "function myvarcharin(cstring,oid,integer)" will be created only locally
2023-11-25 13:48:46.479 UTC [139144] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:958
2023-11-25 13:48:46.479 UTC [139144] WARNING:  0A000: "function myvarcharout(myvarchar)" has dependency on unsupported object "type myvarchar"
2023-11-25 13:48:46.479 UTC [139144] DETAIL:  "function myvarcharout(myvarchar)" will be created only locally
2023-11-25 13:48:46.479 UTC [139144] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:958
2023-11-25 13:48:46.481 UTC [139144] ERROR:  0A000: "table my_table" has dependency on unsupported object "type myvarchar"
2023-11-25 13:48:46.481 UTC [139144] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:958
2023-11-25 13:48:46.481 UTC [139144] STATEMENT:  SELECT create_distributed_table('my_table', 'a');
2023-11-25 13:48:46.491 UTC [139144] ERROR:  22023: EXPLAIN option WAL requires ANALYZE
2023-11-25 13:48:46.491 UTC [139144] LOCATION:  ExplainQuery, explain.c:231
2023-11-25 13:48:46.491 UTC [139144] STATEMENT:  EXPLAIN (WAL) INSERT INTO test_wal VALUES(1,11);
2023-11-25 13:48:46.496 UTC [139143] ERROR:  XX000: Citus does not support COPY FROM with WHERE
2023-11-25 13:48:46.496 UTC [139143] LOCATION:  ProcessCopyStmt, multi_copy.c:2934
2023-11-25 13:48:46.496 UTC [139143] STATEMENT:  copy cptest from STDIN with csv where val < 4;
2023-11-25 13:48:46.497 UTC [139143] ERROR:  42601: syntax error at or near "1" at character 1
2023-11-25 13:48:46.497 UTC [139143] LOCATION:  scanner_yyerror, scan.l:1188
2023-11-25 13:48:46.497 UTC [139143] STATEMENT:  1,6
	2,3
	3,2
	4,9
	5,4
	select sum(id), sum(val) from cptest;
2023-11-25 13:48:46.542 UTC [139143] ERROR:  23503: insert or update on table "collection_users" violates foreign key constraint "collection_users_fkey"
2023-11-25 13:48:46.542 UTC [139143] DETAIL:  Key (key, collection_id)=(1, 1000) is not present in table "collections_list".
2023-11-25 13:48:46.542 UTC [139143] LOCATION:  ri_ReportViolation, ri_triggers.c:2596
2023-11-25 13:48:46.542 UTC [139143] STATEMENT:  INSERT INTO collection_users VALUES (1, 1000, 1);
2023-11-25 13:48:46.580 UTC [139143] ERROR:  23503: insert or update on table "collection_users_60028" violates foreign key constraint "collection_users_fkey_60028"
2023-11-25 13:48:46.580 UTC [139143] DETAIL:  Key (key, collection_id)=(1, 1000) is not present in table "collections_list_60016".
2023-11-25 13:48:46.580 UTC [139143] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:46.580 UTC [139143] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:46.580 UTC [139143] STATEMENT:  INSERT INTO collection_users VALUES (1, 1000, 1);
2023-11-25 13:48:46.608 UTC [139143] ERROR:  25006: cannot execute UPDATE in a read-only transaction
2023-11-25 13:48:46.608 UTC [139143] LOCATION:  PreventCommandIfReadOnly, utility.c:414
2023-11-25 13:48:46.608 UTC [139143] STATEMENT:  UPDATE test SET y = 35;
2023-11-25 13:48:46.611 UTC [139143] ERROR:  25006: cannot execute UPDATE in a read-only transaction
2023-11-25 13:48:46.611 UTC [139143] LOCATION:  PreventCommandIfReadOnly, utility.c:414
2023-11-25 13:48:46.611 UTC [139143] STATEMENT:  UPDATE test SET y = 40;
2023-11-25 13:48:46.618 UTC [139143] ERROR:  0A000: Hash distributed partition columns may not use a non deterministic collation
2023-11-25 13:48:46.618 UTC [139143] LOCATION:  EnsureRelationCanBeDistributed, create_distributed_table.c:1864
2023-11-25 13:48:46.618 UTC [139143] STATEMENT:  select create_distributed_table('col_test', 'val');
2023-11-25 13:48:46.896 UTC [139143] ERROR:  42501: permission denied for schema test_pg12
2023-11-25 13:48:46.896 UTC [139143] LOCATION:  aclcheck_error, aclchk.c:3650
2023-11-25 13:48:46.896 UTC [139143] STATEMENT:  ALTER TABLE test_pg12.superuser_columnar_table SET(columnar.chunk_group_row_limit = 100);
2023-11-25 13:48:46.896 UTC [139143] ERROR:  42501: permission denied for schema test_pg12
2023-11-25 13:48:46.896 UTC [139143] LOCATION:  aclcheck_error, aclchk.c:3650
2023-11-25 13:48:46.896 UTC [139143] STATEMENT:  ALTER TABLE test_pg12.superuser_columnar_table RESET (columnar.chunk_group_row_limit);
2023-11-25 13:48:47.070 UTC [139272] ERROR:  0A000: PROCESS_TOAST required with VACUUM FULL
2023-11-25 13:48:47.070 UTC [139272] LOCATION:  vacuum, vacuum.c:351
2023-11-25 13:48:47.070 UTC [139272] STATEMENT:  VACUUM (FULL, PROCESS_TOAST false) t1;
2023-11-25 13:48:47.072 UTC [139272] ERROR:  42601: index_cleanup requires a Boolean value
2023-11-25 13:48:47.072 UTC [139272] LOCATION:  defGetBoolean, define.c:152
2023-11-25 13:48:47.072 UTC [139272] STATEMENT:  VACUUM (INDEX_CLEANUP "AUTOX") t1;
2023-11-25 13:48:47.091 UTC [139272] ERROR:  42704: tablespace "test_tablespace1" does not exist
2023-11-25 13:48:47.091 UTC [139272] LOCATION:  get_tablespace_oid, tablespace.c:1484
2023-11-25 13:48:47.091 UTC [139272] STATEMENT:  reindex(TABLESPACE test_tablespace1) index idx;
2023-11-25 13:48:47.097 UTC [139272] ERROR:  0A000: only simple column references are allowed in CREATE STATISTICS
2023-11-25 13:48:47.097 UTC [139272] LOCATION:  AppendColumnNames, deparse_statistics_stmts.c:242
2023-11-25 13:48:47.097 UTC [139272] STATEMENT:  CREATE STATISTICS s3 (ndistinct) ON date_trunc('month', a), date_trunc('day', a) FROM tbl1;
2023-11-25 13:48:47.106 UTC [139272] ERROR:  XX000: ALTER TABLE .. DETACH PARTITION .. CONCURRENTLY commands are currently unsupported.
2023-11-25 13:48:47.106 UTC [139272] LOCATION:  ErrorIfUnsupportedAlterTableStmt, table.c:3420
2023-11-25 13:48:47.106 UTC [139272] STATEMENT:  ALTER TABLE par DETACH PARTITION par_2 CONCURRENTLY;
2023-11-25 13:48:47.106 UTC [139272] ERROR:  XX000: ALTER TABLE .. DETACH PARTITION .. FINALIZE commands are currently unsupported.
2023-11-25 13:48:47.106 UTC [139272] LOCATION:  ErrorIfUnsupportedAlterTableStmt, table.c:3398
2023-11-25 13:48:47.106 UTC [139272] STATEMENT:  ALTER TABLE par DETACH PARTITION par_2 FINALIZE;
2023-11-25 13:48:47.180 UTC [139285] LOG:  00000: deferred drop of orphaned resource pg14.col_compression_980010 on localhost:57637 completed
2023-11-25 13:48:47.180 UTC [139285] LOCATION:  DropOrphanedResourcesForCleanup, shard_cleaner.c:298
2023-11-25 13:48:47.180 UTC [139285] STATEMENT:  CALL citus_cleanup_orphaned_resources()
2023-11-25 13:48:47.220 UTC [139272] LOG:  00000: deferred drop of orphaned resource pg14.col_compression_980010 on localhost:57638 completed
2023-11-25 13:48:47.220 UTC [139272] CONTEXT:  SQL statement "CALL pg_catalog.citus_cleanup_orphaned_resources()"
	PL/pgSQL function public.wait_for_resource_cleanup() line 7 at CALL
2023-11-25 13:48:47.220 UTC [139272] LOCATION:  DropOrphanedResourcesForCleanup, shard_cleaner.c:298
2023-11-25 13:48:47.220 UTC [139272] STATEMENT:  SELECT public.wait_for_resource_cleanup();
2023-11-25 13:48:47.447 UTC [139272] ERROR:  22004: jsonb subscript in assignment must not be null
2023-11-25 13:48:47.447 UTC [139272] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:47.447 UTC [139272] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:47.447 UTC [139272] STATEMENT:  update test_jsonb_subscript set test_json[NULL] = '1';
2023-11-25 13:48:47.468 UTC [139272] ERROR:  42P01: invalid reference to FROM-clause entry for table "j1_tbl" at character 57
2023-11-25 13:48:47.468 UTC [139272] HINT:  There is an entry for table "j1_tbl", but it cannot be referenced from this part of the query.
2023-11-25 13:48:47.468 UTC [139272] LOCATION:  errorMissingRTE, parse_relation.c:3597
2023-11-25 13:48:47.468 UTC [139272] STATEMENT:  SELECT * FROM (J1_TBL JOIN J2_TBL USING (i)) AS x WHERE J1_TBL.t = 'one' ORDER BY 1,2,3,4;
2023-11-25 13:48:47.469 UTC [139272] ERROR:  42703: column x.t does not exist at character 55
2023-11-25 13:48:47.469 UTC [139272] LOCATION:  errorMissingColumn, parse_relation.c:3656
2023-11-25 13:48:47.469 UTC [139272] STATEMENT:  SELECT * FROM J1_TBL JOIN J2_TBL USING (i) AS x WHERE x.t = 'one' ORDER BY 1,2,3,4;
2023-11-25 13:48:47.469 UTC [139272] ERROR:  42P01: missing FROM-clause entry for table "x" at character 63
2023-11-25 13:48:47.469 UTC [139272] LOCATION:  errorMissingRTE, parse_relation.c:3608
2023-11-25 13:48:47.469 UTC [139272] STATEMENT:  SELECT * FROM (J1_TBL JOIN J2_TBL USING (i) AS x) AS xx WHERE x.i = 1 ORDER BY 1,2,3,4;
2023-11-25 13:48:47.469 UTC [139272] ERROR:  42712: table name "a1" specified more than once
2023-11-25 13:48:47.469 UTC [139272] LOCATION:  checkNameSpaceConflicts, parse_relation.c:443
2023-11-25 13:48:47.469 UTC [139272] STATEMENT:  SELECT * FROM J1_TBL a1 JOIN J2_TBL a2 USING (i) AS a1 ORDER BY 1,2,3,4;
2023-11-25 13:48:47.486 UTC [139272] ERROR:  0A000: REINDEX TABLE queries on distributed partitioned tables are not supported
2023-11-25 13:48:47.486 UTC [139272] LOCATION:  PreprocessReindexStmt, index.c:635
2023-11-25 13:48:47.486 UTC [139272] STATEMENT:  REINDEX TABLE dist_part_table;
2023-11-25 13:48:47.494 UTC [139272] ERROR:  0A000: recursive CTEs are not supported in distributed queries
2023-11-25 13:48:47.494 UTC [139272] LOCATION:  RecursivelyPlanCTEs, recursive_planning.c:1089
2023-11-25 13:48:47.494 UTC [139272] STATEMENT:  WITH RECURSIVE search_graph(f, t, label) AS (
	    SELECT * FROM graph0 g WHERE f = 1
	    UNION ALL
	    SELECT g.*
	        FROM graph0 g, search_graph sg
	        WHERE g.f = sg.t and  g.f = 1
	) SEARCH DEPTH FIRST BY f, t SET seq
	SELECT * FROM search_graph ORDER BY seq;
2023-11-25 13:48:47.494 UTC [139272] ERROR:  0A000: recursive CTEs are not supported in distributed queries
2023-11-25 13:48:47.494 UTC [139272] LOCATION:  RecursivelyPlanCTEs, recursive_planning.c:1089
2023-11-25 13:48:47.494 UTC [139272] STATEMENT:  WITH RECURSIVE search_graph(f, t, label) AS (
	    SELECT * FROM graph0 g WHERE f = 1
	    UNION ALL
	    SELECT g.*
	        FROM graph0 g, search_graph sg
	        WHERE g.f = sg.t and  g.f = 1
	) SEARCH DEPTH FIRST BY f, t SET seq
	DELETE FROM graph0 WHERE t IN (SELECT t FROM search_graph ORDER BY seq);
2023-11-25 13:48:47.499 UTC [139272] ERROR:  0A000: recursive CTEs are not supported in distributed queries
2023-11-25 13:48:47.499 UTC [139272] LOCATION:  RecursivelyPlanCTEs, recursive_planning.c:1089
2023-11-25 13:48:47.499 UTC [139272] STATEMENT:  WITH RECURSIVE search_graph(f, t, label) AS (
	    SELECT * FROM graph1 g WHERE f = 1
	    UNION ALL
	    SELECT g.*
	        FROM graph1 g, search_graph sg
	        WHERE g.f = sg.t and  g.f = 1
	) SEARCH DEPTH FIRST BY f, t SET seq
	SELECT * FROM search_graph ORDER BY seq;
2023-11-25 13:48:47.500 UTC [139272] ERROR:  0A000: recursive CTEs are not supported in distributed queries
2023-11-25 13:48:47.500 UTC [139272] LOCATION:  RecursivelyPlanCTEs, recursive_planning.c:1089
2023-11-25 13:48:47.500 UTC [139272] STATEMENT:  WITH RECURSIVE search_graph(f, t, label) AS (
	    SELECT * FROM graph1 g WHERE f = 1
	    UNION ALL
	    SELECT g.*
	        FROM graph1 g, search_graph sg
	        WHERE g.f = sg.t and  g.f = 1
	) SEARCH DEPTH FIRST BY f, t SET seq
	DELETE FROM graph1 WHERE t IN (SELECT t FROM search_graph ORDER BY seq);
2023-11-25 13:48:47.500 UTC [139272] ERROR:  0A000: recursive CTEs are not supported in distributed queries
2023-11-25 13:48:47.500 UTC [139272] LOCATION:  RecursivelyPlanCTEs, recursive_planning.c:1089
2023-11-25 13:48:47.500 UTC [139272] STATEMENT:  SELECT * FROM (
	    WITH RECURSIVE search_graph(f, t, label) AS (
	        SELECT *
	        FROM graph0 g
	        WHERE f = 1
	        UNION ALL SELECT g.*
	        FROM graph0 g, search_graph sg
	        WHERE g.f = sg.t AND g.f = 1
	    ) SEARCH DEPTH FIRST BY f, t SET seq
	    SELECT * FROM search_graph ORDER BY seq
	) as foo;
2023-11-25 13:48:47.507 UTC [139272] ERROR:  42883: function "proc_with_out_param(date,int)" does not exist at character 36
2023-11-25 13:48:47.507 UTC [139272] LOCATION:  regprocedurein, regproc.c:275
2023-11-25 13:48:47.507 UTC [139272] STATEMENT:  SELECT create_distributed_function('proc_with_out_param(date,int)');
2023-11-25 13:48:48.977 UTC [139377] LOG:  00000: The LSN of the target subscriptions on node localhost:57638 have caught up with the source LSN 
2023-11-25 13:48:48.977 UTC [139377] LOCATION:  WaitForGroupedLogicalRepTargetsToCatchUp, multi_logical_replication.c:1842
2023-11-25 13:48:48.977 UTC [139377] STATEMENT:  SELECT citus_move_shard_placement(980042, 'localhost', 57637, 'localhost', 57638, shard_transfer_mode := 'force_logical');
2023-11-25 13:48:48.977 UTC [139377] LOG:  00000: The LSN of the target subscriptions on node localhost:57638 have caught up with the source LSN 
2023-11-25 13:48:48.977 UTC [139377] LOCATION:  WaitForGroupedLogicalRepTargetsToCatchUp, multi_logical_replication.c:1842
2023-11-25 13:48:48.977 UTC [139377] STATEMENT:  SELECT citus_move_shard_placement(980042, 'localhost', 57637, 'localhost', 57638, shard_transfer_mode := 'force_logical');
2023-11-25 13:48:48.978 UTC [139377] LOG:  00000: The LSN of the target subscriptions on node localhost:57638 have caught up with the source LSN 
2023-11-25 13:48:48.978 UTC [139377] LOCATION:  WaitForGroupedLogicalRepTargetsToCatchUp, multi_logical_replication.c:1842
2023-11-25 13:48:48.978 UTC [139377] STATEMENT:  SELECT citus_move_shard_placement(980042, 'localhost', 57637, 'localhost', 57638, shard_transfer_mode := 'force_logical');
2023-11-25 13:48:49.013 UTC [139392] LOG:  00000: cleaned up orphaned resource pg14.dist_table_1_980042 on localhost:57638 which was left behind after a failed operation
2023-11-25 13:48:49.013 UTC [139392] LOCATION:  DropOrphanedResourcesForCleanup, shard_cleaner.c:305
2023-11-25 13:48:49.013 UTC [139392] STATEMENT:  CALL citus_cleanup_orphaned_resources()
2023-11-25 13:48:49.014 UTC [139392] LOG:  00000: cleaned up orphaned resource pg14.dist_table_2_980044 on localhost:57638 which was left behind after a failed operation
2023-11-25 13:48:49.014 UTC [139392] LOCATION:  DropOrphanedResourcesForCleanup, shard_cleaner.c:305
2023-11-25 13:48:49.014 UTC [139392] STATEMENT:  CALL citus_cleanup_orphaned_resources()
2023-11-25 13:48:50.058 UTC [139377] LOG:  00000: The LSN of the target subscriptions on node localhost:57638 have caught up with the source LSN 
2023-11-25 13:48:50.058 UTC [139377] LOCATION:  WaitForGroupedLogicalRepTargetsToCatchUp, multi_logical_replication.c:1842
2023-11-25 13:48:50.058 UTC [139377] STATEMENT:  SELECT citus_move_shard_placement(980042, 'localhost', 57637, 'localhost', 57638, shard_transfer_mode := 'force_logical');
2023-11-25 13:48:50.059 UTC [139377] LOG:  00000: The LSN of the target subscriptions on node localhost:57638 have caught up with the source LSN 
2023-11-25 13:48:50.059 UTC [139377] LOCATION:  WaitForGroupedLogicalRepTargetsToCatchUp, multi_logical_replication.c:1842
2023-11-25 13:48:50.059 UTC [139377] STATEMENT:  SELECT citus_move_shard_placement(980042, 'localhost', 57637, 'localhost', 57638, shard_transfer_mode := 'force_logical');
2023-11-25 13:48:50.060 UTC [139377] LOG:  00000: The LSN of the target subscriptions on node localhost:57638 have caught up with the source LSN 
2023-11-25 13:48:50.060 UTC [139377] LOCATION:  WaitForGroupedLogicalRepTargetsToCatchUp, multi_logical_replication.c:1842
2023-11-25 13:48:50.060 UTC [139377] STATEMENT:  SELECT citus_move_shard_placement(980042, 'localhost', 57637, 'localhost', 57638, shard_transfer_mode := 'force_logical');
2023-11-25 13:48:50.227 UTC [139418] ERROR:  42P17: parameter "locale" must be specified
2023-11-25 13:48:50.227 UTC [139418] LOCATION:  DefineCollation, collationcmds.c:242
2023-11-25 13:48:50.227 UTC [139418] STATEMENT:  CREATE COLLATION german_phonebook_test (provider = icu, lc_collate = 'de-u-co-phonebk');
2023-11-25 13:48:50.227 UTC [139418] ERROR:  42P17: parameter "locale" must be specified
2023-11-25 13:48:50.227 UTC [139418] LOCATION:  DefineCollation, collationcmds.c:242
2023-11-25 13:48:50.227 UTC [139418] STATEMENT:  CREATE COLLATION german_phonebook_test (provider = icu, lc_collate = 'de-u-co-phonebk', lc_ctype = 'de-u-co-phonebk');
2023-11-25 13:48:50.407 UTC [139418] ERROR:  XX000: cannot rename trigger "new_record_sale_trigger" on table "sale_newyork"
2023-11-25 13:48:50.407 UTC [139418] HINT:  Rename the trigger on the partitioned table "sale" instead.
2023-11-25 13:48:50.407 UTC [139418] LOCATION:  renametrig, trigger.c:1567
2023-11-25 13:48:50.407 UTC [139418] STATEMENT:  ALTER TRIGGER "new_record_sale_trigger" ON "pg15"."sale_newyork" RENAME TO "another_trigger_name";
2023-11-25 13:48:50.414 UTC [139418] ERROR:  2BP01: cannot drop column col_1 of table generated_stored_ref because other objects depend on it
2023-11-25 13:48:50.414 UTC [139418] DETAIL:  column col_3 of table generated_stored_ref depends on column col_1 of table generated_stored_ref
	column col_5 of table generated_stored_ref depends on column col_1 of table generated_stored_ref
2023-11-25 13:48:50.414 UTC [139418] HINT:  Use DROP ... CASCADE to drop the dependent objects too.
2023-11-25 13:48:50.414 UTC [139418] LOCATION:  reportDependentObjects, dependency.c:1189
2023-11-25 13:48:50.414 UTC [139418] STATEMENT:  ALTER TABLE generated_stored_ref DROP COLUMN col_1;
2023-11-25 13:48:50.443 UTC [139454] LOG:  00000: cleaned up orphaned resource pg14.dist_table_1_980042 on localhost:57638 which was left behind after a failed operation
2023-11-25 13:48:50.443 UTC [139454] LOCATION:  DropOrphanedResourcesForCleanup, shard_cleaner.c:305
2023-11-25 13:48:50.443 UTC [139454] STATEMENT:  CALL citus_cleanup_orphaned_resources()
2023-11-25 13:48:50.444 UTC [139454] LOG:  00000: cleaned up orphaned resource pg14.dist_table_2_980044 on localhost:57638 which was left behind after a failed operation
2023-11-25 13:48:50.444 UTC [139454] LOCATION:  DropOrphanedResourcesForCleanup, shard_cleaner.c:305
2023-11-25 13:48:50.444 UTC [139454] STATEMENT:  CALL citus_cleanup_orphaned_resources()
2023-11-25 13:48:50.605 UTC [139418] ERROR:  0A000: MERGE command is not supported on reference tables yet
2023-11-25 13:48:50.605 UTC [139418] LOCATION:  CheckIfRTETypeIsUnsupported, merge_planner.c:330
2023-11-25 13:48:50.605 UTC [139418] STATEMENT:  MERGE INTO tbl1 USING tbl2 ON (true)
	WHEN MATCHED THEN DELETE;
2023-11-25 13:48:50.613 UTC [139418] ERROR:  0A000: MERGE command is not supported on reference tables yet
2023-11-25 13:48:50.613 UTC [139418] LOCATION:  CheckIfRTETypeIsUnsupported, merge_planner.c:330
2023-11-25 13:48:50.613 UTC [139418] STATEMENT:  MERGE INTO tbl1 USING tbl2 ON (true)
	WHEN MATCHED THEN DELETE;
2023-11-25 13:48:50.675 UTC [139496] ERROR:  0A000: MERGE command is only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:50.675 UTC [139496] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:50.675 UTC [139496] STATEMENT:  MERGE INTO tbl1 USING tbl2 ON (true)
	WHEN MATCHED THEN DELETE;
2023-11-25 13:48:50.676 UTC [139496] ERROR:  0A000: MERGE command is only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:50.676 UTC [139496] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:50.676 UTC [139496] STATEMENT:  WITH targq AS (
	    SELECT * FROM tbl2
	)
	MERGE INTO tbl1 USING targq ON (true)
	WHEN MATCHED THEN DELETE;
2023-11-25 13:48:50.676 UTC [139496] ERROR:  0A000: MERGE not supported in WITH query at character 6
2023-11-25 13:48:50.676 UTC [139496] LOCATION:  transformWithClause, parse_cte.c:131
2023-11-25 13:48:50.676 UTC [139496] STATEMENT:  WITH foo AS (
	  MERGE INTO tbl1 USING tbl2 ON (true)
	  WHEN MATCHED THEN DELETE
	) SELECT * FROM foo;
2023-11-25 13:48:50.676 UTC [139496] ERROR:  0A000: MERGE not supported in COPY
2023-11-25 13:48:50.676 UTC [139496] LOCATION:  DoCopy, copy.c:281
2023-11-25 13:48:50.676 UTC [139496] STATEMENT:  COPY (
	  MERGE INTO tbl1 USING tbl2 ON (true)
	  WHEN MATCHED THEN DELETE
	) TO stdout;
2023-11-25 13:48:50.676 UTC [139496] ERROR:  0A000: MERGE command is only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:50.676 UTC [139496] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:50.676 UTC [139496] STATEMENT:  MERGE INTO tbl1 t
	USING tbl2
	ON (true)
	WHEN MATCHED THEN
	    DO NOTHING;
2023-11-25 13:48:50.676 UTC [139496] ERROR:  0A000: updating the distribution column is not allowed in MERGE actions
2023-11-25 13:48:50.676 UTC [139496] LOCATION:  MergeQualAndTargetListFunctionsSupported, merge_planner.c:651
2023-11-25 13:48:50.676 UTC [139496] STATEMENT:  MERGE INTO tbl1 t
	USING tbl2
	ON (true)
	WHEN MATCHED THEN
	    UPDATE SET x = (SELECT count(*) FROM tbl2);
2023-11-25 13:48:50.678 UTC [139496] ERROR:  0A000: cannot distribute relation: numeric_negative_scale
2023-11-25 13:48:50.678 UTC [139496] DETAIL:  Distribution column must not use numeric type with negative scale
2023-11-25 13:48:50.678 UTC [139496] LOCATION:  EnsureRelationCanBeDistributed, create_distributed_table.c:1838
2023-11-25 13:48:50.678 UTC [139496] STATEMENT:  SELECT create_distributed_table('numeric_negative_scale','numeric_column');
2023-11-25 13:48:50.695 UTC [139496] ERROR:  0A000: cannot distribute relation: numeric_negative_scale_3037880381
2023-11-25 13:48:50.695 UTC [139496] DETAIL:  Distribution column must not use numeric type with negative scale
2023-11-25 13:48:50.695 UTC [139496] LOCATION:  EnsureRelationCanBeDistributed, create_distributed_table.c:1838
2023-11-25 13:48:50.695 UTC [139496] STATEMENT:  SELECT alter_distributed_table('numeric_negative_scale',
	                                distribution_column := 'numeric_column');
2023-11-25 13:48:50.764 UTC [139496] ERROR:  0A000: complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
2023-11-25 13:48:50.764 UTC [139496] LOCATION:  JoinOrderList, multi_join_order.c:310
2023-11-25 13:48:50.764 UTC [139496] STATEMENT:  SELECT count(*)
	FROM numeric_repartition_first f,
	     numeric_repartition_second s
	WHERE f.id = s.numeric_column;
2023-11-25 13:48:50.975 UTC [139496] ERROR:  22P04: column name mismatch in header line field 2: got "data", expected "data_"
2023-11-25 13:48:50.975 UTC [139496] CONTEXT:  COPY copy_test2, line 1: "id	data"
2023-11-25 13:48:50.975 UTC [139496] LOCATION:  NextCopyFromRawFields, copyfromparse.c:806
2023-11-25 13:48:50.975 UTC [139496] STATEMENT:  COPY copy_test2 FROM '/tmp/''copy_test.txt' WITH ( HEADER match, FORMAT text);
2023-11-25 13:48:51.039 UTC [139692] ERROR:  42P01: relation "seq_non_exists" does not exist
2023-11-25 13:48:51.039 UTC [139692] LOCATION:  RangeVarGetRelidExtended, namespace.c:433
2023-11-25 13:48:51.039 UTC [139692] STATEMENT:  ALTER SEQUENCE seq_non_exists SET LOGGED;
2023-11-25 13:48:51.112 UTC [139706] ERROR:  0A000: cannot create foreign key constraint
2023-11-25 13:48:51.112 UTC [139706] DETAIL:  SET NULL or SET DEFAULT is not supported in ON DELETE operation when distribution key is included in the foreign key constraint
2023-11-25 13:48:51.112 UTC [139706] LOCATION:  EnsureSupportedFKeyOnDistKey, foreign_constraint.c:548
2023-11-25 13:48:51.112 UTC [139706] STATEMENT:  SELECT create_distributed_table('FKTABLE', 'tid');
2023-11-25 13:48:51.164 UTC [139711] ERROR:  23505: duplicate key value violates unique constraint "idx2_null_distinct_test_960150"
2023-11-25 13:48:51.164 UTC [139711] DETAIL:  Key (id, c2)=(1, null) already exists.
2023-11-25 13:48:51.164 UTC [139711] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:51.164 UTC [139711] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:51.164 UTC [139711] STATEMENT:  INSERT INTO null_distinct_test VALUES (1, NULL, NULL, 'data4') ;
2023-11-25 13:48:51.176 UTC [139711] ERROR:  23505: could not create unique index "uniq_c1_960150"
2023-11-25 13:48:51.176 UTC [139711] DETAIL:  Key (id, c1)=(1, null) is duplicated.
2023-11-25 13:48:51.176 UTC [139711] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:51.176 UTC [139711] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:51.176 UTC [139711] STATEMENT:  ALTER TABLE null_distinct_test ADD CONSTRAINT uniq_c1 UNIQUE NULLS NOT DISTINCT (id,c1);
2023-11-25 13:48:51.192 UTC [139711] ERROR:  23505: duplicate key value violates unique constraint "reference_uniq_test_x_y_key_960154"
2023-11-25 13:48:51.192 UTC [139711] DETAIL:  Key (x, y)=(1, null) already exists.
2023-11-25 13:48:51.192 UTC [139711] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:51.192 UTC [139711] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:51.192 UTC [139711] STATEMENT:  INSERT INTO reference_uniq_test VALUES (1, NULL);
2023-11-25 13:48:51.193 UTC [139711] WARNING:  01000: not propagating CLUSTER command for partitioned table to worker nodes
2023-11-25 13:48:51.193 UTC [139711] HINT:  Provide a child partition table names in order to CLUSTER distributed partitioned tables.
2023-11-25 13:48:51.193 UTC [139711] LOCATION:  PreprocessClusterStmt, cluster.c:85
2023-11-25 13:48:51.196 UTC [139711] ERROR:  0A000: modifications on partitions when replication factor is greater than 1 is not supported
2023-11-25 13:48:51.196 UTC [139711] HINT:  Run the query on the parent table "sale" instead.
2023-11-25 13:48:51.196 UTC [139711] LOCATION:  DeferErrorIfPartitionTableNotSingleReplicated, distributed_planner.c:1224
2023-11-25 13:48:51.196 UTC [139711] STATEMENT:  CLUSTER sale_newyork USING sale_newyork_pkey;
2023-11-25 13:48:51.221 UTC [139711] WARNING:  01000: not propagating CLUSTER command for partitioned table to worker nodes
2023-11-25 13:48:51.221 UTC [139711] HINT:  Provide a child partition table names in order to CLUSTER distributed partitioned tables.
2023-11-25 13:48:51.221 UTC [139711] LOCATION:  PreprocessClusterStmt, cluster.c:85
2023-11-25 13:48:51.315 UTC [139763] ERROR:  42501: permission denied for table events
2023-11-25 13:48:51.315 UTC [139763] LOCATION:  aclcheck_error, aclchk.c:3650
2023-11-25 13:48:51.315 UTC [139763] STATEMENT:  SELECT * FROM sec_invoker_view ORDER BY event_id;
2023-11-25 13:48:51.357 UTC [139781] ERROR:  42501: permission denied for table events
2023-11-25 13:48:51.357 UTC [139781] LOCATION:  aclcheck_error, aclchk.c:3650
2023-11-25 13:48:51.357 UTC [139781] STATEMENT:  SELECT * FROM sec_definer_view ORDER BY event_id;
2023-11-25 13:48:51.370 UTC [139781] ERROR:  0A000: cannot modify views when the query contains citus tables
2023-11-25 13:48:51.370 UTC [139781] LOCATION:  DeferErrorIfModifyView, multi_router_planner.c:1137
2023-11-25 13:48:51.370 UTC [139781] STATEMENT:  UPDATE sec_invoker_view SET event_id = 5;
2023-11-25 13:48:51.390 UTC [139781] ERROR:  XX000: cannot create foreign key constraint since Citus does not support ON DELETE / UPDATE SET DEFAULT actions on the columns that default to sequences
2023-11-25 13:48:51.390 UTC [139781] LOCATION:  ErrorIfUnsupportedForeignConstraintExists, foreign_constraint.c:274
2023-11-25 13:48:51.390 UTC [139781] STATEMENT:  SELECT create_reference_table('set_on_default_test_referencing');
2023-11-25 13:48:51.650 UTC [139781] ERROR:  XX000: cannot create foreign key constraint since Citus does not support ON DELETE / UPDATE SET DEFAULT actions on the columns that default to sequences
2023-11-25 13:48:51.650 UTC [139781] LOCATION:  ErrorIfUnsupportedForeignConstraintExists, foreign_constraint.c:274
2023-11-25 13:48:51.650 UTC [139781] STATEMENT:  CREATE TABLE set_on_default_test_referencing(
	    col_1 int, col_2 int, col_3 serial, col_4 int,
	    FOREIGN KEY(col_1, col_3)
	    REFERENCES set_on_default_test_referenced(col_1, col_3)
	    ON DELETE SET DEFAULT (col_3)
	);
2023-11-25 13:48:51.675 UTC [139855] ERROR:  42883: operator does not exist: pg15.user_enum ~~ unknown at character 77
2023-11-25 13:48:51.675 UTC [139855] HINT:  No operator matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:48:51.675 UTC [139855] LOCATION:  op_error, parse_oper.c:647
2023-11-25 13:48:51.675 UTC [139855] STATEMENT:  DECLARE c1 CURSOR FOR
	SELECT c0, c1 FROM pg15.foreign_table_test WHERE ((c1 ~~ 'foo')) LIMIT 1::bigint
2023-11-25 13:48:51.676 UTC [139781] ERROR:  42883: operator does not exist: pg15.user_enum ~~ unknown
2023-11-25 13:48:51.676 UTC [139781] HINT:  No operator matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:48:51.676 UTC [139781] CONTEXT:  remote SQL command: SELECT c0, c1 FROM pg15.foreign_table_test WHERE ((c1 ~~ 'foo')) LIMIT 1::bigint
2023-11-25 13:48:51.676 UTC [139781] LOCATION:  pgfdw_report_error, connection.c:895
2023-11-25 13:48:51.676 UTC [139781] STATEMENT:  SELECT * FROM foreign_table WHERE c1 LIKE 'foo' LIMIT 1;
2023-11-25 13:48:51.676 UTC [139855] ERROR:  42883: operator does not exist: pg15.user_enum ~~ unknown at character 77
2023-11-25 13:48:51.676 UTC [139855] HINT:  No operator matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:48:51.676 UTC [139855] LOCATION:  op_error, parse_oper.c:647
2023-11-25 13:48:51.676 UTC [139855] STATEMENT:  DECLARE c1 CURSOR FOR
	SELECT c0, c1 FROM pg15.foreign_table_test WHERE ((c1 ~~ 'foo')) LIMIT 1::bigint
2023-11-25 13:48:51.676 UTC [139781] ERROR:  42883: operator does not exist: pg15.user_enum ~~ unknown
2023-11-25 13:48:51.676 UTC [139781] HINT:  No operator matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:48:51.676 UTC [139781] CONTEXT:  remote SQL command: SELECT c0, c1 FROM pg15.foreign_table_test WHERE ((c1 ~~ 'foo')) LIMIT 1::bigint
2023-11-25 13:48:51.676 UTC [139781] LOCATION:  pgfdw_report_error, connection.c:895
2023-11-25 13:48:51.676 UTC [139781] STATEMENT:  SELECT * FROM foreign_table WHERE c1::text LIKE 'foo' LIMIT 1;
2023-11-25 13:48:51.712 UTC [133094] LOG:  00000: checkpoint starting: immediate force wait
2023-11-25 13:48:51.712 UTC [133094] LOCATION:  LogCheckpointStart, xlog.c:6089
2023-11-25 13:48:51.752 UTC [133094] LOG:  00000: checkpoint complete: wrote 2718 buffers (16.6%); 0 WAL file(s) added, 0 removed, 3 recycled; write=0.011 s, sync=0.001 s, total=0.040 s; sync files=0, longest=0.000 s, average=0.000 s; distance=54939 kB, estimate=54939 kB
2023-11-25 13:48:51.752 UTC [133094] LOCATION:  LogCheckpointEnd, xlog.c:6170
2023-11-25 13:48:52.123 UTC [139883] ERROR:  22P02: invalid input syntax for type jsonpath: ""
2023-11-25 13:48:52.123 UTC [139883] CONTEXT:  while executing command on localhost:57638
2023-11-25 13:48:52.123 UTC [139883] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:52.123 UTC [139883] STATEMENT:  SELECT sample, sample::jsonpath FROM jsonpath_test WHERE sample = '';
2023-11-25 13:48:52.129 UTC [139883] ERROR:  22P02: invalid input syntax for type jsonpath: ""
2023-11-25 13:48:52.129 UTC [139883] LOCATION:  jsonPathFromCstring, jsonpath.c:180
2023-11-25 13:48:52.129 UTC [139883] STATEMENT:  WITH samples as (SELECT id, sample FROM jsonpath_test WHERE sample = '' OFFSET 0)
	SELECT sample, sample::jsonpath FROM samples;
2023-11-25 13:48:52.131 UTC [139883] ERROR:  42601: LAST is allowed only in array subscripts
2023-11-25 13:48:52.131 UTC [139883] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:52.131 UTC [139883] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:52.131 UTC [139883] STATEMENT:  SELECT sample, sample::jsonpath FROM jsonpath_test WHERE sample = 'last';
2023-11-25 13:48:52.140 UTC [139883] ERROR:  42601: LAST is allowed only in array subscripts
2023-11-25 13:48:52.140 UTC [139883] LOCATION:  flattenJsonPathParseItem, jsonpath.c:366
2023-11-25 13:48:52.140 UTC [139883] STATEMENT:  WITH samples as (SELECT id, sample FROM jsonpath_test WHERE sample = 'last' OFFSET 0)
	SELECT sample, sample::jsonpath FROM samples;
2023-11-25 13:48:52.141 UTC [139883] ERROR:  42601: trailing junk after numeric literal at or near "1.t" of jsonpath input
2023-11-25 13:48:52.141 UTC [139883] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:52.141 UTC [139883] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:52.141 UTC [139883] STATEMENT:  SELECT sample, sample::jsonpath FROM jsonpath_test WHERE sample = '1.type()';
2023-11-25 13:48:52.150 UTC [139883] ERROR:  42601: trailing junk after numeric literal at or near "1.t" of jsonpath input
2023-11-25 13:48:52.150 UTC [139883] LOCATION:  jsonpath_yyerror, jsonpath_scan.l:286
2023-11-25 13:48:52.150 UTC [139883] STATEMENT:  WITH samples as (SELECT id, sample FROM jsonpath_test WHERE sample = '1.type()' OFFSET 0)
	SELECT sample, sample::jsonpath FROM samples;
2023-11-25 13:48:52.151 UTC [139883] ERROR:  2201B: invalid regular expression: parentheses () not balanced
2023-11-25 13:48:52.151 UTC [139883] CONTEXT:  while executing command on localhost:57638
2023-11-25 13:48:52.151 UTC [139883] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:52.151 UTC [139883] STATEMENT:  SELECT sample, sample::jsonpath FROM jsonpath_test WHERE sample = '$ ? (@ like_regex "(invalid pattern")';
2023-11-25 13:48:52.161 UTC [139883] ERROR:  2201B: invalid regular expression: parentheses () not balanced
2023-11-25 13:48:52.161 UTC [139883] LOCATION:  RE_compile_and_cache, regexp.c:207
2023-11-25 13:48:52.161 UTC [139883] STATEMENT:  WITH samples as (SELECT id, sample FROM jsonpath_test WHERE sample = '$ ? (@ like_regex "(invalid pattern")' OFFSET 0)
	SELECT sample, sample::jsonpath FROM samples;
2023-11-25 13:48:52.162 UTC [139883] ERROR:  0A000: XQuery "x" flag (expanded regular expressions) is not implemented
2023-11-25 13:48:52.162 UTC [139883] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:52.162 UTC [139883] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:52.162 UTC [139883] STATEMENT:  SELECT sample, sample::jsonpath FROM jsonpath_test WHERE sample = '$ ? (@ like_regex "pattern" flag "xsms")';
2023-11-25 13:48:52.172 UTC [139883] ERROR:  0A000: XQuery "x" flag (expanded regular expressions) is not implemented
2023-11-25 13:48:52.172 UTC [139883] LOCATION:  jspConvertRegexFlags, jsonpath_gram.y:582
2023-11-25 13:48:52.172 UTC [139883] STATEMENT:  WITH samples as (SELECT id, sample FROM jsonpath_test WHERE sample = '$ ? (@ like_regex "pattern" flag "xsms")' OFFSET 0)
	SELECT sample, sample::jsonpath FROM samples;
2023-11-25 13:48:52.173 UTC [139883] ERROR:  42601: @ is not allowed in root expressions
2023-11-25 13:48:52.173 UTC [139883] CONTEXT:  while executing command on localhost:57638
2023-11-25 13:48:52.173 UTC [139883] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:52.173 UTC [139883] STATEMENT:  SELECT sample, sample::jsonpath FROM jsonpath_test WHERE sample = '@ + 1';
2023-11-25 13:48:52.179 UTC [139883] ERROR:  42601: @ is not allowed in root expressions
2023-11-25 13:48:52.179 UTC [139883] LOCATION:  flattenJsonPathParseItem, jsonpath.c:360
2023-11-25 13:48:52.179 UTC [139883] STATEMENT:  WITH samples as (SELECT id, sample FROM jsonpath_test WHERE sample = '@ + 1' OFFSET 0)
	SELECT sample, sample::jsonpath FROM samples;
2023-11-25 13:48:52.180 UTC [139883] ERROR:  42601: trailing junk after numeric literal at or near "00" of jsonpath input
2023-11-25 13:48:52.180 UTC [139883] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:52.180 UTC [139883] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:52.180 UTC [139883] STATEMENT:  SELECT sample, sample::jsonpath FROM jsonpath_test WHERE sample = '00';
2023-11-25 13:48:52.190 UTC [139883] ERROR:  42601: trailing junk after numeric literal at or near "00" of jsonpath input
2023-11-25 13:48:52.190 UTC [139883] LOCATION:  jsonpath_yyerror, jsonpath_scan.l:286
2023-11-25 13:48:52.190 UTC [139883] STATEMENT:  WITH samples as (SELECT id, sample FROM jsonpath_test WHERE sample = '00' OFFSET 0)
	SELECT sample, sample::jsonpath FROM samples;
2023-11-25 13:48:52.191 UTC [139883] ERROR:  42601: trailing junk after numeric literal at or near "1.e" of jsonpath input
2023-11-25 13:48:52.191 UTC [139883] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:52.191 UTC [139883] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:52.191 UTC [139883] STATEMENT:  SELECT sample, sample::jsonpath FROM jsonpath_test WHERE sample = '1.e';
2023-11-25 13:48:52.201 UTC [139883] ERROR:  42601: trailing junk after numeric literal at or near "1.e" of jsonpath input
2023-11-25 13:48:52.201 UTC [139883] LOCATION:  jsonpath_yyerror, jsonpath_scan.l:286
2023-11-25 13:48:52.201 UTC [139883] STATEMENT:  WITH samples as (SELECT id, sample FROM jsonpath_test WHERE sample = '1.e' OFFSET 0)
	SELECT sample, sample::jsonpath FROM samples;
2023-11-25 13:48:52.202 UTC [139883] ERROR:  42601: trailing junk after numeric literal at or near "1.2e3a" of jsonpath input
2023-11-25 13:48:52.202 UTC [139883] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:52.202 UTC [139883] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:52.202 UTC [139883] STATEMENT:  SELECT sample, sample::jsonpath FROM jsonpath_test WHERE sample = '1.2e3a';
2023-11-25 13:48:52.209 UTC [139883] ERROR:  42601: trailing junk after numeric literal at or near "1.2e3a" of jsonpath input
2023-11-25 13:48:52.209 UTC [139883] LOCATION:  jsonpath_yyerror, jsonpath_scan.l:286
2023-11-25 13:48:52.209 UTC [139883] STATEMENT:  WITH samples as (SELECT id, sample FROM jsonpath_test WHERE sample = '1.2e3a' OFFSET 0)
	SELECT sample, sample::jsonpath FROM samples;
2023-11-25 13:48:53.023 UTC [139884] ERROR:  08006: connection to the remote node localhost:57637 failed with the following error: connection not open
2023-11-25 13:48:53.023 UTC [139884] LOCATION:  ReportConnectionError, remote_commands.c:266
2023-11-25 13:48:53.023 UTC [139884] STATEMENT:  SELECT count(*) FROM socket_test_table;
2023-11-25 13:48:54.233 UTC [140242] WARNING:  01000: fake_tuple_insert
2023-11-25 13:48:54.233 UTC [140242] LOCATION:  fake_tuple_insert, fake_am.c:204
2023-11-25 13:48:54.243 UTC [140242] WARNING:  01000: fake_scan_getnextslot
2023-11-25 13:48:54.243 UTC [140242] CONTEXT:  SQL statement "SELECT TRUE FROM test_tableam.test_hash_dist LIMIT 1"
2023-11-25 13:48:54.243 UTC [140242] LOCATION:  fake_scan_getnextslot, fake_am.c:88
2023-11-25 13:48:54.252 UTC [140242] WARNING:  01000: fake_scan_getnextslot
2023-11-25 13:48:54.252 UTC [140242] LOCATION:  fake_scan_getnextslot, fake_am.c:88
2023-11-25 13:48:54.252 UTC [140242] WARNING:  01000: fake_scan_getnextslot
2023-11-25 13:48:54.252 UTC [140242] LOCATION:  fake_scan_getnextslot, fake_am.c:88
2023-11-25 13:48:54.254 UTC [140242] WARNING:  01000: fake_scan_getnextslot
2023-11-25 13:48:54.254 UTC [140242] DETAIL:  from localhost:57637
2023-11-25 13:48:54.254 UTC [140242] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 13:48:54.254 UTC [140242] WARNING:  01000: fake_scan_getnextslot
2023-11-25 13:48:54.254 UTC [140242] DETAIL:  from localhost:57637
2023-11-25 13:48:54.254 UTC [140242] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 13:48:54.254 UTC [140242] WARNING:  01000: fake_scan_getnextslot
2023-11-25 13:48:54.254 UTC [140242] DETAIL:  from localhost:57637
2023-11-25 13:48:54.254 UTC [140242] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 13:48:54.254 UTC [140242] WARNING:  01000: fake_scan_getnextslot
2023-11-25 13:48:54.254 UTC [140242] DETAIL:  from localhost:57638
2023-11-25 13:48:54.254 UTC [140242] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 13:48:54.254 UTC [140242] WARNING:  01000: fake_scan_getnextslot
2023-11-25 13:48:54.254 UTC [140242] DETAIL:  from localhost:57638
2023-11-25 13:48:54.254 UTC [140242] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 13:48:54.256 UTC [140242] WARNING:  01000: fake_tuple_insert
2023-11-25 13:48:54.256 UTC [140242] DETAIL:  from localhost:57637
2023-11-25 13:48:54.256 UTC [140242] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 13:48:54.257 UTC [140242] WARNING:  01000: fake_scan_getnextslot
2023-11-25 13:48:54.257 UTC [140242] DETAIL:  from localhost:57637
2023-11-25 13:48:54.257 UTC [140242] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 13:48:54.257 UTC [140242] ERROR:  XX000: fake_tuple_delete not implemented
2023-11-25 13:48:54.257 UTC [140242] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:54.257 UTC [140242] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:54.257 UTC [140242] STATEMENT:  delete from test_hash_dist where id=1;
2023-11-25 13:48:54.259 UTC [140242] WARNING:  01000: fake_scan_getnextslot
2023-11-25 13:48:54.259 UTC [140242] DETAIL:  from localhost:57638
2023-11-25 13:48:54.259 UTC [140242] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 13:48:54.259 UTC [140242] WARNING:  01000: fake_tuple_insert
2023-11-25 13:48:54.259 UTC [140242] LOCATION:  fake_tuple_insert, fake_am.c:204
2023-11-25 13:48:54.260 UTC [140242] WARNING:  01000: fake_scan_getnextslot
2023-11-25 13:48:54.260 UTC [140242] CONTEXT:  SQL statement "SELECT TRUE FROM test_tableam.test_ref LIMIT 1"
2023-11-25 13:48:54.260 UTC [140242] LOCATION:  fake_scan_getnextslot, fake_am.c:88
2023-11-25 13:48:54.267 UTC [140242] WARNING:  01000: fake_scan_getnextslot
2023-11-25 13:48:54.267 UTC [140242] LOCATION:  fake_scan_getnextslot, fake_am.c:88
2023-11-25 13:48:54.268 UTC [140242] WARNING:  01000: fake_scan_getnextslot
2023-11-25 13:48:54.268 UTC [140242] LOCATION:  fake_scan_getnextslot, fake_am.c:88
2023-11-25 13:48:54.269 UTC [140242] WARNING:  01000: fake_scan_getnextslot
2023-11-25 13:48:54.269 UTC [140242] DETAIL:  from localhost:57637
2023-11-25 13:48:54.269 UTC [140242] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 13:48:54.269 UTC [140242] WARNING:  01000: fake_scan_getnextslot
2023-11-25 13:48:54.269 UTC [140242] DETAIL:  from localhost:57637
2023-11-25 13:48:54.269 UTC [140242] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 13:48:54.269 UTC [140242] WARNING:  01000: fake_tuple_insert
2023-11-25 13:48:54.269 UTC [140242] DETAIL:  from localhost:57637
2023-11-25 13:48:54.269 UTC [140242] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 13:48:54.269 UTC [140242] WARNING:  01000: fake_tuple_insert
2023-11-25 13:48:54.269 UTC [140242] DETAIL:  from localhost:57638
2023-11-25 13:48:54.269 UTC [140242] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 13:48:54.270 UTC [140242] WARNING:  01000: fake_scan_getnextslot
2023-11-25 13:48:54.270 UTC [140242] DETAIL:  from localhost:57637
2023-11-25 13:48:54.270 UTC [140242] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 13:48:54.270 UTC [140242] ERROR:  XX000: fake_tuple_delete not implemented
2023-11-25 13:48:54.270 UTC [140242] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:54.270 UTC [140242] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:54.270 UTC [140242] STATEMENT:  delete from test_ref;
2023-11-25 13:48:54.272 UTC [140242] WARNING:  01000: fake_scan_getnextslot
2023-11-25 13:48:54.272 UTC [140242] DETAIL:  from localhost:57637
2023-11-25 13:48:54.272 UTC [140242] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 13:48:54.272 UTC [140242] ERROR:  XX000: fake_fetch_row_version not implemented
2023-11-25 13:48:54.272 UTC [140242] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:48:54.272 UTC [140242] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:48:54.272 UTC [140242] STATEMENT:  update test_ref set a=2;
2023-11-25 13:48:54.273 UTC [140242] WARNING:  01000: fake_scan_getnextslot
2023-11-25 13:48:54.273 UTC [140242] CONTEXT:  SQL statement "SELECT TRUE FROM test_tableam.test_range_dist LIMIT 1"
2023-11-25 13:48:54.273 UTC [140242] LOCATION:  fake_scan_getnextslot, fake_am.c:88
2023-11-25 13:48:54.273 UTC [140242] WARNING:  01000: fake_scan_getnextslot
2023-11-25 13:48:54.273 UTC [140242] CONTEXT:  SQL statement "SELECT TRUE FROM test_tableam.test_range_dist LIMIT 1"
2023-11-25 13:48:54.273 UTC [140242] LOCATION:  fake_scan_getnextslot, fake_am.c:88
2023-11-25 13:48:54.280 UTC [140242] WARNING:  01000: fake_scan_getnextslot
2023-11-25 13:48:54.280 UTC [140242] DETAIL:  from localhost:57637
2023-11-25 13:48:54.280 UTC [140242] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 13:48:54.280 UTC [140242] WARNING:  01000: fake_scan_getnextslot
2023-11-25 13:48:54.280 UTC [140242] DETAIL:  from localhost:57638
2023-11-25 13:48:54.280 UTC [140242] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 13:48:54.280 UTC [140242] WARNING:  01000: fake_tuple_insert
2023-11-25 13:48:54.280 UTC [140242] DETAIL:  from localhost:57637
2023-11-25 13:48:54.280 UTC [140242] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 13:48:54.326 UTC [140269] WARNING:  01000: fake_tuple_insert
2023-11-25 13:48:54.326 UTC [140269] LOCATION:  fake_tuple_insert, fake_am.c:204
2023-11-25 13:48:54.348 UTC [140269] WARNING:  01000: fake_scan_getnextslot
2023-11-25 13:48:54.348 UTC [140269] CONTEXT:  SQL statement "SELECT TRUE FROM test_tableam.test_partitioned_p2 LIMIT 1"
2023-11-25 13:48:54.348 UTC [140269] LOCATION:  fake_scan_getnextslot, fake_am.c:88
2023-11-25 13:48:54.353 UTC [140269] WARNING:  01000: fake_scan_getnextslot
2023-11-25 13:48:54.353 UTC [140269] LOCATION:  fake_scan_getnextslot, fake_am.c:88
2023-11-25 13:48:54.354 UTC [140269] WARNING:  01000: fake_scan_getnextslot
2023-11-25 13:48:54.354 UTC [140269] LOCATION:  fake_scan_getnextslot, fake_am.c:88
2023-11-25 13:48:54.357 UTC [140269] WARNING:  01000: fake_tuple_insert
2023-11-25 13:48:54.357 UTC [140269] DETAIL:  from localhost:57637
2023-11-25 13:48:54.357 UTC [140269] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 13:48:54.358 UTC [140269] WARNING:  01000: fake_tuple_insert
2023-11-25 13:48:54.358 UTC [140269] DETAIL:  from localhost:57638
2023-11-25 13:48:54.358 UTC [140269] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 13:48:54.359 UTC [140269] WARNING:  01000: fake_scan_getnextslot
2023-11-25 13:48:54.359 UTC [140269] DETAIL:  from localhost:57638
2023-11-25 13:48:54.359 UTC [140269] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 13:48:54.359 UTC [140269] WARNING:  01000: fake_scan_getnextslot
2023-11-25 13:48:54.359 UTC [140269] DETAIL:  from localhost:57638
2023-11-25 13:48:54.359 UTC [140269] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 13:48:54.359 UTC [140269] WARNING:  01000: fake_scan_getnextslot
2023-11-25 13:48:54.359 UTC [140269] DETAIL:  from localhost:57637
2023-11-25 13:48:54.359 UTC [140269] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 13:48:54.359 UTC [140269] WARNING:  01000: fake_scan_getnextslot
2023-11-25 13:48:54.359 UTC [140269] DETAIL:  from localhost:57637
2023-11-25 13:48:54.359 UTC [140269] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 13:48:54.359 UTC [140269] WARNING:  01000: fake_scan_getnextslot
2023-11-25 13:48:54.359 UTC [140269] DETAIL:  from localhost:57638
2023-11-25 13:48:54.359 UTC [140269] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 13:48:54.359 UTC [140269] WARNING:  01000: fake_scan_getnextslot
2023-11-25 13:48:54.359 UTC [140269] DETAIL:  from localhost:57638
2023-11-25 13:48:54.359 UTC [140269] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:174
2023-11-25 13:48:54.377 UTC [140269] ERROR:  0A000: specifying a table access method is not supported on a partitioned table
2023-11-25 13:48:54.377 UTC [140269] LOCATION:  DefineRelation, tablecmds.c:947
2023-11-25 13:48:54.377 UTC [140269] STATEMENT:  CREATE TABLE test_partitioned(id int, p int, val int)
	PARTITION BY RANGE (p) USING fake_am;
2023-11-25 13:48:54.443 UTC [140304] ERROR:  XX000: the backend has already been assigned a transaction id
2023-11-25 13:48:54.443 UTC [140304] LOCATION:  assign_distributed_transaction_id, backend_data.c:168
2023-11-25 13:48:54.443 UTC [140304] STATEMENT:  SELECT assign_distributed_transaction_id(51, 51, '2017-01-01 00:00:00+0');
2023-11-25 13:48:54.444 UTC [140304] ERROR:  22012: division by zero
2023-11-25 13:48:54.444 UTC [140304] LOCATION:  int4div, int.c:840
2023-11-25 13:48:54.444 UTC [140304] STATEMENT:  SELECT 5 / 0;
2023-11-25 13:48:54.453 UTC [140306] ERROR:  XX000: the intermediate result size exceeds citus.max_intermediate_result_size (currently 2 kB)
2023-11-25 13:48:54.453 UTC [140306] DETAIL:  Citus restricts the size of intermediate results of complex subqueries and CTEs to avoid accidentally pulling large result sets into once place.
2023-11-25 13:48:54.453 UTC [140306] HINT:  To run the current query, set citus.max_intermediate_result_size to a higher value or -1 to disable.
2023-11-25 13:48:54.453 UTC [140306] LOCATION:  EnsureIntermediateSizeLimitNotExceeded, tuple_destination.c:164
2023-11-25 13:48:54.453 UTC [140306] STATEMENT:  WITH cte AS MATERIALIZED
	(
		SELECT * FROM users_table
	),
	cte2 AS MATERIALIZED (
		SELECT * FROM events_table
	)
	SELECT cte.user_id, cte.value_2 FROM cte,cte2 ORDER BY 1,2 LIMIT 10;
2023-11-25 13:48:54.456 UTC [140305] WARNING:  CIINF: Query could not find the intermediate result file "squares", it was mostly likely deleted due to an error in a parallel process within the same distributed transaction
2023-11-25 13:48:54.456 UTC [140305] LOCATION:  ReadIntermediateResultsIntoFuncOutput, intermediate_results.c:869
2023-11-25 13:48:54.554 UTC [140305] ERROR:  CIINF: Query could not find the intermediate result file "squares", it was mostly likely deleted due to an error in a parallel process within the same distributed transaction
2023-11-25 13:48:54.554 UTC [140305] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:148
2023-11-25 13:48:54.554 UTC [140305] STATEMENT:  SELECT x, x2
	FROM interesting_squares JOIN (SELECT * FROM read_intermediate_result('squares', 'binary') AS res (x text, x2 int)) squares ON (x = interested_in)
	WHERE user_id = 'jon' OR true
	ORDER BY x;
2023-11-25 13:48:54.555 UTC [140305] ERROR:  CIINF: Query could not find the intermediate result file "squares", it was mostly likely deleted due to an error in a parallel process within the same distributed transaction
2023-11-25 13:48:54.555 UTC [140305] LOCATION:  DefaultCitusNoticeReceiver, worker_log_messages.c:148
2023-11-25 13:48:54.555 UTC [140305] STATEMENT:  SELECT x, x2
	FROM interesting_squares JOIN (SELECT * FROM read_intermediate_result('squares', 'binary') AS res (x text, x2 int)) squares ON (x = interested_in)
	WHERE user_id = 'jon'
	ORDER BY x;
2023-11-25 13:48:54.556 UTC [140305] ERROR:  22021: invalid byte sequence for encoding "UTF8": 0x00
2023-11-25 13:48:54.556 UTC [140305] LOCATION:  report_invalid_encoding, mbutils.c:1665
2023-11-25 13:48:54.556 UTC [140305] STATEMENT:  SELECT * FROM read_intermediate_result('squares', 'binary') AS res (x text, x2 int);
2023-11-25 13:48:54.556 UTC [140305] ERROR:  22P02: invalid input syntax for type integer: "PGCOPY"
2023-11-25 13:48:54.556 UTC [140305] LOCATION:  pg_strtoint32, numutils.c:232
2023-11-25 13:48:54.556 UTC [140305] STATEMENT:  SELECT * FROM read_intermediate_result('squares', 'csv') AS res (x int, x2 int);
2023-11-25 13:48:54.563 UTC [140305] ERROR:  22P04: COPY file signature not recognized
2023-11-25 13:48:54.563 UTC [140305] LOCATION:  ReceiveCopyBinaryHeader, copyfromparse.c:198
2023-11-25 13:48:54.563 UTC [140305] STATEMENT:  SELECT * FROM read_intermediate_result('stored_squares', 'binary') AS res (s intermediate_results.square_type);
BEGIN
COPY 0
SELECT 5
COMMIT
2023-11-25 13:48:54.589 UTC [140305] ERROR:  XX000: cannot execute utility commands
2023-11-25 13:48:54.589 UTC [140305] LOCATION:  ExecuteQueryIntoDestReceiver, multi_executor.c:664
2023-11-25 13:48:54.589 UTC [140305] STATEMENT:  select broadcast_intermediate_result('a', 'create table foo(int serial)');
2023-11-25 13:48:54.590 UTC [140305] ERROR:  XX000: cannot execute utility commands
2023-11-25 13:48:54.590 UTC [140305] LOCATION:  ExecuteQueryIntoDestReceiver, multi_executor.c:664
2023-11-25 13:48:54.590 UTC [140305] STATEMENT:  select broadcast_intermediate_result('a', 'prepare foo as select 1');
2023-11-25 13:48:54.590 UTC [140305] ERROR:  XX000: cannot execute utility commands
2023-11-25 13:48:54.590 UTC [140305] LOCATION:  ExecuteQueryIntoDestReceiver, multi_executor.c:664
2023-11-25 13:48:54.590 UTC [140305] STATEMENT:  select create_intermediate_result('a', 'create table foo(int serial)');
2023-11-25 13:48:54.591 UTC [140305] WARNING:  CIINF: Query could not find the intermediate result file "squares_1", it was mostly likely deleted due to an error in a parallel process within the same distributed transaction
2023-11-25 13:48:54.591 UTC [140305] LOCATION:  ReadIntermediateResultsIntoFuncOutput, intermediate_results.c:869
2023-11-25 13:48:54.592 UTC [140305] WARNING:  CIINF: Query could not find the intermediate result file "notexistingfile", it was mostly likely deleted due to an error in a parallel process within the same distributed transaction
2023-11-25 13:48:54.592 UTC [140305] LOCATION:  ReadIntermediateResultsIntoFuncOutput, intermediate_results.c:869
2023-11-25 13:48:54.592 UTC [140305] WARNING:  CIINF: Query could not find the intermediate result file "notexistingfile", it was mostly likely deleted due to an error in a parallel process within the same distributed transaction
2023-11-25 13:48:54.592 UTC [140305] LOCATION:  ReadIntermediateResultsIntoFuncOutput, intermediate_results.c:869
2023-11-25 13:48:54.592 UTC [140305] ERROR:  22004: null array element not allowed in this context
2023-11-25 13:48:54.592 UTC [140305] LOCATION:  deconstruct_array, arrayfuncs.c:3525
2023-11-25 13:48:54.592 UTC [140305] STATEMENT:  SELECT * FROM read_intermediate_results(ARRAY['squares_1', NULL], 'binary') AS res (x int, x2 int);
2023-11-25 13:48:54.593 UTC [140305] WARNING:  CIINF: Query could not find the intermediate result file "squares_1", it was mostly likely deleted due to an error in a parallel process within the same distributed transaction
2023-11-25 13:48:54.593 UTC [140305] LOCATION:  ReadIntermediateResultsIntoFuncOutput, intermediate_results.c:869
2023-11-25 13:48:54.612 UTC [140305] WARNING:  CIINF: Query could not find the intermediate result file "squares_1", it was mostly likely deleted due to an error in a parallel process within the same distributed transaction
2023-11-25 13:48:54.612 UTC [140305] LOCATION:  ReadIntermediateResultsIntoFuncOutput, intermediate_results.c:869
2023-11-25 13:48:54.612 UTC [140305] WARNING:  CIINF: Query could not find the intermediate result file "squares_2", it was mostly likely deleted due to an error in a parallel process within the same distributed transaction
2023-11-25 13:48:54.612 UTC [140305] LOCATION:  ReadIntermediateResultsIntoFuncOutput, intermediate_results.c:869
2023-11-25 13:48:54.614 UTC [140306] ERROR:  XX000: the intermediate result size exceeds citus.max_intermediate_result_size (currently 2 kB)
2023-11-25 13:48:54.614 UTC [140306] DETAIL:  Citus restricts the size of intermediate results of complex subqueries and CTEs to avoid accidentally pulling large result sets into once place.
2023-11-25 13:48:54.614 UTC [140306] HINT:  To run the current query, set citus.max_intermediate_result_size to a higher value or -1 to disable.
2023-11-25 13:48:54.614 UTC [140306] LOCATION:  EnsureIntermediateSizeLimitNotExceeded, tuple_destination.c:164
2023-11-25 13:48:54.614 UTC [140306] STATEMENT:  WITH cte AS MATERIALIZED (SELECT * FROM users_table WHERE user_id IN (1,2,3,4,5))
	SELECT * FROM cte ORDER BY 1,2,3,4,5 LIMIT 10;
2023-11-25 13:48:54.615 UTC [140305] ERROR:  XX000: cannot connect to localhost:57635 to fetch intermediate results
2023-11-25 13:48:54.615 UTC [140305] LOCATION:  fetch_intermediate_results, intermediate_results.c:929
2023-11-25 13:48:54.615 UTC [140305] STATEMENT:  SELECT * FROM fetch_intermediate_results(ARRAY['squares_1', 'squares_2']::text[], 'localhost', 57635);
2023-11-25 13:48:54.616 UTC [140306] ERROR:  XX000: the intermediate result size exceeds citus.max_intermediate_result_size (currently 0 kB)
2023-11-25 13:48:54.616 UTC [140306] DETAIL:  Citus restricts the size of intermediate results of complex subqueries and CTEs to avoid accidentally pulling large result sets into once place.
2023-11-25 13:48:54.616 UTC [140306] HINT:  To run the current query, set citus.max_intermediate_result_size to a higher value or -1 to disable.
2023-11-25 13:48:54.616 UTC [140306] LOCATION:  EnsureIntermediateSizeLimitNotExceeded, tuple_destination.c:164
2023-11-25 13:48:54.616 UTC [140306] STATEMENT:  WITH cte AS MATERIALIZED (SELECT * FROM users_table WHERE user_id=1),
	cte2 AS MATERIALIZED (SELECT * FROM users_table WHERE user_id=2),
	cte3 AS MATERIALIZED (SELECT * FROM users_table WHERE user_id=3),
	cte4 AS MATERIALIZED (SELECT * FROM users_table WHERE user_id=4),
	cte5 AS MATERIALIZED (SELECT * FROM users_table WHERE user_id=5)
	SELECT * FROM (
	(SELECT * FROM cte)
	UNION
	(SELECT * FROM cte2)
	UNION
	(SELECT * FROM cte3)
	UNION
	(SELECT * FROM cte4)
	UNION
	(SELECT * FROM cte5)
	)a ORDER BY 1,2,3,4,5 LIMIT 10;
2023-11-25 13:48:54.617 UTC [140305] WARNING:  CIINF: Query could not find the intermediate result file "squares_1", it was mostly likely deleted due to an error in a parallel process within the same distributed transaction
2023-11-25 13:48:54.617 UTC [140305] LOCATION:  ReadIntermediateResultsIntoFuncOutput, intermediate_results.c:869
2023-11-25 13:48:54.617 UTC [140305] WARNING:  CIINF: Query could not find the intermediate result file "squares_2", it was mostly likely deleted due to an error in a parallel process within the same distributed transaction
2023-11-25 13:48:54.617 UTC [140305] LOCATION:  ReadIntermediateResultsIntoFuncOutput, intermediate_results.c:869
2023-11-25 13:48:54.630 UTC [140305] ERROR:  22004: worker array object cannot contain null values
2023-11-25 13:48:54.630 UTC [140305] LOCATION:  DeconstructArrayObject, array_type.c:43
2023-11-25 13:48:54.630 UTC [140305] STATEMENT:  SELECT * FROM fetch_intermediate_results(ARRAY[NULL, 'squares_1', 'squares_2']::text[], 'localhost', 57637);
2023-11-25 13:48:54.631 UTC [140306] ERROR:  XX000: the intermediate result size exceeds citus.max_intermediate_result_size (currently 3 kB)
2023-11-25 13:48:54.631 UTC [140306] DETAIL:  Citus restricts the size of intermediate results of complex subqueries and CTEs to avoid accidentally pulling large result sets into once place.
2023-11-25 13:48:54.631 UTC [140306] HINT:  To run the current query, set citus.max_intermediate_result_size to a higher value or -1 to disable.
2023-11-25 13:48:54.631 UTC [140306] LOCATION:  EnsureIntermediateSizeLimitNotExceeded, tuple_destination.c:164
2023-11-25 13:48:54.631 UTC [140306] STATEMENT:  WITH cte AS MATERIALIZED (
		WITH cte2 AS MATERIALIZED (
			SELECT * FROM users_table WHERE user_id IN (3,4,5,6)
		),
		cte3 AS MATERIALIZED(
			SELECT * FROM events_table WHERE event_type = 1
		)
		SELECT * FROM cte2, cte3 WHERE cte2.value_1 IN (SELECT value_2 FROM cte3)
	)
	SELECT count(*) FROM cte;
2023-11-25 13:48:54.632 UTC [140305] WARNING:  CIINF: Query could not find the intermediate result file "squares_1", it was mostly likely deleted due to an error in a parallel process within the same distributed transaction
2023-11-25 13:48:54.632 UTC [140305] LOCATION:  ReadIntermediateResultsIntoFuncOutput, intermediate_results.c:869
2023-11-25 13:48:54.632 UTC [140305] WARNING:  CIINF: Query could not find the intermediate result file "squares_2", it was mostly likely deleted due to an error in a parallel process within the same distributed transaction
2023-11-25 13:48:54.632 UTC [140305] LOCATION:  ReadIntermediateResultsIntoFuncOutput, intermediate_results.c:869
2023-11-25 13:48:54.635 UTC [140306] ERROR:  XX000: the intermediate result size exceeds citus.max_intermediate_result_size (currently 2 kB)
2023-11-25 13:48:54.635 UTC [140306] DETAIL:  Citus restricts the size of intermediate results of complex subqueries and CTEs to avoid accidentally pulling large result sets into once place.
2023-11-25 13:48:54.635 UTC [140306] HINT:  To run the current query, set citus.max_intermediate_result_size to a higher value or -1 to disable.
2023-11-25 13:48:54.635 UTC [140306] LOCATION:  EnsureIntermediateSizeLimitNotExceeded, tuple_destination.c:164
2023-11-25 13:48:54.635 UTC [140306] STATEMENT:  WITH cte AS MATERIALIZED (
		WITH cte2 AS MATERIALIZED (
			SELECT * FROM users_table WHERE user_id IN (1, 2)
		),
		cte3 AS MATERIALIZED (
			SELECT * FROM users_table WHERE user_id = 3
		)
		SELECT * FROM cte2 UNION (SELECT * FROM cte3)
	),
	cte4 AS MATERIALIZED (
		SELECT * FROM events_table
	)
	SELECT * FROM cte UNION ALL
	SELECT * FROM cte4 ORDER BY 1,2,3,4,5 LIMIT 5;
2023-11-25 13:48:54.639 UTC [140306] ERROR:  XX000: the intermediate result size exceeds citus.max_intermediate_result_size (currently 1 kB)
2023-11-25 13:48:54.639 UTC [140306] DETAIL:  Citus restricts the size of intermediate results of complex subqueries and CTEs to avoid accidentally pulling large result sets into once place.
2023-11-25 13:48:54.639 UTC [140306] HINT:  To run the current query, set citus.max_intermediate_result_size to a higher value or -1 to disable.
2023-11-25 13:48:54.639 UTC [140306] LOCATION:  EnsureIntermediateSizeLimitNotExceeded, tuple_destination.c:164
2023-11-25 13:48:54.639 UTC [140306] STATEMENT:  WITH cte AS MATERIALIZED (
		WITH cte2 AS MATERIALIZED (
			SELECT * FROM users_table WHERE user_id IN (1, 2)
		),
		cte3 AS MATERIALIZED (
			SELECT * FROM users_table WHERE user_id = 3
		)
		SELECT * FROM cte2 UNION (SELECT * FROM cte3)
	),
	cte4 AS MATERIALIZED (
		SELECT * FROM events_table
	)
	SELECT * FROM cte UNION ALL
	SELECT * FROM cte4 ORDER BY 1,2,3,4,5 LIMIT 5;
2023-11-25 13:48:55.290 UTC [140602] ERROR:  XX000: query must be distributed and shouldn't require any merging on the coordinator.
2023-11-25 13:48:55.290 UTC [140602] LOCATION:  partition_task_list_results, distributed_intermediate_results.c:60
2023-11-25 13:48:55.290 UTC [140602] STATEMENT:  SELECT partition_task_list_results('test', $$ SELECT avg(a) FROM source_table $$, 'target_table');
2023-11-25 13:48:55.290 UTC [140602] ERROR:  XX000: query must be distributed and shouldn't require any merging on the coordinator.
2023-11-25 13:48:55.290 UTC [140602] LOCATION:  partition_task_list_results, distributed_intermediate_results.c:60
2023-11-25 13:48:55.290 UTC [140602] STATEMENT:  SELECT partition_task_list_results('test', $$ SELECT * FROM generate_series(1, 2) $$, 'target_table');
2023-11-25 13:48:55.411 UTC [140604] ERROR:  25001: cannot perform query with placements that were modified over multiple connections
2023-11-25 13:48:55.411 UTC [140604] LOCATION:  FindPlacementListConnection, placement_connection.c:613
2023-11-25 13:48:55.411 UTC [140604] STATEMENT:  SELECT COUNT(*) FROM test_table JOIN ref_test_table USING (id);
2023-11-25 13:48:55.799 UTC [140762] ERROR:  0A000: repartitioning results of a tasklist is only supported when target relation is hash or range partitioned.
2023-11-25 13:48:55.799 UTC [140762] LOCATION:  PartitionTasklistResults, distributed_intermediate_results.c:152
2023-11-25 13:48:55.799 UTC [140762] STATEMENT:  CREATE TABLE distributed_result_info AS
	  SELECT * FROM redistribute_task_list_results('test', $$ SELECT * FROM source_table $$, 'target_table_reference');
2023-11-25 13:48:55.800 UTC [140762] ERROR:  0A000: repartitioning results of a tasklist is only supported when target relation is hash or range partitioned.
2023-11-25 13:48:55.800 UTC [140762] LOCATION:  PartitionTasklistResults, distributed_intermediate_results.c:152
2023-11-25 13:48:55.800 UTC [140762] STATEMENT:  CREATE TABLE distributed_result_info AS
	  SELECT * FROM redistribute_task_list_results('test', $$ SELECT * FROM source_table $$, 'target_table_append');
2023-11-25 13:48:55.827 UTC [133093] LOG:  00000: received SIGHUP, reloading configuration files
2023-11-25 13:48:55.827 UTC [133093] LOCATION:  SIGHUP_handler, postmaster.c:2769
2023-11-25 13:48:55.828 UTC [133093] LOG:  00000: parameter "deadlock_timeout" changed to "250ms"
2023-11-25 13:48:55.828 UTC [133093] LOCATION:  ProcessConfigFileInternal, guc-file.l:453
2023-11-25 13:48:55.985 UTC [140601] ERROR:  XX000: cannot EXPLAIN ANALYZE multiple queries
2023-11-25 13:48:55.985 UTC [140601] LOCATION:  worker_save_query_explain_analyze, multi_explain.c:1048
2023-11-25 13:48:55.985 UTC [140601] STATEMENT:  SELECT * FROM worker_save_query_explain_analyze('SELECT 1; SELECT 2', '{"costs": false, "timing": false, "summary": false}'::jsonb) as (a int);
2023-11-25 13:48:55.985 UTC [140601] ERROR:  42703: column "x" does not exist at character 8
2023-11-25 13:48:55.985 UTC [140601] LOCATION:  errorMissingColumn, parse_relation.c:3656
2023-11-25 13:48:55.985 UTC [140601] STATEMENT:  SELECT * FROM worker_save_query_explain_analyze('SELECT x', '{"costs": false, "timing": false, "summary": false}'::jsonb) as (a int);
2023-11-25 13:48:55.985 UTC [140601] ERROR:  XX000: Invalid explain analyze format: "invlaid_format"
2023-11-25 13:48:55.985 UTC [140601] LOCATION:  ExtractFieldExplainFormat, multi_explain.c:1163
2023-11-25 13:48:55.985 UTC [140601] STATEMENT:  SELECT * FROM worker_save_query_explain_analyze('SELECT 1', '{"format": "invlaid_format"}') as (a int);
2023-11-25 13:48:56.316 UTC [140604] ERROR:  40P01: canceling the transaction since it was involved in a distributed deadlock
2023-11-25 13:48:56.316 UTC [140604] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 13:48:56.316 UTC [140604] STATEMENT:  SELECT id, pg_advisory_lock(15) FROM test_table;
2023-11-25 13:48:56.342 UTC [140604] ERROR:  40P01: canceling the transaction since it was involved in a distributed deadlock
2023-11-25 13:48:56.342 UTC [140604] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 13:48:56.342 UTC [140604] STATEMENT:  SELECT id, pg_advisory_lock(15) FROM test_table;
2023-11-25 13:48:56.367 UTC [140604] ERROR:  40P01: canceling the transaction since it was involved in a distributed deadlock
2023-11-25 13:48:56.367 UTC [140604] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 13:48:56.367 UTC [140604] STATEMENT:  SELECT id, pg_advisory_lock(15) FROM test_table;
2023-11-25 13:48:56.393 UTC [140604] ERROR:  40P01: canceling the transaction since it was involved in a distributed deadlock
2023-11-25 13:48:56.393 UTC [140604] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 13:48:56.393 UTC [140604] STATEMENT:  SELECT id, pg_advisory_lock(15) FROM test_table;
2023-11-25 13:48:56.418 UTC [140604] ERROR:  40P01: canceling the transaction since it was involved in a distributed deadlock
2023-11-25 13:48:56.418 UTC [140604] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 13:48:56.418 UTC [140604] STATEMENT:  SELECT id, pg_advisory_lock(15) FROM test_table;
2023-11-25 13:48:56.443 UTC [140604] ERROR:  40P01: canceling the transaction since it was involved in a distributed deadlock
2023-11-25 13:48:56.443 UTC [140604] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 13:48:56.443 UTC [140604] STATEMENT:  SELECT id, pg_advisory_lock(15) FROM test_table;
2023-11-25 13:48:56.469 UTC [140604] ERROR:  40P01: canceling the transaction since it was involved in a distributed deadlock
2023-11-25 13:48:56.469 UTC [140604] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 13:48:56.469 UTC [140604] STATEMENT:  SELECT id, pg_advisory_lock(15) FROM test_table;
2023-11-25 13:48:56.494 UTC [140604] ERROR:  40P01: canceling the transaction since it was involved in a distributed deadlock
2023-11-25 13:48:56.494 UTC [140604] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 13:48:56.494 UTC [140604] STATEMENT:  SELECT id, pg_advisory_lock(15) FROM test_table;
2023-11-25 13:48:56.520 UTC [140604] ERROR:  40P01: canceling the transaction since it was involved in a distributed deadlock
2023-11-25 13:48:56.520 UTC [140604] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 13:48:56.520 UTC [140604] STATEMENT:  SELECT id, pg_advisory_lock(15) FROM test_table;
2023-11-25 13:48:56.545 UTC [140604] ERROR:  40P01: canceling the transaction since it was involved in a distributed deadlock
2023-11-25 13:48:56.545 UTC [140604] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 13:48:56.545 UTC [140604] STATEMENT:  SELECT id, pg_advisory_lock(15) FROM test_table;
2023-11-25 13:48:56.723 UTC [133093] LOG:  00000: received SIGHUP, reloading configuration files
2023-11-25 13:48:56.723 UTC [133093] LOCATION:  SIGHUP_handler, postmaster.c:2769
2023-11-25 13:48:56.723 UTC [133093] LOG:  00000: parameter "deadlock_timeout" removed from configuration file, reset to default
2023-11-25 13:48:56.723 UTC [133093] LOCATION:  ProcessConfigFileInternal, guc-file.l:388
2023-11-25 13:48:57.094 UTC [140603] ERROR:  XX000: worker_partition_query_result can only be used in a transaction block
2023-11-25 13:48:57.094 UTC [140603] LOCATION:  worker_partition_query_result, partitioned_intermediate_results.c:151
2023-11-25 13:48:57.094 UTC [140603] STATEMENT:  SELECT * FROM worker_partition_query_result('squares_range',
	                                            'SELECT i, i * i FROM generate_series(1, 10) i',
	                                            1, 'range', '{0}'::text[], '{20}'::text[], true);
2023-11-25 13:48:57.094 UTC [140603] ERROR:  42601: syntax error at or near "SELECxT" at character 1
2023-11-25 13:48:57.094 UTC [140603] LOCATION:  scanner_yyerror, scan.l:1188
2023-11-25 13:48:57.094 UTC [140603] STATEMENT:  SELECT worker_partition_query_result('squares_range',
	                                     'SELECxT i, i * i FROM generate_series(1, 10) i',
	                                     1, 'range',
	                                     '{0,21,41,61}'::text[],
	                                     '{20,40,60,100}'::text[],
	                                     true);
2023-11-25 13:48:57.094 UTC [140603] ERROR:  42602: result key "squares_range/a/" contains invalid character
2023-11-25 13:48:57.094 UTC [140603] HINT:  Result keys may only contain letters, numbers, underscores and hyphens.
2023-11-25 13:48:57.094 UTC [140603] LOCATION:  QueryResultFileName, intermediate_results.c:652
2023-11-25 13:48:57.094 UTC [140603] STATEMENT:  SELECT worker_partition_query_result('squares_range/a/',
	                                     'SELECT i, i * i FROM generate_series(1, 10) i',
	                                     1, 'range',
	                                     '{0,21,41,61}'::text[],
	                                     '{20,40,60,100}'::text[],
	                                     true);
2023-11-25 13:48:57.095 UTC [140603] ERROR:  22023: number of partitions cannot be 0
2023-11-25 13:48:57.095 UTC [140603] LOCATION:  worker_partition_query_result, partitioned_intermediate_results.c:176
2023-11-25 13:48:57.095 UTC [140603] STATEMENT:  SELECT worker_partition_query_result('squares_range',
	                                     'SELECT i, i * i FROM generate_series(1, 10) i',
	                                     1, 'range', ARRAY[]::text[], ARRAY[]::text[], true);
2023-11-25 13:48:57.095 UTC [140603] ERROR:  22023: only hash and range partitiong schemes are supported
2023-11-25 13:48:57.095 UTC [140603] LOCATION:  worker_partition_query_result, partitioned_intermediate_results.c:135
2023-11-25 13:48:57.095 UTC [140603] STATEMENT:  SELECT worker_partition_query_result('squares_range',
	                                     'SELECT i, i * i FROM generate_series(1, 10) i',
	                                     1, 'append',
	                                     '{0,21,41,61}'::text[],
	                                     '{20,40,60,100}'::text[],
	                                     true);
2023-11-25 13:48:57.095 UTC [140603] ERROR:  22023: query must generate a set of rows
2023-11-25 13:48:57.095 UTC [140603] LOCATION:  worker_partition_query_result, partitioned_intermediate_results.c:187
2023-11-25 13:48:57.095 UTC [140603] STATEMENT:  SELECT worker_partition_query_result('squares_range',
	                                     'INSERT INTO t VALUES (1), (2)',
	                                     1, 'range',
	                                     '{0,21,41,61}'::text[],
	                                     '{20,40,60,100}'::text[],
	                                     true);
2023-11-25 13:48:57.096 UTC [140603] ERROR:  22023: partition column index must be between 0 and 1
2023-11-25 13:48:57.096 UTC [140603] LOCATION:  worker_partition_query_result, partitioned_intermediate_results.c:193
2023-11-25 13:48:57.096 UTC [140603] STATEMENT:  SELECT worker_partition_query_result('squares_range',
	                                     'SELECT i, i * i FROM generate_series(1, 10) i',
	                                     -1, 'range',
	                                     '{0,21,41,61}'::text[],
	                                     '{20,40,60,100}'::text[],
	                                     true);
2023-11-25 13:48:57.096 UTC [140603] ERROR:  22023: partition column index must be between 0 and 1
2023-11-25 13:48:57.096 UTC [140603] LOCATION:  worker_partition_query_result, partitioned_intermediate_results.c:193
2023-11-25 13:48:57.096 UTC [140603] STATEMENT:  SELECT worker_partition_query_result('squares_range',
	                                     'SELECT i, i * i FROM generate_series(1, 10) i',
	                                     2, 'range',
	                                     '{0,21,41,61}'::text[],
	                                     '{20,40,60,100}'::text[],
	                                     true);
2023-11-25 13:48:57.096 UTC [140603] ERROR:  22023: min values and max values must have the same number of elements
2023-11-25 13:48:57.096 UTC [140603] LOCATION:  worker_partition_query_result, partitioned_intermediate_results.c:167
2023-11-25 13:48:57.096 UTC [140603] STATEMENT:  SELECT worker_partition_query_result('squares_range',
	                                     'SELECT i, i * i FROM generate_series(1, 10) i',
	                                     1, 'range',
	                                     '{0,21,41,61,101}'::text[],
	                                     '{20,40,60,100}'::text[],
	                                     true);
2023-11-25 13:48:57.096 UTC [140603] ERROR:  XX000: hash partitioned table has uninitialized shards
2023-11-25 13:48:57.096 UTC [140603] LOCATION:  ErrorIfInconsistentShardIntervals, metadata_cache.c:1976
2023-11-25 13:48:57.096 UTC [140603] STATEMENT:  SELECT worker_partition_query_result('squares_hash',
	                                     'SELECT i, i * i FROM generate_series(1, 10) i',
	                                     1, 'hash',
	                                     '{NULL,21,41,61}'::text[],
	                                     '{20,40,60,100}'::text[],
	                                     true);
2023-11-25 13:48:57.096 UTC [140603] ERROR:  XX000: cannot execute multiple utility events
2023-11-25 13:48:57.096 UTC [140603] LOCATION:  ParseTreeRawStmt, worker_data_fetch_protocol.c:319
2023-11-25 13:48:57.096 UTC [140603] STATEMENT:  SELECT worker_partition_query_result('squares_hash',
	                                     'SELECT i, i * i FROM generate_series(1, 10) i; SELECT 4, 16;',
	                                     1, 'hash',
	                                     '{NULL,21,41,61}'::text[],
	                                     '{20,40,60,100}'::text[],
	                                     true);
2023-11-25 13:48:57.614 UTC [140603] ERROR:  55000: could not find shard for partition column value
2023-11-25 13:48:57.614 UTC [140603] CONTEXT:  SQL statement "INSERT INTO t SELECT x, x * x * x FROM generate_series(1, 105) x"
	PL/pgSQL function test_partition_query_results(regclass,text,boolean) line 35 at EXECUTE
2023-11-25 13:48:57.614 UTC [140603] LOCATION:  ShardIdForTuple, multi_copy.c:2614
2023-11-25 13:48:57.614 UTC [140603] STATEMENT:  CALL test_partition_query_results('t', 'SELECT x, x * x * x FROM generate_series(1, 105) x');
2023-11-25 13:48:58.186 UTC [141268] ERROR:  0A000: cannot perform distributed planning on this query
2023-11-25 13:48:58.186 UTC [141268] DETAIL:  Cartesian products are currently unsupported
2023-11-25 13:48:58.186 UTC [141268] LOCATION:  JoinSequenceArray, multi_physical_planner.c:3589
2023-11-25 13:48:58.186 UTC [141268] STATEMENT:  SELECT count(*) FROM events_reference_table e1 CROSS JOIN events_table e2 CROSS JOIN users_table u;
2023-11-25 13:48:58.187 UTC [141268] ERROR:  0A000: cannot perform distributed planning on this query
2023-11-25 13:48:58.187 UTC [141268] DETAIL:  Cartesian products are currently unsupported
2023-11-25 13:48:58.187 UTC [141268] LOCATION:  JoinSequenceArray, multi_physical_planner.c:3589
2023-11-25 13:48:58.187 UTC [141268] STATEMENT:  SELECT count(*) FROM events_reference_table e1, events_table e2, users_table u;
2023-11-25 13:48:58.271 UTC [141264] ERROR:  0A000: cannot perform distributed planning on this query
2023-11-25 13:48:58.271 UTC [141264] DETAIL:  Cartesian products are currently unsupported
2023-11-25 13:48:58.271 UTC [141264] LOCATION:  JoinSequenceArray, multi_physical_planner.c:3589
2023-11-25 13:48:58.271 UTC [141264] STATEMENT:  SELECT
		avg(unit_price)
	FROM
		(SELECT
			l_orderkey,
			avg(o_totalprice / l_quantity) AS unit_price
		FROM
			lineitem_subquery,
			orders_subquery
		GROUP BY
			l_orderkey) AS unit_prices;
2023-11-25 13:48:58.392 UTC [141265] ERROR:  0A000: complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
2023-11-25 13:48:58.392 UTC [141265] LOCATION:  JoinOrderList, multi_join_order.c:310
2023-11-25 13:48:58.392 UTC [141265] STATEMENT:  SELECT count(*) FROM lineitem, orders WHERE l_orderkey + 1 = o_orderkey;
2023-11-25 13:48:58.430 UTC [141268] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:58.430 UTC [141268] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:58.430 UTC [141268] STATEMENT:  SELECT count(*) FROM users_table u1 CROSS JOIN users_ref_test_table ref2 LEFT JOIN users_table u2 ON (ref2.id = u2.user_id);
2023-11-25 13:48:58.431 UTC [141268] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:58.431 UTC [141268] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:58.431 UTC [141268] STATEMENT:  SELECT count(*) FROM users_table u1 CROSS JOIN users_ref_test_table ref2 FULL JOIN users_table u2 ON (ref2.id = u2.user_id);
2023-11-25 13:48:58.432 UTC [141268] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:58.432 UTC [141268] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:58.432 UTC [141268] STATEMENT:  SELECT count(*) FROM users_table u1 CROSS JOIN users_ref_test_table ref2 RIGHT JOIN users_table u2 ON (ref2.id = u2.user_id);
2023-11-25 13:48:58.451 UTC [141269] ERROR:  0A000: complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
2023-11-25 13:48:58.451 UTC [141269] LOCATION:  JoinOrderList, multi_join_order.c:310
2023-11-25 13:48:58.451 UTC [141269] STATEMENT:  SELECT ("final_query"."event_types") as types, count(*) AS sumOfEventType
	FROM
	  ( SELECT *, random()
	   FROM
	     ( SELECT "t"."user_id", "t"."time", unnest("t"."collected_events") AS "event_types"
	      FROM
	        ( SELECT "t1"."user_id", min("t1"."time") AS "time", array_agg(("t1"."event") ORDER BY TIME ASC, event DESC) AS collected_events
	         FROM (
	                 (SELECT
	                    *
	                  FROM
	                   (SELECT
	                      "events"."user_id", "events"."time", 0 AS event
	                    FROM
	                      events_table as  "events"
	                    WHERE
	                      event_type IN (1, 2) ) events_subquery_1)
	                 UNION
	                 (SELECT
	                    *
	                  FROM
	                    (SELECT
	                        "events"."user_id", "events"."time", 1 AS event
	                     FROM
	                        events_table as "events"
	                     WHERE
	                      event_type IN (3, 4) ) events_subquery_2)
	               UNION
	                 (SELECT
	                    *
	                  FROM
	                    (SELECT
	                        "events"."user_id", "events"."time", 2 AS event
	                     FROM
	                        events_table as  "events",  users_table as "users"
	                     WHERE
	                      event_type IN (5, 6)  AND users.user_id != events.user_id ) events_subquery_3)
	                UNION
	                  (SELECT
	                      *
	                   FROM
	                    (SELECT
	                        "events"."user_id", "events"."time", 3 AS event
	                     FROM
	                      events_table as "events"
	                     WHERE
	                      event_type IN (4, 5)) events_subquery_4)) t1
	         GROUP BY "t1"."user_id") AS t) "q"
	INNER JOIN
	     (SELECT
	        "users"."user_id"
	      FROM
	        users_table as "users"
	      WHERE
	        value_1 > 0 and value_1 < 4) AS t
	    ON (t.user_id = q.user_id)) as final_query
	GROUP BY
	  types
	ORDER BY
	  types;
2023-11-25 13:48:58.617 UTC [141264] ERROR:  0A000: cannot push down this subquery
2023-11-25 13:48:58.617 UTC [141264] DETAIL:  Limit in subquery without limit in the outermost query is unsupported
2023-11-25 13:48:58.617 UTC [141264] LOCATION:  DeferErrorIfCannotPushdownSubquery, query_pushdown_planning.c:1048
2023-11-25 13:48:58.617 UTC [141264] STATEMENT:  SELECT
		avg(o_totalprice/l_quantity)
	FROM
			(SELECT
				l_orderkey,
				l_quantity
			FROM
				lineitem_subquery
			ORDER BY
				l_orderkey, l_quantity
			LIMIT 10
			) lineitem_quantities
		JOIN LATERAL
			(SELECT
				o_totalprice
			FROM
				orders_subquery
			WHERE
				lineitem_quantities.l_orderkey = o_orderkey) orders_price ON true;
2023-11-25 13:48:58.626 UTC [141269] ERROR:  0A000: cannot push down this subquery
2023-11-25 13:48:58.626 UTC [141269] DETAIL:  Limit clause is currently unsupported when a lateral subquery references a column from complex subqueries, CTEs or local tables
2023-11-25 13:48:58.626 UTC [141269] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 13:48:58.626 UTC [141269] STATEMENT:  SELECT "some_users_data".user_id, lastseen
	FROM
	     (SELECT user_id, max(time) AS lastseen
	      FROM
	        (SELECT user_id, time
	         FROM
	           (SELECT
	              user_id, time
	            FROM
	              events_table as "events"
	            WHERE
	              user_id > 1 and user_id < 4) "events_1"
	         ORDER BY
	           time DESC
	         LIMIT 1000) "recent_events_1"
	      GROUP BY
	        user_id
	      ORDER BY
	        max(TIME) DESC) "some_recent_users"
	   JOIN LATERAL
	     (SELECT
	        "users".user_id
	      FROM
	        users_table as "users"
	      WHERE
	        "users"."value_1" = "some_recent_users"."user_id" AND
	        users.value_2 > 1 and users.value_2 < 3
	      ORDER BY 1 LIMIT 1) "some_users_data"
	     ON TRUE
	ORDER BY
	  user_id
	limit 50;
2023-11-25 13:48:58.627 UTC [141269] ERROR:  0A000: cannot push down this subquery
2023-11-25 13:48:58.627 UTC [141269] DETAIL:  Limit clause is currently unsupported when a lateral subquery references a column from complex subqueries, CTEs or local tables
2023-11-25 13:48:58.627 UTC [141269] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 13:48:58.627 UTC [141269] STATEMENT:  SELECT "some_users_data".user_id, lastseen
	FROM
	     (SELECT 2 * user_id as user_id, max(time) AS lastseen
	      FROM
	        (SELECT user_id, time
	         FROM
	           (SELECT
	              user_id, time
	            FROM
	              events_table as "events"
	            WHERE
	              user_id > 1 and user_id < 4) "events_1"
	         ORDER BY
	           time DESC
	         LIMIT 1000) "recent_events_1"
	      GROUP BY
	        user_id
	      ORDER BY
	        max(TIME) DESC) "some_recent_users"
	   JOIN LATERAL
	     (SELECT
	        "users".user_id
	      FROM
	        users_table as "users"
	      WHERE
	        "users"."user_id" = "some_recent_users"."user_id" AND
	        users.value_2 > 1 and users.value_2 < 3
	      ORDER BY 1 LIMIT 1) "some_users_data"
	     ON TRUE
	ORDER BY
	  user_id
	limit 50;
2023-11-25 13:48:58.632 UTC [141266] ERROR:  0A000: cannot push down this subquery
2023-11-25 13:48:58.632 UTC [141266] DETAIL:  Distinct on columns without partition column is currently unsupported
2023-11-25 13:48:58.632 UTC [141266] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 13:48:58.632 UTC [141266] STATEMENT:  SELECT a.user_id, avg(b.value_2) as subquery_avg
	FROM
		(SELECT
	      user_id
	   FROM
	      users_table
		 WHERE
	      (value_1 > 2)
		 GROUP BY
	      user_id
		 HAVING
	      count(distinct value_1) > 2
		) as a
		LEFT JOIN
		(SELECT
	      DISTINCT ON (value_2) value_2 , user_id, value_3
		 FROM
	      users_table
		 WHERE
	      (value_1 > 3)
		 ORDER BY
	      1,2,3
		) AS b
		USING (user_id)
	GROUP BY user_id;
2023-11-25 13:48:58.645 UTC [141269] ERROR:  0A000: cannot push down this subquery
2023-11-25 13:48:58.645 UTC [141269] DETAIL:  Limit clause is currently unsupported when a lateral subquery references a column from complex subqueries, CTEs or local tables
2023-11-25 13:48:58.645 UTC [141269] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 13:48:58.645 UTC [141269] STATEMENT:  SELECT user_id, lastseen
	FROM
	  (SELECT
	      "some_users_data".user_id, lastseen
	   FROM
	     (SELECT
	        filter_users_1.user_id, time AS lastseen
	      FROM
	        (SELECT
	            user_where_1_1.user_id
	         FROM
	           (SELECT
	              "users"."user_id"
	            FROM
	              users_table as "users"
	            WHERE
	              user_id > 1 and user_id < 4 and value_1 > 2) user_where_1_1
	         INNER JOIN
	           (SELECT
	              "users"."user_id"
	            FROM
	              users_table as "users"
	            WHERE
	              user_id > 1 and user_id < 4  and value_2 > 3) user_where_1_join_1
	           ON ("user_where_1_1".user_id != "user_where_1_join_1".user_id)) filter_users_1
	      JOIN LATERAL
	        (SELECT
	            user_id, time
	         FROM
	          events_table as "events"
	         WHERE
	          user_id > 1 and user_id < 4 and user_id = filter_users_1.user_id
	         ORDER BY
	          time DESC
	         LIMIT 1) "last_events_1" ON true
	      ORDER BY time DESC
	      LIMIT 10) "some_recent_users"
	   JOIN LATERAL
	     (SELECT
	        "users".user_id
	      FROM
	        users_table  as "users"
	      WHERE
	        "users"."user_id" = "some_recent_users"."user_id" AND
	        "users"."value_2" > 4
	      ORDER BY 1 LIMIT 1) "some_users_data" ON true
	   ORDER BY
	    lastseen DESC
	   LIMIT 10) "some_users"
	ORDER BY
	  user_id DESC, lastseen DESC
	LIMIT 10;
2023-11-25 13:48:58.646 UTC [141264] ERROR:  0A000: complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
2023-11-25 13:48:58.646 UTC [141264] LOCATION:  JoinOrderList, multi_join_order.c:310
2023-11-25 13:48:58.646 UTC [141264] STATEMENT:  SELECT l_orderkey
	FROM
		lineitem_subquery l
	JOIN
		orders_subquery o
	ON (l_orderkey::int8 = o_orderkey::int4)
	WHERE
		(o_orderkey < l_quantity + 3)
	ORDER BY l_orderkey DESC
	LIMIT 10;
2023-11-25 13:48:58.648 UTC [141264] ERROR:  0A000: complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
2023-11-25 13:48:58.648 UTC [141264] LOCATION:  JoinOrderList, multi_join_order.c:310
2023-11-25 13:48:58.648 UTC [141264] STATEMENT:  SELECT l_orderkey
	FROM
		lineitem_subquery l
	JOIN
		orders_subquery o
	ON (l_orderkey::int4 = o_orderkey::int8)
	ORDER BY l_orderkey DESC
	LIMIT 10;
2023-11-25 13:48:58.648 UTC [141269] ERROR:  0A000: cannot push down this subquery
2023-11-25 13:48:58.648 UTC [141269] DETAIL:  Limit clause is currently unsupported when a lateral subquery references a column from complex subqueries, CTEs or local tables
2023-11-25 13:48:58.648 UTC [141269] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 13:48:58.648 UTC [141269] STATEMENT:  SELECT user_id, lastseen
	FROM
	  (SELECT
	      "some_users_data".user_id, lastseen
	   FROM
	     (SELECT
	        filter_users_1.user_id, time AS lastseen
	      FROM
	        (SELECT
	            user_where_1_1.user_id
	         FROM
	           (SELECT
	              "users"."user_id"
	            FROM
	              users_table as "users"
	            WHERE
	              user_id > 1 and user_id < 4 and value_1 > 2) user_where_1_1
	         INNER JOIN
	           (SELECT
	              "users"."user_id", "users"."value_1"
	            FROM
	              users_table as "users"
	            WHERE
	              user_id > 1 and user_id < 4 and value_2 > 3) user_where_1_join_1
	           ON ("user_where_1_1".user_id = "user_where_1_join_1".value_1)) filter_users_1
	      JOIN LATERAL
	        (SELECT
	            user_id, time
	         FROM
	          events_table as "events"
	         WHERE
	          user_id > 1 and user_id < 4 and user_id = filter_users_1.user_id
	         ORDER BY
	          time DESC
	         LIMIT 1) "last_events_1" ON true
	      ORDER BY time DESC
	      LIMIT 10) "some_recent_users"
	   JOIN LATERAL
	     (SELECT
	        "users".user_id
	      FROM
	        users_table  as "users"
	      WHERE
	        "users"."user_id" = "some_recent_users"."user_id" AND
	        "users"."value_2" > 4
	      ORDER BY 1 LIMIT 1) "some_users_data" ON true
	   ORDER BY
	    lastseen DESC
	   LIMIT 10) "some_users"
	ORDER BY
	  user_id DESC, lastseen DESC
	LIMIT 10;
2023-11-25 13:48:58.650 UTC [141269] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:58.650 UTC [141269] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:58.650 UTC [141269] STATEMENT:  SELECT user_id, lastseen
	FROM
	  (SELECT
	      "some_users_data".user_id, lastseen
	   FROM
	     (SELECT
	        filter_users_1.user_id, time AS lastseen
	      FROM
	        (SELECT
	            user_where_1_1.user_id
	         FROM
	           (SELECT
	              "users"."user_id"
	            FROM
	              users_table as "users"
	            WHERE
	              user_id > 1 and user_id < 3 and value_1 > 2) user_where_1_1
	         INNER JOIN
	           (SELECT
	              "users"."user_id", "users"."value_1"
	            FROM
	              users_table as "users"
	            WHERE
	              user_id > 1 and user_id < 3 and value_2 > 3) user_where_1_join_1
	           ON ("user_where_1_1".user_id = "user_where_1_join_1".user_id)) filter_users_1
	      JOIN LATERAL
	        (SELECT
	            user_id, time
	         FROM
	          events_table as "events"
	         WHERE
	          user_id > 1 and user_id < 3 and user_id != filter_users_1.user_id
	         ORDER BY
	          time DESC
	         LIMIT 1) "last_events_1" ON true
	      ORDER BY time DESC
	      LIMIT 10) "some_recent_users"
	   JOIN LATERAL
	     (SELECT
	        "users".user_id
	      FROM
	        users_table  as "users"
	      WHERE
	        "users"."user_id" = "some_recent_users"."user_id" AND
	        "users"."value_2" > 4
	      LIMIT 1) "some_users_data" ON true
	   ORDER BY
	    lastseen DESC
	   LIMIT 10) "some_users"
	ORDER BY
	  user_id DESC, lastseen DESC
	LIMIT 10;
2023-11-25 13:48:58.652 UTC [141269] ERROR:  0A000: cannot push down this subquery
2023-11-25 13:48:58.652 UTC [141269] DETAIL:  Limit clause is currently unsupported when a lateral subquery references a column from complex subqueries, CTEs or local tables
2023-11-25 13:48:58.652 UTC [141269] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 13:48:58.652 UTC [141269] STATEMENT:  SELECT user_id, lastseen
	FROM
	  (SELECT
	      "some_users_data".user_id, lastseen
	   FROM
	     (SELECT
	        filter_users_1.user_id, time AS lastseen
	      FROM
	        (SELECT
	            user_where_1_1.user_id
	         FROM
	           (SELECT
	              "users"."user_id"
	            FROM
	              users_table as "users"
	            WHERE
	              user_id > 1 and user_id < 3 and value_1 > 2) user_where_1_1
	         INNER JOIN
	           (SELECT
	              "users"."user_id", "users"."value_1"
	            FROM
	              users_table as "users"
	            WHERE
	              user_id > 1 and user_id < 3 and value_2 > 3) user_where_1_join_1
	           ON ("user_where_1_1".user_id = "user_where_1_join_1".user_id)) filter_users_1
	      JOIN LATERAL
	        (SELECT
	            user_id, time
	         FROM
	          events_table as "events"
	         WHERE
	          user_id > 1 and user_id < 3 and user_id = filter_users_1.user_id
	         ORDER BY
	          time DESC
	         LIMIT 1) "last_events_1" ON true
	      ORDER BY time DESC
	      LIMIT 10) "some_recent_users"
	   JOIN LATERAL
	     (SELECT
	        "users".user_id
	      FROM
	        users_table  as "users"
	      WHERE
	        "users"."value_1" = "some_recent_users"."user_id" AND
	        "users"."value_2" > 4
	      ORDER BY 1 LIMIT 1) "some_users_data" ON true
	   ORDER BY
	    lastseen DESC
	   LIMIT 10) "some_users"
	ORDER BY
	  user_id DESC, lastseen DESC
	LIMIT 10;
2023-11-25 13:48:58.669 UTC [141264] ERROR:  0A000: complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
2023-11-25 13:48:58.669 UTC [141264] LOCATION:  JoinOrderList, multi_join_order.c:310
2023-11-25 13:48:58.669 UTC [141264] STATEMENT:  SELECT l_orderkey
	FROM
		lineitem_subquery l
	JOIN
		orders_subquery o
	ON (l_orderkey = o_orderkey + 1)
	WHERE
		(o_orderkey < l_quantity)
	ORDER BY l_orderkey DESC
	LIMIT 10;
2023-11-25 13:48:58.669 UTC [141264] ERROR:  0A000: complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
2023-11-25 13:48:58.669 UTC [141264] LOCATION:  JoinOrderList, multi_join_order.c:310
2023-11-25 13:48:58.669 UTC [141264] STATEMENT:  SELECT l_orderkey
	FROM
		lineitem_subquery l
	JOIN
		orders_subquery o
	ON (l_orderkey = o_orderkey + 1)
	ORDER BY l_orderkey DESC
	LIMIT 10;
2023-11-25 13:48:58.670 UTC [141264] ERROR:  0A000: complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
2023-11-25 13:48:58.670 UTC [141264] LOCATION:  JoinOrderList, multi_join_order.c:310
2023-11-25 13:48:58.670 UTC [141264] STATEMENT:  SELECT l_orderkey
	FROM
		lineitem_subquery l
	JOIN
		orders_subquery o
	ON (l_orderkey < o_orderkey)
	WHERE
		(o_orderkey < l_quantity)
	ORDER BY l_orderkey DESC
	LIMIT 10;
2023-11-25 13:48:58.695 UTC [141264] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:58.695 UTC [141264] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:58.695 UTC [141264] STATEMENT:  SELECT DISTINCT ON (t1.user_id) t1.user_id, t2.value_1, t2.value_2, t2.value_3
	FROM events_table t1
	LEFT JOIN users_table t2 ON t1.user_id > t2.user_id
	ORDER BY 1 DESC, 2 DESC, 3 DESC, 4 DESC
	LIMIT 5;
2023-11-25 13:48:58.727 UTC [141264] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:58.727 UTC [141264] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:58.727 UTC [141264] STATEMENT:  SELECT DISTINCT ON (t1.user_id) t1.user_id, t2.value_1, t2.value_2, t2.value_3
	 FROM
	 users_table t0 LEFT JOIN
	 events_table t1  ON t0.user_id = trunc(t1.user_id)
	 LEFT JOIN users_reference_table t2 ON t1.user_id = trunc(t2.user_id)
	 ORDER BY 1 DESC, 2 DESC, 3 DESC, 4 DESC
	 LIMIT 5;
2023-11-25 13:48:58.752 UTC [141266] ERROR:  0A000: cannot push down this subquery
2023-11-25 13:48:58.752 UTC [141266] DETAIL:  Subqueries without a FROM clause can only contain immutable functions
2023-11-25 13:48:58.752 UTC [141266] LOCATION:  DeferErrorIfCannotPushdownSubquery, query_pushdown_planning.c:1048
2023-11-25 13:48:58.752 UTC [141266] STATEMENT:  SELECT count(*) as subquery_count
	FROM (
	  SELECT
	      user_id
	    FROM
	    users_table
	  WHERE
	    (value_1 = '1' OR value_1 = '3')
	  GROUP BY user_id
	  HAVING count(distinct value_1) = 2
	  ) as a
	  INNER JOIN (
	  SELECT
	    random()::int as user_id
	  ) AS b
	  ON a.user_id = b.user_id
	WHERE b.user_id IS NULL
	GROUP BY a.user_id;
2023-11-25 13:48:58.783 UTC [141264] ERROR:  0A000: cannot compute aggregate (distinct)
2023-11-25 13:48:58.783 UTC [141264] DETAIL:  Only count(distinct) aggregate is supported in subqueries
2023-11-25 13:48:58.783 UTC [141264] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4111
2023-11-25 13:48:58.783 UTC [141264] STATEMENT:  SELECT
		sum(DISTINCT a)
	FROM (
		SELECT
			count(*) a
		FROM
			lineitem_subquery
		GROUP BY
		   l_orderkey
	) z;
2023-11-25 13:48:58.783 UTC [141264] ERROR:  0A000: cannot compute aggregate (distinct)
2023-11-25 13:48:58.783 UTC [141264] DETAIL:  Only count(distinct) aggregate is supported in subqueries
2023-11-25 13:48:58.783 UTC [141264] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4111
2023-11-25 13:48:58.783 UTC [141264] STATEMENT:  SELECT
		avg(DISTINCT a)
	FROM (
		SELECT
			count(*) a
		FROM
			lineitem_subquery
		GROUP BY
		   l_orderkey
	) z;
2023-11-25 13:48:58.800 UTC [141269] ERROR:  0A000: cannot push down this subquery
2023-11-25 13:48:58.800 UTC [141269] DETAIL:  Limit clause is currently unsupported when a lateral subquery references a column from complex subqueries, CTEs or local tables
2023-11-25 13:48:58.800 UTC [141269] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 13:48:58.800 UTC [141269] STATEMENT:  SELECT *
	FROM
	  (SELECT
	      "some_users_data".user_id, value_2
	   FROM
	     (SELECT user_id, max(value_2) AS value_2
	      FROM
	        (SELECT user_id, value_2
	         FROM
	           (SELECT
	              user_id, value_2
	            FROM
	              events_table as "events"
	            WHERE
	              user_id > 1 and user_id < 3) "events_1"
	         ORDER BY
	          value_2 DESC
	         LIMIT 10000) "recent_events_1"
	      GROUP BY
	        user_id
	      ORDER BY
	        max(value_2) DESC) "some_recent_users"
	   JOIN LATERAL
	     (SELECT
	        "users".user_id
	      FROM
	        users_table as "users"
	      WHERE
	        "users"."value_2" = "some_recent_users"."user_id" AND
	        value_2 > 4
	      ORDER BY 1 LIMIT 1) "some_users_data" ON true
	   ORDER BY
	    value_2 DESC
	   LIMIT 10) "some_users"
	ORDER BY
	    value_2 DESC, user_id DESC
	LIMIT 10;
2023-11-25 13:48:58.804 UTC [141266] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:58.804 UTC [141266] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:58.804 UTC [141266] STATEMENT:  SELECT user_id, array_length(events_table, 1)
	FROM (
	  SELECT user_id, array_agg(event ORDER BY time) AS events_table
	  FROM (
	    SELECT u.user_id, e.event_type::text AS event, e.time
	    FROM users_table AS u,
	         events_table AS e
	    WHERE test_join_function_2(u.user_id, e.user_id)
	  ) t
	  GROUP BY user_id
	) q
	ORDER BY 2 DESC, 1;
2023-11-25 13:48:58.823 UTC [141266] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:58.823 UTC [141266] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:58.823 UTC [141266] STATEMENT:  SELECT
	  count(*)
	FROM
	  (SELECT
	    event_type, random()
	  FROM
	    events_table, users_table
	  WHERE
	    events_table.user_id > users_table.user_id AND
	    events_table.time = users_table.time AND
	    events_table.value_2 IN (0, 4)
	  ) as foo;
2023-11-25 13:48:58.832 UTC [141266] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:48:58.832 UTC [141266] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:48:58.832 UTC [141266] STATEMENT:  SELECT
	  count(*)
	FROM
	  (SELECT
	    event_type, random()
	  FROM
	    events_table, users_table
	  WHERE
	    events_table.user_id = users_table.user_id AND
	    events_table.value_2 IN (0, 4)
	  ) as foo,
	(SELECT
	    event_type, random()
	  FROM
	    events_table, users_table
	  WHERE
	    events_table.user_id = users_table.user_id AND
	    events_table.value_2 IN (1, 5)
	  ) as bar
	WHERE foo.event_type = bar.event_type;
2023-11-25 13:48:58.837 UTC [141264] ERROR:  0A000: shard counts of co-located tables do not match
2023-11-25 13:48:58.837 UTC [141264] LOCATION:  QueryPushdownSqlTaskList, multi_physical_planner.c:2210
2023-11-25 13:48:58.837 UTC [141264] STATEMENT:  SELECT
		avg(unit_price)
	FROM
		(SELECT
			l_orderkey,
			avg(o_totalprice / l_quantity) AS unit_price
		FROM
			lineitem_subquery,
			orders_subquery
		WHERE
			l_orderkey = o_orderkey
		GROUP BY
			l_orderkey) AS unit_prices;
2023-11-25 13:48:58.864 UTC [141268] ERROR:  0A000: cannot perform distributed planning on this query
2023-11-25 13:48:58.864 UTC [141268] DETAIL:  Cartesian products are currently unsupported
2023-11-25 13:48:58.864 UTC [141268] LOCATION:  QueryTargetList, multi_physical_planner.c:818
2023-11-25 13:48:58.864 UTC [141268] STATEMENT:  SELECT dist2.c0 FROM dist1, dist3, dist4, dist2 WHERE (dist3.c0) IN (dist4.c0);
2023-11-25 13:48:58.865 UTC [141268] ERROR:  0A000: cannot perform distributed planning on this query
2023-11-25 13:48:58.865 UTC [141268] DETAIL:  Cartesian products are currently unsupported
2023-11-25 13:48:58.865 UTC [141268] LOCATION:  QueryTargetList, multi_physical_planner.c:818
2023-11-25 13:48:58.865 UTC [141268] STATEMENT:  SELECT 1 FROM dist3, dist4, dist2 WHERE (dist3.c0) IN (dist4.c0);
2023-11-25 13:48:58.865 UTC [141268] ERROR:  0A000: cannot perform distributed planning on this query
2023-11-25 13:48:58.865 UTC [141268] DETAIL:  Cartesian products are currently unsupported
2023-11-25 13:48:58.865 UTC [141268] LOCATION:  QueryTargetList, multi_physical_planner.c:818
2023-11-25 13:48:58.865 UTC [141268] STATEMENT:  SELECT  FROM dist3, dist4, dist2 WHERE (dist3.c0) IN (dist4.c0);
2023-11-25 13:48:58.865 UTC [141268] ERROR:  0A000: cannot perform distributed planning on this query
2023-11-25 13:48:58.865 UTC [141268] DETAIL:  Cartesian products are currently unsupported
2023-11-25 13:48:58.865 UTC [141268] LOCATION:  QueryTargetList, multi_physical_planner.c:818
2023-11-25 13:48:58.865 UTC [141268] STATEMENT:  SELECT dist2.c0 FROM dist3, dist4, dist2 WHERE (dist3.c0) IN (dist4.c0);
2023-11-25 13:48:58.866 UTC [141268] ERROR:  0A000: cannot perform distributed planning on this query
2023-11-25 13:48:58.866 UTC [141268] DETAIL:  Cartesian products are currently unsupported
2023-11-25 13:48:58.866 UTC [141268] LOCATION:  QueryTargetList, multi_physical_planner.c:818
2023-11-25 13:48:58.866 UTC [141268] STATEMENT:  SELECT dist2.* FROM dist3, dist4, dist2 WHERE (dist3.c0) IN (dist4.c0);
2023-11-25 13:48:58.866 UTC [141264] ERROR:  0A000: Subqueries in HAVING cannot refer to outer query
2023-11-25 13:48:58.866 UTC [141264] LOCATION:  RecursivelyPlanSubqueriesAndCTEs, recursive_planning.c:323
2023-11-25 13:48:58.866 UTC [141264] STATEMENT:  WITH cte_1 AS (SELECT b max FROM subquery_pruning_varchar_test_table)
	SELECT a
	FROM subquery_pruning_varchar_test_table
	JOIN cte_1 ON a = max::text
	GROUP BY a HAVING a = (SELECT a)
	ORDER BY 1;
2023-11-25 13:49:04.684 UTC [141800] ERROR:  0A000: cannot perform distributed planning on this query because parameterized queries for SQL functions referencing distributed tables are not supported
2023-11-25 13:49:04.684 UTC [141800] HINT:  Consider using PL/pgSQL functions instead.
2023-11-25 13:49:04.684 UTC [141800] LOCATION:  distributed_planner, distributed_planner.c:301
2023-11-25 13:49:04.684 UTC [141800] STATEMENT:  SELECT * FROM test_parameterized_sql_function(1);
2023-11-25 13:49:04.685 UTC [141800] ERROR:  0A000: cannot perform distributed planning on this query because parameterized queries for SQL functions referencing distributed tables are not supported
2023-11-25 13:49:04.685 UTC [141800] HINT:  Consider using PL/pgSQL functions instead.
2023-11-25 13:49:04.685 UTC [141800] LOCATION:  GetRTEIdentity, distributed_planner.c:533
2023-11-25 13:49:04.685 UTC [141800] STATEMENT:  SELECT (SELECT 1 FROM test_parameterized_sql limit 1) FROM test_parameterized_sql_function(1);
2023-11-25 13:49:04.685 UTC [141800] ERROR:  0A000: could not create distributed plan
2023-11-25 13:49:04.685 UTC [141800] DETAIL:  Possibly this is caused by the use of parameters in SQL functions, which is not supported in Citus.
2023-11-25 13:49:04.685 UTC [141800] HINT:  Consider using PL/pgSQL functions instead.
2023-11-25 13:49:04.685 UTC [141800] CONTEXT:  SQL function "test_parameterized_sql_function_in_subquery_where" statement 1
2023-11-25 13:49:04.685 UTC [141800] LOCATION:  CreateDistributedPlannedStmt, distributed_planner.c:751
2023-11-25 13:49:04.685 UTC [141800] STATEMENT:  SELECT test_parameterized_sql_function_in_subquery_where(1);
2023-11-25 13:49:04.712 UTC [141800] ERROR:  23505: duplicate key value violates unique constraint "table_with_unique_constraint_a_key_1230009"
2023-11-25 13:49:04.712 UTC [141800] DETAIL:  Key (a)=(4) already exists.
2023-11-25 13:49:04.712 UTC [141800] CONTEXT:  while executing command on localhost:57638
	SQL function "insert_twice" statement 2
2023-11-25 13:49:04.712 UTC [141800] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:04.712 UTC [141800] STATEMENT:  SELECT insert_twice();
2023-11-25 13:49:05.193 UTC [141801] ERROR:  0A000: cannot push down this subquery
2023-11-25 13:49:05.193 UTC [141801] DETAIL:  Limit in subquery without limit in the outermost query is unsupported
2023-11-25 13:49:05.193 UTC [141801] LOCATION:  DeferErrorIfCannotPushdownSubquery, query_pushdown_planning.c:1048
2023-11-25 13:49:05.193 UTC [141801] STATEMENT:  SELECT * FROM recent_10_users;
2023-11-25 13:49:05.193 UTC [141801] ERROR:  0A000: cannot push down this subquery
2023-11-25 13:49:05.193 UTC [141801] DETAIL:  Limit in subquery without limit in the outermost query is unsupported
2023-11-25 13:49:05.193 UTC [141801] LOCATION:  DeferErrorIfCannotPushdownSubquery, query_pushdown_planning.c:1048
2023-11-25 13:49:05.193 UTC [141801] STATEMENT:  SELECT et.* FROM recent_10_users JOIN events_table et USING(user_id);
2023-11-25 13:49:05.656 UTC [141798] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:49:05.656 UTC [141798] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:49:05.656 UTC [141798] STATEMENT:  SELECT
	  u1.user_id, count(*)
	FROM
	  events_table as e1, users_table as u1
	WHERE
	  event_type IN
	            (SELECT
	                event_type
	             FROM
	              events_reference_table as e2
	             WHERE
	              value_2 = 1 AND
	              value_3 > 3 AND
	              e1.value_2 > e2.value_2
	            )
	            AND u1.user_id > e1.user_id
	GROUP BY 1
	ORDER BY 2 DESC, 1 DESC
	LIMIT 5;
2023-11-25 13:49:05.808 UTC [141801] ERROR:  0A000: cannot modify views when the query contains citus tables
2023-11-25 13:49:05.808 UTC [141801] LOCATION:  DeferErrorIfModifyView, multi_router_planner.c:1137
2023-11-25 13:49:05.808 UTC [141801] STATEMENT:  UPDATE small_view SET id = 1;
2023-11-25 13:49:05.808 UTC [141801] ERROR:  0A000: cannot modify views when the query contains citus tables
2023-11-25 13:49:05.808 UTC [141801] LOCATION:  DeferErrorIfModifyView, multi_router_planner.c:1137
2023-11-25 13:49:05.808 UTC [141801] STATEMENT:  DELETE FROM small_view;
2023-11-25 13:49:05.808 UTC [141801] ERROR:  0A000: cannot modify views when the query contains citus tables
2023-11-25 13:49:05.808 UTC [141801] LOCATION:  DeferErrorIfModifyView, multi_router_planner.c:1137
2023-11-25 13:49:05.808 UTC [141801] STATEMENT:  INSERT INTO small_view VALUES(8, 5) ON CONFLICT(tenant_id) DO UPDATE SET tenant_id=99;
2023-11-25 13:49:05.867 UTC [141798] ERROR:  0A000: FULL JOIN is only supported with merge-joinable or hash-joinable join conditions
2023-11-25 13:49:05.867 UTC [141798] LOCATION:  populate_joinrel_with_paths, joinrels.c:853
2023-11-25 13:49:05.867 UTC [141798] STATEMENT:  SELECT count(*) FROM users_ref_test_table ref1 INNER JOIN users_ref_test_table ref2 on ref1.id = ref2.id FULL JOIN user_buy_test_table ON (ref1.id > 5);
2023-11-25 13:49:05.867 UTC [141798] ERROR:  0A000: FULL JOIN is only supported with merge-joinable or hash-joinable join conditions
2023-11-25 13:49:05.867 UTC [141798] LOCATION:  populate_joinrel_with_paths, joinrels.c:853
2023-11-25 13:49:05.867 UTC [141798] STATEMENT:  SELECT count(*) FROM users_ref_test_table ref1 INNER JOIN users_ref_test_table ref2 on ref1.id = ref2.id FULL JOIN user_buy_test_table ON (user_buy_test_table.user_id > 5);
2023-11-25 13:49:05.913 UTC [141798] ERROR:  0A000: FULL JOIN is only supported with merge-joinable or hash-joinable join conditions
2023-11-25 13:49:05.913 UTC [141798] LOCATION:  populate_joinrel_with_paths, joinrels.c:853
2023-11-25 13:49:05.913 UTC [141798] STATEMENT:  SELECT count(*) FROM user_buy_test_table FULL JOIN users_ref_test_table ref1 INNER JOIN users_ref_test_table ref2 on ref1.id = ref2.id ON (ref1.id > 5);
2023-11-25 13:49:05.913 UTC [141798] ERROR:  0A000: FULL JOIN is only supported with merge-joinable or hash-joinable join conditions
2023-11-25 13:49:05.913 UTC [141798] LOCATION:  populate_joinrel_with_paths, joinrels.c:853
2023-11-25 13:49:05.913 UTC [141798] STATEMENT:  SELECT count(*) FROM user_buy_test_table FULL JOIN users_ref_test_table ref1 INNER JOIN users_ref_test_table ref2 on ref1.id = ref2.id ON (user_buy_test_table.user_id > 5);
2023-11-25 13:49:05.916 UTC [141801] ERROR:  0A000: cannot modify views when the query contains citus tables
2023-11-25 13:49:05.916 UTC [141801] LOCATION:  DeferErrorIfModifyView, multi_router_planner.c:1137
2023-11-25 13:49:05.916 UTC [141801] STATEMENT:  INSERT INTO small_view VALUES(3, 3);
2023-11-25 13:49:05.949 UTC [141798] ERROR:  0A000: FULL JOIN is only supported with merge-joinable or hash-joinable join conditions
2023-11-25 13:49:05.949 UTC [141798] LOCATION:  populate_joinrel_with_paths, joinrels.c:853
2023-11-25 13:49:05.949 UTC [141798] STATEMENT:  SELECT count(*) FROM (SELECT ref1.*, random() FROM users_ref_test_table ref1 INNER JOIN users_ref_test_table ref2 on ref1.id = ref2.id) as foo FULL JOIN user_buy_test_table ON (foo.id > 5);
2023-11-25 13:49:05.949 UTC [141798] ERROR:  0A000: FULL JOIN is only supported with merge-joinable or hash-joinable join conditions
2023-11-25 13:49:05.949 UTC [141798] LOCATION:  populate_joinrel_with_paths, joinrels.c:853
2023-11-25 13:49:05.949 UTC [141798] STATEMENT:  SELECT count(*) FROM (SELECT ref1.*, random() FROM users_ref_test_table ref1 LEFT JOIN users_ref_test_table ref2 on ref1.id = ref2.id) as foo FULL JOIN user_buy_test_table ON (user_buy_test_table.user_id > 19);
2023-11-25 13:49:05.971 UTC [141798] ERROR:  0A000: FULL JOIN is only supported with merge-joinable or hash-joinable join conditions
2023-11-25 13:49:05.971 UTC [141798] LOCATION:  populate_joinrel_with_paths, joinrels.c:853
2023-11-25 13:49:05.971 UTC [141798] STATEMENT:  SELECT count(*) FROM user_buy_test_table FULL JOIN (SELECT ref1.*, random() FROM users_ref_test_table ref1 INNER JOIN users_ref_test_table ref2 on ref1.id = ref2.id) as foo ON (foo.id > 5);
2023-11-25 13:49:05.971 UTC [141798] ERROR:  0A000: FULL JOIN is only supported with merge-joinable or hash-joinable join conditions
2023-11-25 13:49:05.971 UTC [141798] LOCATION:  populate_joinrel_with_paths, joinrels.c:853
2023-11-25 13:49:05.971 UTC [141798] STATEMENT:  SELECT count(*) FROM user_buy_test_table FULL JOIN (SELECT ref1.*, random() FROM users_ref_test_table ref1 LEFT JOIN users_ref_test_table ref2 on ref1.id = ref2.id) as foo ON (user_buy_test_table.user_id > 19);
2023-11-25 13:49:06.029 UTC [141801] ERROR:  0A000: cannot modify views when the query contains citus tables
2023-11-25 13:49:06.029 UTC [141801] LOCATION:  DeferErrorIfModifyView, multi_router_planner.c:1137
2023-11-25 13:49:06.029 UTC [141801] STATEMENT:  UPDATE small_view SET id = 1;
2023-11-25 13:49:06.029 UTC [141801] ERROR:  0A000: cannot modify views when the query contains citus tables
2023-11-25 13:49:06.029 UTC [141801] LOCATION:  DeferErrorIfModifyView, multi_router_planner.c:1137
2023-11-25 13:49:06.029 UTC [141801] STATEMENT:  DELETE FROM small_view;
2023-11-25 13:49:06.222 UTC [141801] ERROR:  55000: cannot insert into view "small_view"
2023-11-25 13:49:06.222 UTC [141801] DETAIL:  Views that do not select from a single table or view are not automatically updatable.
2023-11-25 13:49:06.222 UTC [141801] HINT:  To enable inserting into the view, provide an INSTEAD OF INSERT trigger or an unconditional ON INSERT DO INSTEAD rule.
2023-11-25 13:49:06.222 UTC [141801] LOCATION:  rewriteTargetView, rewriteHandler.c:3096
2023-11-25 13:49:06.222 UTC [141801] STATEMENT:  INSERT INTO small_view VALUES(3, 3);
2023-11-25 13:49:06.720 UTC [142609] ERROR:  0A000: COMMIT is not allowed in an SQL function
2023-11-25 13:49:06.720 UTC [142609] CONTEXT:  SQL function "test_procedure_commit" during startup
2023-11-25 13:49:06.720 UTC [142609] LOCATION:  init_execution_state, functions.c:517
2023-11-25 13:49:06.720 UTC [142609] STATEMENT:  CALL test_procedure_commit(2,5);
2023-11-25 13:49:06.731 UTC [142609] ERROR:  0A000: ROLLBACK is not allowed in an SQL function
2023-11-25 13:49:06.731 UTC [142609] CONTEXT:  SQL function "test_procedure_rollback" during startup
2023-11-25 13:49:06.731 UTC [142609] LOCATION:  init_execution_state, functions.c:517
2023-11-25 13:49:06.731 UTC [142609] STATEMENT:  CALL test_procedure_rollback(2,15);
2023-11-25 13:49:06.786 UTC [142609] ERROR:  23505: duplicate key value violates unique constraint "idx_table_100503"
2023-11-25 13:49:06.786 UTC [142609] DETAIL:  Key (id, org_id)=(2, 12) already exists.
2023-11-25 13:49:06.786 UTC [142609] CONTEXT:  while executing command on localhost:57637
	SQL statement "INSERT INTO test_table VALUES (tt_id, tt_org_id)"
	PL/pgSQL function test_procedure_modify_insert(integer,integer) line 5 at SQL statement
2023-11-25 13:49:06.786 UTC [142609] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:06.786 UTC [142609] STATEMENT:  CALL test_procedure_modify_insert(2,12);
2023-11-25 13:49:06.804 UTC [142607] ERROR:  0A000: input of anonymous composite types is not implemented
2023-11-25 13:49:06.804 UTC [142607] LOCATION:  record_in, rowtypes.c:103
2023-11-25 13:49:06.804 UTC [142607] STATEMENT:  SELECT ARRAY[(x,(y,x),y),(y,(x,y))] FROM test ORDER BY x, y;
2023-11-25 13:49:06.804 UTC [142609] ERROR:  23505: duplicate key value violates unique constraint "idx_table_100503"
2023-11-25 13:49:06.804 UTC [142609] DETAIL:  Key (id, org_id)=(2, 30) already exists.
2023-11-25 13:49:06.804 UTC [142609] CONTEXT:  while executing command on localhost:57637
	SQL statement "INSERT INTO test_table VALUES (tt_id, tt_org_id)"
	PL/pgSQL function test_procedure_modify_insert_commit(integer,integer) line 5 at SQL statement
2023-11-25 13:49:06.804 UTC [142609] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:06.804 UTC [142609] STATEMENT:  CALL test_procedure_modify_insert_commit(2,30);
2023-11-25 13:49:06.819 UTC [142607] ERROR:  0A000: input of anonymous composite types is not implemented
2023-11-25 13:49:06.819 UTC [142607] LOCATION:  record_in, rowtypes.c:103
2023-11-25 13:49:06.819 UTC [142607] STATEMENT:  SELECT ARRAY[[(x,(y,x))],[((x,x),y)]] FROM test ORDER BY x, y;
2023-11-25 13:49:06.830 UTC [142610] ERROR:  P0001: Task failed to execute
2023-11-25 13:49:06.830 UTC [142610] CONTEXT:  PL/pgSQL function raise_failed_execution_func_join(text) line 8 at RAISE
2023-11-25 13:49:06.830 UTC [142610] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:49:06.830 UTC [142610] STATEMENT:  SELECT raise_failed_execution_func_join($$
	  WITH one_row AS (
	      SELECT * FROM table1 WHERE id=52
	      )
	  SELECT table1.id, table1.data
	  FROM one_row, table1, next_k_integers(one_row.id, 5) next_five_ids
	  WHERE table1.id = next_five_ids;
	$$);
2023-11-25 13:49:06.835 UTC [142610] ERROR:  P0001: Task failed to execute
2023-11-25 13:49:06.835 UTC [142610] CONTEXT:  PL/pgSQL function raise_failed_execution_func_join(text) line 8 at RAISE
2023-11-25 13:49:06.835 UTC [142610] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:49:06.835 UTC [142610] STATEMENT:  SELECT raise_failed_execution_func_join($$
	  SELECT * FROM table1 JOIN the_answer_to_life() the_answer ON (id = the_answer);
	$$);
2023-11-25 13:49:06.835 UTC [142607] ERROR:  0A000: input of anonymous composite types is not implemented
2023-11-25 13:49:06.835 UTC [142607] LOCATION:  record_in, rowtypes.c:103
2023-11-25 13:49:06.835 UTC [142607] STATEMENT:  SELECT CASE x WHEN 2 THEN (x, y, x) ELSE (y, x) END FROM test ORDER BY x, y;
2023-11-25 13:49:06.844 UTC [142610] ERROR:  P0001: Task failed to execute
2023-11-25 13:49:06.844 UTC [142610] CONTEXT:  PL/pgSQL function raise_failed_execution_func_join(text) line 8 at RAISE
2023-11-25 13:49:06.844 UTC [142610] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:49:06.844 UTC [142610] STATEMENT:  SELECT raise_failed_execution_func_join($$
	  SELECT *
	  FROM table1
	         JOIN next_k_integers(10,5) WITH ORDINALITY next_integers
	           ON (id = next_integers.result);
	$$);
2023-11-25 13:49:06.852 UTC [142610] ERROR:  P0001: Task failed to execute
2023-11-25 13:49:06.852 UTC [142610] CONTEXT:  PL/pgSQL function raise_failed_execution_func_join(text) line 8 at RAISE
2023-11-25 13:49:06.852 UTC [142610] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:49:06.852 UTC [142610] STATEMENT:  SELECT raise_failed_execution_func_join($$
	  SELECT *
	  FROM table1
	         JOIN next_k_integers(10,5) WITH ORDINALITY next_integers
	           ON (id = next_integers.result)
	  ORDER BY id ASC;
	$$);
2023-11-25 13:49:06.858 UTC [142607] ERROR:  0A000: input of anonymous composite types is not implemented
2023-11-25 13:49:06.858 UTC [142607] LOCATION:  record_in, rowtypes.c:103
2023-11-25 13:49:06.858 UTC [142607] STATEMENT:  SELECT identity_returner((x, y)) FROM test ORDER BY x, y;
2023-11-25 13:49:06.867 UTC [142607] ERROR:  0A000: input of anonymous composite types is not implemented
2023-11-25 13:49:06.867 UTC [142607] LOCATION:  record_in, rowtypes.c:103
2023-11-25 13:49:06.867 UTC [142607] STATEMENT:  SELECT array_agg((x, y)) FROM test;
2023-11-25 13:49:06.888 UTC [142607] ERROR:  0A000: input of anonymous composite types is not implemented
2023-11-25 13:49:06.888 UTC [142607] LOCATION:  record_in, rowtypes.c:103
2023-11-25 13:49:06.888 UTC [142607] STATEMENT:  SELECT ARRAY[(x,(y,x),y),(y,(x,y))] FROM test WHERE x = 1 ORDER BY x, y;
2023-11-25 13:49:06.896 UTC [142607] ERROR:  0A000: input of anonymous composite types is not implemented
2023-11-25 13:49:06.896 UTC [142607] LOCATION:  record_in, rowtypes.c:103
2023-11-25 13:49:06.896 UTC [142607] STATEMENT:  SELECT ARRAY[[(x,(y,x))],[((x,x),y)]] FROM test WHERE x = 1 ORDER BY x, y;
2023-11-25 13:49:06.904 UTC [142607] ERROR:  0A000: input of anonymous composite types is not implemented
2023-11-25 13:49:06.904 UTC [142607] LOCATION:  record_in, rowtypes.c:103
2023-11-25 13:49:06.904 UTC [142607] STATEMENT:  SELECT CASE x WHEN 2 THEN (x, y, x) ELSE (y, x) END FROM test WHERE x = 1 ORDER BY x, y;
2023-11-25 13:49:06.913 UTC [142607] ERROR:  0A000: input of anonymous composite types is not implemented
2023-11-25 13:49:06.913 UTC [142607] LOCATION:  record_in, rowtypes.c:103
2023-11-25 13:49:06.913 UTC [142607] STATEMENT:  SELECT identity_returner((x, y)) FROM test WHERE x = 1 ORDER BY x, y;
2023-11-25 13:49:06.922 UTC [142607] ERROR:  0A000: input of anonymous composite types is not implemented
2023-11-25 13:49:06.922 UTC [142607] LOCATION:  record_in, rowtypes.c:103
2023-11-25 13:49:06.922 UTC [142607] STATEMENT:  SELECT array_agg((x, y)) FROM test WHERE x = 1;
2023-11-25 13:49:06.930 UTC [142607] ERROR:  0A000: input of anonymous composite types is not implemented
2023-11-25 13:49:06.930 UTC [142607] LOCATION:  record_in, rowtypes.c:103
2023-11-25 13:49:06.930 UTC [142607] STATEMENT:  SELECT (x,table_returner(x)) FROM test WHERE x = 1 ORDER BY x, y;
2023-11-25 13:49:07.462 UTC [142608] ERROR:  55000: materialized view "materialized_view" has not been populated
2023-11-25 13:49:07.462 UTC [142608] HINT:  Use the REFRESH MATERIALIZED VIEW command.
2023-11-25 13:49:07.462 UTC [142608] LOCATION:  ExecOpenScanRelation, execUtils.c:740
2023-11-25 13:49:07.462 UTC [142608] STATEMENT:  SELECT count(*) FROM materialized_view;
2023-11-25 13:49:07.560 UTC [142608] ERROR:  42809: cannot change materialized view "small_view"
2023-11-25 13:49:07.560 UTC [142608] LOCATION:  CheckValidResultRel, execMain.c:1060
2023-11-25 13:49:07.560 UTC [142608] STATEMENT:  UPDATE small_view SET id = 1;
2023-11-25 13:49:07.582 UTC [142608] ERROR:  42809: cannot change materialized view "small_view"
2023-11-25 13:49:07.582 UTC [142608] LOCATION:  CheckValidResultRel, execMain.c:1060
2023-11-25 13:49:07.582 UTC [142608] STATEMENT:  INSERT INTO small_view VALUES(3, 3);
2023-11-25 13:49:07.668 UTC [142608] ERROR:  42809: cannot change materialized view "small_view"
2023-11-25 13:49:07.668 UTC [142608] LOCATION:  CheckValidResultRel, execMain.c:1060
2023-11-25 13:49:07.668 UTC [142608] STATEMENT:  UPDATE small_view SET id = 1;
2023-11-25 13:49:07.668 UTC [142608] ERROR:  42809: cannot change materialized view "small_view"
2023-11-25 13:49:07.668 UTC [142608] LOCATION:  CheckValidResultRel, execMain.c:1060
2023-11-25 13:49:07.668 UTC [142608] STATEMENT:  DELETE FROM small_view;
2023-11-25 13:49:07.945 UTC [142970] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a reference table
2023-11-25 13:49:07.945 UTC [142970] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:671
2023-11-25 13:49:07.945 UTC [142970] STATEMENT:  SELECT
	  user_id
	FROM
	  users_reference_table
	WHERE
	  NOT EXISTS
	      (SELECT
	          value_2
	       FROM
	          events_table
	       WHERE
	          users_reference_table.user_id = events_table.user_id
	      )
	LIMIT 3;
2023-11-25 13:49:07.945 UTC [142970] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a set returning function
2023-11-25 13:49:07.945 UTC [142970] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:677
2023-11-25 13:49:07.945 UTC [142970] STATEMENT:  SELECT
	  user_id
	FROM
	  (SELECT user_id FROM generate_series(1,10) AS series(user_id)) users_reference_table
	WHERE
	  NOT EXISTS
	      (SELECT
	          value_2
	       FROM
	          events_table
	       WHERE
	          users_reference_table.user_id = events_table.user_id
	      )
	LIMIT 3;
2023-11-25 13:49:07.946 UTC [142970] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a CTE or subquery
2023-11-25 13:49:07.946 UTC [142970] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:684
2023-11-25 13:49:07.946 UTC [142970] STATEMENT:  SELECT
	  user_id
	FROM
	  (SELECT  5 AS user_id UNION ALL SELECT 6) users_reference_table
	WHERE
	  NOT EXISTS
	      (SELECT
	          value_2
	       FROM
	          events_table
	       WHERE
	          users_reference_table.user_id = events_table.user_id
	      )
	LIMIT 3;
2023-11-25 13:49:07.957 UTC [142970] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a CTE or subquery
2023-11-25 13:49:07.957 UTC [142970] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:684
2023-11-25 13:49:07.957 UTC [142970] STATEMENT:  SELECT
	  DISTINCT user_id
	FROM
	  users_table RIGHT JOIN users_reference_table USING (user_id)
	WHERE
	  users_table.value_2 IN
	      (SELECT
	          value_2
	       FROM
	          events_table
	       WHERE
	          users_table.user_id = events_table.user_id
	      )
	ORDER BY user_id
	LIMIT 3;
2023-11-25 13:49:07.959 UTC [142972] WARNING:  25001: SET TRANSACTION ISOLATION LEVEL must be called before any query
2023-11-25 13:49:07.959 UTC [142972] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:49:07.959 UTC [142972] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:07.959 UTC [142972] ERROR:  XX000: failure on connection marked as essential: localhost:57637
2023-11-25 13:49:07.959 UTC [142972] LOCATION:  MarkRemoteTransactionFailed, remote_transaction.c:840
2023-11-25 13:49:07.959 UTC [142972] STATEMENT:  SET LOCAL TRANSACTION ISOLATION LEVEL REPEATABLE READ;
2023-11-25 13:49:07.959 UTC [142972] ERROR:  25006: cannot execute INSERT in a read-only transaction
2023-11-25 13:49:07.959 UTC [142972] LOCATION:  PreventCommandIfReadOnly, utility.c:414
2023-11-25 13:49:07.959 UTC [142972] STATEMENT:  INSERT INTO test VALUES (2,2);
2023-11-25 13:49:07.966 UTC [142972] WARNING:  25001: there is already a transaction in progress
2023-11-25 13:49:07.966 UTC [142972] LOCATION:  BeginTransactionBlock, xact.c:3778
2023-11-25 13:49:07.966 UTC [142972] WARNING:  25001: there is already a transaction in progress
2023-11-25 13:49:07.966 UTC [142972] LOCATION:  BeginTransactionBlock, xact.c:3778
2023-11-25 13:49:07.966 UTC [142972] ERROR:  25001: SET TRANSACTION ISOLATION LEVEL must be called before any query
2023-11-25 13:49:07.966 UTC [142972] LOCATION:  call_enum_check_hook, guc.c:12007
2023-11-25 13:49:07.966 UTC [142972] STATEMENT:  BEGIN TRANSACTION ISOLATION LEVEL REPEATABLE READ;
2023-11-25 13:49:07.981 UTC [142970] ERROR:  0A000: cannot perform a lateral outer join when a distributed subquery references a reference table
2023-11-25 13:49:07.981 UTC [142970] LOCATION:  DeferredErrorIfUnsupportedRecurringTuplesJoin, query_pushdown_planning.c:913
2023-11-25 13:49:07.981 UTC [142970] STATEMENT:  SELECT user_id, value_2 FROM users_table WHERE
	  value_1 > 1 AND value_1 < 3
	  AND value_2 >= 5
	  AND user_id IN
	  (
			SELECT
			  e1.user_id
			FROM (
			  -- Get the first time each user viewed the homepage.
			  SELECT
			    user_id,
			    1 AS view_homepage,
			    min(time) AS view_homepage_time
			  FROM events_reference_table
			     WHERE
			     event_type IN (1, 2)
			  GROUP BY user_id
			) e1 LEFT JOIN LATERAL (
			  SELECT
			    user_id,
			    1 AS use_demo,
			    time AS use_demo_time
			  FROM events_table
			  WHERE
			    user_id = e1.user_id AND
			       event_type IN (2, 3)
			  ORDER BY time
			) e2 ON true LEFT JOIN LATERAL (
			  SELECT
			    user_id,
			    1 AS enter_credit_card,
			    time AS enter_credit_card_time
			  FROM  events_reference_table
			  WHERE
			    user_id = e2.user_id AND
			    event_type IN (3, 4)
			  ORDER BY time
			) e3 ON true LEFT JOIN LATERAL (
			  SELECT
			    1 AS submit_card_info,
			    user_id,
			    time AS enter_credit_card_time
			  FROM  events_reference_table
			  WHERE
			    user_id = e3.user_id AND
			    event_type IN (4, 5)
			  ORDER BY time
			) e4 ON true LEFT JOIN LATERAL (
			  SELECT
			    1 AS see_bought_screen
			  FROM  events_reference_table
			  WHERE
			    user_id = e4.user_id AND
			    event_type IN (5, 6)
			  ORDER BY time
			) e5 ON true
			group by e1.user_id
			HAVING sum(submit_card_info) > 0
	)
	ORDER BY 1, 2;
2023-11-25 13:49:08.018 UTC [142970] ERROR:  0A000: correlated subqueries are not supported when the FROM clause contains a reference table
2023-11-25 13:49:08.018 UTC [142970] LOCATION:  DeferErrorIfFromClauseRecurs, query_pushdown_planning.c:671
2023-11-25 13:49:08.018 UTC [142970] STATEMENT:  SELECT user_id,
	       count(*)
	FROM users_reference_table
	WHERE value_2 > ALL
	    (SELECT min(value_2)
	     FROM events_table
	     WHERE event_type > 2 AND users_reference_table.user_id = events_table.user_id
	     GROUP BY user_id)
	GROUP BY user_id
	HAVING count(*) > 3
	ORDER BY 2 DESC,
	         1 DESC
	LIMIT 5;
2023-11-25 13:49:08.870 UTC [143243] ERROR:  23505: duplicate key value violates unique constraint "test_forcepushdown_pkey_900015"
2023-11-25 13:49:08.870 UTC [143243] DETAIL:  Key (intcol)=(3) already exists.
2023-11-25 13:49:08.870 UTC [143243] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.test_forcepushdown VALUES (a)"
	PL/pgSQL function forcepushdown_schema.insert_data(integer) line 3 at SQL statement
	while executing command on localhost:57638
2023-11-25 13:49:08.870 UTC [143243] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:08.870 UTC [143243] STATEMENT:  SELECT insert_data(3);
2023-11-25 13:49:08.870 UTC [143243] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 13:49:08.870 UTC [143243] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 13:49:08.870 UTC [143243] STATEMENT:  INSERT INTO forcepushdown_schema.test_forcepushdown VALUES (5);
2023-11-25 13:49:08.872 UTC [143243] ERROR:  23505: duplicate key value violates unique constraint "test_forcepushdown_pkey_900000"
2023-11-25 13:49:08.872 UTC [143243] DETAIL:  Key (intcol)=(8) already exists.
2023-11-25 13:49:08.872 UTC [143243] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:49:08.872 UTC [143243] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:08.872 UTC [143243] STATEMENT:  INSERT INTO forcepushdown_schema.test_forcepushdown VALUES (8);
2023-11-25 13:49:08.873 UTC [143243] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 13:49:08.873 UTC [143243] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 13:49:08.873 UTC [143243] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.test_forcepushdown VALUES (a+1)"
	PL/pgSQL function forcepushdown_schema.insert_data_non_distarg(integer) line 3 at SQL statement
	while executing command on localhost:57637
2023-11-25 13:49:08.873 UTC [143243] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:08.873 UTC [143243] STATEMENT:  SELECT insert_data_non_distarg(9);
2023-11-25 13:49:08.874 UTC [143243] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 13:49:08.874 UTC [143243] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 13:49:08.874 UTC [143243] CONTEXT:  SQL statement "UPDATE forcepushdown_schema.test_forcepushdown SET data = 'non-default'"
	PL/pgSQL function forcepushdown_schema.update_data_nonlocal(integer) line 3 at SQL statement
	while executing command on localhost:57637
2023-11-25 13:49:08.874 UTC [143243] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:08.874 UTC [143243] STATEMENT:  SELECT update_data_nonlocal(12);
2023-11-25 13:49:08.874 UTC [143243] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 13:49:08.874 UTC [143243] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 13:49:08.874 UTC [143243] STATEMENT:  INSERT INTO forcepushdown_schema.test_forcepushdown VALUES (13);
2023-11-25 13:49:08.876 UTC [143243] ERROR:  XX000: cannot execute a distributed query from a query on a shard
2023-11-25 13:49:08.876 UTC [143243] DETAIL:  Executing a distributed query in a function call that may be pushed to a remote node can lead to incorrect results.
2023-11-25 13:49:08.876 UTC [143243] HINT:  Avoid nesting of distributed queries or use alter user current_user set citus.allow_nested_distributed_execution to on to allow it with possible incorrectness.
2023-11-25 13:49:08.876 UTC [143243] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.test_forcepushdown VALUES (a)"
	PL/pgSQL function forcepushdown_schema.insert_data(integer) line 3 at SQL statement
	while executing command on localhost:57638
2023-11-25 13:49:08.876 UTC [143243] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:08.876 UTC [143243] STATEMENT:  SELECT insert_data(intcol+17) from test_forcepushdown where intcol = 1;
2023-11-25 13:49:08.877 UTC [143243] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 13:49:08.877 UTC [143243] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 13:49:08.877 UTC [143243] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.test_forcepushdown_noncolocate VALUES (a)"
	PL/pgSQL function forcepushdown_schema.insert_data_noncolocation(integer) line 4 at SQL statement
	while executing command on localhost:57638
2023-11-25 13:49:08.877 UTC [143243] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:08.877 UTC [143243] STATEMENT:  SELECT insert_data_noncolocation(19);
2023-11-25 13:49:08.877 UTC [143243] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 13:49:08.877 UTC [143243] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 13:49:08.877 UTC [143243] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.test_forcepushdown_noncolocate VALUES (a)"
	PL/pgSQL function forcepushdown_schema.insert_data_noncolocation(integer) line 4 at SQL statement
	while executing command on localhost:57638
2023-11-25 13:49:08.877 UTC [143243] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:08.877 UTC [143243] STATEMENT:  SELECT insert_data_noncolocation(19);
2023-11-25 13:49:08.933 UTC [143243] ERROR:  XX000: cannot execute a distributed query from a query on a shard
2023-11-25 13:49:08.933 UTC [143243] DETAIL:  Executing a distributed query in a function call that may be pushed to a remote node can lead to incorrect results.
2023-11-25 13:49:08.933 UTC [143243] HINT:  Avoid nesting of distributed queries or use alter user current_user set citus.allow_nested_distributed_execution to on to allow it with possible incorrectness.
2023-11-25 13:49:08.933 UTC [143243] CONTEXT:  SQL statement "SELECT max(id)::numeric+1               FROM forcepushdown_schema.test_nested WHERE id = $1"
	PL/pgSQL function forcepushdown_schema.inner_force_delegation_function(integer) line 4 at SQL statement
	while executing command on localhost:57638
2023-11-25 13:49:08.933 UTC [143243] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:08.933 UTC [143243] STATEMENT:  SELECT inner_force_delegation_function(id) FROM test_nested WHERE id = 300;
2023-11-25 13:49:08.934 UTC [143243] ERROR:  XX000: cannot execute a distributed query from a query on a shard
2023-11-25 13:49:08.934 UTC [143243] DETAIL:  Executing a distributed query in a function call that may be pushed to a remote node can lead to incorrect results.
2023-11-25 13:49:08.934 UTC [143243] HINT:  Avoid nesting of distributed queries or use alter user current_user set citus.allow_nested_distributed_execution to on to allow it with possible incorrectness.
2023-11-25 13:49:08.934 UTC [143243] CONTEXT:  SQL statement "SELECT max(id)::numeric+1               FROM forcepushdown_schema.test_nested WHERE id = $1"
	PL/pgSQL function forcepushdown_schema.inner_force_delegation_function(integer) line 4 at SQL statement
	while executing command on localhost:57637
2023-11-25 13:49:08.934 UTC [143243] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:08.934 UTC [143243] STATEMENT:  SELECT inner_force_delegation_function((SELECT id+112 FROM test_nested WHERE id=400));
2023-11-25 13:49:09.019 UTC [143243] ERROR:  XX000: cannot execute a distributed query from a query on a shard
2023-11-25 13:49:09.019 UTC [143243] DETAIL:  Executing a distributed query in a function call that may be pushed to a remote node can lead to incorrect results.
2023-11-25 13:49:09.019 UTC [143243] HINT:  Avoid nesting of distributed queries or use alter user current_user set citus.allow_nested_distributed_execution to on to allow it with possible incorrectness.
2023-11-25 13:49:09.019 UTC [143243] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.test_forcepushdown SELECT(a+1)"
	PL/pgSQL function forcepushdown_schema.insert_select_data(integer) line 3 at SQL statement
	while executing command on localhost:57637
2023-11-25 13:49:09.019 UTC [143243] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:09.019 UTC [143243] STATEMENT:  SELECT insert_select_data(20);
2023-11-25 13:49:09.030 UTC [143243] ERROR:  XX000: cannot execute a distributed query from a query on a shard
2023-11-25 13:49:09.030 UTC [143243] DETAIL:  Executing a distributed query in a function call that may be pushed to a remote node can lead to incorrect results.
2023-11-25 13:49:09.030 UTC [143243] HINT:  Avoid nesting of distributed queries or use alter user current_user set citus.allow_nested_distributed_execution to on to allow it with possible incorrectness.
2023-11-25 13:49:09.030 UTC [143243] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.test_forcepushdown SELECT(a+1)"
	PL/pgSQL function forcepushdown_schema.insert_select_data(integer) line 3 at SQL statement
	while executing command on localhost:57638
2023-11-25 13:49:09.030 UTC [143243] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:09.030 UTC [143243] STATEMENT:  SELECT insert_select_data(22);
2023-11-25 13:49:09.056 UTC [143243] ERROR:  XX000: cannot execute a distributed query from a query on a shard
2023-11-25 13:49:09.056 UTC [143243] DETAIL:  Executing a distributed query in a function call that may be pushed to a remote node can lead to incorrect results.
2023-11-25 13:49:09.056 UTC [143243] HINT:  Avoid nesting of distributed queries or use alter user current_user set citus.allow_nested_distributed_execution to on to allow it with possible incorrectness.
2023-11-25 13:49:09.056 UTC [143243] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.test_forcepushdown(intcol)
			SELECT intcol FROM forcepushdown_schema.test_forcepushdown_noncolocate"
	PL/pgSQL function forcepushdown_schema.insert_select_data_nonlocal(integer) line 3 at SQL statement
	while executing command on localhost:57638
2023-11-25 13:49:09.056 UTC [143243] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:09.056 UTC [143243] STATEMENT:  SELECT insert_select_data_nonlocal(41);
2023-11-25 13:49:09.160 UTC [143243] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 13:49:09.160 UTC [143243] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 13:49:09.160 UTC [143243] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.test_forcepushdown_char VALUES (a)"
	PL/pgSQL function forcepushdown_schema.insert_data_char(character) line 3 at SQL statement
	while executing command on localhost:57638
2023-11-25 13:49:09.160 UTC [143243] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:09.160 UTC [143243] STATEMENT:  SELECT insert_data_char('CHAR');
2023-11-25 13:49:09.167 UTC [143243] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 13:49:09.167 UTC [143243] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 13:49:09.167 UTC [143243] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.test_forcepushdown_char VALUES (a)"
	PL/pgSQL function forcepushdown_schema.insert_data_char(character) line 3 at SQL statement
	while executing command on localhost:57638
2023-11-25 13:49:09.167 UTC [143243] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:09.167 UTC [143243] STATEMENT:  SELECT insert_data_char('CHAR');
2023-11-25 13:49:09.257 UTC [143243] ERROR:  XX000: cannot execute a distributed query from a query on a shard
2023-11-25 13:49:09.257 UTC [143243] DETAIL:  Executing a distributed query in a function call that may be pushed to a remote node can lead to incorrect results.
2023-11-25 13:49:09.257 UTC [143243] HINT:  Avoid nesting of distributed queries or use alter user current_user set citus.allow_nested_distributed_execution to on to allow it with possible incorrectness.
2023-11-25 13:49:09.257 UTC [143243] CONTEXT:  SQL statement "SELECT result          FROM forcepushdown_schema.test_subquery WHERE data =
			(SELECT data FROM forcepushdown_schema.test_subquery WHERE data = a)"
	PL/pgSQL function forcepushdown_schema.select_data(integer) line 4 at SQL statement
	while executing command on localhost:57637
2023-11-25 13:49:09.257 UTC [143243] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:09.257 UTC [143243] STATEMENT:  SELECT select_data(100);
2023-11-25 13:49:09.265 UTC [143243] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 13:49:09.265 UTC [143243] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 13:49:09.265 UTC [143243] CONTEXT:  SQL statement "SELECT data          FROM forcepushdown_schema.test_subquery WHERE data =
			(SELECT id FROM forcepushdown_schema.test_non_colocated WHERE id = a)"
	PL/pgSQL function forcepushdown_schema.select_data_noncolocate(integer) line 5 at SQL statement
	while executing command on localhost:57637
2023-11-25 13:49:09.265 UTC [143243] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:09.265 UTC [143243] STATEMENT:  SELECT select_data_noncolocate(100);
2023-11-25 13:49:09.272 UTC [143243] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 13:49:09.272 UTC [143243] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 13:49:09.272 UTC [143243] CONTEXT:  SQL statement "SELECT data          FROM forcepushdown_schema.test_subquery WHERE data =
			(SELECT id FROM forcepushdown_schema.test_non_colocated WHERE id = a)"
	PL/pgSQL function forcepushdown_schema.select_data_noncolocate(integer) line 5 at SQL statement
	while executing command on localhost:57637
2023-11-25 13:49:09.272 UTC [143243] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:09.272 UTC [143243] STATEMENT:  SELECT select_data_noncolocate(100);
2023-11-25 13:49:09.286 UTC [143243] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 13:49:09.286 UTC [143243] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 13:49:09.286 UTC [143243] CONTEXT:  SQL statement "WITH ins AS (INSERT INTO forcepushdown_schema.test_subquery VALUES (a+1) RETURNING data)
			SELECT ins.data          FROM forcepushdown_schema.test_subquery, ins WHERE forcepushdown_schema.test_subquery.data = a"
	PL/pgSQL function forcepushdown_schema.insert_data_cte_nondist(integer) line 5 at SQL statement
	while executing command on localhost:57637
2023-11-25 13:49:09.286 UTC [143243] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:09.286 UTC [143243] STATEMENT:  SELECT insert_data_cte_nondist(400);
2023-11-25 13:49:09.293 UTC [143243] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 13:49:09.293 UTC [143243] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 13:49:09.293 UTC [143243] CONTEXT:  SQL statement "WITH ins AS (INSERT INTO forcepushdown_schema.test_subquery VALUES (a+1) RETURNING data)
			SELECT ins.data          FROM forcepushdown_schema.test_subquery, ins WHERE forcepushdown_schema.test_subquery.data = a"
	PL/pgSQL function forcepushdown_schema.insert_data_cte_nondist(integer) line 5 at SQL statement
	while executing command on localhost:57637
2023-11-25 13:49:09.293 UTC [143243] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:09.293 UTC [143243] STATEMENT:  SELECT insert_data_cte_nondist(400);
2023-11-25 13:49:09.317 UTC [143243] ERROR:  XX000: cannot execute a distributed query from a query on a shard
2023-11-25 13:49:09.317 UTC [143243] DETAIL:  Executing a distributed query in a function call that may be pushed to a remote node can lead to incorrect results.
2023-11-25 13:49:09.317 UTC [143243] HINT:  Avoid nesting of distributed queries or use alter user current_user set citus.allow_nested_distributed_execution to on to allow it with possible incorrectness.
2023-11-25 13:49:09.317 UTC [143243] CONTEXT:  SQL statement "SELECT result          FROM forcepushdown_schema.test_subquery WHERE data =
			(SELECT data FROM forcepushdown_schema.test_subquery WHERE data = a)"
	PL/pgSQL function forcepushdown_schema.select_data(integer) line 4 at SQL statement
	while executing command on localhost:57637
2023-11-25 13:49:09.317 UTC [143243] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:09.317 UTC [143243] STATEMENT:  SELECT 1,2,3 FROM select_data(100);
2023-11-25 13:49:09.336 UTC [143243] ERROR:  42883: function test_prepare(integer, integer) does not exist
2023-11-25 13:49:09.336 UTC [143243] LOCATION:  LookupFuncWithArgs, parse_func.c:2444
2023-11-25 13:49:09.336 UTC [143243] STATEMENT:  DROP FUNCTION test_prepare(int, int);
2023-11-25 13:49:09.341 UTC [143243] ERROR:  42883: function outer_test_prepare(integer, integer) does not exist
2023-11-25 13:49:09.341 UTC [143243] LOCATION:  LookupFuncWithArgs, parse_func.c:2444
2023-11-25 13:49:09.341 UTC [143243] STATEMENT:  DROP FUNCTION outer_test_prepare(int, int);
2023-11-25 13:49:09.357 UTC [143243] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 13:49:09.357 UTC [143243] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 13:49:09.357 UTC [143243] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.table_test_prepare VALUES (y, x)"
	PL/pgSQL function forcepushdown_schema.test_prepare(integer,integer) line 5 at SQL statement
	while executing command on localhost:57638
	SQL statement "SELECT FROM test_prepare(x, y)"
	PL/pgSQL function outer_test_prepare(integer,integer) line 5 at PERFORM
2023-11-25 13:49:09.357 UTC [143243] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:09.357 UTC [143243] STATEMENT:  SELECT outer_test_prepare(1,2);
2023-11-25 13:49:09.443 UTC [143243] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 13:49:09.443 UTC [143243] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 13:49:09.443 UTC [143243] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.testnested_table VALUES (x,x)"
	PL/pgSQL function forcepushdown_schema.inner_fn(integer) line 4 at SQL statement
	SQL statement "SELECT 1 FROM forcepushdown_schema.inner_fn(z)"
	PL/pgSQL function forcepushdown_schema.outer_fn(integer,integer) line 6 at PERFORM
	while executing command on localhost:57638
2023-11-25 13:49:09.443 UTC [143243] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:09.443 UTC [143243] STATEMENT:  SELECT outer_fn(1, 2);
2023-11-25 13:49:09.450 UTC [143243] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 13:49:09.450 UTC [143243] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 13:49:09.450 UTC [143243] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.testnested_table VALUES (x,x)"
	PL/pgSQL function forcepushdown_schema.inner_fn(integer) line 4 at SQL statement
	SQL statement "SELECT 1 FROM forcepushdown_schema.inner_fn(z)"
	PL/pgSQL function forcepushdown_schema.outer_fn(integer,integer) line 6 at PERFORM
	while executing command on localhost:57638
2023-11-25 13:49:09.450 UTC [143243] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:09.450 UTC [143243] STATEMENT:  SELECT outer_fn(1, 2);
2023-11-25 13:49:09.464 UTC [143243] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 13:49:09.464 UTC [143243] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 13:49:09.464 UTC [143243] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.testnested_table VALUES (y,y)"
	PL/pgSQL function forcepushdown_schema.force_push_inner(integer) line 4 at SQL statement
	SQL statement "SELECT forcepushdown_schema.force_push_inner(x+1) LIMIT 1"
	PL/pgSQL function forcepushdown_schema.force_push_outer(integer) line 5 at PERFORM
	while executing command on localhost:57637
2023-11-25 13:49:09.464 UTC [143243] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:09.464 UTC [143243] STATEMENT:  SELECT force_push_outer(7);
2023-11-25 13:49:09.471 UTC [143243] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 13:49:09.471 UTC [143243] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 13:49:09.471 UTC [143243] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.testnested_table VALUES (y,y)"
	PL/pgSQL function forcepushdown_schema.force_push_inner(integer) line 4 at SQL statement
	SQL statement "SELECT forcepushdown_schema.force_push_inner(x+1) LIMIT 1"
	PL/pgSQL function forcepushdown_schema.force_push_outer(integer) line 5 at PERFORM
	while executing command on localhost:57637
2023-11-25 13:49:09.471 UTC [143243] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:09.471 UTC [143243] STATEMENT:  SELECT force_push_outer(8);
2023-11-25 13:49:09.471 UTC [143243] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 13:49:09.471 UTC [143243] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 13:49:09.471 UTC [143243] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.testnested_table VALUES (y,y)"
	PL/pgSQL function forcepushdown_schema.force_push_inner(integer) line 4 at SQL statement
	SQL statement "SELECT forcepushdown_schema.force_push_inner(x+1) LIMIT 1"
	PL/pgSQL function forcepushdown_schema.force_push_outer(integer) line 5 at PERFORM
	while executing command on localhost:57637
2023-11-25 13:49:09.471 UTC [143243] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:09.471 UTC [143243] STATEMENT:  SELECT force_push_outer(14);
2023-11-25 13:49:09.497 UTC [143243] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 13:49:09.497 UTC [143243] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 13:49:09.497 UTC [143243] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.testnested_table VALUES (y,y)"
	PL/pgSQL function forcepushdown_schema.force_push_2(integer) line 4 at SQL statement
	SQL statement "SELECT forcepushdown_schema.force_push_2(x+1) LIMIT 1"
	PL/pgSQL function forcepushdown_schema.force_push_1(integer) line 5 at PERFORM
	while executing command on localhost:57637
2023-11-25 13:49:09.497 UTC [143243] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:09.497 UTC [143243] STATEMENT:  SELECT force_push_1(7);
2023-11-25 13:49:09.498 UTC [143243] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 13:49:09.498 UTC [143243] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 13:49:09.498 UTC [143243] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.testnested_table VALUES (y,y)"
	PL/pgSQL function forcepushdown_schema.force_push_2(integer) line 4 at SQL statement
	SQL statement "SELECT forcepushdown_schema.force_push_2(x+1) LIMIT 1"
	PL/pgSQL function forcepushdown_schema.force_push_1(integer) line 5 at PERFORM
	while executing command on localhost:57638
2023-11-25 13:49:09.498 UTC [143243] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:09.498 UTC [143243] STATEMENT:  SELECT force_push_1(13);
2023-11-25 13:49:09.522 UTC [143243] ERROR:  0A000: queries must filter by the distribution argument in the same colocation group when using the forced function pushdown
2023-11-25 13:49:09.522 UTC [143243] HINT:  consider disabling forced delegation through create_distributed_table(..., force_delegation := false)
2023-11-25 13:49:09.522 UTC [143243] CONTEXT:  SQL statement "INSERT INTO forcepushdown_schema.testnested_table VALUES (x+1,x+1)"
	PL/pgSQL function forcepushdown_schema.force_push_outer(integer) line 5 at SQL statement
	while executing command on localhost:57637
2023-11-25 13:49:09.522 UTC [143243] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:09.522 UTC [143243] STATEMENT:  SELECT force_push_outer(7);
2023-11-25 13:49:09.945 UTC [143435] ERROR:  0A000: cannot push down this subquery
2023-11-25 13:49:09.945 UTC [143435] DETAIL:  Shards of relations in subquery need to have 1-to-1 shard partitioning
2023-11-25 13:49:09.945 UTC [143435] LOCATION:  ErrorIfUnsupportedShardDistribution, multi_physical_planner.c:2432
2023-11-25 13:49:09.945 UTC [143435] STATEMENT:  SELECT * FROM test_table_1 full join test_table_2 using(id);
2023-11-25 13:49:10.639 UTC [143941] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:49:10.639 UTC [143941] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:49:10.639 UTC [143941] STATEMENT:  SELECT
	  user_id
	FROM
	  users_table
	WHERE
	  value_2 >
	          (SELECT
	              max(value_2)
	           FROM
	              events_table
	           WHERE
	              users_table.user_id > events_table.user_id AND event_type = 1 AND
	              users_table.time = events_table.time
	           GROUP BY
	              user_id
	          )
	GROUP BY user_id
	HAVING count(*) > 1
	ORDER BY user_id
	LIMIT 5;
2023-11-25 13:49:10.701 UTC [143941] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:49:10.701 UTC [143941] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:49:10.701 UTC [143941] STATEMENT:  SELECT user_id, value_2 FROM users_table WHERE
	  value_1 > 1 AND value_1 < 2
	  AND value_2 >= 1
	  AND user_id IN
	  (
			SELECT
			  e1.user_id
			FROM (
			  -- Get the first time each user viewed the homepage.
			  SELECT
			    user_id,
			    1 AS view_homepage,
			    min(time) AS view_homepage_time
			  FROM events_table
			     WHERE
			     event_type IN (0, 1)
			  GROUP BY user_id
			) e1 LEFT JOIN LATERAL (
			  SELECT
			    user_id,
			    1 AS use_demo,
			    time AS use_demo_time
			  FROM events_table
			  WHERE
			    user_id = e1.user_id AND
			       event_type IN (1, 2)
			  ORDER BY time
			) e2 ON true LEFT JOIN LATERAL (
			  SELECT
			    user_id,
			    1 AS enter_credit_card,
			    time AS enter_credit_card_time
			  FROM  events_table
			  WHERE
			    user_id = e2.user_id AND
			    event_type IN (2, 3)
			  ORDER BY time
			) e3 ON true LEFT JOIN LATERAL (
			  SELECT
			    1 AS submit_card_info,
			    user_id,
			    time AS enter_credit_card_time
			  FROM  events_table
			  WHERE
			    value_2 = e3.user_id AND
			    event_type IN (3, 4)
			  ORDER BY time
			) e4 ON true LEFT JOIN LATERAL (
			  SELECT
			    1 AS see_bought_screen
			  FROM  events_table
			  WHERE
			    user_id = e4.user_id AND
			    event_type IN (5, 6)
			  ORDER BY time
			) e5 ON true
			group by e1.user_id
			HAVING sum(submit_card_info) > 0
	);
2023-11-25 13:49:10.725 UTC [143941] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:49:10.725 UTC [143941] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:49:10.725 UTC [143941] STATEMENT:  SELECT user_id, array_length(events_table, 1)
	FROM (
	  SELECT user_id, array_agg(event ORDER BY time) AS events_table
	  FROM (
	    SELECT
	    	u.user_id, e.event_type::text AS event, e.time
	    FROM
	    	users_table AS u,
	        events_table AS e
	    WHERE u.user_id = e.user_id AND
	    		u.user_id IN
	    		(
	    			SELECT
	    				user_id
	    			FROM
	    				users_table
	    			WHERE value_2 >= 5
				    AND  EXISTS (SELECT user_id FROM events_table WHERE event_type > 1 AND event_type <= 3 AND value_3 > 1 AND user_id = users_table.user_id)
					AND  NOT EXISTS (SELECT user_id FROM events_table WHERE event_type > 3 AND event_type <= 4  AND value_3 > 1 AND user_id != users_table.user_id)
	    		)
	  ) t
	  GROUP BY user_id
	) q
	ORDER BY 2 DESC, 1;
2023-11-25 13:49:10.772 UTC [143942] ERROR:  0A000: could not create distributed plan
2023-11-25 13:49:10.772 UTC [143942] DETAIL:  Possibly this is caused by the use of parameters in SQL functions, which is not supported in Citus.
2023-11-25 13:49:10.772 UTC [143942] HINT:  Consider using PL/pgSQL functions instead.
2023-11-25 13:49:10.772 UTC [143942] CONTEXT:  SQL function "sql_subquery_test" statement 1
2023-11-25 13:49:10.772 UTC [143942] LOCATION:  CreateDistributedPlannedStmt, distributed_planner.c:751
2023-11-25 13:49:10.772 UTC [143942] STATEMENT:  SELECT sql_subquery_test(1,1);
2023-11-25 13:49:10.793 UTC [143941] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:49:10.793 UTC [143941] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:49:10.793 UTC [143941] STATEMENT:  SELECT user_id, value_2 FROM users_table WHERE
	  value_1 = 1
	  AND value_2 >= 2
	  AND NOT EXISTS (SELECT user_id FROM events_table WHERE event_type=1 AND value_3 > 1 AND test_join_function(events_table.user_id, users_table.user_id))
	ORDER BY 1 DESC, 2 DESC
	LIMIT 3;
2023-11-25 13:49:10.818 UTC [143939] ERROR:  0A000: complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
2023-11-25 13:49:10.818 UTC [143939] LOCATION:  JoinOrderList, multi_join_order.c:310
2023-11-25 13:49:10.818 UTC [143939] STATEMENT:  SELECT user_id, sum(counter)
	FROM (
	    SELECT user_id, sum(value_2) AS counter FROM users_table GROUP BY user_id
	      UNION
	    SELECT events_table.user_id, sum(events_table.value_2) AS counter FROM events_table, users_table WHERE users_table.user_id > events_table.user_id GROUP BY 1
	) user_id
	GROUP BY user_id;
2023-11-25 13:49:10.819 UTC [143939] ERROR:  0A000: complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
2023-11-25 13:49:10.819 UTC [143939] LOCATION:  JoinOrderList, multi_join_order.c:310
2023-11-25 13:49:10.819 UTC [143939] STATEMENT:  SELECT user_id, sum(counter)
	FROM (
	    SELECT user_id, sum(value_2) AS counter FROM users_table GROUP BY user_id
	      UNION ALL
	    SELECT events_table.user_id, sum(events_table.value_2) AS counter FROM events_table, users_table WHERE users_table.user_id > events_table.user_id GROUP BY 1
	) user_id
	GROUP BY user_id;
2023-11-25 13:49:10.880 UTC [143939] ERROR:  0A000: cannot perform distributed planning on this query because parameterized queries for SQL functions referencing distributed tables are not supported
2023-11-25 13:49:10.880 UTC [143939] HINT:  Consider using PL/pgSQL functions instead.
2023-11-25 13:49:10.880 UTC [143939] LOCATION:  distributed_planner, distributed_planner.c:301
2023-11-25 13:49:10.880 UTC [143939] STATEMENT:  SELECT user_id FROM users_table
	UNION SELECT u.user_id FROM users_table, users_udf() u;
2023-11-25 13:49:11.776 UTC [143940] ERROR:  40P01: canceling the transaction since it was involved in a distributed deadlock
2023-11-25 13:49:11.776 UTC [143940] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 13:49:11.776 UTC [143940] STATEMENT:  SELECT id, pg_advisory_lock(15) FROM t_lock;
2023-11-25 13:49:11.788 UTC [143940] ERROR:  57014: canceling statement due to statement timeout
2023-11-25 13:49:11.788 UTC [143940] LOCATION:  ProcessInterrupts, postgres.c:3326
2023-11-25 13:49:11.788 UTC [143940] STATEMENT:  INSERT INTO t_unrelated SELECT i FROM generate_series(1, 10) i;
2023-11-25 13:49:12.219 UTC [144194] ERROR:  25001: cannot perform query on placements that were modified in this transaction by a different user
2023-11-25 13:49:12.219 UTC [144194] LOCATION:  FindPlacementListConnection, placement_connection.c:647
2023-11-25 13:49:12.219 UTC [144194] STATEMENT:  SELECT * FROM t;
2023-11-25 13:49:12.220 UTC [144194] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 13:49:12.220 UTC [144194] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 13:49:12.220 UTC [144194] STATEMENT:  INSERT INTO t values (1);
2023-11-25 13:49:12.220 UTC [144194] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 13:49:12.220 UTC [144194] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 13:49:12.220 UTC [144194] STATEMENT:  SELECT * from t;
2023-11-25 13:49:12.245 UTC [144194] ERROR:  25001: cannot perform query on placements that were modified in this transaction by a different user
2023-11-25 13:49:12.245 UTC [144194] LOCATION:  FindPlacementListConnection, placement_connection.c:647
2023-11-25 13:49:12.245 UTC [144194] STATEMENT:  INSERT INTO t values (2);
2023-11-25 13:49:12.276 UTC [144194] ERROR:  25001: cannot perform query on placements that were modified in this transaction by a different user
2023-11-25 13:49:12.276 UTC [144194] CONTEXT:  COPY t, line 1: "1"
2023-11-25 13:49:12.276 UTC [144194] LOCATION:  FindPlacementListConnection, placement_connection.c:647
2023-11-25 13:49:12.276 UTC [144194] STATEMENT:  COPY t FROM STDIN;
2023-11-25 13:49:12.374 UTC [144193] LOG:  00000: join order: [ "multi_outer_join_left_hash" ]
2023-11-25 13:49:12.374 UTC [144193] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:12.374 UTC [144193] STATEMENT:  SELECT
		min(r_custkey), max(r_custkey)
	FROM
		multi_outer_join_left_hash a RIGHT JOIN multi_outer_join_right_reference b ON (l_custkey = r_custkey);
2023-11-25 13:49:12.400 UTC [144193] ERROR:  XX000: hash partitioned table has overlapping shards
2023-11-25 13:49:12.400 UTC [144193] LOCATION:  ErrorIfInconsistentShardIntervals, metadata_cache.c:1981
2023-11-25 13:49:12.400 UTC [144193] STATEMENT:  SELECT
		min(l_custkey), max(l_custkey)
	FROM
		multi_outer_join_left_hash a LEFT JOIN multi_outer_join_right_hash b ON (l_custkey = r_custkey);
2023-11-25 13:49:12.566 UTC [144193] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:49:12.566 UTC [144193] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:49:12.566 UTC [144193] STATEMENT:  SELECT
		count(*)
	FROM
		multi_outer_join_left_hash a LEFT JOIN multi_outer_join_right_hash b ON (l_nationkey = r_nationkey);
2023-11-25 13:49:12.584 UTC [144193] LOG:  00000: join order: [ "multi_outer_join_left_hash" ]
2023-11-25 13:49:12.584 UTC [144193] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:12.584 UTC [144193] STATEMENT:  SELECT
		min(r_custkey), max(r_custkey)
	FROM
		multi_outer_join_left_hash a RIGHT JOIN multi_outer_join_right_reference b ON (l_custkey = r_custkey);
2023-11-25 13:49:12.594 UTC [144193] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:49:12.594 UTC [144193] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:49:12.594 UTC [144193] STATEMENT:  SELECT
		*
	FROM
		multi_outer_join_left_hash l1
		LEFT JOIN multi_outer_join_right_reference r1 ON (l1.l_custkey = r1.r_custkey)
		LEFT JOIN multi_outer_join_right_reference r2 ON (l1.l_custkey  = r2.r_custkey)
		RIGHT JOIN multi_outer_join_left_hash l2 ON (r2.r_custkey = l2.l_custkey);
2023-11-25 13:49:12.594 UTC [144193] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:49:12.594 UTC [144193] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:49:12.594 UTC [144193] STATEMENT:  SELECT
		*
	FROM
		multi_outer_join_left_hash l1
		LEFT JOIN multi_outer_join_right_reference r1 ON (l1.l_custkey = r1.r_custkey)
		LEFT JOIN multi_outer_join_right_reference r2 ON (l1.l_custkey  = r2.r_custkey)
		RIGHT JOIN multi_outer_join_left_hash l2 ON (r2.r_custkey = l2.l_custkey)
	WHERE
		r1.r_custkey is NULL;
2023-11-25 13:49:12.599 UTC [144193] LOG:  00000: join order: [ "multi_outer_join_left_hash" ]
2023-11-25 13:49:12.599 UTC [144193] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:12.599 UTC [144193] STATEMENT:  SELECT
		l_custkey, r_custkey, t_custkey
	FROM
		multi_outer_join_left_hash l1
		LEFT JOIN multi_outer_join_right_hash r1 ON (l1.l_custkey = r1.r_custkey)
		RIGHT JOIN multi_outer_join_third_reference t1 ON (r1.r_custkey  = t1.t_custkey)
	ORDER BY 1,2,3;
2023-11-25 13:49:12.599 UTC [144193] LOG:  00000: join order: [ "multi_outer_join_right_hash" ]
2023-11-25 13:49:12.599 UTC [144193] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:12.599 UTC [144193] STATEMENT:  SELECT
		l_custkey, r_custkey, t_custkey
	FROM
		multi_outer_join_left_hash l1
		LEFT JOIN multi_outer_join_right_hash r1 ON (l1.l_custkey = r1.r_custkey)
		RIGHT JOIN multi_outer_join_third_reference t1 ON (r1.r_custkey  = t1.t_custkey)
	ORDER BY 1,2,3;
2023-11-25 13:49:12.648 UTC [144193] LOG:  00000: join order: [ "multi_outer_join_left_hash" ]
2023-11-25 13:49:12.648 UTC [144193] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:12.648 UTC [144193] STATEMENT:  SELECT
		l_custkey, t_custkey
	FROM
		multi_outer_join_left_hash l1
		FULL JOIN multi_outer_join_third_reference t1 ON (l1.l_custkey = t1.t_custkey)
	ORDER BY 1,2;
2023-11-25 13:49:12.718 UTC [144192] ERROR:  0A000: cannot compute aggregate (distinct)
2023-11-25 13:49:12.718 UTC [144192] DETAIL:  table partitioning is unsuitable for aggregate (distinct)
2023-11-25 13:49:12.718 UTC [144192] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4205
2023-11-25 13:49:12.718 UTC [144192] STATEMENT:  SELECT SUM(distinct l_partkey) FROM lineitem_hash;
2023-11-25 13:49:12.718 UTC [144192] ERROR:  0A000: cannot compute aggregate (distinct)
2023-11-25 13:49:12.718 UTC [144192] DETAIL:  table partitioning is unsuitable for aggregate (distinct)
2023-11-25 13:49:12.718 UTC [144192] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4205
2023-11-25 13:49:12.718 UTC [144192] STATEMENT:  SELECT l_shipmode, sum(distinct l_partkey) FROM lineitem_hash GROUP BY l_shipmode;
2023-11-25 13:49:13.115 UTC [144559] ERROR:  0A000: could not run distributed query with FOR UPDATE/SHARE commands
2023-11-25 13:49:13.115 UTC [144559] HINT:  Consider using an equality filter on the distributed table's partition column.
2023-11-25 13:49:13.115 UTC [144559] LOCATION:  DeferErrorIfQueryNotSupported, multi_logical_planner.c:945
2023-11-25 13:49:13.115 UTC [144559] STATEMENT:  SELECT * FROM
		test_table_1_rf1 as tt1 INNER JOIN test_table_1_rf1 as tt2 on tt1.id = tt2.id
		ORDER BY 1
		FOR UPDATE;
2023-11-25 13:49:13.116 UTC [144559] ERROR:  0A000: could not run distributed query with FOR UPDATE/SHARE commands
2023-11-25 13:49:13.116 UTC [144559] LOCATION:  DeferErrorIfQueryNotSupported, multi_logical_planner.c:945
2023-11-25 13:49:13.116 UTC [144559] STATEMENT:  SELECT * FROM
		test_table_1_rf1 as tt1 INNER JOIN test_table_3_rf2 as tt3 on tt1.id = tt3.id
		WHERE tt1.id = 1
		ORDER BY 1
		FOR UPDATE;
2023-11-25 13:49:13.116 UTC [144559] ERROR:  0A000: could not run distributed query with FOR UPDATE/SHARE commands
2023-11-25 13:49:13.116 UTC [144559] LOCATION:  DeferErrorIfQueryNotSupported, multi_logical_planner.c:945
2023-11-25 13:49:13.116 UTC [144559] STATEMENT:  SELECT * FROM
		test_table_1_rf1 as tt1 INNER JOIN test_table_3_rf2 as tt3 on tt1.id = tt3.id
		ORDER BY 1
		FOR UPDATE;
2023-11-25 13:49:13.116 UTC [144559] ERROR:  0A000: could not run distributed query with FOR UPDATE/SHARE commands
2023-11-25 13:49:13.116 UTC [144559] HINT:  Consider using an equality filter on the distributed table's partition column.
2023-11-25 13:49:13.116 UTC [144559] LOCATION:  DeferErrorIfQueryNotSupported, multi_logical_planner.c:945
2023-11-25 13:49:13.116 UTC [144559] STATEMENT:  SELECT * FROM
		test_table_3_rf2 as tt3 INNER JOIN test_table_4_rf2 as tt4 on tt3.id = tt4.id
		WHERE tt3.id = 1
		ORDER BY 1
		FOR UPDATE;
2023-11-25 13:49:13.152 UTC [144561] ERROR:  23505: duplicate key value violates unique constraint "reference_table_test_fourth_pkey_1250003"
2023-11-25 13:49:13.152 UTC [144561] DETAIL:  Key (value_2)=(1) already exists.
2023-11-25 13:49:13.152 UTC [144561] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:49:13.152 UTC [144561] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:13.152 UTC [144561] STATEMENT:  INSERT INTO reference_table_test_fourth VALUES (1, 1.0, '1', '2016-12-01');
2023-11-25 13:49:13.153 UTC [144561] ERROR:  23502: null value in column "value_2" of relation "reference_table_test_fourth_1250003" violates not-null constraint
2023-11-25 13:49:13.153 UTC [144561] DETAIL:  Failing row contains (1, null, 1.0, 2016-12-01 00:00:00).
2023-11-25 13:49:13.153 UTC [144561] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:49:13.153 UTC [144561] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:13.153 UTC [144561] STATEMENT:  INSERT INTO reference_table_test_fourth (value_1, value_3, value_4) VALUES (1, '1.0', '2016-12-01');
2023-11-25 13:49:13.270 UTC [144561] LOG:  00000: join order: [ "colocated_table_test" ][ reference join "reference_table_test" ]
2023-11-25 13:49:13.270 UTC [144561] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:13.270 UTC [144561] STATEMENT:  SELECT
		reference_table_test.value_1
	FROM
		reference_table_test, colocated_table_test
	WHERE
		colocated_table_test.value_1 = reference_table_test.value_1
	ORDER BY 1;
2023-11-25 13:49:13.275 UTC [144561] LOG:  00000: join order: [ "colocated_table_test" ][ reference join "reference_table_test" ]
2023-11-25 13:49:13.275 UTC [144561] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:13.275 UTC [144561] STATEMENT:  SELECT
		colocated_table_test.value_2
	FROM
		reference_table_test, colocated_table_test
	WHERE
		colocated_table_test.value_2 = reference_table_test.value_2
	ORDER BY 1;
2023-11-25 13:49:13.284 UTC [144561] LOG:  00000: join order: [ "colocated_table_test" ][ reference join "reference_table_test" ]
2023-11-25 13:49:13.284 UTC [144561] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:13.284 UTC [144561] STATEMENT:  SELECT
		colocated_table_test.value_2
	FROM
		colocated_table_test, reference_table_test
	WHERE
		reference_table_test.value_1 = colocated_table_test.value_1
	ORDER BY 1;
2023-11-25 13:49:13.291 UTC [144561] LOG:  00000: join order: [ "colocated_table_test_2" ][ cartesian product reference join "reference_table_test" ][ dual partition join "colocated_table_test" ]
2023-11-25 13:49:13.291 UTC [144561] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:13.291 UTC [144561] STATEMENT:  SELECT
		colocated_table_test.value_2
	FROM
		reference_table_test, colocated_table_test, colocated_table_test_2
	WHERE
		colocated_table_test.value_2 = reference_table_test.value_2
	ORDER BY colocated_table_test.value_2;
2023-11-25 13:49:13.448 UTC [144561] LOG:  00000: join order: [ "colocated_table_test" ][ reference join "reference_table_test" ][ local partition join "colocated_table_test_2" ]
2023-11-25 13:49:13.448 UTC [144561] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:13.448 UTC [144561] STATEMENT:  SELECT
		colocated_table_test.value_2
	FROM
		reference_table_test, colocated_table_test, colocated_table_test_2
	WHERE
		colocated_table_test.value_1 = colocated_table_test_2.value_1 AND colocated_table_test.value_2 = reference_table_test.value_2
	ORDER BY 1;
2023-11-25 13:49:13.454 UTC [144561] LOG:  00000: join order: [ "colocated_table_test" ][ reference join "reference_table_test" ][ dual partition join "colocated_table_test_2" ]
2023-11-25 13:49:13.454 UTC [144561] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:13.454 UTC [144561] STATEMENT:  SELECT
		colocated_table_test.value_2
	FROM
		reference_table_test, colocated_table_test, colocated_table_test_2
	WHERE
		colocated_table_test.value_2 = colocated_table_test_2.value_2 AND colocated_table_test.value_2 = reference_table_test.value_2
	ORDER BY colocated_table_test.value_2;
2023-11-25 13:49:13.591 UTC [144561] LOG:  00000: join order: [ "colocated_table_test" ][ reference join "reference_table_test" ][ dual partition join "colocated_table_test_2" ]
2023-11-25 13:49:13.591 UTC [144561] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:13.591 UTC [144561] STATEMENT:  SELECT
		reference_table_test.value_2
	FROM
		reference_table_test, colocated_table_test, colocated_table_test_2
	WHERE
		colocated_table_test.value_1 = reference_table_test.value_1 AND colocated_table_test_2.value_1 = reference_table_test.value_1
	ORDER BY reference_table_test.value_2;
2023-11-25 13:49:13.778 UTC [144561] ERROR:  0A000: relation reference_table_test should be a hash distributed table
2023-11-25 13:49:13.778 UTC [144561] LOCATION:  EnsureHashDistributedTable, metadata_utility.c:2284
2023-11-25 13:49:13.778 UTC [144561] STATEMENT:  SELECT update_distributed_table_colocation('colocated_table_test_2', colocate_with => 'reference_table_test');
2023-11-25 13:49:13.778 UTC [144561] ERROR:  0A000: relation reference_table_test_fifth should be a hash distributed table
2023-11-25 13:49:13.778 UTC [144561] LOCATION:  EnsureHashDistributedTable, metadata_utility.c:2284
2023-11-25 13:49:13.778 UTC [144561] STATEMENT:  SELECT update_distributed_table_colocation('reference_table_test', colocate_with => 'reference_table_test_fifth');
2023-11-25 13:49:13.921 UTC [145112] ERROR:  XX000: relation "reference_schema.reference_table_ddl" is a reference table
2023-11-25 13:49:13.921 UTC [145112] DETAIL:  We currently don't support creating shards on reference tables
2023-11-25 13:49:13.921 UTC [145112] LOCATION:  master_create_empty_shard, stage_protocol.c:143
2023-11-25 13:49:13.921 UTC [145112] STATEMENT:  SELECT master_create_empty_shard('reference_schema.reference_table_ddl');
2023-11-25 13:49:14.254 UTC [145173] ERROR:  42704: type "hll" does not exist at character 51
2023-11-25 13:49:14.254 UTC [145173] LOCATION:  typenameType, parse_type.c:270
2023-11-25 13:49:14.254 UTC [145173] STATEMENT:  CREATE TABLE daily_uniques(day date, unique_users hll);
2023-11-25 13:49:14.270 UTC [145173] ERROR:  42P01: relation "daily_uniques" does not exist at character 33
2023-11-25 13:49:14.270 UTC [145173] LOCATION:  RangeVarGetRelidExtended, namespace.c:433
2023-11-25 13:49:14.270 UTC [145173] STATEMENT:  SELECT create_distributed_table('daily_uniques', 'day');
2023-11-25 13:49:14.327 UTC [145172] ERROR:  0A000: cannot compute aggregate (distinct)
2023-11-25 13:49:14.327 UTC [145172] DETAIL:  table partitioning is unsuitable for aggregate (distinct)
2023-11-25 13:49:14.327 UTC [145172] LOCATION:  DeferErrorIfUnsupportedAggregateDistinct, multi_logical_optimizer.c:4205
2023-11-25 13:49:14.327 UTC [145172] STATEMENT:  select key, sum2(distinct val), sum2_strict(distinct val), psum(distinct val, valf::int), psum_strict(distinct val, valf::int) from aggdata group by key order by key;
2023-11-25 13:49:14.329 UTC [145174] ERROR:  42883: function tdigest(double precision, integer) does not exist at character 37
2023-11-25 13:49:14.329 UTC [145174] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:49:14.329 UTC [145174] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 13:49:14.329 UTC [145174] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT tdigest(latency, 100)
	FROM latencies;
2023-11-25 13:49:14.329 UTC [145174] ERROR:  42883: function tdigest(double precision, integer) does not exist at character 40
2023-11-25 13:49:14.329 UTC [145174] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:49:14.329 UTC [145174] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 13:49:14.329 UTC [145174] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT a, tdigest(latency, 100)
	FROM latencies
	GROUP BY a;
2023-11-25 13:49:14.329 UTC [145174] ERROR:  42883: function tdigest(double precision, integer) does not exist at character 40
2023-11-25 13:49:14.329 UTC [145174] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:49:14.329 UTC [145174] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 13:49:14.329 UTC [145174] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT b, tdigest(latency, 100)
	FROM latencies
	GROUP BY b;
2023-11-25 13:49:14.329 UTC [145174] ERROR:  42883: function tdigest_percentile(double precision, integer, numeric) does not exist at character 37
2023-11-25 13:49:14.329 UTC [145174] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:49:14.329 UTC [145174] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 13:49:14.329 UTC [145174] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT tdigest_percentile(latency, 100, 0.99)
	FROM latencies;
2023-11-25 13:49:14.329 UTC [145174] ERROR:  42883: function tdigest_percentile(double precision, integer, numeric) does not exist at character 40
2023-11-25 13:49:14.329 UTC [145174] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:49:14.329 UTC [145174] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 13:49:14.329 UTC [145174] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT a, tdigest_percentile(latency, 100, 0.99)
	FROM latencies
	GROUP BY a;
2023-11-25 13:49:14.330 UTC [145174] ERROR:  42883: function tdigest_percentile(double precision, integer, numeric) does not exist at character 40
2023-11-25 13:49:14.330 UTC [145174] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:49:14.330 UTC [145174] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 13:49:14.330 UTC [145174] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT b, tdigest_percentile(latency, 100, 0.99)
	FROM latencies
	GROUP BY b;
2023-11-25 13:49:14.330 UTC [145174] ERROR:  42883: function tdigest_percentile(double precision, integer, numeric[]) does not exist at character 37
2023-11-25 13:49:14.330 UTC [145174] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:49:14.330 UTC [145174] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 13:49:14.330 UTC [145174] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT tdigest_percentile(latency, 100, ARRAY[0.99, 0.95])
	FROM latencies;
2023-11-25 13:49:14.330 UTC [145174] ERROR:  42883: function tdigest_percentile(double precision, integer, numeric[]) does not exist at character 40
2023-11-25 13:49:14.330 UTC [145174] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:49:14.330 UTC [145174] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 13:49:14.330 UTC [145174] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT a, tdigest_percentile(latency, 100, ARRAY[0.99, 0.95])
	FROM latencies
	GROUP BY a;
2023-11-25 13:49:14.330 UTC [145174] ERROR:  42883: function tdigest_percentile(double precision, integer, numeric[]) does not exist at character 40
2023-11-25 13:49:14.330 UTC [145174] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:49:14.330 UTC [145174] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 13:49:14.330 UTC [145174] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT b, tdigest_percentile(latency, 100, ARRAY[0.99, 0.95])
	FROM latencies
	GROUP BY b;
2023-11-25 13:49:14.330 UTC [145174] ERROR:  42883: function tdigest_percentile_of(double precision, integer, integer) does not exist at character 37
2023-11-25 13:49:14.330 UTC [145174] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:49:14.330 UTC [145174] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 13:49:14.330 UTC [145174] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT tdigest_percentile_of(latency, 100, 9000)
	FROM latencies;
2023-11-25 13:49:14.330 UTC [145174] ERROR:  42883: function tdigest_percentile_of(double precision, integer, integer) does not exist at character 40
2023-11-25 13:49:14.330 UTC [145174] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:49:14.330 UTC [145174] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 13:49:14.330 UTC [145174] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT a, tdigest_percentile_of(latency, 100, 9000)
	FROM latencies
	GROUP BY a;
2023-11-25 13:49:14.331 UTC [145174] ERROR:  42883: function tdigest_percentile_of(double precision, integer, integer) does not exist at character 40
2023-11-25 13:49:14.331 UTC [145174] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:49:14.331 UTC [145174] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 13:49:14.331 UTC [145174] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT b, tdigest_percentile_of(latency, 100, 9000)
	FROM latencies
	GROUP BY b;
2023-11-25 13:49:14.331 UTC [145174] ERROR:  42883: function tdigest_percentile_of(double precision, integer, integer[]) does not exist at character 37
2023-11-25 13:49:14.331 UTC [145174] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:49:14.331 UTC [145174] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 13:49:14.331 UTC [145174] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT tdigest_percentile_of(latency, 100, ARRAY[9000, 9500])
	FROM latencies;
2023-11-25 13:49:14.331 UTC [145174] ERROR:  42883: function tdigest_percentile_of(double precision, integer, integer[]) does not exist at character 40
2023-11-25 13:49:14.331 UTC [145174] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:49:14.331 UTC [145174] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 13:49:14.331 UTC [145174] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT a, tdigest_percentile_of(latency, 100, ARRAY[9000, 9500])
	FROM latencies
	GROUP BY a;
2023-11-25 13:49:14.331 UTC [145174] ERROR:  42883: function tdigest_percentile_of(double precision, integer, integer[]) does not exist at character 40
2023-11-25 13:49:14.331 UTC [145174] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:49:14.331 UTC [145174] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 13:49:14.331 UTC [145174] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT b, tdigest_percentile_of(latency, 100, ARRAY[9000, 9500])
	FROM latencies
	GROUP BY b;
2023-11-25 13:49:14.331 UTC [145174] ERROR:  42883: function tdigest(double precision, integer) does not exist at character 8
2023-11-25 13:49:14.331 UTC [145174] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:49:14.331 UTC [145174] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 13:49:14.331 UTC [145174] STATEMENT:  SELECT tdigest(latency, 100) FROM latencies;
2023-11-25 13:49:14.331 UTC [145174] ERROR:  42883: function tdigest_percentile(double precision, integer, numeric) does not exist at character 8
2023-11-25 13:49:14.331 UTC [145174] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:49:14.331 UTC [145174] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 13:49:14.331 UTC [145174] STATEMENT:  SELECT tdigest_percentile(latency, 100, 0.99) FROM latencies;
2023-11-25 13:49:14.331 UTC [145174] ERROR:  42883: function tdigest_percentile(double precision, integer, numeric[]) does not exist at character 8
2023-11-25 13:49:14.331 UTC [145174] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:49:14.331 UTC [145174] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 13:49:14.331 UTC [145174] STATEMENT:  SELECT tdigest_percentile(latency, 100, ARRAY[0.99, 0.95]) FROM latencies;
2023-11-25 13:49:14.331 UTC [145174] ERROR:  42883: function tdigest_percentile_of(double precision, integer, integer) does not exist at character 8
2023-11-25 13:49:14.331 UTC [145174] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:49:14.331 UTC [145174] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 13:49:14.331 UTC [145174] STATEMENT:  SELECT tdigest_percentile_of(latency, 100, 9000) FROM latencies;
2023-11-25 13:49:14.332 UTC [145174] ERROR:  42883: function tdigest_percentile_of(double precision, integer, integer[]) does not exist at character 8
2023-11-25 13:49:14.332 UTC [145174] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:49:14.332 UTC [145174] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 13:49:14.332 UTC [145174] STATEMENT:  SELECT tdigest_percentile_of(latency, 100, ARRAY[9000, 9500]) FROM latencies;
2023-11-25 13:49:14.332 UTC [145174] ERROR:  42704: type "tdigest" does not exist at character 47
2023-11-25 13:49:14.332 UTC [145174] LOCATION:  typenameType, parse_type.c:270
2023-11-25 13:49:14.332 UTC [145174] STATEMENT:  CREATE TABLE latencies_rollup (a int, tdigest tdigest);
2023-11-25 13:49:14.332 UTC [145174] ERROR:  42P01: relation "latencies_rollup" does not exist at character 33
2023-11-25 13:49:14.332 UTC [145174] LOCATION:  RangeVarGetRelidExtended, namespace.c:433
2023-11-25 13:49:14.332 UTC [145174] STATEMENT:  SELECT create_distributed_table('latencies_rollup', 'a', colocate_with => 'latencies');
2023-11-25 13:49:14.332 UTC [145174] ERROR:  42P01: relation "latencies_rollup" does not exist at character 13
2023-11-25 13:49:14.332 UTC [145174] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 13:49:14.332 UTC [145174] STATEMENT:  INSERT INTO latencies_rollup
	SELECT a, tdigest(latency, 100)
	FROM latencies
	GROUP BY a;
2023-11-25 13:49:14.332 UTC [145174] ERROR:  42P01: relation "latencies_rollup" does not exist at character 59
2023-11-25 13:49:14.332 UTC [145174] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 13:49:14.332 UTC [145174] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT tdigest(tdigest)
	FROM latencies_rollup;
2023-11-25 13:49:14.332 UTC [145174] ERROR:  42P01: relation "latencies_rollup" does not exist at character 62
2023-11-25 13:49:14.332 UTC [145174] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 13:49:14.332 UTC [145174] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT a, tdigest(tdigest)
	FROM latencies_rollup
	GROUP BY a;
2023-11-25 13:49:14.332 UTC [145174] ERROR:  42P01: relation "latencies_rollup" does not exist at character 76
2023-11-25 13:49:14.332 UTC [145174] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 13:49:14.332 UTC [145174] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT tdigest_percentile(tdigest, 0.99)
	FROM latencies_rollup;
2023-11-25 13:49:14.332 UTC [145174] ERROR:  42P01: relation "latencies_rollup" does not exist at character 79
2023-11-25 13:49:14.332 UTC [145174] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 13:49:14.332 UTC [145174] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT a, tdigest_percentile(tdigest, 0.99)
	FROM latencies_rollup
	GROUP BY a;
2023-11-25 13:49:14.332 UTC [145174] ERROR:  42P01: relation "latencies_rollup" does not exist at character 89
2023-11-25 13:49:14.332 UTC [145174] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 13:49:14.332 UTC [145174] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT tdigest_percentile(tdigest, ARRAY[0.99, 0.95])
	FROM latencies_rollup;
2023-11-25 13:49:14.333 UTC [145174] ERROR:  42P01: relation "latencies_rollup" does not exist at character 92
2023-11-25 13:49:14.333 UTC [145174] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 13:49:14.333 UTC [145174] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT a, tdigest_percentile(tdigest, ARRAY[0.99, 0.95])
	FROM latencies_rollup
	GROUP BY a;
2023-11-25 13:49:14.333 UTC [145174] ERROR:  42P01: relation "latencies_rollup" does not exist at character 79
2023-11-25 13:49:14.333 UTC [145174] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 13:49:14.333 UTC [145174] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT tdigest_percentile_of(tdigest, 9000)
	FROM latencies_rollup;
2023-11-25 13:49:14.333 UTC [145174] ERROR:  42P01: relation "latencies_rollup" does not exist at character 82
2023-11-25 13:49:14.333 UTC [145174] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 13:49:14.333 UTC [145174] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT a, tdigest_percentile_of(tdigest, 9000)
	FROM latencies_rollup
	GROUP BY a;
2023-11-25 13:49:14.333 UTC [145174] ERROR:  42P01: relation "latencies_rollup" does not exist at character 92
2023-11-25 13:49:14.333 UTC [145174] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 13:49:14.333 UTC [145174] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT tdigest_percentile_of(tdigest, ARRAY[9000, 9500])
	FROM latencies_rollup;
2023-11-25 13:49:14.333 UTC [145174] ERROR:  42P01: relation "latencies_rollup" does not exist at character 95
2023-11-25 13:49:14.333 UTC [145174] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 13:49:14.333 UTC [145174] STATEMENT:  EXPLAIN (COSTS OFF, VERBOSE)
	SELECT a, tdigest_percentile_of(tdigest, ARRAY[9000, 9500])
	FROM latencies_rollup
	GROUP BY a;
2023-11-25 13:49:14.333 UTC [145174] ERROR:  42P01: relation "latencies_rollup" does not exist at character 30
2023-11-25 13:49:14.333 UTC [145174] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 13:49:14.333 UTC [145174] STATEMENT:  SELECT tdigest(tdigest) FROM latencies_rollup;
2023-11-25 13:49:14.333 UTC [145174] ERROR:  42P01: relation "latencies_rollup" does not exist at character 47
2023-11-25 13:49:14.333 UTC [145174] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 13:49:14.333 UTC [145174] STATEMENT:  SELECT tdigest_percentile(tdigest, 0.99) FROM latencies_rollup;
2023-11-25 13:49:14.333 UTC [145174] ERROR:  42P01: relation "latencies_rollup" does not exist at character 60
2023-11-25 13:49:14.333 UTC [145174] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 13:49:14.333 UTC [145174] STATEMENT:  SELECT tdigest_percentile(tdigest, ARRAY[0.99, 0.95]) FROM latencies_rollup;
2023-11-25 13:49:14.333 UTC [145174] ERROR:  42P01: relation "latencies_rollup" does not exist at character 50
2023-11-25 13:49:14.333 UTC [145174] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 13:49:14.333 UTC [145174] STATEMENT:  SELECT tdigest_percentile_of(tdigest, 9000) FROM latencies_rollup;
2023-11-25 13:49:14.333 UTC [145174] ERROR:  42P01: relation "latencies_rollup" does not exist at character 63
2023-11-25 13:49:14.333 UTC [145174] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 13:49:14.333 UTC [145174] STATEMENT:  SELECT tdigest_percentile_of(tdigest, ARRAY[9000, 9500]) FROM latencies_rollup;
2023-11-25 13:49:14.335 UTC [145172] ERROR:  XX000: unsupported aggregate function sum2
2023-11-25 13:49:14.335 UTC [145172] LOCATION:  GetAggregateType, multi_logical_optimizer.c:3481
2023-11-25 13:49:14.335 UTC [145172] STATEMENT:  select key, sum2(val order by valf), sum2_strict(val order by valf), psum(val, valf::int order by valf), psum_strict(val, valf::int order by valf) from aggdata group by key order by key;
2023-11-25 13:49:14.360 UTC [145173] ERROR:  42883: function hll_hash_integer(integer) does not exist at character 72
2023-11-25 13:49:14.360 UTC [145173] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:49:14.360 UTC [145173] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 13:49:14.360 UTC [145173] STATEMENT:  SELECT hll_cardinality(hll_union_agg(agg))
	FROM (
	  SELECT hll_add_agg(hll_hash_integer(user_id)) AS agg
	  FROM raw_table)a;
2023-11-25 13:49:14.360 UTC [145173] ERROR:  42P01: relation "daily_uniques" does not exist at character 13
2023-11-25 13:49:14.360 UTC [145173] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 13:49:14.360 UTC [145173] STATEMENT:  INSERT INTO daily_uniques
	  SELECT day, hll_add_agg(hll_hash_integer(user_id))
	  FROM raw_table
	  GROUP BY 1;
2023-11-25 13:49:14.360 UTC [145173] ERROR:  42P01: relation "daily_uniques" does not exist at character 48
2023-11-25 13:49:14.360 UTC [145173] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 13:49:14.360 UTC [145173] STATEMENT:  SELECT day, hll_cardinality(unique_users)
	FROM daily_uniques
	WHERE day >= '2018-06-20' and day <= '2018-06-30'
	ORDER BY 2 DESC,1
	LIMIT 10;
2023-11-25 13:49:14.360 UTC [145173] ERROR:  42P01: relation "daily_uniques" does not exist at character 58
2023-11-25 13:49:14.360 UTC [145173] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 13:49:14.360 UTC [145173] STATEMENT:  SELECT hll_cardinality(hll_union_agg(unique_users))
	FROM daily_uniques
	WHERE day >= '2018-05-24'::date AND day <= '2018-05-31'::date;
2023-11-25 13:49:14.360 UTC [145173] ERROR:  42P01: relation "daily_uniques" does not exist at character 92
2023-11-25 13:49:14.360 UTC [145173] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 13:49:14.360 UTC [145173] STATEMENT:  SELECT EXTRACT(MONTH FROM day) AS month, hll_cardinality(hll_union_agg(unique_users))
	FROM daily_uniques
	WHERE day >= '2018-06-23' AND day <= '2018-07-01'
	GROUP BY 1
	ORDER BY 1;
2023-11-25 13:49:14.360 UTC [145173] ERROR:  42P01: relation "daily_uniques" does not exist at character 79
2023-11-25 13:49:14.360 UTC [145173] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 13:49:14.360 UTC [145173] STATEMENT:  SELECT day, hll_cardinality(hll_union_agg(unique_users) OVER seven_days)
	FROM daily_uniques
	WINDOW seven_days AS (ORDER BY day ASC ROWS 6 PRECEDING)
	ORDER BY 1;
2023-11-25 13:49:14.360 UTC [145173] ERROR:  42P01: relation "daily_uniques" does not exist at character 127
2023-11-25 13:49:14.360 UTC [145173] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 13:49:14.360 UTC [145173] STATEMENT:  SELECT day, (hll_cardinality(hll_union_agg(unique_users) OVER two_days)) - hll_cardinality(unique_users) AS lost_uniques
	FROM daily_uniques
	WINDOW two_days AS (ORDER BY day ASC ROWS 1 PRECEDING)
	ORDER BY 1;
2023-11-25 13:49:14.361 UTC [145173] ERROR:  42P01: relation "daily_uniques" does not exist at character 69
2023-11-25 13:49:14.361 UTC [145173] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 13:49:14.361 UTC [145173] STATEMENT:  EXPLAIN(COSTS OFF)
	SELECT
	  day, hll_union_agg(unique_users)
	FROM
	  daily_uniques
	GROUP BY(1);
2023-11-25 13:49:14.362 UTC [145173] ERROR:  42P01: relation "daily_uniques" does not exist at character 69
2023-11-25 13:49:14.362 UTC [145173] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 13:49:14.362 UTC [145173] STATEMENT:  EXPLAIN(COSTS OFF)
	SELECT
	  day, hll_union_agg(unique_users)
	FROM
	  daily_uniques
	GROUP BY(1);
2023-11-25 13:49:14.362 UTC [145173] ERROR:  42P01: relation "daily_uniques" does not exist at character 100
2023-11-25 13:49:14.362 UTC [145173] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 13:49:14.362 UTC [145173] STATEMENT:  EXPLAIN(COSTS OFF)
	SELECT
	  day, hll_union_agg(unique_users) || hll_union_agg(unique_users)
	FROM
	  daily_uniques
	GROUP BY(1);
2023-11-25 13:49:14.362 UTC [145173] ERROR:  42P01: relation "daily_uniques" does not exist at character 100
2023-11-25 13:49:14.362 UTC [145173] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 13:49:14.362 UTC [145173] STATEMENT:  EXPLAIN(COSTS OFF)
	SELECT
	  day, hll_union_agg(unique_users) || hll_union_agg(unique_users)
	FROM
	  daily_uniques
	GROUP BY(1);
2023-11-25 13:49:14.362 UTC [145173] ERROR:  42P01: relation "daily_uniques" does not exist at character 86
2023-11-25 13:49:14.362 UTC [145173] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 13:49:14.362 UTC [145173] STATEMENT:  EXPLAIN(COSTS OFF)
	SELECT
	  day, hll_cardinality(hll_union_agg(unique_users))
	FROM
	  daily_uniques
	GROUP BY(1);
2023-11-25 13:49:14.362 UTC [145173] ERROR:  42P01: relation "daily_uniques" does not exist at character 86
2023-11-25 13:49:14.362 UTC [145173] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 13:49:14.362 UTC [145173] STATEMENT:  EXPLAIN(COSTS OFF)
	SELECT
	  day, hll_cardinality(hll_union_agg(unique_users))
	FROM
	  daily_uniques
	GROUP BY(1);
2023-11-25 13:49:14.363 UTC [145173] ERROR:  42P01: relation "daily_uniques" does not exist at character 86
2023-11-25 13:49:14.363 UTC [145173] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 13:49:14.363 UTC [145173] STATEMENT:  EXPLAIN(COSTS OFF)
	SELECT
	  day, hll_cardinality(hll_union_agg(unique_users))
	FROM
	  daily_uniques
	GROUP BY(1);
2023-11-25 13:49:14.363 UTC [145173] ERROR:  42P01: relation "daily_uniques" does not exist at character 86
2023-11-25 13:49:14.363 UTC [145173] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 13:49:14.363 UTC [145173] STATEMENT:  EXPLAIN(COSTS OFF)
	SELECT
	  day, hll_cardinality(hll_union_agg(unique_users))
	FROM
	  daily_uniques
	GROUP BY(1)
	HAVING hll_cardinality(hll_union_agg(unique_users)) > 1;
2023-11-25 13:49:14.377 UTC [145173] ERROR:  42P01: table "daily_uniques" does not exist
2023-11-25 13:49:14.377 UTC [145173] LOCATION:  DropErrorMsgNonExistent, tablecmds.c:1290
2023-11-25 13:49:14.377 UTC [145173] STATEMENT:  DROP TABLE daily_uniques;
2023-11-25 13:49:14.517 UTC [145173] ERROR:  42883: function topn_add_agg(text) does not exist at character 42
2023-11-25 13:49:14.517 UTC [145173] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:49:14.517 UTC [145173] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 13:49:14.517 UTC [145173] STATEMENT:  SELECT (topn(agg, 10)).*
	FROM (
	  SELECT topn_add_agg(user_id::text) AS agg
	  FROM customer_reviews
	  )a
	ORDER BY 2 DESC, 1;
2023-11-25 13:49:14.517 UTC [145173] ERROR:  42883: function topn_add_agg(text) does not exist at character 44
2023-11-25 13:49:14.517 UTC [145173] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:49:14.517 UTC [145173] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 13:49:14.517 UTC [145173] STATEMENT:  INSERT INTO popular_reviewer
	  SELECT day, topn_add_agg(user_id::text)
	  FROM customer_reviews
	  GROUP BY 1;
2023-11-25 13:49:14.517 UTC [145173] ERROR:  42883: function topn(jsonb, integer) does not exist at character 14
2023-11-25 13:49:14.517 UTC [145173] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:49:14.517 UTC [145173] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 13:49:14.517 UTC [145173] STATEMENT:  SELECT day, (topn(reviewers, 10)).*
	FROM popular_reviewer
	WHERE day >= '2018-06-20' and day <= '2018-06-30'
	ORDER BY 3 DESC, 1, 2
	LIMIT 10;
2023-11-25 13:49:14.517 UTC [145173] ERROR:  42883: function topn_union_agg(jsonb) does not exist at character 41
2023-11-25 13:49:14.517 UTC [145173] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:49:14.517 UTC [145173] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 13:49:14.517 UTC [145173] STATEMENT:  SELECT (topn(agg, 10)).*
	FROM (
		SELECT topn_union_agg(reviewers) AS agg
		FROM popular_reviewer
		WHERE day >= '2018-05-24'::date AND day <= '2018-05-31'::date
		)a
	ORDER BY 2 DESC, 1;
2023-11-25 13:49:14.518 UTC [145173] ERROR:  42883: function topn_union_agg(jsonb) does not exist at character 81
2023-11-25 13:49:14.518 UTC [145173] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:49:14.518 UTC [145173] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 13:49:14.518 UTC [145173] STATEMENT:  SELECT month, (topn(agg, 5)).*
	FROM (
		SELECT EXTRACT(MONTH FROM day) AS month, topn_union_agg(reviewers) AS agg
		FROM popular_reviewer
		WHERE day >= '2018-06-23' AND day <= '2018-07-01'
		GROUP BY 1
		ORDER BY 1
		)a
	ORDER BY 1, 3 DESC, 2;
2023-11-25 13:49:14.518 UTC [145173] ERROR:  42883: function topn_union_agg(jsonb) does not exist at character 14
2023-11-25 13:49:14.518 UTC [145173] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:49:14.518 UTC [145173] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 13:49:14.518 UTC [145173] STATEMENT:  SELECT (topn(topn_union_agg(reviewers), 10)).*
	FROM popular_reviewer
	WHERE day >= '2018-05-24'::date AND day <= '2018-05-31'::date
	ORDER BY 2 DESC, 1;
2023-11-25 13:49:14.518 UTC [145173] ERROR:  42883: function topn_add_agg(text) does not exist at character 14
2023-11-25 13:49:14.518 UTC [145173] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:49:14.518 UTC [145173] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 13:49:14.518 UTC [145173] STATEMENT:  SELECT (topn(topn_add_agg(user_id::text), 10)).*
	FROM customer_reviews
	ORDER BY 2 DESC, 1;
2023-11-25 13:49:14.518 UTC [145173] ERROR:  42883: function topn_union_agg(jsonb) does not exist at character 51
2023-11-25 13:49:14.518 UTC [145173] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:49:14.518 UTC [145173] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 13:49:14.518 UTC [145173] STATEMENT:  SELECT day, (topn(agg, 10)).*
	FROM (
		SELECT day, topn_union_agg(reviewers) OVER seven_days AS agg
		FROM popular_reviewer
		WINDOW seven_days AS (ORDER BY day ASC ROWS 6 PRECEDING)
		)a
	ORDER BY 3 DESC, 1, 2
	LIMIT 10;
2023-11-25 13:49:14.518 UTC [145173] ERROR:  42883: function topn_add_agg(text) does not exist at character 19
2023-11-25 13:49:14.518 UTC [145173] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:49:14.518 UTC [145173] LOCATION:  ParseFuncOrColumn, parse_func.c:629
2023-11-25 13:49:14.518 UTC [145173] STATEMENT:  SELECT day, (topn(topn_add_agg(user_id::text) OVER seven_days, 10)).*
	FROM customer_reviews
	WINDOW seven_days AS (ORDER BY day ASC ROWS 6 PRECEDING)
	ORDER BY 3 DESC, 1, 2
	LIMIT 10;
2023-11-25 13:49:14.696 UTC [145172] ERROR:  XX000: unsupported aggregate function first
2023-11-25 13:49:14.696 UTC [145172] LOCATION:  GetAggregateType, multi_logical_optimizer.c:3481
2023-11-25 13:49:14.696 UTC [145172] STATEMENT:  SELECT key, first(val ORDER BY id), last(val ORDER BY id)
	FROM aggdata GROUP BY key ORDER BY key;
2023-11-25 13:49:14.699 UTC [145172] ERROR:  XX000: unsupported aggregate function first
2023-11-25 13:49:14.699 UTC [145172] LOCATION:  GetAggregateType, multi_logical_optimizer.c:3481
2023-11-25 13:49:14.699 UTC [145172] STATEMENT:  SELECT id%5, first(val ORDER BY key), last(val ORDER BY key)
	FROM aggdata GROUP BY id%5 ORDER BY id%5;
2023-11-25 13:49:14.801 UTC [145172] ERROR:  0A000: could not run distributed query with GROUPING
2023-11-25 13:49:14.801 UTC [145172] HINT:  Consider using an equality filter on the distributed table's partition column.
2023-11-25 13:49:14.801 UTC [145172] LOCATION:  DeferErrorIfQueryNotSupported, multi_logical_planner.c:945
2023-11-25 13:49:14.801 UTC [145172] STATEMENT:  select grouping(id)
	from aggdata group by id order by 1 limit 3;
2023-11-25 13:49:14.801 UTC [145172] ERROR:  0A000: could not run distributed query with GROUPING
2023-11-25 13:49:14.801 UTC [145172] HINT:  Consider using an equality filter on the distributed table's partition column.
2023-11-25 13:49:14.801 UTC [145172] LOCATION:  DeferErrorIfQueryNotSupported, multi_logical_planner.c:945
2023-11-25 13:49:14.801 UTC [145172] STATEMENT:  select key, grouping(val)
	from aggdata group by key, val order by 1, 2;
2023-11-25 13:49:14.802 UTC [145172] ERROR:  0A000: could not run distributed query with GROUPING
2023-11-25 13:49:14.802 UTC [145172] HINT:  Consider using an equality filter on the distributed table's partition column.
2023-11-25 13:49:14.802 UTC [145172] LOCATION:  DeferErrorIfQueryNotSupported, multi_logical_planner.c:945
2023-11-25 13:49:14.802 UTC [145172] STATEMENT:  select key, grouping(val), sum(distinct valf)
	from aggdata group by key, val order by 1, 2;
2023-11-25 13:49:14.802 UTC [145172] ERROR:  XX000: worker_partial_agg_sfunc could not confirm type correctness
2023-11-25 13:49:14.802 UTC [145172] LOCATION:  worker_partial_agg_sfunc, aggregate_utils.c:517
2023-11-25 13:49:14.802 UTC [145172] STATEMENT:  select pg_catalog.worker_partial_agg('string_agg(text,text)'::regprocedure, id) from nulltable;
2023-11-25 13:49:14.802 UTC [145172] ERROR:  XX000: worker_partial_agg_sfunc could not confirm type correctness
2023-11-25 13:49:14.802 UTC [145172] LOCATION:  worker_partial_agg_sfunc, aggregate_utils.c:517
2023-11-25 13:49:14.802 UTC [145172] STATEMENT:  select pg_catalog.worker_partial_agg('sum(int8)'::regprocedure, id) from nulltable;
2023-11-25 13:49:14.803 UTC [145172] ERROR:  XX000: coord_combine_agg_ffunc could not confirm type correctness
2023-11-25 13:49:14.803 UTC [145172] LOCATION:  coord_combine_agg_ffunc, aggregate_utils.c:817
2023-11-25 13:49:14.803 UTC [145172] STATEMENT:  select pg_catalog.coord_combine_agg('sum(float8)'::regprocedure, id::text::cstring, null::text) from nulltable;
2023-11-25 13:49:14.803 UTC [145172] ERROR:  XX000: coord_combine_agg_ffunc could not confirm type correctness
2023-11-25 13:49:14.803 UTC [145172] LOCATION:  coord_combine_agg_ffunc, aggregate_utils.c:817
2023-11-25 13:49:14.803 UTC [145172] STATEMENT:  select pg_catalog.coord_combine_agg('avg(float8)'::regprocedure, ARRAY[id,id,id]::text::cstring, null::text) from nulltable;
2023-11-25 13:49:14.837 UTC [145172] ERROR:  42883: function aggregate_support.square_func(integer) does not exist
2023-11-25 13:49:14.837 UTC [145172] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:49:14.837 UTC [145172] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:49:14.837 UTC [145172] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:14.837 UTC [145172] STATEMENT:  SELECT square_func(5), a FROM t1 GROUP BY a;
2023-11-25 13:49:14.844 UTC [145172] ERROR:  42883: function aggregate_support.square_func(integer) does not exist
2023-11-25 13:49:14.844 UTC [145172] HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
2023-11-25 13:49:14.844 UTC [145172] CONTEXT:  while executing command on localhost:57638
2023-11-25 13:49:14.844 UTC [145172] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:14.844 UTC [145172] STATEMENT:  SELECT square_func(5), a, count(a) FROM t1 GROUP BY a;
2023-11-25 13:49:14.895 UTC [145172] WARNING:  0A000: "function dummy_fnc(dummy_tbl,double precision)" has dependency to "table dummy_tbl" that is not in Citus' metadata
2023-11-25 13:49:14.895 UTC [145172] DETAIL:  "function dummy_fnc(dummy_tbl,double precision)" will be created only locally
2023-11-25 13:49:14.895 UTC [145172] HINT:  Distribute "table dummy_tbl" first to distribute "function dummy_fnc(dummy_tbl,double precision)"
2023-11-25 13:49:14.895 UTC [145172] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:950
2023-11-25 13:49:14.896 UTC [145172] WARNING:  0A000: "function dependent_agg(double precision)" has dependency to "table dummy_tbl" that is not in Citus' metadata
2023-11-25 13:49:14.896 UTC [145172] DETAIL:  "function dependent_agg(double precision)" will be created only locally
2023-11-25 13:49:14.896 UTC [145172] HINT:  Distribute "table dummy_tbl" first to distribute "function dependent_agg(double precision)"
2023-11-25 13:49:14.896 UTC [145172] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:950
2023-11-25 13:49:15.674 UTC [145549] ERROR:  0A000: array_agg (distinct) is unsupported
2023-11-25 13:49:15.674 UTC [145549] LOCATION:  DeferErrorIfUnsupportedArrayAggregate, multi_logical_optimizer.c:4016
2023-11-25 13:49:15.674 UTC [145549] STATEMENT:  SELECT array_agg(distinct l_orderkey) FROM lineitem;
2023-11-25 13:49:15.675 UTC [145549] ERROR:  0A000: array_agg with order by is unsupported
2023-11-25 13:49:15.675 UTC [145549] LOCATION:  DeferErrorIfUnsupportedArrayAggregate, multi_logical_optimizer.c:4008
2023-11-25 13:49:15.675 UTC [145549] STATEMENT:  SELECT array_agg(l_orderkey ORDER BY l_partkey) FROM lineitem;
2023-11-25 13:49:15.675 UTC [145549] ERROR:  0A000: array_agg with order by is unsupported
2023-11-25 13:49:15.675 UTC [145549] LOCATION:  DeferErrorIfUnsupportedArrayAggregate, multi_logical_optimizer.c:4008
2023-11-25 13:49:15.675 UTC [145549] STATEMENT:  SELECT array_agg(distinct l_orderkey ORDER BY l_orderkey) FROM lineitem;
2023-11-25 13:49:15.922 UTC [145547] ERROR:  0A000: subquery in LIMIT is not supported in multi-shard queries
2023-11-25 13:49:15.922 UTC [145547] LOCATION:  DeferErrorIfQueryNotSupported, multi_logical_planner.c:945
2023-11-25 13:49:15.922 UTC [145547] STATEMENT:  SELECT l_orderkey FROM lineitem_hash ORDER BY l_orderkey LIMIT (SELECT 10);
2023-11-25 13:49:15.922 UTC [145547] ERROR:  0A000: subquery in OFFSET is not supported in multi-shard queries
2023-11-25 13:49:15.922 UTC [145547] LOCATION:  DeferErrorIfQueryNotSupported, multi_logical_planner.c:945
2023-11-25 13:49:15.922 UTC [145547] STATEMENT:  SELECT l_orderkey FROM lineitem_hash ORDER BY l_orderkey LIMIT 10 OFFSET (SELECT 10);
2023-11-25 13:49:16.129 UTC [145708] ERROR:  0A000: jsonb_agg (distinct) is unsupported
2023-11-25 13:49:16.129 UTC [145708] LOCATION:  DeferErrorIfUnsupportedJsonAggregate, multi_logical_optimizer.c:4051
2023-11-25 13:49:16.129 UTC [145708] STATEMENT:  SELECT jsonb_agg(distinct l_orderkey) FROM lineitem;
2023-11-25 13:49:16.130 UTC [145708] ERROR:  0A000: jsonb_agg with order by is unsupported
2023-11-25 13:49:16.130 UTC [145708] LOCATION:  DeferErrorIfUnsupportedJsonAggregate, multi_logical_optimizer.c:4051
2023-11-25 13:49:16.130 UTC [145708] STATEMENT:  SELECT jsonb_agg(l_orderkey ORDER BY l_partkey) FROM lineitem;
2023-11-25 13:49:16.130 UTC [145708] ERROR:  0A000: jsonb_agg with order by is unsupported
2023-11-25 13:49:16.130 UTC [145708] LOCATION:  DeferErrorIfUnsupportedJsonAggregate, multi_logical_optimizer.c:4051
2023-11-25 13:49:16.130 UTC [145708] STATEMENT:  SELECT jsonb_agg(distinct l_orderkey ORDER BY l_orderkey) FROM lineitem;
2023-11-25 13:49:16.135 UTC [145712] ERROR:  0A000: json_agg (distinct) is unsupported
2023-11-25 13:49:16.135 UTC [145712] LOCATION:  DeferErrorIfUnsupportedJsonAggregate, multi_logical_optimizer.c:4051
2023-11-25 13:49:16.135 UTC [145712] STATEMENT:  SELECT json_agg(distinct l_orderkey) FROM lineitem;
2023-11-25 13:49:16.137 UTC [145712] ERROR:  0A000: json_agg with order by is unsupported
2023-11-25 13:49:16.137 UTC [145712] LOCATION:  DeferErrorIfUnsupportedJsonAggregate, multi_logical_optimizer.c:4051
2023-11-25 13:49:16.137 UTC [145712] STATEMENT:  SELECT json_agg(l_orderkey ORDER BY l_partkey) FROM lineitem;
2023-11-25 13:49:16.137 UTC [145712] ERROR:  0A000: json_agg with order by is unsupported
2023-11-25 13:49:16.137 UTC [145712] LOCATION:  DeferErrorIfUnsupportedJsonAggregate, multi_logical_optimizer.c:4051
2023-11-25 13:49:16.137 UTC [145712] STATEMENT:  SELECT json_agg(distinct l_orderkey ORDER BY l_orderkey) FROM lineitem;
2023-11-25 13:49:16.140 UTC [145713] ERROR:  0A000: json_object_agg (distinct) is unsupported
2023-11-25 13:49:16.140 UTC [145713] LOCATION:  DeferErrorIfUnsupportedJsonAggregate, multi_logical_optimizer.c:4051
2023-11-25 13:49:16.140 UTC [145713] STATEMENT:  SELECT json_object_agg(distinct l_shipmode, l_orderkey) FROM lineitem;
2023-11-25 13:49:16.140 UTC [145713] ERROR:  0A000: json_object_agg with order by is unsupported
2023-11-25 13:49:16.140 UTC [145713] LOCATION:  DeferErrorIfUnsupportedJsonAggregate, multi_logical_optimizer.c:4051
2023-11-25 13:49:16.140 UTC [145713] STATEMENT:  SELECT json_object_agg(l_shipmode, l_orderkey ORDER BY l_shipmode) FROM lineitem;
2023-11-25 13:49:16.140 UTC [145713] ERROR:  0A000: json_object_agg with order by is unsupported
2023-11-25 13:49:16.140 UTC [145713] LOCATION:  DeferErrorIfUnsupportedJsonAggregate, multi_logical_optimizer.c:4051
2023-11-25 13:49:16.140 UTC [145713] STATEMENT:  SELECT json_object_agg(distinct l_orderkey, l_shipmode ORDER BY l_orderkey) FROM lineitem;
2023-11-25 13:49:16.177 UTC [145710] ERROR:  0A000: jsonb_object_agg (distinct) is unsupported
2023-11-25 13:49:16.177 UTC [145710] LOCATION:  DeferErrorIfUnsupportedJsonAggregate, multi_logical_optimizer.c:4051
2023-11-25 13:49:16.177 UTC [145710] STATEMENT:  SELECT jsonb_object_agg(distinct l_shipmode, l_orderkey) FROM lineitem;
2023-11-25 13:49:16.178 UTC [145710] ERROR:  0A000: jsonb_object_agg with order by is unsupported
2023-11-25 13:49:16.178 UTC [145710] LOCATION:  DeferErrorIfUnsupportedJsonAggregate, multi_logical_optimizer.c:4051
2023-11-25 13:49:16.178 UTC [145710] STATEMENT:  SELECT jsonb_object_agg(l_shipmode, l_orderkey ORDER BY l_shipmode) FROM lineitem;
2023-11-25 13:49:16.178 UTC [145710] ERROR:  0A000: jsonb_object_agg with order by is unsupported
2023-11-25 13:49:16.178 UTC [145710] LOCATION:  DeferErrorIfUnsupportedJsonAggregate, multi_logical_optimizer.c:4051
2023-11-25 13:49:16.178 UTC [145710] STATEMENT:  SELECT jsonb_object_agg(distinct l_orderkey, l_shipmode ORDER BY l_orderkey) FROM lineitem;
2023-11-25 13:49:16.326 UTC [145715] ERROR:  0A000: Subqueries in HAVING cannot refer to outer query
2023-11-25 13:49:16.326 UTC [145715] LOCATION:  RecursivelyPlanSubqueriesAndCTEs, recursive_planning.c:323
2023-11-25 13:49:16.326 UTC [145715] STATEMENT:  select     s_i_id, sum(s_order_cnt) as ordercount
	from     stock s
	where   s_order_cnt > (select sum(s_order_cnt) * .005 as where_query from stock)
	group by s_i_id
	having   (select max(s_order_cnt) > 2 as having_query from stock where s_i_id = s.s_i_id)
	order by s_i_id;
2023-11-25 13:49:16.326 UTC [145715] ERROR:  0A000: Subqueries in HAVING cannot refer to outer query
2023-11-25 13:49:16.326 UTC [145715] LOCATION:  RecursivelyPlanSubqueriesAndCTEs, recursive_planning.c:323
2023-11-25 13:49:16.326 UTC [145715] STATEMENT:  select     s_i_id, sum(s_order_cnt) as ordercount
	from     stock s
	group by s_i_id
	having   (select max(s_order_cnt) > 2 as having_query from stock where s_i_id = s.s_i_id)
	order by s_i_id;
2023-11-25 13:49:16.463 UTC [145717] ERROR:  0A000: complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
2023-11-25 13:49:16.463 UTC [145717] LOCATION:  JoinOrderList, multi_join_order.c:310
2023-11-25 13:49:16.463 UTC [145717] STATEMENT:  SELECT *
	FROM
	    test t1 JOIN test t2 USING (y), -- causes repartition, which makes this not routable or pushdownable
	    test a,
	    test b
	WHERE t2.y - a.x - b.x = 0
	ORDER BY 1,2,3;
2023-11-25 13:49:16.646 UTC [145716] LOG:  00000: join order: [ "order_line" ]
2023-11-25 13:49:16.646 UTC [145716] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:16.646 UTC [145716] STATEMENT:  SELECT
	    ol_number,
	    sum(ol_quantity) as sum_qty,
	    sum(ol_amount) as sum_amount,
	    avg(ol_quantity) as avg_qty,
	    avg(ol_amount) as avg_amount,
	    count(*) as count_order
	FROM order_line
	WHERE ol_delivery_d > '2007-01-02 00:00:00.000000'
	GROUP BY ol_number
	ORDER BY ol_number;
2023-11-25 13:49:16.651 UTC [145716] LOG:  00000: join order: [ "stock" ][ reference join "supplier" ][ reference join "nation" ][ reference join "region" ]
2023-11-25 13:49:16.651 UTC [145716] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:16.651 UTC [145716] STATEMENT:  SELECT
	    su_suppkey,
	    su_name,
	    n_name,
	    i_id,
	    i_name,
	    su_address,
	    su_phone,
	    su_comment
	FROM
	    item,
	    supplier,
	    stock,
	    nation,
	    region,
	    (SELECT
	         s_i_id AS m_i_id,
	         min(s_quantity) as m_s_quantity
	     FROM
	         stock,
	         supplier,
	         nation,
	         region
	     WHERE mod((s_w_id*s_i_id),10000)=su_suppkey
	       AND su_nationkey=n_nationkey
	       AND n_regionkey=r_regionkey
	       AND r_name LIKE 'Europ%'
	     GROUP BY s_i_id) m
	WHERE i_id = s_i_id
	  AND mod((s_w_id * s_i_id), 10000) = su_suppkey
	  AND su_nationkey = n_nationkey
	  AND n_regionkey = r_regionkey
	  AND i_data LIKE '%b'
	  AND r_name LIKE 'Europ%'
	  AND i_id = m_i_id
	  AND s_quantity = m_s_quantity
	ORDER BY
	    n_name,
	    su_name,
	    i_id;
2023-11-25 13:49:16.658 UTC [145716] LOG:  00000: join order: [ "customer" ][ local partition join "oorder" ][ local partition join "new_order" ][ local partition join "order_line" ]
2023-11-25 13:49:16.658 UTC [145716] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:16.658 UTC [145716] STATEMENT:  SELECT
	    ol_o_id,
	    ol_w_id,
	    ol_d_id,
	    sum(ol_amount) AS revenue,
	    o_entry_d
	FROM
	    customer,
	    new_order,
	    oorder,
	    order_line
	WHERE c_state LIKE 'C%' -- used to ba A%, but C% works with our small data
	  AND c_id = o_c_id
	  AND c_w_id = o_w_id
	  AND c_d_id = o_d_id
	  AND no_w_id = o_w_id
	  AND no_d_id = o_d_id
	  AND no_o_id = o_id
	  AND ol_w_id = o_w_id
	  AND ol_d_id = o_d_id
	  AND ol_o_id = o_id
	  AND o_entry_d > '2007-01-02 00:00:00.000000'
	GROUP BY
	    ol_o_id,
	    ol_w_id,
	    ol_d_id,
	    o_entry_d
	ORDER BY
	    revenue DESC,
	    o_entry_d;
2023-11-25 13:49:16.667 UTC [145716] LOG:  00000: join order: [ "customer" ][ local partition join "oorder" ][ local partition join "order_line" ][ local partition join "stock" ][ reference join "supplier" ][ reference join "nation" ][ reference join "region" ]
2023-11-25 13:49:16.667 UTC [145716] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:16.667 UTC [145716] STATEMENT:  SELECT
	    n_name,
	    sum(ol_amount) AS revenue
	FROM
	    customer,
	    oorder,
	    order_line,
	    stock,
	    supplier,
	    nation,
	    region
	WHERE c_id = o_c_id
	  AND c_w_id = o_w_id
	  AND c_d_id = o_d_id
	  AND ol_o_id = o_id
	  AND ol_w_id = o_w_id
	  AND ol_d_id=o_d_id
	  AND ol_w_id = s_w_id
	  AND ol_i_id = s_i_id
	  AND mod((s_w_id * s_i_id),10000) = su_suppkey
	-- our dataset does not have the supplier in the same nation as the customer causing this
	-- join to filter out all the data. We verify later on that we can actually perform an
	-- ascii(substr(c_state,1,1)) == reference table column join later on so it should not
	-- matter we skip this filter here.
	--AND ascii(substr(c_state,1,1)) = su_nationkey
	  AND su_nationkey = n_nationkey
	  AND n_regionkey = r_regionkey
	  AND r_name = 'Europe'
	  AND o_entry_d >= '2007-01-02 00:00:00.000000'
	GROUP BY n_name
	ORDER BY revenue DESC;
2023-11-25 13:49:16.676 UTC [145716] LOG:  00000: join order: [ "order_line" ]
2023-11-25 13:49:16.676 UTC [145716] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:16.676 UTC [145716] STATEMENT:  SELECT
	    sum(ol_amount) AS revenue
	FROM order_line
	WHERE ol_delivery_d >= '1999-01-01 00:00:00.000000'
	  AND ol_delivery_d < '2020-01-01 00:00:00.000000'
	  AND ol_quantity BETWEEN 1 AND 100000;
2023-11-25 13:49:16.682 UTC [145716] LOG:  00000: join order: [ "order_line" ][ local partition join "oorder" ][ local partition join "customer" ][ reference join "nation" ][ reference join "nation" ][ reference join "supplier" ][ dual partition join "stock" ]
2023-11-25 13:49:16.682 UTC [145716] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:16.682 UTC [145716] STATEMENT:  SELECT
	    su_nationkey as supp_nation,
	    substr(c_state,1,1) as cust_nation,
	    extract(year from o_entry_d) as l_year,
	    sum(ol_amount) as revenue
	FROM
	    supplier,
	    stock,
	    order_line,
	    oorder,
	    customer,
	    nation n1,
	    nation n2
	WHERE ol_supply_w_id = s_w_id
	  AND ol_i_id = s_i_id
	  AND mod((s_w_id * s_i_id), 10000) = su_suppkey
	  AND ol_w_id = o_w_id
	  AND ol_d_id = o_d_id
	  AND ol_o_id = o_id
	  AND c_id = o_c_id
	  AND c_w_id = o_w_id
	  AND c_d_id = o_d_id
	  AND su_nationkey = n1.n_nationkey
	  AND ascii(substr(c_state,1,1)) = n2.n_nationkey
	  AND (
	         (n1.n_name = 'Germany' AND n2.n_name = 'Cambodia')
	      OR (n1.n_name = 'Cambodia' AND n2.n_name = 'Germany')
	      )
	  AND ol_delivery_d BETWEEN '2007-01-02 00:00:00.000000' AND '2012-01-02 00:00:00.000000'
	GROUP BY
	    su_nationkey,
	    substr(c_state,1,1),
	    extract(year from o_entry_d)
	ORDER BY
	    su_nationkey,
	    cust_nation,
	    l_year;
2023-11-25 13:49:16.767 UTC [145716] LOG:  00000: join order: [ "order_line" ][ reference join "item" ][ local partition join "oorder" ][ local partition join "customer" ][ reference join "nation" ][ reference join "region" ][ dual partition join "stock" ][ reference join "supplier" ][ reference join "nation" ]
2023-11-25 13:49:16.767 UTC [145716] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:16.767 UTC [145716] STATEMENT:  SELECT
	    extract(year from o_entry_d) as l_year,
	    sum(case when n2.n_name = 'Germany' then ol_amount else 0 end) / sum(ol_amount) as mkt_share
	FROM
	    item,
	    supplier,
	    stock,
	    order_line,
	    oorder,
	    customer,
	    nation n1,
	    nation n2,
	    region
	WHERE i_id = s_i_id
	  AND ol_i_id = s_i_id
	  AND ol_supply_w_id = s_w_id
	  AND mod((s_w_id * s_i_id),10000) = su_suppkey
	  AND ol_w_id = o_w_id
	  AND ol_d_id = o_d_id
	  AND ol_o_id = o_id
	  AND c_id = o_c_id
	  AND c_w_id = o_w_id
	  AND c_d_id = o_d_id
	  AND n1.n_nationkey = ascii(substr(c_state,1,1))
	  AND n1.n_regionkey = r_regionkey
	  AND ol_i_id < 1000
	  AND r_name = 'Europe'
	  AND su_nationkey = n2.n_nationkey
	  AND o_entry_d BETWEEN '2007-01-02 00:00:00.000000' AND '2012-01-02 00:00:00.000000'
	  AND i_data LIKE '%b'
	  AND i_id = ol_i_id
	GROUP BY extract(YEAR FROM o_entry_d)
	ORDER BY l_year;
2023-11-25 13:49:16.851 UTC [145716] LOG:  00000: join order: [ "order_line" ][ reference join "item" ][ local partition join "oorder" ][ dual partition join "stock" ][ reference join "supplier" ][ reference join "nation" ]
2023-11-25 13:49:16.851 UTC [145716] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:16.851 UTC [145716] STATEMENT:  SELECT
	    n_name,
	    extract(year from o_entry_d) as l_year,
	    sum(ol_amount) as sum_profit
	FROM
	    item,
	    stock,
	    supplier,
	    order_line,
	    oorder,
	    nation
	WHERE ol_i_id = s_i_id
	  AND ol_supply_w_id = s_w_id
	  AND mod((s_w_id * s_i_id), 10000) = su_suppkey
	  AND ol_w_id = o_w_id
	  AND ol_d_id = o_d_id
	  AND ol_o_id = o_id
	  AND ol_i_id = i_id
	  AND su_nationkey = n_nationkey
	  AND i_data LIKE '%b' -- this used to be %BB but that will not work with our small dataset
	GROUP BY
	    n_name,
	    extract(YEAR FROM o_entry_d)
	ORDER BY
	    n_name,
	    l_year DESC;
2023-11-25 13:49:16.936 UTC [145716] LOG:  00000: join order: [ "customer" ][ reference join "nation" ][ local partition join "oorder" ][ local partition join "order_line" ]
2023-11-25 13:49:16.936 UTC [145716] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:16.936 UTC [145716] STATEMENT:  SELECT
	    c_id,
	    c_last,
	    sum(ol_amount) AS revenue,
	    c_city,
	    c_phone,
	    n_name
	FROM
	    customer,
	    oorder,
	    order_line,
	    nation
	WHERE c_id = o_c_id
	  AND c_w_id = o_w_id
	  AND c_d_id = o_d_id
	  AND ol_w_id = o_w_id
	  AND ol_d_id = o_d_id
	  AND ol_o_id = o_id
	  AND o_entry_d >= '2007-01-02 00:00:00.000000'
	  AND o_entry_d <= ol_delivery_d
	  AND n_nationkey = ascii(substr(c_state,1,1))
	GROUP BY
	    c_id,
	    c_last,
	    c_city,
	    c_phone,
	    n_name
	ORDER BY revenue DESC;
2023-11-25 13:49:16.941 UTC [145716] LOG:  00000: join order: [ "stock" ][ reference join "supplier" ][ reference join "nation" ]
2023-11-25 13:49:16.941 UTC [145716] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:16.941 UTC [145716] STATEMENT:  SELECT
	    s_i_id,
	    sum(s_order_cnt) AS ordercount
	FROM
	    stock,
	    supplier,
	    nation
	WHERE mod((s_w_id * s_i_id),10000) = su_suppkey
	  AND su_nationkey = n_nationkey
	  AND n_name = 'Germany'
	GROUP BY s_i_id
	HAVING sum(s_order_cnt) >
	         (SELECT sum(s_order_cnt) * .005
	          FROM
	              stock,
	              supplier,
	              nation
	          WHERE mod((s_w_id * s_i_id),10000) = su_suppkey
	            AND su_nationkey = n_nationkey
	            AND n_name = 'Germany')
	ORDER BY ordercount DESC;
2023-11-25 13:49:16.947 UTC [145716] LOG:  00000: join order: [ "oorder" ][ local partition join "order_line" ]
2023-11-25 13:49:16.947 UTC [145716] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:16.947 UTC [145716] STATEMENT:  SELECT
	    o_ol_cnt,
	    sum(case when o_carrier_id = 1 or o_carrier_id = 2 then 1 else 0 end) as high_line_count,
	    sum(case when o_carrier_id <> 1 and o_carrier_id <> 2 then 1 else 0 end) as low_line_count
	FROM
	    oorder,
	    order_line
	WHERE ol_w_id = o_w_id
	  AND ol_d_id = o_d_id
	  AND ol_o_id = o_id
	  AND o_entry_d <= ol_delivery_d
	  AND ol_delivery_d < '2020-01-01 00:00:00.000000'
	GROUP BY o_ol_cnt
	ORDER BY o_ol_cnt;
2023-11-25 13:49:16.955 UTC [145716] LOG:  00000: join order: [ "order_line" ][ reference join "item" ]
2023-11-25 13:49:16.955 UTC [145716] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:16.955 UTC [145716] STATEMENT:  SELECT
	    100.00 * sum(CASE WHEN i_data LIKE 'PR%' THEN ol_amount ELSE 0 END) / (1+sum(ol_amount)) AS promo_revenue
	FROM
	    order_line,
	    item
	WHERE ol_i_id = i_id
	  AND ol_delivery_d >= '2007-01-02 00:00:00.000000'
	  AND ol_delivery_d < '2020-01-02 00:00:00.000000';
2023-11-25 13:49:16.959 UTC [145716] LOG:  00000: join order: [ "order_line" ][ dual partition join "stock" ]
2023-11-25 13:49:16.959 UTC [145716] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:16.959 UTC [145716] STATEMENT:  WITH revenue (supplier_no, total_revenue) AS (
	    SELECT
	        mod((s_w_id * s_i_id),10000) AS supplier_no,
	        sum(ol_amount) AS total_revenue
	    FROM
	        order_line,
	        stock
	    WHERE ol_i_id = s_i_id
	      AND ol_supply_w_id = s_w_id
	      AND ol_delivery_d >= '2007-01-02 00:00:00.000000'
	    GROUP BY mod((s_w_id * s_i_id),10000))
	SELECT
	    su_suppkey,
	    su_name,
	    su_address,
	    su_phone,
	    total_revenue
	FROM
	    supplier,
	    revenue
	WHERE su_suppkey = supplier_no
	  AND total_revenue = (SELECT max(total_revenue) FROM revenue)
	ORDER BY su_suppkey;
2023-11-25 13:49:17.046 UTC [145716] LOG:  00000: join order: [ "order_line" ][ reference join "item" ]
2023-11-25 13:49:17.046 UTC [145716] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:17.046 UTC [145716] STATEMENT:  SELECT
	       sum(ol_amount) / 2.0 AS avg_yearly
	FROM
	    order_line,
	    (SELECT
	         i_id,
	         avg(ol_quantity) AS a
	     FROM
	         item,
	         order_line
	     WHERE i_data LIKE '%b'
	       AND ol_i_id = i_id
	     GROUP BY i_id) t
	WHERE ol_i_id = t.i_id;
2023-11-25 13:49:17.051 UTC [145716] LOG:  00000: join order: [ "customer" ][ local partition join "oorder" ][ local partition join "order_line" ]
2023-11-25 13:49:17.051 UTC [145716] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:17.051 UTC [145716] STATEMENT:  SELECT
	    c_last,
	    c_id o_id,
	    o_entry_d,
	    o_ol_cnt,
	    sum(ol_amount)
	FROM
	    customer,
	    oorder,
	    order_line
	WHERE c_id = o_c_id
	  AND c_w_id = o_w_id
	  AND c_d_id = o_d_id
	  AND ol_w_id = o_w_id
	  AND ol_d_id = o_d_id
	  AND ol_o_id = o_id
	GROUP BY
	    o_id,
	    o_w_id,
	    o_d_id,
	    c_id,
	    c_last,
	    o_entry_d,
	    o_ol_cnt
	HAVING sum(ol_amount) > 5 -- was 200, but thats too big for the dataset
	ORDER BY
	    sum(ol_amount) DESC,
	    o_entry_d;
2023-11-25 13:49:17.055 UTC [145716] LOG:  00000: join order: [ "order_line" ][ reference join "item" ]
2023-11-25 13:49:17.055 UTC [145716] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:17.055 UTC [145716] STATEMENT:  SELECT
	    sum(ol_amount) AS revenue
	FROM
	    order_line,
	     item
	WHERE (     ol_i_id = i_id
	        AND i_data LIKE '%a'
	        AND ol_quantity >= 1
	        AND ol_quantity <= 10
	        AND i_price BETWEEN 1 AND 400000
	        AND ol_w_id IN (1,2,3))
	   OR (     ol_i_id = i_id
	        AND i_data LIKE '%b'
	        AND ol_quantity >= 1
	        AND ol_quantity <= 10
	        AND i_price BETWEEN 1 AND 400000
	        AND ol_w_id IN (1,2,4))
	   OR (     ol_i_id = i_id
	        AND i_data LIKE '%c'
	        AND ol_quantity >= 1
	        AND ol_quantity <= 10
	        AND i_price BETWEEN 1 AND 400000
	        AND ol_w_id IN (1,5,3));
2023-11-25 13:49:17.059 UTC [145716] LOG:  00000: join order: [ "stock" ][ reference join "item" ][ dual partition join "order_line" ]
2023-11-25 13:49:17.059 UTC [145716] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:17.059 UTC [145716] STATEMENT:  SELECT
	    su_name,
	    su_address
	FROM
	    supplier,
	    nation
	WHERE su_suppkey in
	      (SELECT
	           mod(s_i_id * s_w_id, 10000)
	       FROM
	           stock,
	           order_line
	       WHERE s_i_id IN
	             (SELECT i_id
	              FROM item
	              WHERE i_data LIKE 'co%')
	       AND ol_i_id = s_i_id
	       AND ol_delivery_d > '2008-05-23 12:00:00' -- was 2010, but our order is in 2008
	       GROUP BY s_i_id, s_w_id, s_quantity
	       HAVING   2*s_quantity > sum(ol_quantity))
	  AND su_nationkey = n_nationkey
	  AND n_name = 'Germany'
	ORDER BY su_name;
2023-11-25 13:49:17.147 UTC [145716] LOG:  00000: join order: [ "customer" ]
2023-11-25 13:49:17.147 UTC [145716] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:17.147 UTC [145716] STATEMENT:  SELECT
	    substr(c_state,1,1) AS country,
	    count(*) AS numcust,
	    sum(c_balance) AS totacctbal
	FROM customer
	WHERE substr(c_phone,1,1) in ('1','2','3','4','5','6','7')
	  AND c_balance > (SELECT avg(c_BALANCE)
	                   FROM customer
	                   WHERE  c_balance > 0.00
	                     AND substr(c_phone,1,1) in ('1','2','3','4','5','6','7'))
	  AND NOT exists (SELECT *
	                  FROM oorder
	                  WHERE o_c_id = c_id
	                    AND o_w_id = c_w_id
	                    AND o_d_id = c_d_id)
	GROUP BY substr(c_state,1,1)
	ORDER BY substr(c_state,1,1);
2023-11-25 13:49:17.156 UTC [145716] LOG:  00000: join order: [ "order_line" ][ local partition join "oorder" ][ local partition join "customer" ][ reference join "nation" ][ reference join "nation" ][ reference join "supplier" ][ single hash partition join "stock" ]
2023-11-25 13:49:17.156 UTC [145716] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:17.156 UTC [145716] STATEMENT:  SELECT
	    su_nationkey as supp_nation,
	    substr(c_state,1,1) as cust_nation,
	    extract(year from o_entry_d) as l_year,
	    sum(ol_amount) as revenue
	FROM
	    supplier,
	    stock,
	    order_line,
	    oorder,
	    customer,
	    nation n1,
	    nation n2
	WHERE ol_supply_w_id = s_w_id
	  AND ol_i_id = s_i_id
	  AND mod((s_w_id * s_i_id), 10000) = su_suppkey
	  AND ol_w_id = o_w_id
	  AND ol_d_id = o_d_id
	  AND ol_o_id = o_id
	  AND c_id = o_c_id
	  AND c_w_id = o_w_id
	  AND c_d_id = o_d_id
	  AND su_nationkey = n1.n_nationkey
	  AND ascii(substr(c_state,1,1)) = n2.n_nationkey
	  AND (
	        (n1.n_name = 'Germany' AND n2.n_name = 'Cambodia')
	        OR (n1.n_name = 'Cambodia' AND n2.n_name = 'Germany')
	    )
	  AND ol_delivery_d BETWEEN '2007-01-02 00:00:00.000000' AND '2012-01-02 00:00:00.000000'
	GROUP BY
	    su_nationkey,
	    substr(c_state,1,1),
	    extract(year from o_entry_d)
	ORDER BY
	    su_nationkey,
	    cust_nation,
	    l_year;
2023-11-25 13:49:17.214 UTC [145716] LOG:  00000: join order: [ "order_line" ][ reference join "item" ][ local partition join "oorder" ][ local partition join "customer" ][ reference join "nation" ][ reference join "region" ][ single hash partition join "stock" ][ reference join "supplier" ][ reference join "nation" ]
2023-11-25 13:49:17.214 UTC [145716] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:17.214 UTC [145716] STATEMENT:  SELECT
	    extract(year from o_entry_d) as l_year,
	    sum(case when n2.n_name = 'Germany' then ol_amount else 0 end) / sum(ol_amount) as mkt_share
	FROM
	    item,
	    supplier,
	    stock,
	    order_line,
	    oorder,
	    customer,
	    nation n1,
	    nation n2,
	    region
	WHERE i_id = s_i_id
	  AND ol_i_id = s_i_id
	  AND ol_supply_w_id = s_w_id
	  AND mod((s_w_id * s_i_id),10000) = su_suppkey
	  AND ol_w_id = o_w_id
	  AND ol_d_id = o_d_id
	  AND ol_o_id = o_id
	  AND c_id = o_c_id
	  AND c_w_id = o_w_id
	  AND c_d_id = o_d_id
	  AND n1.n_nationkey = ascii(substr(c_state,1,1))
	  AND n1.n_regionkey = r_regionkey
	  AND ol_i_id < 1000
	  AND r_name = 'Europe'
	  AND su_nationkey = n2.n_nationkey
	  AND o_entry_d BETWEEN '2007-01-02 00:00:00.000000' AND '2012-01-02 00:00:00.000000'
	  AND i_data LIKE '%b'
	  AND i_id = ol_i_id
	GROUP BY extract(YEAR FROM o_entry_d)
	ORDER BY l_year;
2023-11-25 13:49:17.271 UTC [145716] LOG:  00000: join order: [ "order_line" ][ reference join "item" ][ local partition join "oorder" ][ single hash partition join "stock" ][ reference join "supplier" ][ reference join "nation" ]
2023-11-25 13:49:17.271 UTC [145716] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:17.271 UTC [145716] STATEMENT:  SELECT
	    n_name,
	    extract(year from o_entry_d) as l_year,
	    sum(ol_amount) as sum_profit
	FROM
	    item,
	    stock,
	    supplier,
	    order_line,
	    oorder,
	    nation
	WHERE ol_i_id = s_i_id
	  AND ol_supply_w_id = s_w_id
	  AND mod((s_w_id * s_i_id), 10000) = su_suppkey
	  AND ol_w_id = o_w_id
	  AND ol_d_id = o_d_id
	  AND ol_o_id = o_id
	  AND ol_i_id = i_id
	  AND su_nationkey = n_nationkey
	  AND i_data LIKE '%b' -- this used to be %BB but that will not work with our small dataset
	GROUP BY
	    n_name,
	    extract(YEAR FROM o_entry_d)
	ORDER BY
	    n_name,
	    l_year DESC;
2023-11-25 13:49:17.968 UTC [146600] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:49:17.968 UTC [146600] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:49:17.968 UTC [146600] STATEMENT:  select  s_i_id
	    from stock, order_line
	    where
	        s_i_id in (select i_im_id from item)
	        AND s_i_id = ol_i_id;
2023-11-25 13:49:17.968 UTC [146600] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:49:17.968 UTC [146600] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:49:17.968 UTC [146600] STATEMENT:  select  s_i_id
	    from stock, order_line
	    where
	        s_i_id not in (select i_id from item)
	        AND s_i_id = ol_i_id;
2023-11-25 13:49:17.969 UTC [146600] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:49:17.969 UTC [146600] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:49:17.969 UTC [146600] STATEMENT:  select  s_i_id
	    from stock, order_line
	    where
	        s_i_id not in (select i_im_id from item)
	        AND s_i_id = ol_i_id;
2023-11-25 13:49:17.972 UTC [146600] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:49:17.972 UTC [146600] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:49:17.972 UTC [146600] STATEMENT:  select  s_i_id
	    from stock, order_line
	    where
	        s_i_id in (select i_id from item)
	        AND s_i_id not in (select i_id from item)
	        AND s_i_id = ol_i_id;
2023-11-25 13:49:17.973 UTC [146600] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:49:17.973 UTC [146600] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:49:17.973 UTC [146600] STATEMENT:  select  s_i_id
	    from stock, order_line
	    where
	        s_i_id in (select i_id from item)
	        AND s_i_id not in (select i_im_id from item)
	        AND s_i_id = ol_i_id;
2023-11-25 13:49:17.973 UTC [146600] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:49:17.973 UTC [146600] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:49:17.973 UTC [146600] STATEMENT:  select  s_i_id
	    from stock, order_line
	    where
	        s_i_id in (select i_im_id from item)
	        AND s_i_id not in (select i_id from item)
	        AND s_i_id = ol_i_id;
2023-11-25 13:49:17.973 UTC [146600] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:49:17.973 UTC [146600] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:49:17.973 UTC [146600] STATEMENT:  select  s_i_id
	    from stock, order_line
	    where
	        s_i_id in (select i_im_id from item)
	        AND s_i_id not in (select i_im_id from item)
	        AND s_i_id = ol_i_id;
2023-11-25 13:49:18.530 UTC [146931] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:49:18.530 UTC [146931] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:49:18.530 UTC [146931] STATEMENT:  SELECT count(*)
	FROM distributed_table u1
	JOIN local_table u2 USING(value)
	JOIN LATERAL
	  (SELECT value,
	          random()
	   FROM distributed_table
	   WHERE u2.value = 15) AS u3 USING (value)
	WHERE (u2.value > 2
	       AND FALSE);
2023-11-25 13:49:18.795 UTC [147046] LOG:  00000: join order: [ "single_hash_repartition_first" ][ single hash partition join "single_hash_repartition_second" ]
2023-11-25 13:49:18.795 UTC [147046] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:18.795 UTC [147046] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		single_hash_repartition_first t1, single_hash_repartition_second t2
	WHERE
		t1.id = t2.sum;
2023-11-25 13:49:18.796 UTC [147046] ERROR:  XX000: the query contains a join that requires repartitioning
2023-11-25 13:49:18.796 UTC [147046] HINT:  Set citus.enable_repartition_joins to on to enable repartitioning
2023-11-25 13:49:18.796 UTC [147046] LOCATION:  JobExecutorType, multi_server_executor.c:68
2023-11-25 13:49:18.796 UTC [147046] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		single_hash_repartition_first t1, single_hash_repartition_second t2
	WHERE
		t1.id = t2.sum;
2023-11-25 13:49:18.796 UTC [147046] LOG:  00000: join order: [ "single_hash_repartition_second" ][ single hash partition join "single_hash_repartition_first" ]
2023-11-25 13:49:18.796 UTC [147046] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:18.796 UTC [147046] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		single_hash_repartition_second t1, single_hash_repartition_first t2
	WHERE
		t2.sum = t1.id;
2023-11-25 13:49:18.797 UTC [147046] ERROR:  XX000: the query contains a join that requires repartitioning
2023-11-25 13:49:18.797 UTC [147046] HINT:  Set citus.enable_repartition_joins to on to enable repartitioning
2023-11-25 13:49:18.797 UTC [147046] LOCATION:  JobExecutorType, multi_server_executor.c:68
2023-11-25 13:49:18.797 UTC [147046] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		single_hash_repartition_second t1, single_hash_repartition_first t2
	WHERE
		t2.sum = t1.id;
2023-11-25 13:49:18.797 UTC [147046] LOG:  00000: join order: [ "single_hash_repartition_second" ][ reference join "ref_table" ][ single hash partition join "single_hash_repartition_first" ]
2023-11-25 13:49:18.797 UTC [147046] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:18.797 UTC [147046] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		ref_table r1, single_hash_repartition_second t1, single_hash_repartition_first t2
	WHERE
		r1.id = t1.id AND t2.sum = t1.id;
2023-11-25 13:49:18.798 UTC [147046] ERROR:  XX000: the query contains a join that requires repartitioning
2023-11-25 13:49:18.798 UTC [147046] HINT:  Set citus.enable_repartition_joins to on to enable repartitioning
2023-11-25 13:49:18.798 UTC [147046] LOCATION:  JobExecutorType, multi_server_executor.c:68
2023-11-25 13:49:18.798 UTC [147046] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		ref_table r1, single_hash_repartition_second t1, single_hash_repartition_first t2
	WHERE
		r1.id = t1.id AND t2.sum = t1.id;
2023-11-25 13:49:18.798 UTC [147046] LOG:  00000: join order: [ "single_hash_repartition_first" ][ local partition join "single_hash_repartition_first" ][ single hash partition join "single_hash_repartition_second" ]
2023-11-25 13:49:18.798 UTC [147046] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:18.798 UTC [147046] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		single_hash_repartition_first t1, single_hash_repartition_first t2, single_hash_repartition_second t3
	WHERE
		t1.id = t2.id AND t1.sum = t3.id;
2023-11-25 13:49:18.799 UTC [147046] ERROR:  XX000: the query contains a join that requires repartitioning
2023-11-25 13:49:18.799 UTC [147046] HINT:  Set citus.enable_repartition_joins to on to enable repartitioning
2023-11-25 13:49:18.799 UTC [147046] LOCATION:  JobExecutorType, multi_server_executor.c:68
2023-11-25 13:49:18.799 UTC [147046] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		single_hash_repartition_first t1, single_hash_repartition_first t2, single_hash_repartition_second t3
	WHERE
		t1.id = t2.id AND t1.sum = t3.id;
2023-11-25 13:49:18.799 UTC [147046] LOG:  00000: join order: [ "single_hash_repartition_first" ][ single hash partition join "single_hash_repartition_second" ][ dual partition join "single_hash_repartition_first" ]
2023-11-25 13:49:18.799 UTC [147046] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:18.799 UTC [147046] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		single_hash_repartition_first t1, single_hash_repartition_first t2, single_hash_repartition_second t3
	WHERE
		t1.sum = t2.sum AND t1.sum = t3.id;
2023-11-25 13:49:18.801 UTC [147046] ERROR:  XX000: the query contains a join that requires repartitioning
2023-11-25 13:49:18.801 UTC [147046] HINT:  Set citus.enable_repartition_joins to on to enable repartitioning
2023-11-25 13:49:18.801 UTC [147046] LOCATION:  JobExecutorType, multi_server_executor.c:68
2023-11-25 13:49:18.801 UTC [147046] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		single_hash_repartition_first t1, single_hash_repartition_first t2, single_hash_repartition_second t3
	WHERE
		t1.sum = t2.sum AND t1.sum = t3.id;
2023-11-25 13:49:18.801 UTC [147046] LOG:  00000: join order: [ "single_hash_repartition_second" ][ cartesian product "single_hash_repartition_first" ][ dual partition join "single_hash_repartition_first" ]
2023-11-25 13:49:18.801 UTC [147046] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:18.801 UTC [147046] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		single_hash_repartition_first t1, single_hash_repartition_first t2, single_hash_repartition_second t3
	WHERE
		t1.id = t2.id AND t1.avg = t3.id;
2023-11-25 13:49:18.802 UTC [147046] ERROR:  0A000: cannot perform distributed planning on this query
2023-11-25 13:49:18.802 UTC [147046] DETAIL:  Cartesian products are currently unsupported
2023-11-25 13:49:18.802 UTC [147046] LOCATION:  JoinSequenceArray, multi_physical_planner.c:3589
2023-11-25 13:49:18.802 UTC [147046] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		single_hash_repartition_first t1, single_hash_repartition_first t2, single_hash_repartition_second t3
	WHERE
		t1.id = t2.id AND t1.avg = t3.id;
2023-11-25 13:49:18.802 UTC [147046] LOG:  00000: join order: [ "single_hash_repartition_first" ][ single hash partition join "single_hash_repartition_second" ]
2023-11-25 13:49:18.802 UTC [147046] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:18.802 UTC [147046] STATEMENT:  EXPLAIN WITH cte1 AS
	(
		SELECT
			t1.id * t2.avg as data
		FROM
			single_hash_repartition_first t1, single_hash_repartition_second t2
		WHERE
			t1.id = t2.sum
		AND t1.sum > 5000
		ORDER BY 1 DESC
		LIMIT 50
	)
	SELECT
		count(*)
	FROM
		cte1, single_hash_repartition_first
	WHERE
		cte1.data > single_hash_repartition_first.id;
2023-11-25 13:49:18.803 UTC [147046] LOG:  00000: join order: [ "single_hash_repartition_first" ][ single hash partition join "single_hash_repartition_second" ]
2023-11-25 13:49:18.803 UTC [147046] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:18.803 UTC [147046] STATEMENT:  EXPLAIN WITH cte1 AS
	(
		SELECT
			t1.id * t2.avg as data
		FROM
			single_hash_repartition_first t1, single_hash_repartition_second t2
		WHERE
			t1.id = t2.sum
		AND t1.sum > 5000
		ORDER BY 1 DESC
		LIMIT 50
	)
	SELECT
		count(*)
	FROM
		cte1, single_hash_repartition_first
	WHERE
		cte1.data > single_hash_repartition_first.id;
2023-11-25 13:49:18.803 UTC [147046] ERROR:  XX000: the query contains a join that requires repartitioning
2023-11-25 13:49:18.803 UTC [147046] HINT:  Set citus.enable_repartition_joins to on to enable repartitioning
2023-11-25 13:49:18.803 UTC [147046] LOCATION:  JobExecutorType, multi_server_executor.c:68
2023-11-25 13:49:18.803 UTC [147046] STATEMENT:  EXPLAIN WITH cte1 AS
	(
		SELECT
			t1.id * t2.avg as data
		FROM
			single_hash_repartition_first t1, single_hash_repartition_second t2
		WHERE
			t1.id = t2.sum
		AND t1.sum > 5000
		ORDER BY 1 DESC
		LIMIT 50
	)
	SELECT
		count(*)
	FROM
		cte1, single_hash_repartition_first
	WHERE
		cte1.data > single_hash_repartition_first.id;
2023-11-25 13:49:18.803 UTC [147046] LOG:  00000: join order: [ "single_hash_repartition_first" ][ single hash partition join "single_hash_repartition_second" ][ single hash partition join "single_hash_repartition_second" ]
2023-11-25 13:49:18.803 UTC [147046] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:18.803 UTC [147046] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		single_hash_repartition_first t1, single_hash_repartition_second t2, single_hash_repartition_second t3
	WHERE
		t1.id = t2.sum AND t2.sum = t3.id;
2023-11-25 13:49:18.804 UTC [147046] ERROR:  XX000: the query contains a join that requires repartitioning
2023-11-25 13:49:18.804 UTC [147046] HINT:  Set citus.enable_repartition_joins to on to enable repartitioning
2023-11-25 13:49:18.804 UTC [147046] LOCATION:  JobExecutorType, multi_server_executor.c:68
2023-11-25 13:49:18.804 UTC [147046] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		single_hash_repartition_first t1, single_hash_repartition_second t2, single_hash_repartition_second t3
	WHERE
		t1.id = t2.sum AND t2.sum = t3.id;
2023-11-25 13:49:18.804 UTC [147046] LOG:  00000: join order: [ "single_hash_repartition_second" ][ single hash partition join "single_hash_repartition_second" ][ single hash partition join "single_hash_repartition_first" ]
2023-11-25 13:49:18.804 UTC [147046] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:18.804 UTC [147046] STATEMENT:  EXPLAIN SELECT
		avg(t1.avg + t2.avg)
	FROM
		single_hash_repartition_first t1, single_hash_repartition_second t2, single_hash_repartition_second t3
	WHERE
		t1.id = t2.sum AND t2.id = t3.sum
	LIMIT 10;
2023-11-25 13:49:18.805 UTC [147046] ERROR:  XX000: the query contains a join that requires repartitioning
2023-11-25 13:49:18.805 UTC [147046] HINT:  Set citus.enable_repartition_joins to on to enable repartitioning
2023-11-25 13:49:18.805 UTC [147046] LOCATION:  JobExecutorType, multi_server_executor.c:68
2023-11-25 13:49:18.805 UTC [147046] STATEMENT:  EXPLAIN SELECT
		avg(t1.avg + t2.avg)
	FROM
		single_hash_repartition_first t1, single_hash_repartition_second t2, single_hash_repartition_second t3
	WHERE
		t1.id = t2.sum AND t2.id = t3.sum
	LIMIT 10;
2023-11-25 13:49:18.806 UTC [147046] LOG:  00000: join order: [ "single_hash_repartition_first" ][ single hash partition join "single_hash_repartition_second" ]
2023-11-25 13:49:18.806 UTC [147046] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:18.806 UTC [147046] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		single_hash_repartition_first t1, single_hash_repartition_second t2
	WHERE
		t1.id = t2.sum;
2023-11-25 13:49:18.807 UTC [147046] ERROR:  XX000: the query contains a join that requires repartitioning
2023-11-25 13:49:18.807 UTC [147046] HINT:  Set citus.enable_repartition_joins to on to enable repartitioning
2023-11-25 13:49:18.807 UTC [147046] LOCATION:  JobExecutorType, multi_server_executor.c:68
2023-11-25 13:49:18.807 UTC [147046] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		single_hash_repartition_first t1, single_hash_repartition_second t2
	WHERE
		t1.id = t2.sum;
2023-11-25 13:49:18.807 UTC [147046] LOG:  00000: join order: [ "single_hash_repartition_first" ][ single hash partition join "single_hash_repartition_second" ]
2023-11-25 13:49:18.807 UTC [147046] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:18.807 UTC [147046] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		single_hash_repartition_first t1, single_hash_repartition_second t2
	WHERE
		t1.sum = t2.id;
2023-11-25 13:49:18.807 UTC [147046] ERROR:  XX000: the query contains a join that requires repartitioning
2023-11-25 13:49:18.807 UTC [147046] HINT:  Set citus.enable_repartition_joins to on to enable repartitioning
2023-11-25 13:49:18.807 UTC [147046] LOCATION:  JobExecutorType, multi_server_executor.c:68
2023-11-25 13:49:18.807 UTC [147046] STATEMENT:  EXPLAIN SELECT
		count(*)
	FROM
		single_hash_repartition_first t1, single_hash_repartition_second t2
	WHERE
		t1.sum = t2.id;
2023-11-25 13:49:18.812 UTC [147047] ERROR:  0A000: cannot push down this subquery
2023-11-25 13:49:18.812 UTC [147047] DETAIL:  Limit clause is currently unsupported when a lateral subquery references a column from a reference table (ref)
2023-11-25 13:49:18.812 UTC [147047] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 13:49:18.812 UTC [147047] STATEMENT:  SELECT count(*)
	FROM ref,
	    LATERAL (
	        SELECT
	            test.y
	        FROM test
	        WHERE
	            test.y = ref.a
	        LIMIT 2
	    ) q;
2023-11-25 13:49:18.812 UTC [147047] ERROR:  0A000: cannot push down this subquery
2023-11-25 13:49:18.812 UTC [147047] DETAIL:  Limit clause is currently unsupported when a lateral subquery references a column from a VALUES clause
2023-11-25 13:49:18.812 UTC [147047] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 13:49:18.812 UTC [147047] STATEMENT:  SELECT count(*)
	FROM (VALUES (1), (3)) ref(a),
	    LATERAL (
	        SELECT
	            test.y
	        FROM test
	        WHERE
	            test.y = ref.a
	        LIMIT 2
	    ) q;
2023-11-25 13:49:18.813 UTC [147047] ERROR:  0A000: cannot push down this subquery
2023-11-25 13:49:18.813 UTC [147047] DETAIL:  Limit clause is currently unsupported when a lateral subquery references a column from complex subqueries, CTEs or local tables
2023-11-25 13:49:18.813 UTC [147047] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 13:49:18.813 UTC [147047] STATEMENT:  WITH ref(a) as (select y from test)
	SELECT count(*)
	FROM ref,
	    LATERAL (
	        SELECT
	            test.y
	        FROM test
	        WHERE
	            test.y = ref.a
	        LIMIT 2
	    ) q;
2023-11-25 13:49:18.813 UTC [147047] ERROR:  0A000: cannot push down this subquery
2023-11-25 13:49:18.813 UTC [147047] DETAIL:  Limit clause is currently unsupported when a lateral subquery references a column from a table function (ref)
2023-11-25 13:49:18.813 UTC [147047] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 13:49:18.813 UTC [147047] STATEMENT:  SELECT count(*)
	FROM generate_series(1, 3) ref(a),
	    LATERAL (
	        SELECT
	            test.y
	        FROM test
	        WHERE
	            test.y = ref.a
	        LIMIT 2
	    ) q;
2023-11-25 13:49:18.813 UTC [147047] ERROR:  0A000: cannot push down this subquery
2023-11-25 13:49:18.813 UTC [147047] DETAIL:  Limit clause is currently unsupported when a lateral subquery references a column from a subquery without FROM (ref)
2023-11-25 13:49:18.813 UTC [147047] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 13:49:18.813 UTC [147047] STATEMENT:  SELECT count(*)
	FROM (SELECT generate_series(1, 3)) ref(a),
	    LATERAL (
	        SELECT
	            test.y
	        FROM test
	        WHERE
	            test.y = ref.a
	        LIMIT 2
	    ) q;
2023-11-25 13:49:18.813 UTC [147047] ERROR:  0A000: cannot push down this subquery
2023-11-25 13:49:18.813 UTC [147047] DETAIL:  Limit clause is currently unsupported when a lateral subquery references a column from a reference table (ref_table)
2023-11-25 13:49:18.813 UTC [147047] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 13:49:18.813 UTC [147047] STATEMENT:  SELECT count(*)
	FROM ref ref_table,
	    (VALUES (1), (3)) rec_values(a),
	    LATERAL (
	        SELECT
	            test.y
	        FROM test
	        WHERE
	            test.y = ref_table.a
	        LIMIT 2
	    ) q;
2023-11-25 13:49:18.814 UTC [147047] ERROR:  0A000: cannot push down this subquery
2023-11-25 13:49:18.814 UTC [147047] DETAIL:  Limit clause is currently unsupported when a lateral subquery references a column from a VALUES clause
2023-11-25 13:49:18.814 UTC [147047] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 13:49:18.814 UTC [147047] STATEMENT:  SELECT count(*)
	FROM ref as ref_table,
	    (VALUES (1), (3)) ref_values(a),
	    LATERAL (
	        SELECT
	            test.y
	        FROM test
	        WHERE
	            test.y = ref_values.a
	        LIMIT 2
	    ) q;
2023-11-25 13:49:18.814 UTC [147047] ERROR:  0A000: cannot push down this subquery
2023-11-25 13:49:18.814 UTC [147047] DETAIL:  Limit clause is currently unsupported when a lateral subquery references a column from a reference table (ref_outer)
2023-11-25 13:49:18.814 UTC [147047] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 13:49:18.814 UTC [147047] STATEMENT:  SELECT count(*) FROM
	    ref ref_outer,
	    LATERAL (
	        SELECT * FROM
	            LATERAL ( SELECT *
	            FROM ref ref_inner,
	                LATERAL (
	                    SELECT
	                        test.y
	                    FROM test
	                    WHERE
	                        test.y = ref_outer.a
	                    LIMIT 2
	                ) q
	            ) q2
	    ) q3;
2023-11-25 13:49:18.814 UTC [147047] ERROR:  0A000: cannot push down this subquery
2023-11-25 13:49:18.814 UTC [147047] DETAIL:  Limit clause is currently unsupported when a lateral subquery references a column from a reference table (ref_inner)
2023-11-25 13:49:18.814 UTC [147047] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 13:49:18.814 UTC [147047] STATEMENT:  SELECT count(*) FROM
	    ref ref_outer,
	    LATERAL (
	        SELECT * FROM
	            LATERAL ( SELECT *
	            FROM ref ref_inner,
	                LATERAL (
	                    SELECT
	                        test.y
	                    FROM test
	                    WHERE
	                        test.y = ref_inner.a
	                    LIMIT 2
	                ) q
	            ) q2
	    ) q3;
2023-11-25 13:49:18.814 UTC [147047] ERROR:  0A000: cannot push down this subquery
2023-11-25 13:49:18.814 UTC [147047] DETAIL:  Limit clause is currently unsupported when a lateral subquery references a column from a reference table (ref)
2023-11-25 13:49:18.814 UTC [147047] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 13:49:18.814 UTC [147047] STATEMENT:  SELECT count(*)
	FROM ref,
	    LATERAL (
	        SELECT
	            test.x
	        FROM test
	        WHERE
	            test.x = ref.a
	        LIMIT 2
	    ) q;
2023-11-25 13:49:18.814 UTC [147047] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:49:18.814 UTC [147047] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:49:18.814 UTC [147047] STATEMENT:  SELECT count(*)
	FROM test,
	    LATERAL (
	        SELECT
	            test_2.x
	        FROM test test_2
	        WHERE
	            test_2.x = test.y
	        LIMIT 2
	    ) q ;
2023-11-25 13:49:18.815 UTC [147047] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:49:18.815 UTC [147047] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:49:18.815 UTC [147047] STATEMENT:  SELECT count(*)
	FROM ref JOIN test on ref.b = test.x,
	    LATERAL (
	        SELECT
	            test_2.x
	        FROM test test_2
	        WHERE
	            test_2.x = ref.a
	        LIMIT 2
	    ) q
	;
2023-11-25 13:49:18.815 UTC [147047] ERROR:  0A000: complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:49:18.815 UTC [147047] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:49:18.815 UTC [147047] STATEMENT:  SELECT count(*)
	FROM ref JOIN test on ref.b = test.x,
	    LATERAL (
	        SELECT
	            test_2.y
	        FROM test test_2
	        WHERE
	            test_2.y = ref.a
	        LIMIT 2
	    ) q
	;
2023-11-25 13:49:18.851 UTC [147046] LOG:  00000: join order: [ "test_numeric" ][ single hash partition join "test_numeric" ]
2023-11-25 13:49:18.851 UTC [147046] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:18.851 UTC [147046] STATEMENT:  SELECT count(*) FROM test_numeric t1 JOIN test_numeric as t2 ON (t1.a = t2.b);
2023-11-25 13:49:19.036 UTC [147046] LOG:  00000: join order: [ "dist_1" ][ single hash partition join "dist_1" ]
2023-11-25 13:49:19.036 UTC [147046] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:19.036 UTC [147046] STATEMENT:  SELECT COUNT(*) FROM dist_1 f, dist_1 s WHERE f.a = s.b;
2023-11-25 13:49:19.910 UTC [147501] LOG:  00000: join order: [ "lineitem" ]
2023-11-25 13:49:19.910 UTC [147501] CONTEXT:  PL/pgSQL function public.coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 13:49:19.910 UTC [147501] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:19.910 UTC [147501] STATEMENT:  SELECT public.coordinator_plan($Q$
	EXPLAIN (COSTS FALSE)
	SELECT l_orderkey, l_linenumber, l_shipdate FROM lineitem WHERE l_orderkey = 9030 or l_orderkey = 1;
	$Q$);
2023-11-25 13:49:19.912 UTC [147501] LOG:  00000: join order: [ "lineitem" ][ local partition join "orders" ]
2023-11-25 13:49:19.912 UTC [147501] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:19.912 UTC [147501] STATEMENT:  EXPLAIN (COSTS FALSE)
	SELECT sum(l_linenumber), avg(l_linenumber) FROM lineitem, orders
		WHERE l_orderkey = o_orderkey;
2023-11-25 13:49:19.915 UTC [147501] LOG:  00000: join order: [ "lineitem" ]
2023-11-25 13:49:19.915 UTC [147501] CONTEXT:  PL/pgSQL function public.coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 13:49:19.915 UTC [147501] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:19.915 UTC [147501] STATEMENT:  SELECT public.coordinator_plan($Q$
	EXPLAIN (COSTS FALSE)
	SELECT l_orderkey, l_linenumber, l_shipdate FROM lineitem WHERE l_orderkey = 9030;
	$Q$);
2023-11-25 13:49:19.917 UTC [147501] LOG:  00000: join order: [ "lineitem" ][ dual partition join "orders" ]
2023-11-25 13:49:19.917 UTC [147501] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:19.917 UTC [147501] STATEMENT:  EXPLAIN (COSTS FALSE)
	SELECT sum(l_linenumber), avg(l_linenumber) FROM lineitem, orders
		WHERE l_partkey = o_custkey;
2023-11-25 13:49:19.918 UTC [147501] LOG:  00000: join order: [ "lineitem" ]
2023-11-25 13:49:19.918 UTC [147501] CONTEXT:  PL/pgSQL function public.coordinator_plan(text) line 3 at FOR over EXECUTE statement
2023-11-25 13:49:19.918 UTC [147501] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:19.918 UTC [147501] STATEMENT:  SELECT public.coordinator_plan($Q$
	EXPLAIN (COSTS FALSE)
	SELECT l_orderkey, l_linenumber, l_shipdate FROM lineitem WHERE l_orderkey = 9030;
	$Q$);
2023-11-25 13:49:19.919 UTC [147501] LOG:  00000: join order: [ "lineitem" ][ dual partition join "orders" ]
2023-11-25 13:49:19.919 UTC [147501] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:19.919 UTC [147501] STATEMENT:  EXPLAIN (COSTS FALSE)
	SELECT sum(l_linenumber), avg(l_linenumber) FROM lineitem, orders
		WHERE l_partkey = o_custkey;
2023-11-25 13:49:19.921 UTC [147501] LOG:  00000: join order: [ "lineitem" ][ dual partition join "orders" ]
2023-11-25 13:49:19.921 UTC [147501] LOCATION:  PrintJoinOrderList, multi_join_order.c:629
2023-11-25 13:49:19.921 UTC [147501] STATEMENT:  EXPLAIN (COSTS FALSE)
	SELECT sum(l_linenumber), avg(l_linenumber) FROM lineitem, orders
		WHERE l_partkey = o_custkey;
2023-11-25 13:49:20.126 UTC [147551] ERROR:  0A000: modifying the partition value of rows is not allowed
2023-11-25 13:49:20.126 UTC [147551] LOCATION:  TargetlistAndFunctionsSupported, multi_router_planner.c:585
2023-11-25 13:49:20.126 UTC [147551] STATEMENT:  UPDATE test SET a = 5, c = 5;
2023-11-25 13:49:20.126 UTC [147551] ERROR:  0A000: modifying the partition value of rows is not allowed
2023-11-25 13:49:20.126 UTC [147551] LOCATION:  TargetlistAndFunctionsSupported, multi_router_planner.c:585
2023-11-25 13:49:20.126 UTC [147551] STATEMENT:  UPDATE test SET a = 5, c = d, d =3;
2023-11-25 13:49:20.127 UTC [147551] ERROR:  0A000: modifying the partition value of rows is not allowed
2023-11-25 13:49:20.127 UTC [147551] LOCATION:  TargetlistAndFunctionsSupported, multi_router_planner.c:585
2023-11-25 13:49:20.127 UTC [147551] STATEMENT:  UPDATE test SET c = d, d =3;
2023-11-25 13:49:20.127 UTC [147551] ERROR:  0A000: modifying the partition value of rows is not allowed
2023-11-25 13:49:20.127 UTC [147551] LOCATION:  TargetlistAndFunctionsSupported, multi_router_planner.c:585
2023-11-25 13:49:20.127 UTC [147551] STATEMENT:  UPDATE test SET d=c, c = d;
2023-11-25 13:49:20.140 UTC [147551] ERROR:  42601: multiple assignments to same column "c"
2023-11-25 13:49:20.140 UTC [147551] LOCATION:  process_matched_tle, rewriteHandler.c:1105
2023-11-25 13:49:20.140 UTC [147551] STATEMENT:  UPDATE test SET c = c, c = 3;
2023-11-25 13:49:20.154 UTC [147551] ERROR:  0A000: modifying the partition value of rows is not allowed
2023-11-25 13:49:20.154 UTC [147551] LOCATION:  ErrorIfOnConflictNotSupported, multi_router_planner.c:1255
2023-11-25 13:49:20.154 UTC [147551] STATEMENT:  INSERT INTO test (c,d) VALUES(3,4) ON CONFLICT(c) DO UPDATE SET c=7;
2023-11-25 13:49:20.157 UTC [147551] ERROR:  0A000: modifying the partition value of rows is not allowed
2023-11-25 13:49:20.157 UTC [147551] LOCATION:  ErrorIfOnConflictNotSupported, multi_router_planner.c:1255
2023-11-25 13:49:20.157 UTC [147551] STATEMENT:  INSERT INTO test (d,c) VALUES(3,4) ON CONFLICT(c) DO UPDATE SET c=7;
2023-11-25 13:49:20.166 UTC [147550] ERROR:  42501: permission denied for table reference_table_1
2023-11-25 13:49:20.166 UTC [147550] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:49:20.166 UTC [147550] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:20.166 UTC [147550] STATEMENT:  INSERT INTO reference_table_2 SELECT * FROM reference_table_1;
2023-11-25 13:49:20.179 UTC [147551] ERROR:  42601: multiple assignments to same column "c"
2023-11-25 13:49:20.179 UTC [147551] LOCATION:  process_matched_tle, rewriteHandler.c:1105
2023-11-25 13:49:20.179 UTC [147551] STATEMENT:  UPDATE test SET c = c, c = 3;
2023-11-25 13:49:20.247 UTC [147551] ERROR:  0A000: modifying the partition value of rows is not allowed
2023-11-25 13:49:20.247 UTC [147551] LOCATION:  TargetlistAndFunctionsSupported, multi_router_planner.c:585
2023-11-25 13:49:20.247 UTC [147551] STATEMENT:  UPDATE test SET a = 5,d  = 2, c = 5 FROM (SELECT * FROM test LIMIT 10) t2 WHERE t2.d = test.c  and test.c = 6;
2023-11-25 13:49:20.486 UTC [147719] ERROR:  XX000: multi-task query about to be executed
2023-11-25 13:49:20.486 UTC [147719] HINT:  Queries are split to multiple tasks if they have to be split into several queries on the workers.
2023-11-25 13:49:20.486 UTC [147719] LOCATION:  FinalizePlan, distributed_planner.c:1392
2023-11-25 13:49:20.486 UTC [147719] STATEMENT:  SELECT * FROM multi_task_table;
2023-11-25 13:49:20.511 UTC [147719] ERROR:  XX000: multi-task query about to be executed
2023-11-25 13:49:20.511 UTC [147719] HINT:  Queries are split to multiple tasks if they have to be split into several queries on the workers.
2023-11-25 13:49:20.511 UTC [147719] LOCATION:  FinalizePlan, distributed_planner.c:1392
2023-11-25 13:49:20.511 UTC [147719] STATEMENT:  INSERT INTO summary_table SELECT id, SUM(order_count) FROM raw_table GROUP BY id;
2023-11-25 13:49:20.522 UTC [147719] ERROR:  XX000: multi-task query about to be executed
2023-11-25 13:49:20.522 UTC [147719] HINT:  Queries are split to multiple tasks if they have to be split into several queries on the workers.
2023-11-25 13:49:20.522 UTC [147719] LOCATION:  FinalizePlan, distributed_planner.c:1392
2023-11-25 13:49:20.522 UTC [147719] STATEMENT:  INSERT INTO summary_table SELECT id, SUM(order_count) FROM raw_table GROUP BY id;
2023-11-25 13:49:20.778 UTC [147839] ERROR:  42P01: relation "customer_few" does not exist at character 52
2023-11-25 13:49:20.778 UTC [147839] LOCATION:  parserOpenTable, parse_relation.c:1392
2023-11-25 13:49:20.778 UTC [147839] STATEMENT:  SELECT customer_key, c_name, c_address
	       FROM customer_few ORDER BY customer_key LIMIT 5;
2023-11-25 13:49:20.861 UTC [147839] ERROR:  34000: cursor "noholdcursor" does not exist
2023-11-25 13:49:20.861 UTC [147839] LOCATION:  PerformPortalFetch, portalcmds.c:187
2023-11-25 13:49:20.861 UTC [147839] STATEMENT:  FETCH ABSOLUTE 5 FROM noHoldCursor;
2023-11-25 13:49:21.054 UTC [147871] ERROR:  XX000: cannot modify table "on_update_fkey_table" because there was a parallel operation on a distributed table in the transaction
2023-11-25 13:49:21.054 UTC [147871] DETAIL:  When there is a foreign key to a reference table, Citus needs to perform all operations over a single connection per node to ensure consistency.
2023-11-25 13:49:21.054 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.054 UTC [147871] LOCATION:  SetupExecutionModeForAlterTable, table.c:3638
2023-11-25 13:49:21.054 UTC [147871] STATEMENT:  ALTER TABLE on_update_fkey_table DROP COLUMN value_1 CASCADE;
2023-11-25 13:49:21.058 UTC [147871] ERROR:  XX000: cannot modify table "on_update_fkey_table" because there was a parallel operation on a distributed table in the transaction
2023-11-25 13:49:21.058 UTC [147871] DETAIL:  When there is a foreign key to a reference table, Citus needs to perform all operations over a single connection per node to ensure consistency.
2023-11-25 13:49:21.058 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.058 UTC [147871] LOCATION:  SetupExecutionModeForAlterTable, table.c:3638
2023-11-25 13:49:21.058 UTC [147871] STATEMENT:  ALTER TABLE on_update_fkey_table DROP COLUMN value_1 CASCADE;
2023-11-25 13:49:21.063 UTC [147871] ERROR:  XX000: cannot execute parallel DDL on table "on_update_fkey_table" after SELECT command on reference table "reference_table" because there is a foreign key between them and "reference_table" has been accessed in this transaction
2023-11-25 13:49:21.063 UTC [147871] DETAIL:  When there is a foreign key to a reference table, Citus needs to perform all operations over a single connection per node to ensure consistency.
2023-11-25 13:49:21.063 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.063 UTC [147871] LOCATION:  CheckConflictingParallelRelationAccesses, relation_access_tracking.c:880
2023-11-25 13:49:21.063 UTC [147871] STATEMENT:  ALTER TABLE on_update_fkey_table ADD COLUMN X INT;
2023-11-25 13:49:21.068 UTC [147871] ERROR:  XX000: cannot execute parallel DDL on table "on_update_fkey_table" after SELECT command on reference table "transitive_reference_table" because there is a foreign key between them and "transitive_reference_table" has been accessed in this transaction
2023-11-25 13:49:21.068 UTC [147871] DETAIL:  When there is a foreign key to a reference table, Citus needs to perform all operations over a single connection per node to ensure consistency.
2023-11-25 13:49:21.068 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.068 UTC [147871] LOCATION:  CheckConflictingParallelRelationAccesses, relation_access_tracking.c:880
2023-11-25 13:49:21.068 UTC [147871] STATEMENT:  ALTER TABLE on_update_fkey_table ADD COLUMN X INT;
2023-11-25 13:49:21.103 UTC [147871] ERROR:  23503: insert or update on table "on_update_fkey_table_2380002" violates foreign key constraint "fkey_2380002"
2023-11-25 13:49:21.103 UTC [147871] DETAIL:  Key (value_1)=(101) is not present in table "reference_table_2380001".
2023-11-25 13:49:21.103 UTC [147871] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:49:21.103 UTC [147871] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:49:21.103 UTC [147871] STATEMENT:  UPDATE on_update_fkey_table SET value_1 = 101 WHERE id = 1;
2023-11-25 13:49:21.103 UTC [147871] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 13:49:21.103 UTC [147871] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 13:49:21.103 UTC [147871] STATEMENT:  UPDATE on_update_fkey_table SET value_1 = 101 WHERE id = 2;
2023-11-25 13:49:21.104 UTC [147871] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 13:49:21.104 UTC [147871] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 13:49:21.104 UTC [147871] STATEMENT:  UPDATE on_update_fkey_table SET value_1 = 101 WHERE id = 3;
2023-11-25 13:49:21.104 UTC [147871] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 13:49:21.104 UTC [147871] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 13:49:21.104 UTC [147871] STATEMENT:  UPDATE on_update_fkey_table SET value_1 = 101 WHERE id = 4;
2023-11-25 13:49:21.126 UTC [147871] ERROR:  XX000: insert or update on table "on_update_fkey_table_2380005" violates foreign key constraint "fkey_2380005"
2023-11-25 13:49:21.126 UTC [147871] DETAIL:  Key (value_1)=(101) is not present in table "reference_table_2380001".
2023-11-25 13:49:21.126 UTC [147871] LOCATION:  ReportCopyError, multi_copy.c:1193
2023-11-25 13:49:21.126 UTC [147871] STATEMENT:  COPY on_update_fkey_table FROM STDIN WITH CSV;
2023-11-25 13:49:21.261 UTC [147871] ERROR:  XX000: cannot modify table "reference_table" because there was a parallel operation on a distributed table
2023-11-25 13:49:21.261 UTC [147871] DETAIL:  When there is a foreign key to a reference table or to a local table, Citus needs to perform all operations over a single connection per node to ensure consistency.
2023-11-25 13:49:21.261 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.261 UTC [147871] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:795
2023-11-25 13:49:21.261 UTC [147871] STATEMENT:  UPDATE reference_table SET id = 101 WHERE id = 99;
2023-11-25 13:49:21.265 UTC [147871] ERROR:  XX000: cannot modify table "transitive_reference_table" because there was a parallel operation on a distributed table
2023-11-25 13:49:21.265 UTC [147871] DETAIL:  When there is a foreign key to a reference table or to a local table, Citus needs to perform all operations over a single connection per node to ensure consistency.
2023-11-25 13:49:21.265 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.265 UTC [147871] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:795
2023-11-25 13:49:21.265 UTC [147871] STATEMENT:  UPDATE transitive_reference_table SET id = 101 WHERE id = 99;
2023-11-25 13:49:21.269 UTC [147871] ERROR:  XX000: cannot execute DDL on table "reference_table" because there was a parallel SELECT access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 13:49:21.269 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.269 UTC [147871] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 13:49:21.269 UTC [147871] STATEMENT:  ALTER TABLE reference_table ADD COLUMN X INT;
2023-11-25 13:49:21.273 UTC [147871] ERROR:  XX000: cannot execute DDL on table "transitive_reference_table" because there was a parallel SELECT access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 13:49:21.273 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.273 UTC [147871] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 13:49:21.273 UTC [147871] STATEMENT:  ALTER TABLE transitive_reference_table ADD COLUMN X INT;
2023-11-25 13:49:21.281 UTC [147871] ERROR:  XX000: cannot execute DDL on table "reference_table" because there was a parallel SELECT access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 13:49:21.281 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.281 UTC [147871] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 13:49:21.281 UTC [147871] STATEMENT:  ALTER TABLE reference_table ALTER COLUMN id SET DATA TYPE smallint;
2023-11-25 13:49:21.288 UTC [147871] ERROR:  XX000: cannot execute DDL on table "transitive_reference_table" because there was a parallel SELECT access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 13:49:21.288 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.288 UTC [147871] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 13:49:21.288 UTC [147871] STATEMENT:  ALTER TABLE transitive_reference_table ALTER COLUMN id SET DATA TYPE smallint;
2023-11-25 13:49:21.293 UTC [147871] ERROR:  XX000: cannot execute DDL on table "reference_table" because there was a parallel SELECT access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 13:49:21.293 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.293 UTC [147871] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 13:49:21.293 UTC [147871] STATEMENT:  TRUNCATE reference_table CASCADE;
2023-11-25 13:49:21.297 UTC [147871] ERROR:  XX000: cannot execute DDL on table "transitive_reference_table" because there was a parallel SELECT access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 13:49:21.297 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.297 UTC [147871] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 13:49:21.297 UTC [147871] STATEMENT:  TRUNCATE transitive_reference_table CASCADE;
2023-11-25 13:49:21.321 UTC [147871] ERROR:  XX000: cannot execute DDL on table because there was a parallel SELECT access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 13:49:21.321 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.321 UTC [147871] CONTEXT:  SQL statement "SELECT citus_drop_all_shards(v_obj.objid, v_obj.schema_name, v_obj.object_name, drop_shards_metadata_only := false)"
	PL/pgSQL function citus_drop_trigger() line 25 at PERFORM
2023-11-25 13:49:21.321 UTC [147871] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:761
2023-11-25 13:49:21.321 UTC [147871] STATEMENT:  DROP TABLE reference_table CASCADE;
2023-11-25 13:49:21.338 UTC [147871] ERROR:  XX000: cannot execute DML on table "reference_table" because there was a parallel DML access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 13:49:21.338 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.338 UTC [147871] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 13:49:21.338 UTC [147871] STATEMENT:  UPDATE reference_table SET id = 160 WHERE id = 15;
2023-11-25 13:49:21.342 UTC [147871] ERROR:  XX000: cannot execute DML on table "transitive_reference_table" because there was a parallel DML access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 13:49:21.342 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.342 UTC [147871] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 13:49:21.342 UTC [147871] STATEMENT:  UPDATE transitive_reference_table SET id = 160 WHERE id = 15;
2023-11-25 13:49:21.347 UTC [147871] ERROR:  XX000: cannot execute DML on table "reference_table" because there was a parallel DML access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 13:49:21.347 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.347 UTC [147871] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 13:49:21.347 UTC [147871] STATEMENT:  UPDATE reference_table SET id = 160 WHERE id = 15;
2023-11-25 13:49:21.351 UTC [147871] ERROR:  XX000: cannot execute DML on table "transitive_reference_table" because there was a parallel DML access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 13:49:21.351 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.351 UTC [147871] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 13:49:21.351 UTC [147871] STATEMENT:  UPDATE transitive_reference_table SET id = 160 WHERE id = 15;
2023-11-25 13:49:21.355 UTC [147871] ERROR:  XX000: cannot execute DDL on table "reference_table" because there was a parallel DML access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 13:49:21.355 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.355 UTC [147871] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 13:49:21.355 UTC [147871] STATEMENT:  ALTER TABLE reference_table ADD COLUMN X INT;
2023-11-25 13:49:21.359 UTC [147871] ERROR:  XX000: cannot execute DDL on table "transitive_reference_table" because there was a parallel DML access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 13:49:21.359 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.359 UTC [147871] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 13:49:21.359 UTC [147871] STATEMENT:  ALTER TABLE transitive_reference_table ADD COLUMN X INT;
2023-11-25 13:49:21.366 UTC [147871] ERROR:  XX000: cannot execute DDL on table "reference_table" because there was a parallel DML access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 13:49:21.366 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.366 UTC [147871] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 13:49:21.366 UTC [147871] STATEMENT:  ALTER TABLE reference_table ALTER COLUMN id SET DATA TYPE smallint;
2023-11-25 13:49:21.374 UTC [147871] ERROR:  XX000: cannot execute DDL on table "transitive_reference_table" because there was a parallel DML access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 13:49:21.374 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.374 UTC [147871] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 13:49:21.374 UTC [147871] STATEMENT:  ALTER TABLE transitive_reference_table ALTER COLUMN id SET DATA TYPE smallint;
2023-11-25 13:49:21.378 UTC [147871] ERROR:  XX000: cannot execute SELECT on table "reference_table" because there was a parallel DDL access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 13:49:21.378 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.378 UTC [147871] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 13:49:21.378 UTC [147871] STATEMENT:  SELECT count(*) FROM reference_table;
2023-11-25 13:49:21.382 UTC [147871] ERROR:  XX000: cannot execute SELECT on table "transitive_reference_table" because there was a parallel DDL access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 13:49:21.382 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.382 UTC [147871] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 13:49:21.382 UTC [147871] STATEMENT:  SELECT count(*) FROM transitive_reference_table;
2023-11-25 13:49:21.402 UTC [147871] ERROR:  XX000: cannot execute SELECT on table "reference_table" because there was a parallel DDL access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 13:49:21.402 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.402 UTC [147871] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 13:49:21.402 UTC [147871] STATEMENT:  SELECT count(*) FROM reference_table;
2023-11-25 13:49:21.406 UTC [147871] ERROR:  XX000: cannot execute SELECT on table "transitive_reference_table" because there was a parallel DDL access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 13:49:21.406 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.406 UTC [147871] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 13:49:21.406 UTC [147871] STATEMENT:  SELECT count(*) FROM transitive_reference_table;
2023-11-25 13:49:21.410 UTC [147871] ERROR:  XX000: cannot execute DML on table "reference_table" because there was a parallel DDL access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 13:49:21.410 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.410 UTC [147871] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 13:49:21.410 UTC [147871] STATEMENT:  UPDATE reference_table SET id = 160 WHERE id = 15;
2023-11-25 13:49:21.414 UTC [147871] ERROR:  XX000: cannot execute DML on table "transitive_reference_table" because there was a parallel DDL access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 13:49:21.414 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.414 UTC [147871] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 13:49:21.414 UTC [147871] STATEMENT:  UPDATE transitive_reference_table SET id = 160 WHERE id = 15;
2023-11-25 13:49:21.419 UTC [147871] ERROR:  XX000: cannot execute DDL on table "reference_table" because there was a parallel DDL access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 13:49:21.419 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.419 UTC [147871] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 13:49:21.419 UTC [147871] STATEMENT:  ALTER TABLE reference_table ADD COLUMN X int;
2023-11-25 13:49:21.423 UTC [147871] ERROR:  XX000: cannot execute DDL on table "transitive_reference_table" because there was a parallel DDL access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 13:49:21.423 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.423 UTC [147871] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 13:49:21.423 UTC [147871] STATEMENT:  ALTER TABLE transitive_reference_table ADD COLUMN X int;
2023-11-25 13:49:21.427 UTC [147871] ERROR:  XX000: cannot modify table "on_update_fkey_table" because there was a parallel operation on a distributed table in the transaction
2023-11-25 13:49:21.427 UTC [147871] DETAIL:  When there is a foreign key to a reference table, Citus needs to perform all operations over a single connection per node to ensure consistency.
2023-11-25 13:49:21.427 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.427 UTC [147871] LOCATION:  SetupExecutionModeForAlterTable, table.c:3638
2023-11-25 13:49:21.427 UTC [147871] STATEMENT:  ALTER TABLE on_update_fkey_table ALTER COLUMN value_1 SET DATA TYPE smallint;
2023-11-25 13:49:21.436 UTC [147871] ERROR:  XX000: cannot execute DML on table "reference_table" because there was a parallel DML access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 13:49:21.436 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.436 UTC [147871] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 13:49:21.436 UTC [147871] STATEMENT:  DELETE FROM reference_table  WHERE id = 99;
2023-11-25 13:49:21.446 UTC [147871] ERROR:  XX000: cannot execute DML on table "transitive_reference_table" because there was a parallel DML access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 13:49:21.446 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.446 UTC [147871] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 13:49:21.446 UTC [147871] STATEMENT:  DELETE FROM transitive_reference_table  WHERE id = 99;
2023-11-25 13:49:21.456 UTC [147871] ERROR:  XX000: cannot execute DML on table "reference_table" because there was a parallel DML access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 13:49:21.456 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.456 UTC [147871] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 13:49:21.456 UTC [147871] STATEMENT:  UPDATE reference_table SET id = 101 WHERE id = 99;
2023-11-25 13:49:21.466 UTC [147871] ERROR:  XX000: cannot execute DML on table "transitive_reference_table" because there was a parallel DML access to distributed table "on_update_fkey_table" in the same transaction
2023-11-25 13:49:21.466 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.466 UTC [147871] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 13:49:21.466 UTC [147871] STATEMENT:  UPDATE transitive_reference_table SET id = 101 WHERE id = 99;
2023-11-25 13:49:21.470 UTC [147871] ERROR:  XX000: cannot modify table "reference_table" because there was a parallel operation on a distributed table
2023-11-25 13:49:21.470 UTC [147871] DETAIL:  When there is a foreign key to a reference table or to a local table, Citus needs to perform all operations over a single connection per node to ensure consistency.
2023-11-25 13:49:21.470 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.470 UTC [147871] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:795
2023-11-25 13:49:21.470 UTC [147871] STATEMENT:  UPDATE reference_table SET id = 101 WHERE id = 99;
2023-11-25 13:49:21.470 UTC [147871] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 13:49:21.470 UTC [147871] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 13:49:21.470 UTC [147871] STATEMENT:  UPDATE on_update_fkey_table SET value_1 = 5 WHERE id != 11;
2023-11-25 13:49:21.504 UTC [147871] ERROR:  XX000: cannot distribute relation "test_table_2" in this transaction because it has a foreign key to a reference table
2023-11-25 13:49:21.504 UTC [147871] DETAIL:  If a hash distributed table has a foreign key to a reference table, it has to be created in sequential mode before any parallel commands have been executed in the same transaction
2023-11-25 13:49:21.504 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.504 UTC [147871] LOCATION:  CanUseExclusiveConnections, create_distributed_table.c:2256
2023-11-25 13:49:21.504 UTC [147871] STATEMENT:  SELECT create_distributed_table('test_table_2', 'id');
2023-11-25 13:49:21.506 UTC [147871] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 13:49:21.506 UTC [147871] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 13:49:21.506 UTC [147871] STATEMENT:  SET LOCAL client_min_messages TO ERROR;
2023-11-25 13:49:21.506 UTC [147871] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 13:49:21.506 UTC [147871] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 13:49:21.506 UTC [147871] STATEMENT:  DROP TABLE test_table_1 CASCADE;
2023-11-25 13:49:21.546 UTC [147871] ERROR:  XX000: cannot modify table "test_table_2" because there was a parallel operation on a distributed table in the transaction
2023-11-25 13:49:21.546 UTC [147871] DETAIL:  When there is a foreign key to a reference table, Citus needs to perform all operations over a single connection per node to ensure consistency.
2023-11-25 13:49:21.546 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.546 UTC [147871] LOCATION:  SetupExecutionModeForAlterTable, table.c:3638
2023-11-25 13:49:21.546 UTC [147871] STATEMENT:  ALTER TABLE test_table_2 ADD CONSTRAINT c_check FOREIGN KEY (value_1) REFERENCES test_table_1(id);
2023-11-25 13:49:21.547 UTC [147871] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 13:49:21.547 UTC [147871] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 13:49:21.547 UTC [147871] STATEMENT:  SET LOCAL client_min_messages TO ERROR;
2023-11-25 13:49:21.547 UTC [147871] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 13:49:21.547 UTC [147871] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 13:49:21.547 UTC [147871] STATEMENT:  DROP TABLE test_table_1, test_table_2;
2023-11-25 13:49:21.582 UTC [147871] ERROR:  XX000: cannot modify table "test_table_2" because there was a parallel operation on a distributed table in the transaction
2023-11-25 13:49:21.582 UTC [147871] DETAIL:  When there is a foreign key to a reference table, Citus needs to perform all operations over a single connection per node to ensure consistency.
2023-11-25 13:49:21.582 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.582 UTC [147871] LOCATION:  SetupExecutionModeForAlterTable, table.c:3638
2023-11-25 13:49:21.582 UTC [147871] STATEMENT:  ALTER TABLE test_table_2 ADD CONSTRAINT c_check FOREIGN KEY (value_1) REFERENCES test_table_1(id);
2023-11-25 13:49:21.583 UTC [147871] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 13:49:21.583 UTC [147871] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 13:49:21.583 UTC [147871] STATEMENT:  SET LOCAL client_min_messages TO ERROR;
2023-11-25 13:49:21.583 UTC [147871] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 13:49:21.583 UTC [147871] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 13:49:21.583 UTC [147871] STATEMENT:  DROP TABLE test_table_1 CASCADE;
2023-11-25 13:49:21.605 UTC [147871] ERROR:  XX000: cannot distribute "test_table_2" in sequential mode because a parallel query was executed in this transaction
2023-11-25 13:49:21.605 UTC [147871] HINT:  If you have manually set citus.multi_shard_modify_mode to 'sequential', try with 'parallel' option. 
2023-11-25 13:49:21.605 UTC [147871] LOCATION:  CanUseExclusiveConnections, create_distributed_table.c:2269
2023-11-25 13:49:21.605 UTC [147871] STATEMENT:  SELECT create_distributed_table('test_table_2', 'id');
2023-11-25 13:49:21.606 UTC [147871] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 13:49:21.606 UTC [147871] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 13:49:21.606 UTC [147871] STATEMENT:  CREATE TABLE test_table_1(id int PRIMARY KEY);
2023-11-25 13:49:21.606 UTC [147871] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 13:49:21.606 UTC [147871] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 13:49:21.606 UTC [147871] STATEMENT:  SELECT create_reference_table('test_table_1');
2023-11-25 13:49:21.606 UTC [147871] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 13:49:21.606 UTC [147871] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 13:49:21.606 UTC [147871] STATEMENT:  ALTER TABLE test_table_2 ADD CONSTRAINT c_check FOREIGN KEY (value_1) REFERENCES test_table_1(id);
2023-11-25 13:49:21.606 UTC [147871] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 13:49:21.606 UTC [147871] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 13:49:21.606 UTC [147871] STATEMENT:  SET LOCAL client_min_messages TO ERROR;
2023-11-25 13:49:21.606 UTC [147871] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 13:49:21.606 UTC [147871] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 13:49:21.606 UTC [147871] STATEMENT:  DROP TABLE test_table_1 CASCADE;
2023-11-25 13:49:21.733 UTC [147871] ERROR:  XX000: cannot execute DML on table "reference_table" because there was a parallel DML access to distributed table "distributed_table" in the same transaction
2023-11-25 13:49:21.733 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.733 UTC [147871] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 13:49:21.733 UTC [147871] STATEMENT:  WITH t1 AS (DELETE FROM distributed_table RETURNING id),
		t2 AS (DELETE FROM reference_table RETURNING id)
		SELECT count(*) FROM distributed_table, t1, t2 WHERE  value_1 = t1.id AND value_1 = t2.id;
2023-11-25 13:49:21.736 UTC [147871] ERROR:  XX000: cannot execute DML on table "reference_table" because there was a parallel DML access to distributed table "distributed_table" in the same transaction
2023-11-25 13:49:21.736 UTC [147871] HINT:  Try re-running the transaction with "SET LOCAL citus.multi_shard_modify_mode TO 'sequential';"
2023-11-25 13:49:21.736 UTC [147871] LOCATION:  CheckConflictingRelationAccesses, relation_access_tracking.c:772
2023-11-25 13:49:21.736 UTC [147871] STATEMENT:  WITH t1 AS (DELETE FROM distributed_table RETURNING id)
		DELETE FROM reference_table RETURNING id;
2023-11-25 13:49:22.134 UTC [148038] ERROR:  XX000: you cannot alter access method of a partitioned table
2023-11-25 13:49:22.134 UTC [148038] LOCATION:  AlterTableSetAccessMethod, alter_table.c:487
2023-11-25 13:49:22.134 UTC [148038] STATEMENT:  SELECT alter_table_set_access_method('partitioned_table', 'columnar');
2023-11-25 13:49:22.176 UTC [148038] ERROR:  P0001: partition column of partitioned_table cannot be cast to a timestamptz
2023-11-25 13:49:22.176 UTC [148038] CONTEXT:  PL/pgSQL function alter_old_partitions_set_access_method(regclass,timestamp with time zone,name) line 14 at RAISE
2023-11-25 13:49:22.176 UTC [148038] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:49:22.176 UTC [148038] STATEMENT:  CALL alter_old_partitions_set_access_method('partitioned_table', '2021-01-01', 'columnar');
2023-11-25 13:49:22.536 UTC [148038] ERROR:  0A000: Foreign keys and AFTER ROW triggers are not supported for columnar tables
2023-11-25 13:49:22.536 UTC [148038] HINT:  Consider an AFTER STATEMENT trigger instead.
2023-11-25 13:49:22.536 UTC [148038] CONTEXT:  SQL statement "ALTER TABLE alter_table_set_access_method.test_fk_p ATTACH PARTITION alter_table_set_access_method.test_fk_p1 FOR VALUES FROM (10) TO (20);"
2023-11-25 13:49:22.536 UTC [148038] LOCATION:  ColumnarTriggerCreateHook, columnar_tableam.c:2140
2023-11-25 13:49:22.536 UTC [148038] STATEMENT:  select alter_table_set_access_method('test_fk_p1', 'columnar');
2023-11-25 13:49:22.537 UTC [148038] ERROR:  XX000: the access method of alter_table_set_access_method.same_access_method is already heap
2023-11-25 13:49:22.537 UTC [148038] LOCATION:  AlterTableSetAccessMethod, alter_table.c:514
2023-11-25 13:49:22.537 UTC [148038] STATEMENT:  SELECT alter_table_set_access_method('same_access_method', 'heap');
2023-11-25 13:49:22.542 UTC [148038] WARNING:  0A000: "view v_local" has dependency to "table local" that is not in Citus' metadata
2023-11-25 13:49:22.542 UTC [148038] DETAIL:  "view v_local" will be created only locally
2023-11-25 13:49:22.542 UTC [148038] HINT:  Distribute "table local" first to distribute "view v_local"
2023-11-25 13:49:22.542 UTC [148038] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:950
2023-11-25 13:49:22.686 UTC [148038] ERROR:  XX000: you cannot alter access method of a view
2023-11-25 13:49:22.686 UTC [148038] LOCATION:  AlterTableSetAccessMethod, alter_table.c:492
2023-11-25 13:49:22.686 UTC [148038] STATEMENT:  select alter_table_set_access_method('view_test_view','columnar');
2023-11-25 13:49:22.732 UTC [148038] ERROR:  XX000: tuple concurrently deleted
2023-11-25 13:49:22.732 UTC [148038] LOCATION:  simple_heap_delete, heapam.c:3106
2023-11-25 13:49:22.732 UTC [148038] STATEMENT:  SELECT 1 FROM master_remove_node('localhost', 57636);
2023-11-25 13:49:23.256 UTC [148145] ERROR:  XX000: cannot complete operation because table is a partition
2023-11-25 13:49:23.256 UTC [148145] HINT:  the parent table is "partitioned_table"
2023-11-25 13:49:23.256 UTC [148145] LOCATION:  EnsureTableNotPartition, alter_table.c:1172
2023-11-25 13:49:23.256 UTC [148145] STATEMENT:  SELECT alter_distributed_table('partitioned_table_1_5', shard_count := 10, distribution_column := 'a');
2023-11-25 13:49:23.374 UTC [148145] ERROR:  0A000: cannot create foreign key constraint since foreign keys from reference tables and local tables to distributed tables are not supported
2023-11-25 13:49:23.374 UTC [148145] DETAIL:  When adding a foreign key from a local table to a reference table, Citus applies a conversion to all the local tables in the foreign key graph
2023-11-25 13:49:23.374 UTC [148145] LOCATION:  ErrorIfUnsupportedForeignConstraintExists, foreign_constraint.c:309
2023-11-25 13:49:23.374 UTC [148145] STATEMENT:  SELECT create_reference_table('referenced_ref_table');
2023-11-25 13:49:23.375 UTC [148145] ERROR:  42P16: referenced table "referenced_ref_table" must be a distributed table or a reference table
2023-11-25 13:49:23.375 UTC [148145] DETAIL:  To enforce foreign keys, the referencing and referenced rows need to be stored on the same node.
2023-11-25 13:49:23.375 UTC [148145] HINT:  You could use SELECT create_reference_table('referenced_ref_table') to replicate the referenced table to all nodes or consider dropping the foreign key
2023-11-25 13:49:23.375 UTC [148145] LOCATION:  ErrorIfUnsupportedForeignConstraintExists, foreign_constraint.c:232
2023-11-25 13:49:23.375 UTC [148145] STATEMENT:  SELECT create_distributed_table('table_with_references', 'a1', colocate_with:='referenced_dist_table');
2023-11-25 13:49:23.376 UTC [148145] ERROR:  42P16: referenced table "table_with_references" must be a distributed table or a reference table
2023-11-25 13:49:23.376 UTC [148145] DETAIL:  To enforce foreign keys, the referencing and referenced rows need to be stored on the same node.
2023-11-25 13:49:23.376 UTC [148145] HINT:  You could use SELECT create_reference_table('table_with_references') to replicate the referenced table to all nodes or consider dropping the foreign key
2023-11-25 13:49:23.376 UTC [148145] LOCATION:  ErrorIfUnsupportedForeignConstraintExists, foreign_constraint.c:232
2023-11-25 13:49:23.376 UTC [148145] STATEMENT:  SELECT create_distributed_table('referencing_dist_table', 'a', colocate_with:='referenced_dist_table');
2023-11-25 13:49:23.376 UTC [148145] ERROR:  XX000: cannot alter table because the table is not distributed
2023-11-25 13:49:23.376 UTC [148145] LOCATION:  AlterDistributedTable, alter_table.c:434
2023-11-25 13:49:23.376 UTC [148145] STATEMENT:  SELECT alter_distributed_table('table_with_references', shard_count := 12, cascade_to_colocated := true);
2023-11-25 13:49:23.376 UTC [148145] ERROR:  XX000: cannot alter table because the table is not distributed
2023-11-25 13:49:23.376 UTC [148145] LOCATION:  AlterDistributedTable, alter_table.c:434
2023-11-25 13:49:23.376 UTC [148145] STATEMENT:  SELECT alter_distributed_table('table_with_references', shard_count := 10, cascade_to_colocated := false);
2023-11-25 13:49:23.399 UTC [148145] ERROR:  42P16: referenced table "referenced_ref_table" must be a distributed table or a reference table
2023-11-25 13:49:23.399 UTC [148145] DETAIL:  To enforce foreign keys, the referencing and referenced rows need to be stored on the same node.
2023-11-25 13:49:23.399 UTC [148145] HINT:  You could use SELECT create_reference_table('referenced_ref_table') to replicate the referenced table to all nodes or consider dropping the foreign key
2023-11-25 13:49:23.399 UTC [148145] LOCATION:  ErrorIfUnsupportedForeignConstraintExists, foreign_constraint.c:232
2023-11-25 13:49:23.399 UTC [148145] STATEMENT:  SELECT create_distributed_table('ref_to_ref_table', 'a', colocate_with:='none');
2023-11-25 13:49:23.399 UTC [148145] ERROR:  XX000: cannot alter table because the table is not distributed
2023-11-25 13:49:23.399 UTC [148145] LOCATION:  AlterDistributedTable, alter_table.c:434
2023-11-25 13:49:23.399 UTC [148145] STATEMENT:  SELECT alter_distributed_table('ref_to_ref_table', shard_count:=6);
2023-11-25 13:49:23.403 UTC [148145] ERROR:  XX000: cannot alter table because the table is not distributed
2023-11-25 13:49:23.403 UTC [148145] LOCATION:  AlterDistributedTable, alter_table.c:434
2023-11-25 13:49:23.403 UTC [148145] STATEMENT:  SELECT alter_distributed_table('ref_to_ref_table', colocate_with:='none');
2023-11-25 13:49:23.403 UTC [148145] ERROR:  XX000: relation ref_to_ref_table is not distributed
2023-11-25 13:49:23.403 UTC [148145] LOCATION:  GetCitusTableCacheEntry, metadata_cache.c:1396
2023-11-25 13:49:23.403 UTC [148145] STATEMENT:  SELECT create_distributed_table('col_with_ref_to_ref', 'a', colocate_with:='ref_to_ref_table');
2023-11-25 13:49:23.403 UTC [148145] ERROR:  XX000: cannot alter table because the table is not distributed
2023-11-25 13:49:23.403 UTC [148145] LOCATION:  AlterDistributedTable, alter_table.c:434
2023-11-25 13:49:23.403 UTC [148145] STATEMENT:  SELECT alter_distributed_table('col_with_ref_to_ref', shard_count:=8, cascade_to_colocated:=true);
2023-11-25 13:49:23.404 UTC [148145] ERROR:  XX000: cannot alter table because the table is not distributed
2023-11-25 13:49:23.404 UTC [148145] LOCATION:  AlterDistributedTable, alter_table.c:434
2023-11-25 13:49:23.404 UTC [148145] STATEMENT:  SELECT alter_distributed_table('col_with_ref_to_ref', shard_count:=10, cascade_to_colocated:=false);
2023-11-25 13:49:23.712 UTC [148145] LOG:  00000: performing blocking isolate_tenant_to_new_shard 
2023-11-25 13:49:23.712 UTC [148145] LOCATION:  SplitShard, shard_split.c:507
2023-11-25 13:49:23.712 UTC [148145] STATEMENT:  SELECT 1 FROM isolate_tenant_to_new_shard('shard_split_table', 5, shard_transfer_mode => 'block_writes');
2023-11-25 13:49:23.713 UTC [148145] LOG:  00000: creating child shards for isolate_tenant_to_new_shard
2023-11-25 13:49:23.713 UTC [148145] LOCATION:  BlockingShardSplit, shard_split.c:571
2023-11-25 13:49:23.713 UTC [148145] STATEMENT:  SELECT 1 FROM isolate_tenant_to_new_shard('shard_split_table', 5, shard_transfer_mode => 'block_writes');
2023-11-25 13:49:23.722 UTC [148145] LOG:  00000: performing copy for isolate_tenant_to_new_shard
2023-11-25 13:49:23.722 UTC [148145] LOCATION:  BlockingShardSplit, shard_split.c:577
2023-11-25 13:49:23.722 UTC [148145] STATEMENT:  SELECT 1 FROM isolate_tenant_to_new_shard('shard_split_table', 5, shard_transfer_mode => 'block_writes');
2023-11-25 13:49:23.722 UTC [148145] LOG:  00000: creating auxillary structures (indexes, stats, replicaindentities, triggers) for isolate_tenant_to_new_shard
2023-11-25 13:49:23.722 UTC [148145] LOCATION:  BlockingShardSplit, shard_split.c:587
2023-11-25 13:49:23.722 UTC [148145] STATEMENT:  SELECT 1 FROM isolate_tenant_to_new_shard('shard_split_table', 5, shard_transfer_mode => 'block_writes');
2023-11-25 13:49:23.722 UTC [148145] LOG:  00000: marking deferred cleanup of source shard(s) for isolate_tenant_to_new_shard
2023-11-25 13:49:23.722 UTC [148145] LOCATION:  BlockingShardSplit, shard_split.c:609
2023-11-25 13:49:23.722 UTC [148145] STATEMENT:  SELECT 1 FROM isolate_tenant_to_new_shard('shard_split_table', 5, shard_transfer_mode => 'block_writes');
2023-11-25 13:49:23.724 UTC [148145] LOG:  00000: creating foreign key constraints (if any) for isolate_tenant_to_new_shard
2023-11-25 13:49:23.724 UTC [148145] LOCATION:  BlockingShardSplit, shard_split.c:625
2023-11-25 13:49:23.724 UTC [148145] STATEMENT:  SELECT 1 FROM isolate_tenant_to_new_shard('shard_split_table', 5, shard_transfer_mode => 'block_writes');
2023-11-25 13:49:23.760 UTC [148145] ERROR:  XX000: you have to specify at least one of the distribution_column, shard_count or colocate_with parameters
2023-11-25 13:49:23.760 UTC [148145] LOCATION:  CheckAlterDistributedTableConversionParameters, alter_table.c:1813
2023-11-25 13:49:23.760 UTC [148145] STATEMENT:  SELECT alter_distributed_table('dist_table');
2023-11-25 13:49:23.761 UTC [148145] ERROR:  XX000: you have to specify at least one of the distribution_column, shard_count or colocate_with parameters
2023-11-25 13:49:23.761 UTC [148145] LOCATION:  CheckAlterDistributedTableConversionParameters, alter_table.c:1813
2023-11-25 13:49:23.761 UTC [148145] STATEMENT:  SELECT alter_distributed_table('dist_table', cascade_to_colocated := false);
2023-11-25 13:49:23.761 UTC [148145] ERROR:  XX000: this call doesn't change any properties of the table
2023-11-25 13:49:23.761 UTC [148145] HINT:  check citus_tables view to see current properties of the table
2023-11-25 13:49:23.761 UTC [148145] LOCATION:  CheckAlterDistributedTableConversionParameters, alter_table.c:1891
2023-11-25 13:49:23.761 UTC [148145] STATEMENT:  SELECT alter_distributed_table('dist_table', distribution_column := 'b');
2023-11-25 13:49:23.761 UTC [148145] ERROR:  XX000: this call doesn't change any properties of the table
2023-11-25 13:49:23.761 UTC [148145] HINT:  check citus_tables view to see current properties of the table
2023-11-25 13:49:23.761 UTC [148145] LOCATION:  CheckAlterDistributedTableConversionParameters, alter_table.c:1891
2023-11-25 13:49:23.761 UTC [148145] STATEMENT:  SELECT alter_distributed_table('dist_table', shard_count := 10);
2023-11-25 13:49:23.781 UTC [148145] ERROR:  XX000: this call doesn't change any properties of the table
2023-11-25 13:49:23.781 UTC [148145] HINT:  check citus_tables view to see current properties of the table
2023-11-25 13:49:23.781 UTC [148145] LOCATION:  CheckAlterDistributedTableConversionParameters, alter_table.c:1891
2023-11-25 13:49:23.781 UTC [148145] STATEMENT:  SELECT alter_distributed_table('dist_table', colocate_with := 'colocation_table');
2023-11-25 13:49:23.828 UTC [148145] ERROR:  XX000: distribution_column cannot be cascaded to colocated tables
2023-11-25 13:49:23.828 UTC [148145] LOCATION:  CheckAlterDistributedTableConversionParameters, alter_table.c:1873
2023-11-25 13:49:23.828 UTC [148145] STATEMENT:  SELECT alter_distributed_table('dist_table', distribution_column := 'b', cascade_to_colocated := true);
2023-11-25 13:49:23.828 UTC [148145] ERROR:  XX000: distribution_column cannot be cascaded to colocated tables
2023-11-25 13:49:23.828 UTC [148145] LOCATION:  CheckAlterDistributedTableConversionParameters, alter_table.c:1873
2023-11-25 13:49:23.828 UTC [148145] STATEMENT:  SELECT alter_distributed_table('dist_table', distribution_column := 'b', shard_count:=12, colocate_with:='colocation_table_2', cascade_to_colocated := true);
2023-11-25 13:49:23.828 UTC [148145] ERROR:  XX000: shard_count or colocate_with is necessary for cascading to colocated tables
2023-11-25 13:49:23.828 UTC [148145] LOCATION:  CheckAlterDistributedTableConversionParameters, alter_table.c:1879
2023-11-25 13:49:23.828 UTC [148145] STATEMENT:  SELECT alter_distributed_table('dist_table', cascade_to_colocated := true);
2023-11-25 13:49:23.828 UTC [148145] ERROR:  XX000: colocate_with := 'none' cannot be cascaded to colocated tables
2023-11-25 13:49:23.828 UTC [148145] LOCATION:  CheckAlterDistributedTableConversionParameters, alter_table.c:1899
2023-11-25 13:49:23.828 UTC [148145] STATEMENT:  SELECT alter_distributed_table('dist_table', colocate_with := 'none', cascade_to_colocated := true);
2023-11-25 13:49:23.828 UTC [148145] ERROR:  XX000: cascade_to_colocated parameter is necessary
2023-11-25 13:49:23.828 UTC [148145] DETAIL:  this table is colocated with some other tables
2023-11-25 13:49:23.828 UTC [148145] HINT:  cascade_to_colocated := false will break the current colocation, cascade_to_colocated := true will change the shard count of colocated tables too.
2023-11-25 13:49:23.828 UTC [148145] LOCATION:  CheckAlterDistributedTableConversionParameters, alter_table.c:1907
2023-11-25 13:49:23.828 UTC [148145] STATEMENT:  SELECT alter_distributed_table('dist_table', shard_count := 14);
2023-11-25 13:49:23.860 UTC [148145] ERROR:  XX000: shard_count cannot be different than the shard count of the table in colocate_with
2023-11-25 13:49:23.860 UTC [148145] HINT:  if no shard_count is specified shard count will be same with colocate_with table's
2023-11-25 13:49:23.860 UTC [148145] LOCATION:  CheckAlterDistributedTableConversionParameters, alter_table.c:1927
2023-11-25 13:49:23.860 UTC [148145] STATEMENT:  SELECT alter_distributed_table('dist_table', colocate_with := 'colocation_table', shard_count := 16);
2023-11-25 13:49:23.867 UTC [148145] ERROR:  XX000: cannot colocate with different_type_table because data type of its distribution column is different than dist_table
2023-11-25 13:49:23.867 UTC [148145] LOCATION:  CheckAlterDistributedTableConversionParameters, alter_table.c:1968
2023-11-25 13:49:23.867 UTC [148145] STATEMENT:  SELECT alter_distributed_table('dist_table', colocate_with := 'different_type_table');
2023-11-25 13:49:23.867 UTC [148145] ERROR:  XX000: cannot colocate with different_type_table and change distribution column to a because data type of column a is different then the distribution column of the different_type_table
2023-11-25 13:49:23.867 UTC [148145] LOCATION:  CheckAlterDistributedTableConversionParameters, alter_table.c:1959
2023-11-25 13:49:23.867 UTC [148145] STATEMENT:  SELECT alter_distributed_table('dist_table', distribution_column := 'a', colocate_with := 'different_type_table');
2023-11-25 13:49:23.867 UTC [148145] ERROR:  XX000: shard_count cannot be 0
2023-11-25 13:49:23.867 UTC [148145] HINT:  if you no longer want this to be a distributed table you can try undistribute_table() function
2023-11-25 13:49:23.867 UTC [148145] LOCATION:  CheckAlterDistributedTableConversionParameters, alter_table.c:1864
2023-11-25 13:49:23.867 UTC [148145] STATEMENT:  SELECT alter_distributed_table('dist_table', shard_count := 0);
2023-11-25 13:49:23.872 UTC [148145] ERROR:  XX000: cannot colocate with reference_table because it is not a distributed table
2023-11-25 13:49:23.872 UTC [148145] LOCATION:  CheckAlterDistributedTableConversionParameters, alter_table.c:1855
2023-11-25 13:49:23.872 UTC [148145] STATEMENT:  SELECT alter_distributed_table('dist_table', colocate_with:='reference_table');
2023-11-25 13:49:23.873 UTC [148145] ERROR:  0A000: relation append_table should be a hash distributed table
2023-11-25 13:49:23.873 UTC [148145] LOCATION:  EnsureHashDistributedTable, metadata_utility.c:2284
2023-11-25 13:49:23.873 UTC [148145] STATEMENT:  SELECT alter_distributed_table('append_table', shard_count:=6);
2023-11-25 13:49:25.969 UTC [148338] ERROR:  0A000: cannot push down subquery on the target list
2023-11-25 13:49:25.969 UTC [148338] DETAIL:  Subqueries in the SELECT part of the query can only be pushed down if they happen before aggregates and window functions
2023-11-25 13:49:25.969 UTC [148338] LOCATION:  MultiLogicalPlanOptimize, multi_logical_optimizer.c:479
2023-11-25 13:49:25.969 UTC [148338] STATEMENT:  /*
	 * Test that we don't get a crash. See #5248.
	 */
	SELECT   subq_3.c15 AS c0,
	         subq_3.c0  AS c1,
	         subq_3.c15 AS c2,
	         subq_0.c1  AS c3,
	         pg_catalog.String_agg( Cast(
	                                      (
	                                      SELECT tgargs
	                                      FROM   pg_catalog.pg_trigger limit 1 offset 1) AS BYTEA), Cast(
	                                                                                                      (
	                                                                                                      SELECT minimum_value
	                                                                                                      FROM   columnar.chunk limit 1 offset 5) AS BYTEA)) OVER (partition BY subq_3.c10 ORDER BY subq_3.c12,subq_0.c2) AS c4,
	         subq_0.c1                                                                                                                                                                                                    AS c5
	FROM     (
	                    SELECT     ref_1.address                      AS c0,
	                               ref_1.error                        AS c1,
	                               sample_0.NAME                      AS c2,
	                               sample_2.trftosql                  AS c3
	                    FROM       pg_catalog.pg_statio_all_sequences AS ref_0
	                    INNER JOIN pg_catalog.pg_hba_file_rules       AS ref_1
	                    ON         ((
	                                                 SELECT pg_catalog.Max(aggnumdirectargs)
	                                                 FROM   pg_catalog.pg_aggregate) <= ref_0.blks_hit)
	                    INNER JOIN countries  AS sample_0 TABLESAMPLE system (6.4)
	                    INNER JOIN local_data AS sample_1 TABLESAMPLE bernoulli (8)
	                    ON         ((
	                                                     true)
	                               OR         (
	                                                     sample_0.NAME IS NOT NULL))
	                    INNER JOIN pg_catalog.pg_transform AS sample_2 TABLESAMPLE bernoulli (1.2)
	                    INNER JOIN pg_catalog.pg_language  AS ref_2
	                    ON         ((
	                                                 SELECT shard_cost_function
	                                                 FROM   pg_catalog.pg_dist_rebalance_strategy limit 1 offset 1) IS NULL)
	                    RIGHT JOIN pg_catalog.pg_index AS sample_3 TABLESAMPLE system (0.3)
	                    ON         ((
	                                                     cast(NULL AS bpchar) ~<=~ cast(NULL AS bpchar))
	                               OR         ((
	                                                                EXISTS
	                                                                (
	                                                                       SELECT sample_3.indnkeyatts        AS c0,
	                                                                              sample_2.trflang            AS c1,
	                                                                              sample_2.trftype            AS c2
	                                                                       FROM   pg_catalog.pg_statistic_ext AS sample_4 TABLESAMPLE bernoulli (8.6)
	                                                                       WHERE  sample_2.trftype IS NOT NULL))
	                                          AND        (
	                                                                false)))
	                    ON         (
	                                          EXISTS
	                                          (
	                                                 SELECT sample_0.id           AS c0,
	                                                        sample_3.indisprimary AS c1
	                                                 FROM   orgs           AS sample_5 TABLESAMPLE system (5.3)
	                                                 WHERE  false))
	                    ON         (
	                                          cast(NULL AS float8) >
	                                          (
	                                                 SELECT pg_catalog.avg(enumsortorder)
	                                                 FROM   pg_catalog.pg_enum) )
	                    WHERE      cast(COALESCE(
	                               CASE
	                                          WHEN ref_1.auth_method ~>=~ ref_1.auth_method THEN cast(NULL AS path)
	                                          ELSE cast(NULL AS path)
	                               END , cast(NULL AS path)) AS path) = cast(NULL AS path)) AS subq_0,
	         lateral
	         (
	                SELECT
	                       (
	                              SELECT pg_catalog.stddev(total_time)
	                              FROM   pg_catalog.pg_stat_user_functions) AS c0,
	                       subq_0.c1                                        AS c1,
	                       subq_2.c0                                        AS c2,
	                       subq_0.c2                                        AS c3,
	                       subq_0.c0                                        AS c4,
	                       cast(COALESCE(subq_2.c0, subq_2.c0) AS text)     AS c5,
	                       subq_2.c0                                        AS c6,
	                       subq_2.c1                                        AS c7,
	                       subq_2.c1                                        AS c8,
	                       subq_2.c1                                        AS c9,
	                       subq_0.c3                                        AS c10,
	                       pg_catalog.pg_stat_get_db_temp_files( cast(
	                                                                   (
	                                                                   SELECT objoid
	                                                                   FROM   pg_catalog.pg_description limit 1 offset 1) AS oid)) AS c11,
	                       subq_0.c3                                                                                               AS c12,
	                       subq_2.c1                                                                                               AS c13,
	                       subq_0.c0                                                                                               AS c14,
	                       subq_0.c3                                                                                               AS c15,
	                       subq_0.c3                                                                                               AS c16,
	                       subq_0.c1                                                                                               AS c17,
	                       subq_0.c2                                                                                               AS c18
	                FROM   (
	                              SELECT subq_1.c2                        AS c0,
	                                     subq_0.c3                        AS c1
	                              FROM   information_schema.element_types AS ref_3,
	                                     lateral
	                                     (
	                                            SELECT subq_0.c1            AS c0,
	                                                   sample_6.info        AS c1,
	                                                   subq_0.c2            AS c2,
	                                                   subq_0.c3            AS c3,
	                                                   ref_3.domain_default AS c4,
	                                                   sample_6.user_id     AS c5,
	                                                   ref_3.collation_name AS c6
	                                            FROM   orders        AS sample_6 TABLESAMPLE system (3.8)
	                                            WHERE  sample_6.price = sample_6.org_id limit 58) AS subq_1
	                              WHERE  (
	                                            subq_1.c2 <= subq_0.c2)
	                              AND    (
	                                            cast(NULL AS line) ?-| cast(NULL AS line)) limit 59) AS subq_2
	                WHERE  cast(COALESCE(pg_catalog.age( cast(
	                                                           (
	                                                           SELECT pg_catalog.max(event_time)
	                                                           FROM   events) AS "timestamp")),
	                       (
	                              SELECT write_lag
	                              FROM   pg_catalog.pg_stat_replication limit 1 offset 3) ) AS "interval") >
	                       (
	                              SELECT utc_offset
	                              FROM   pg_catalog.pg_timezone_names limit 1 offset 4) limit 91) AS subq_3
	WHERE    pg_catalog.pg_backup_stop() > cast(NULL AS record) limit 100;
2023-11-25 13:49:26.307 UTC [148454] ERROR:  22023: relation with OID 31797 does not exist
2023-11-25 13:49:26.307 UTC [148454] LOCATION:  EnsureRelationExists, create_distributed_table.c:969
2023-11-25 13:49:26.307 UTC [148454] STATEMENT:  SELECT undistribute_table('dist_table'), create_distributed_table('dist_table', 'a');
2023-11-25 13:49:26.372 UTC [148454] ERROR:  XX000: cannot complete operation because table referenced_table is referenced by a foreign key
2023-11-25 13:49:26.372 UTC [148454] HINT:  Use cascade option to undistribute all the relations involved in a foreign key relationship with undistribute_table.referenced_table by executing SELECT undistribute_table($$undistribute_table.referenced_table$$, cascade_via_foreign_keys=>true)
2023-11-25 13:49:26.372 UTC [148454] LOCATION:  EnsureTableNotReferenced, alter_table.c:1129
2023-11-25 13:49:26.372 UTC [148454] STATEMENT:  SELECT undistribute_table('referenced_table');
2023-11-25 13:49:26.372 UTC [148454] ERROR:  XX000: cannot complete operation because table referencing_table has a foreign key
2023-11-25 13:49:26.372 UTC [148454] HINT:  Use cascade option to undistribute all the relations involved in a foreign key relationship with undistribute_table.referencing_table by executing SELECT undistribute_table($$undistribute_table.referencing_table$$, cascade_via_foreign_keys=>true)
2023-11-25 13:49:26.372 UTC [148454] LOCATION:  EnsureTableNotReferencing, alter_table.c:1100
2023-11-25 13:49:26.372 UTC [148454] STATEMENT:  SELECT undistribute_table('referencing_table');
2023-11-25 13:49:26.420 UTC [148454] ERROR:  XX000: cannot complete operation because table is a partition
2023-11-25 13:49:26.420 UTC [148454] HINT:  the parent table is "partitioned_table"
2023-11-25 13:49:26.420 UTC [148454] LOCATION:  EnsureTableNotPartition, alter_table.c:1172
2023-11-25 13:49:26.420 UTC [148454] STATEMENT:  SELECT undistribute_table('partitioned_table_1_5');
2023-11-25 13:49:26.516 UTC [148454] ERROR:  XX000: cannot alter table because an extension depends on it
2023-11-25 13:49:26.516 UTC [148454] LOCATION:  ErrorIfUnsupportedCascadeObjects, alter_table.c:1405
2023-11-25 13:49:26.516 UTC [148454] STATEMENT:  SELECT undistribute_table ('extension_table');
2023-11-25 13:49:26.533 UTC [148454] ERROR:  XX000: cannot alter table because an extension depends on it
2023-11-25 13:49:26.533 UTC [148454] LOCATION:  ErrorIfUnsupportedCascadeObjects, alter_table.c:1405
2023-11-25 13:49:26.533 UTC [148454] STATEMENT:  SELECT undistribute_table('rule_table_2');
2023-11-25 13:49:26.753 UTC [148526] WARNING:  01000: Error on node with node id 16: failed to connect to localhost:0
2023-11-25 13:49:26.753 UTC [148526] CONTEXT:  PL/pgSQL function run_command_on_all_nodes(text,boolean,boolean) line 46 at RAISE
2023-11-25 13:49:26.753 UTC [148526] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:49:26.756 UTC [148526] WARNING:  01000: Error on node with node id 40: failed to connect to localhost:0
2023-11-25 13:49:26.756 UTC [148526] CONTEXT:  PL/pgSQL function run_command_on_coordinator(text,boolean) line 46 at RAISE
2023-11-25 13:49:26.756 UTC [148526] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:49:26.854 UTC [148576] ERROR:  P0001: the coordinator is not added to the metadata
2023-11-25 13:49:26.854 UTC [148576] HINT:  Add the node as a coordinator by using: SELECT citus_set_coordinator_host('<hostname>')
2023-11-25 13:49:26.854 UTC [148576] CONTEXT:  PL/pgSQL function run_command_on_coordinator(text,boolean) line 36 at RAISE
2023-11-25 13:49:26.854 UTC [148576] LOCATION:  exec_stmt_raise, pl_exec.c:3903
2023-11-25 13:49:26.854 UTC [148576] STATEMENT:  SELECT success, result FROM run_command_on_coordinator('select inet_server_port()');
2023-11-25 13:49:27.089 UTC [133093] LOG:  00000: received SIGHUP, reloading configuration files
2023-11-25 13:49:27.089 UTC [133093] LOCATION:  SIGHUP_handler, postmaster.c:2769
2023-11-25 13:49:27.089 UTC [133093] LOG:  00000: parameter "citus.background_task_queue_interval" changed to "1s"
2023-11-25 13:49:27.089 UTC [133093] LOCATION:  ProcessConfigFileInternal, guc-file.l:453
2023-11-25 13:49:28.092 UTC [133163] LOG:  00000: found scheduled background tasks, starting new background task queue monitor
2023-11-25 13:49:28.092 UTC [133163] CONTEXT:  Citus maintenance daemon for database 16384 user 10
2023-11-25 13:49:28.092 UTC [133163] LOCATION:  CitusMaintenanceDaemonMain, maintenanced.c:772
2023-11-25 13:49:28.095 UTC [148647] LOG:  00000: task jobid/taskid started: 1450000/1450000
2023-11-25 13:49:28.095 UTC [148647] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:28.095 UTC [148647] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 13:49:28.097 UTC [148647] LOG:  00000: task jobid/taskid succeeded: 1450000/1450000
2023-11-25 13:49:28.097 UTC [148647] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:28.097 UTC [148647] LOCATION:  ConsumeExecutorQueue, background_jobs.c:777
2023-11-25 13:49:29.101 UTC [133163] LOG:  00000: found scheduled background tasks, starting new background task queue monitor
2023-11-25 13:49:29.101 UTC [133163] CONTEXT:  Citus maintenance daemon for database 16384 user 10
2023-11-25 13:49:29.101 UTC [133163] LOCATION:  CitusMaintenanceDaemonMain, maintenanced.c:772
2023-11-25 13:49:29.104 UTC [148649] LOG:  00000: task jobid/taskid started: 1450001/1450001
2023-11-25 13:49:29.104 UTC [148649] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:29.104 UTC [148649] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 13:49:30.097 UTC [148650] ERROR:  57014: canceling statement due to user request
2023-11-25 13:49:30.097 UTC [148650] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450001/1450001)
2023-11-25 13:49:30.097 UTC [148650] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 13:49:30.097 UTC [148649] LOG:  00000: task jobid/taskid is cancelled: 1450001/1450001
2023-11-25 13:49:30.097 UTC [148649] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:30.097 UTC [148649] LOCATION:  TaskConcurrentCancelCheck, background_jobs.c:720
2023-11-25 13:49:30.098 UTC [133093] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450001/1450001)" (PID 148650) exited with exit code 1
2023-11-25 13:49:30.098 UTC [133093] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 13:49:31.102 UTC [133163] LOG:  00000: found scheduled background tasks, starting new background task queue monitor
2023-11-25 13:49:31.102 UTC [133163] CONTEXT:  Citus maintenance daemon for database 16384 user 10
2023-11-25 13:49:31.102 UTC [133163] LOCATION:  CitusMaintenanceDaemonMain, maintenanced.c:772
2023-11-25 13:49:31.104 UTC [148651] LOG:  00000: task jobid/taskid started: 1450002/1450002
2023-11-25 13:49:31.104 UTC [148651] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:31.104 UTC [148651] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 13:49:31.106 UTC [148652] ERROR:  22012: division by zero
2023-11-25 13:49:31.106 UTC [148652] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450002/1450002)
2023-11-25 13:49:31.106 UTC [148652] LOCATION:  int4div, int.c:840
2023-11-25 13:49:31.106 UTC [148651] LOG:  00000: task jobid/taskid failed: 1450002/1450002
2023-11-25 13:49:31.106 UTC [148651] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:31.106 UTC [148651] LOCATION:  ConsumeExecutorQueue, background_jobs.c:770
2023-11-25 13:49:31.107 UTC [133093] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450002/1450002)" (PID 148652) exited with exit code 1
2023-11-25 13:49:31.107 UTC [133093] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 13:49:33.112 UTC [133163] LOG:  00000: found scheduled background tasks, starting new background task queue monitor
2023-11-25 13:49:33.112 UTC [133163] CONTEXT:  Citus maintenance daemon for database 16384 user 10
2023-11-25 13:49:33.112 UTC [133163] LOCATION:  CitusMaintenanceDaemonMain, maintenanced.c:772
2023-11-25 13:49:33.115 UTC [148653] LOG:  00000: task jobid/taskid started: 1450003/1450003
2023-11-25 13:49:33.115 UTC [148653] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:33.115 UTC [148653] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 13:49:33.117 UTC [148653] LOG:  00000: task jobid/taskid succeeded: 1450003/1450003
2023-11-25 13:49:33.117 UTC [148653] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:33.117 UTC [148653] LOCATION:  ConsumeExecutorQueue, background_jobs.c:777
2023-11-25 13:49:33.119 UTC [148653] LOG:  00000: task jobid/taskid started: 1450003/1450004
2023-11-25 13:49:33.119 UTC [148653] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:33.119 UTC [148653] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 13:49:33.121 UTC [148653] LOG:  00000: task jobid/taskid succeeded: 1450003/1450004
2023-11-25 13:49:33.121 UTC [148653] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:33.121 UTC [148653] LOCATION:  ConsumeExecutorQueue, background_jobs.c:777
2023-11-25 13:49:33.123 UTC [148653] LOG:  00000: task jobid/taskid started: 1450003/1450005
2023-11-25 13:49:33.123 UTC [148653] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:33.123 UTC [148653] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 13:49:33.125 UTC [148653] LOG:  00000: task jobid/taskid succeeded: 1450003/1450005
2023-11-25 13:49:33.125 UTC [148653] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:33.125 UTC [148653] LOCATION:  ConsumeExecutorQueue, background_jobs.c:777
2023-11-25 13:49:34.130 UTC [133163] LOG:  00000: found scheduled background tasks, starting new background task queue monitor
2023-11-25 13:49:34.130 UTC [133163] CONTEXT:  Citus maintenance daemon for database 16384 user 10
2023-11-25 13:49:34.130 UTC [133163] LOCATION:  CitusMaintenanceDaemonMain, maintenanced.c:772
2023-11-25 13:49:34.132 UTC [148657] LOG:  00000: task jobid/taskid started: 1450004/1450006
2023-11-25 13:49:34.132 UTC [148657] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:34.132 UTC [148657] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 13:49:34.135 UTC [148657] LOG:  00000: task jobid/taskid succeeded: 1450004/1450006
2023-11-25 13:49:34.135 UTC [148657] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:34.135 UTC [148657] LOCATION:  ConsumeExecutorQueue, background_jobs.c:777
2023-11-25 13:49:34.136 UTC [148657] LOG:  00000: task jobid/taskid started: 1450004/1450007
2023-11-25 13:49:34.136 UTC [148657] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:34.136 UTC [148657] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 13:49:34.138 UTC [148659] ERROR:  22012: division by zero
2023-11-25 13:49:34.138 UTC [148659] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450004/1450007)
2023-11-25 13:49:34.138 UTC [148659] LOCATION:  int4div, int.c:840
2023-11-25 13:49:34.138 UTC [148657] LOG:  00000: task jobid/taskid failed: 1450004/1450007
2023-11-25 13:49:34.138 UTC [148657] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:34.138 UTC [148657] LOCATION:  ConsumeExecutorQueue, background_jobs.c:770
2023-11-25 13:49:34.139 UTC [133093] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450004/1450007)" (PID 148659) exited with exit code 1
2023-11-25 13:49:34.139 UTC [133093] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 13:49:36.144 UTC [133163] LOG:  00000: found scheduled background tasks, starting new background task queue monitor
2023-11-25 13:49:36.144 UTC [133163] CONTEXT:  Citus maintenance daemon for database 16384 user 10
2023-11-25 13:49:36.144 UTC [133163] LOCATION:  CitusMaintenanceDaemonMain, maintenanced.c:772
2023-11-25 13:49:36.147 UTC [148660] LOG:  00000: task jobid/taskid started: 1450005/1450009
2023-11-25 13:49:36.147 UTC [148660] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:36.147 UTC [148660] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 13:49:36.148 UTC [148660] LOG:  00000: task jobid/taskid started: 1450005/1450010
2023-11-25 13:49:36.148 UTC [148660] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:36.148 UTC [148660] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 13:49:36.148 UTC [148660] LOG:  00000: task jobid/taskid started: 1450005/1450011
2023-11-25 13:49:36.148 UTC [148660] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:36.148 UTC [148660] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 13:49:36.149 UTC [148660] LOG:  00000: task jobid/taskid started: 1450006/1450012
2023-11-25 13:49:36.149 UTC [148660] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:36.149 UTC [148660] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 13:49:36.149 UTC [148660] WARNING:  01000: unable to start background worker for background task execution
2023-11-25 13:49:36.149 UTC [148660] DETAIL:  Already reached the maximum number of task executors: 4/4
2023-11-25 13:49:36.149 UTC [148660] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:36.149 UTC [148660] LOCATION:  NewExecutorExceedsCitusLimit, background_jobs.c:491
2023-11-25 13:49:37.084 UTC [148664] ERROR:  57014: canceling statement due to user request
2023-11-25 13:49:37.084 UTC [148664] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450006/1450012)
2023-11-25 13:49:37.084 UTC [148664] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 13:49:37.084 UTC [148660] LOG:  00000: task jobid/taskid is cancelled: 1450006/1450012
2023-11-25 13:49:37.084 UTC [148660] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:37.084 UTC [148660] LOCATION:  TaskConcurrentCancelCheck, background_jobs.c:720
2023-11-25 13:49:37.085 UTC [133093] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450006/1450012)" (PID 148664) exited with exit code 1
2023-11-25 13:49:37.085 UTC [133093] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 13:49:37.086 UTC [148660] LOG:  00000: able to start a background worker with 0 seconds delay
2023-11-25 13:49:37.086 UTC [148660] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:37.086 UTC [148660] LOCATION:  CheckAndResetLastWorkerAllocationFailure, background_jobs.c:679
2023-11-25 13:49:37.086 UTC [148660] LOG:  00000: task jobid/taskid started: 1450007/1450013
2023-11-25 13:49:37.086 UTC [148660] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:37.086 UTC [148660] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 13:49:38.086 UTC [148663] ERROR:  57014: canceling statement due to user request
2023-11-25 13:49:38.086 UTC [148663] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450005/1450011)
2023-11-25 13:49:38.086 UTC [148663] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 13:49:38.086 UTC [148662] ERROR:  57014: canceling statement due to user request
2023-11-25 13:49:38.086 UTC [148662] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450005/1450010)
2023-11-25 13:49:38.086 UTC [148662] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 13:49:38.086 UTC [148661] ERROR:  57014: canceling statement due to user request
2023-11-25 13:49:38.086 UTC [148661] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450005/1450009)
2023-11-25 13:49:38.086 UTC [148661] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 13:49:38.087 UTC [148660] LOG:  00000: task jobid/taskid is cancelled: 1450005/1450011
2023-11-25 13:49:38.087 UTC [148660] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:38.087 UTC [148660] LOCATION:  TaskConcurrentCancelCheck, background_jobs.c:720
2023-11-25 13:49:38.088 UTC [133093] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450005/1450009)" (PID 148661) exited with exit code 1
2023-11-25 13:49:38.088 UTC [133093] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 13:49:38.088 UTC [133093] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450005/1450010)" (PID 148662) exited with exit code 1
2023-11-25 13:49:38.088 UTC [133093] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 13:49:38.088 UTC [133093] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450005/1450011)" (PID 148663) exited with exit code 1
2023-11-25 13:49:38.088 UTC [133093] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 13:49:38.088 UTC [148660] LOG:  00000: task jobid/taskid is cancelled: 1450005/1450009
2023-11-25 13:49:38.088 UTC [148660] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:38.088 UTC [148660] LOCATION:  TaskConcurrentCancelCheck, background_jobs.c:720
2023-11-25 13:49:38.088 UTC [148660] LOG:  00000: task jobid/taskid is cancelled: 1450005/1450010
2023-11-25 13:49:38.088 UTC [148660] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:38.088 UTC [148660] LOCATION:  TaskConcurrentCancelCheck, background_jobs.c:720
2023-11-25 13:49:38.088 UTC [148665] ERROR:  57014: canceling statement due to user request
2023-11-25 13:49:38.088 UTC [148665] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450007/1450013)
2023-11-25 13:49:38.088 UTC [148665] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 13:49:38.088 UTC [148660] LOG:  00000: task jobid/taskid is cancelled: 1450007/1450013
2023-11-25 13:49:38.088 UTC [148660] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:38.088 UTC [148660] LOCATION:  TaskConcurrentCancelCheck, background_jobs.c:720
2023-11-25 13:49:38.090 UTC [133093] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450007/1450013)" (PID 148665) exited with exit code 1
2023-11-25 13:49:38.090 UTC [133093] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 13:49:39.091 UTC [133163] LOG:  00000: found scheduled background tasks, starting new background task queue monitor
2023-11-25 13:49:39.091 UTC [133163] CONTEXT:  Citus maintenance daemon for database 16384 user 10
2023-11-25 13:49:39.091 UTC [133163] LOCATION:  CitusMaintenanceDaemonMain, maintenanced.c:772
2023-11-25 13:49:39.093 UTC [148666] LOG:  00000: task jobid/taskid started: 1450008/1450014
2023-11-25 13:49:39.093 UTC [148666] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:39.093 UTC [148666] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 13:49:39.093 UTC [148666] LOG:  00000: task jobid/taskid started: 1450008/1450015
2023-11-25 13:49:39.093 UTC [148666] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:39.093 UTC [148666] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 13:49:39.094 UTC [148666] LOG:  00000: task jobid/taskid started: 1450008/1450016
2023-11-25 13:49:39.094 UTC [148666] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:39.094 UTC [148666] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 13:49:39.094 UTC [148666] LOG:  00000: task jobid/taskid started: 1450009/1450017
2023-11-25 13:49:39.094 UTC [148666] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:39.094 UTC [148666] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 13:49:39.094 UTC [148666] WARNING:  01000: unable to start background worker for background task execution
2023-11-25 13:49:39.094 UTC [148666] DETAIL:  Already reached the maximum number of task executors: 4/4
2023-11-25 13:49:39.094 UTC [148666] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:39.094 UTC [148666] LOCATION:  NewExecutorExceedsCitusLimit, background_jobs.c:491
2023-11-25 13:49:40.093 UTC [133093] LOG:  00000: received SIGHUP, reloading configuration files
2023-11-25 13:49:40.093 UTC [133093] LOCATION:  SIGHUP_handler, postmaster.c:2769
2023-11-25 13:49:40.093 UTC [133093] LOG:  00000: parameter "citus.max_background_task_executors" changed to "5"
2023-11-25 13:49:40.093 UTC [133093] LOCATION:  ProcessConfigFileInternal, guc-file.l:453
2023-11-25 13:49:40.095 UTC [148666] LOG:  00000: able to start a background worker with 1 seconds delay
2023-11-25 13:49:40.095 UTC [148666] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:40.095 UTC [148666] LOCATION:  CheckAndResetLastWorkerAllocationFailure, background_jobs.c:679
2023-11-25 13:49:40.095 UTC [148666] LOG:  00000: task jobid/taskid started: 1450010/1450018
2023-11-25 13:49:40.095 UTC [148666] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:40.095 UTC [148666] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 13:49:41.096 UTC [148667] ERROR:  57014: canceling statement due to user request
2023-11-25 13:49:41.096 UTC [148667] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450008/1450014)
2023-11-25 13:49:41.096 UTC [148667] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 13:49:41.096 UTC [148668] ERROR:  57014: canceling statement due to user request
2023-11-25 13:49:41.096 UTC [148668] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450008/1450015)
2023-11-25 13:49:41.096 UTC [148668] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 13:49:41.096 UTC [148669] ERROR:  57014: canceling statement due to user request
2023-11-25 13:49:41.096 UTC [148669] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450008/1450016)
2023-11-25 13:49:41.096 UTC [148669] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 13:49:41.096 UTC [148666] LOG:  00000: task jobid/taskid is cancelled: 1450008/1450015
2023-11-25 13:49:41.096 UTC [148666] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:41.096 UTC [148666] LOCATION:  TaskConcurrentCancelCheck, background_jobs.c:720
2023-11-25 13:49:41.097 UTC [133093] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450008/1450014)" (PID 148667) exited with exit code 1
2023-11-25 13:49:41.097 UTC [133093] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 13:49:41.097 UTC [133093] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450008/1450015)" (PID 148668) exited with exit code 1
2023-11-25 13:49:41.097 UTC [133093] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 13:49:41.097 UTC [133093] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450008/1450016)" (PID 148669) exited with exit code 1
2023-11-25 13:49:41.097 UTC [133093] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 13:49:41.098 UTC [148666] LOG:  00000: task jobid/taskid is cancelled: 1450008/1450014
2023-11-25 13:49:41.098 UTC [148666] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:41.098 UTC [148666] LOCATION:  TaskConcurrentCancelCheck, background_jobs.c:720
2023-11-25 13:49:41.098 UTC [148666] LOG:  00000: task jobid/taskid is cancelled: 1450008/1450016
2023-11-25 13:49:41.098 UTC [148666] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:41.098 UTC [148666] LOCATION:  TaskConcurrentCancelCheck, background_jobs.c:720
2023-11-25 13:49:41.098 UTC [148670] ERROR:  57014: canceling statement due to user request
2023-11-25 13:49:41.098 UTC [148670] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450009/1450017)
2023-11-25 13:49:41.098 UTC [148670] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 13:49:41.098 UTC [148666] LOG:  00000: task jobid/taskid is cancelled: 1450009/1450017
2023-11-25 13:49:41.098 UTC [148666] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:41.098 UTC [148666] LOCATION:  TaskConcurrentCancelCheck, background_jobs.c:720
2023-11-25 13:49:41.099 UTC [133093] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450009/1450017)" (PID 148670) exited with exit code 1
2023-11-25 13:49:41.099 UTC [133093] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 13:49:41.099 UTC [148671] ERROR:  57014: canceling statement due to user request
2023-11-25 13:49:41.099 UTC [148671] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450010/1450018)
2023-11-25 13:49:41.099 UTC [148671] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 13:49:41.099 UTC [148666] LOG:  00000: task jobid/taskid is cancelled: 1450010/1450018
2023-11-25 13:49:41.099 UTC [148666] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:41.099 UTC [148666] LOCATION:  TaskConcurrentCancelCheck, background_jobs.c:720
2023-11-25 13:49:41.100 UTC [133093] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450010/1450018)" (PID 148671) exited with exit code 1
2023-11-25 13:49:41.100 UTC [133093] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 13:49:42.100 UTC [133163] LOG:  00000: found scheduled background tasks, starting new background task queue monitor
2023-11-25 13:49:42.100 UTC [133163] CONTEXT:  Citus maintenance daemon for database 16384 user 10
2023-11-25 13:49:42.100 UTC [133163] LOCATION:  CitusMaintenanceDaemonMain, maintenanced.c:772
2023-11-25 13:49:42.102 UTC [148674] LOG:  00000: task jobid/taskid started: 1450011/1450019
2023-11-25 13:49:42.102 UTC [148674] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:42.102 UTC [148674] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 13:49:42.103 UTC [148674] LOG:  00000: task jobid/taskid started: 1450012/1450020
2023-11-25 13:49:42.103 UTC [148674] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:42.103 UTC [148674] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 13:49:43.102 UTC [148674] LOG:  00000: handling termination signal
2023-11-25 13:49:43.102 UTC [148674] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:43.102 UTC [148674] LOCATION:  CitusBackgroundTaskQueueMonitorMain, background_jobs.c:1232
2023-11-25 13:49:43.102 UTC [148676] FATAL:  57P01: terminating background worker "Citus Background Task Queue Executor: regression/postgres for (1450012/1450020)" due to administrator command
2023-11-25 13:49:43.102 UTC [148676] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450012/1450020)
2023-11-25 13:49:43.102 UTC [148676] LOCATION:  ProcessInterrupts, postgres.c:3213
2023-11-25 13:49:43.102 UTC [148675] FATAL:  57P01: terminating background worker "Citus Background Task Queue Executor: regression/postgres for (1450011/1450019)" due to administrator command
2023-11-25 13:49:43.102 UTC [148675] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450011/1450019)
2023-11-25 13:49:43.102 UTC [148675] LOCATION:  ProcessInterrupts, postgres.c:3213
2023-11-25 13:49:43.102 UTC [148674] LOG:  00000: task jobid/taskid failed: 1450012/1450020
2023-11-25 13:49:43.102 UTC [148674] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:43.102 UTC [148674] LOCATION:  ConsumeExecutorQueue, background_jobs.c:770
2023-11-25 13:49:43.104 UTC [133093] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450011/1450019)" (PID 148675) exited with exit code 1
2023-11-25 13:49:43.104 UTC [133093] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 13:49:43.104 UTC [133093] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450012/1450020)" (PID 148676) exited with exit code 1
2023-11-25 13:49:43.104 UTC [133093] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 13:49:43.104 UTC [148674] LOG:  00000: task jobid/taskid failed: 1450011/1450019
2023-11-25 13:49:43.104 UTC [148674] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:43.104 UTC [148674] LOCATION:  ConsumeExecutorQueue, background_jobs.c:770
2023-11-25 13:49:46.111 UTC [133163] LOG:  00000: found scheduled background tasks, starting new background task queue monitor
2023-11-25 13:49:46.111 UTC [133163] CONTEXT:  Citus maintenance daemon for database 16384 user 10
2023-11-25 13:49:46.111 UTC [133163] LOCATION:  CitusMaintenanceDaemonMain, maintenanced.c:772
2023-11-25 13:49:46.113 UTC [148679] LOG:  00000: task jobid/taskid started: 1450013/1450021
2023-11-25 13:49:46.113 UTC [148679] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:46.113 UTC [148679] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 13:49:46.114 UTC [148679] LOG:  00000: task jobid/taskid started: 1450014/1450022
2023-11-25 13:49:46.114 UTC [148679] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:46.114 UTC [148679] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 13:49:47.112 UTC [148679] LOG:  00000: handling cancellation signal
2023-11-25 13:49:47.112 UTC [148679] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:47.112 UTC [148679] LOCATION:  CitusBackgroundTaskQueueMonitorMain, background_jobs.c:1239
2023-11-25 13:49:47.113 UTC [148679] LOG:  00000: task jobid/taskid is cancelled: 1450014/1450022
2023-11-25 13:49:47.113 UTC [148679] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:47.113 UTC [148679] LOCATION:  TaskConcurrentCancelCheck, background_jobs.c:720
2023-11-25 13:49:47.113 UTC [148680] ERROR:  57014: canceling statement due to user request
2023-11-25 13:49:47.113 UTC [148680] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450013/1450021)
2023-11-25 13:49:47.113 UTC [148680] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 13:49:47.113 UTC [148681] ERROR:  57014: canceling statement due to user request
2023-11-25 13:49:47.113 UTC [148681] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450014/1450022)
2023-11-25 13:49:47.113 UTC [148681] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 13:49:47.115 UTC [133093] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450013/1450021)" (PID 148680) exited with exit code 1
2023-11-25 13:49:47.115 UTC [133093] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 13:49:47.115 UTC [133093] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450014/1450022)" (PID 148681) exited with exit code 1
2023-11-25 13:49:47.115 UTC [133093] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 13:49:47.115 UTC [148679] LOG:  00000: task jobid/taskid is cancelled: 1450013/1450021
2023-11-25 13:49:47.115 UTC [148679] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:47.115 UTC [148679] LOCATION:  TaskConcurrentCancelCheck, background_jobs.c:720
2023-11-25 13:49:48.118 UTC [133163] LOG:  00000: found scheduled background tasks, starting new background task queue monitor
2023-11-25 13:49:48.118 UTC [133163] CONTEXT:  Citus maintenance daemon for database 16384 user 10
2023-11-25 13:49:48.118 UTC [133163] LOCATION:  CitusMaintenanceDaemonMain, maintenanced.c:772
2023-11-25 13:49:48.121 UTC [148682] LOG:  00000: task jobid/taskid started: 1450015/1450023
2023-11-25 13:49:48.121 UTC [148682] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:48.121 UTC [148682] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 13:49:48.121 UTC [148682] LOG:  00000: task jobid/taskid started: 1450016/1450024
2023-11-25 13:49:48.121 UTC [148682] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:48.121 UTC [148682] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 13:49:48.123 UTC [148682] LOG:  00000: task jobid/taskid succeeded: 1450016/1450024
2023-11-25 13:49:48.123 UTC [148682] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:48.123 UTC [148682] LOCATION:  ConsumeExecutorQueue, background_jobs.c:777
2023-11-25 13:49:49.118 UTC [148683] ERROR:  57014: canceling statement due to user request
2023-11-25 13:49:49.118 UTC [148683] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450015/1450023)
2023-11-25 13:49:49.118 UTC [148683] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 13:49:49.118 UTC [148682] LOG:  00000: task jobid/taskid is cancelled: 1450015/1450023
2023-11-25 13:49:49.118 UTC [148682] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:49.118 UTC [148682] LOCATION:  TaskConcurrentCancelCheck, background_jobs.c:720
2023-11-25 13:49:49.120 UTC [133093] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450015/1450023)" (PID 148683) exited with exit code 1
2023-11-25 13:49:49.120 UTC [133093] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 13:49:50.123 UTC [133163] LOG:  00000: found scheduled background tasks, starting new background task queue monitor
2023-11-25 13:49:50.123 UTC [133163] CONTEXT:  Citus maintenance daemon for database 16384 user 10
2023-11-25 13:49:50.123 UTC [133163] LOCATION:  CitusMaintenanceDaemonMain, maintenanced.c:772
2023-11-25 13:49:50.125 UTC [148685] LOG:  00000: task jobid/taskid started: 1450017/1450025
2023-11-25 13:49:50.125 UTC [148685] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:50.125 UTC [148685] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 13:49:50.126 UTC [148685] LOG:  00000: task jobid/taskid started: 1450017/1450026
2023-11-25 13:49:50.126 UTC [148685] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:50.126 UTC [148685] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 13:49:52.129 UTC [148685] LOG:  00000: task jobid/taskid succeeded: 1450017/1450025
2023-11-25 13:49:52.129 UTC [148685] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:52.129 UTC [148685] LOCATION:  ConsumeExecutorQueue, background_jobs.c:777
2023-11-25 13:49:52.131 UTC [148685] LOG:  00000: task jobid/taskid started: 1450017/1450027
2023-11-25 13:49:52.131 UTC [148685] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:52.131 UTC [148685] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 13:49:52.131 UTC [148685] LOG:  00000: task jobid/taskid succeeded: 1450017/1450026
2023-11-25 13:49:52.131 UTC [148685] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:52.131 UTC [148685] LOCATION:  ConsumeExecutorQueue, background_jobs.c:777
2023-11-25 13:49:53.101 UTC [133093] LOG:  00000: received SIGHUP, reloading configuration files
2023-11-25 13:49:53.101 UTC [133093] LOCATION:  SIGHUP_handler, postmaster.c:2769
2023-11-25 13:49:53.101 UTC [133093] LOG:  00000: parameter "citus.max_background_task_executors_per_node" changed to "2"
2023-11-25 13:49:53.101 UTC [133093] LOCATION:  ProcessConfigFileInternal, guc-file.l:453
2023-11-25 13:49:53.103 UTC [148685] LOG:  00000: task jobid/taskid started: 1450017/1450028
2023-11-25 13:49:53.103 UTC [148685] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:53.103 UTC [148685] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 13:49:53.103 UTC [148685] LOG:  00000: task jobid/taskid started: 1450017/1450029
2023-11-25 13:49:53.103 UTC [148685] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:53.103 UTC [148685] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 13:49:56.135 UTC [148685] LOG:  00000: task jobid/taskid succeeded: 1450017/1450027
2023-11-25 13:49:56.135 UTC [148685] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:56.135 UTC [148685] LOCATION:  ConsumeExecutorQueue, background_jobs.c:777
2023-11-25 13:49:56.137 UTC [148685] LOG:  00000: task jobid/taskid started: 1450017/1450030
2023-11-25 13:49:56.137 UTC [148685] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:56.137 UTC [148685] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 13:49:57.109 UTC [148685] LOG:  00000: task jobid/taskid succeeded: 1450017/1450028
2023-11-25 13:49:57.109 UTC [148685] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:57.109 UTC [148685] LOCATION:  ConsumeExecutorQueue, background_jobs.c:777
2023-11-25 13:49:57.111 UTC [148685] LOG:  00000: task jobid/taskid started: 1450017/1450031
2023-11-25 13:49:57.111 UTC [148685] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:57.111 UTC [148685] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 13:49:57.111 UTC [148685] LOG:  00000: task jobid/taskid succeeded: 1450017/1450029
2023-11-25 13:49:57.111 UTC [148685] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:57.111 UTC [148685] LOCATION:  ConsumeExecutorQueue, background_jobs.c:777
2023-11-25 13:49:57.114 UTC [133093] LOG:  00000: received SIGHUP, reloading configuration files
2023-11-25 13:49:57.114 UTC [133093] LOCATION:  SIGHUP_handler, postmaster.c:2769
2023-11-25 13:49:57.114 UTC [133093] LOG:  00000: parameter "citus.max_background_task_executors_per_node" changed to "3"
2023-11-25 13:49:57.114 UTC [133093] LOCATION:  ProcessConfigFileInternal, guc-file.l:453
2023-11-25 13:49:57.115 UTC [148685] LOG:  00000: task jobid/taskid started: 1450017/1450032
2023-11-25 13:49:57.115 UTC [148685] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:57.115 UTC [148685] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 13:49:58.117 UTC [133093] LOG:  00000: received SIGHUP, reloading configuration files
2023-11-25 13:49:58.117 UTC [133093] LOCATION:  SIGHUP_handler, postmaster.c:2769
2023-11-25 13:49:58.117 UTC [148691] ERROR:  57014: canceling statement due to user request
2023-11-25 13:49:58.117 UTC [148691] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450017/1450030)
2023-11-25 13:49:58.117 UTC [148691] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 13:49:58.117 UTC [148685] LOG:  00000: task jobid/taskid failed: 1450017/1450030
2023-11-25 13:49:58.117 UTC [148685] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:49:58.117 UTC [148685] LOCATION:  ConsumeExecutorQueue, background_jobs.c:770
2023-11-25 13:49:58.117 UTC [133093] LOG:  00000: parameter "citus.max_background_task_executors_per_node" removed from configuration file, reset to default
2023-11-25 13:49:58.117 UTC [133093] LOCATION:  ProcessConfigFileInternal, guc-file.l:388
2023-11-25 13:49:58.119 UTC [133093] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450017/1450030)" (PID 148691) exited with exit code 1
2023-11-25 13:49:58.119 UTC [133093] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 13:50:03.120 UTC [148685] LOG:  00000: task jobid/taskid succeeded: 1450017/1450031
2023-11-25 13:50:03.120 UTC [148685] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:50:03.120 UTC [148685] LOCATION:  ConsumeExecutorQueue, background_jobs.c:777
2023-11-25 13:50:03.123 UTC [148685] LOG:  00000: task jobid/taskid succeeded: 1450017/1450032
2023-11-25 13:50:03.123 UTC [148685] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:50:03.123 UTC [148685] LOCATION:  ConsumeExecutorQueue, background_jobs.c:777
2023-11-25 13:50:03.125 UTC [148685] LOG:  00000: task jobid/taskid started: 1450017/1450030
2023-11-25 13:50:03.125 UTC [148685] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:50:03.125 UTC [148685] LOCATION:  AssignRunnableTaskToNewExecutor, background_jobs.c:603
2023-11-25 13:50:04.126 UTC [148698] ERROR:  57014: canceling statement due to user request
2023-11-25 13:50:04.126 UTC [148698] CONTEXT:  Citus Background Task Queue Executor: regression/postgres for (1450017/1450030)
2023-11-25 13:50:04.126 UTC [148698] LOCATION:  ProcessInterrupts, postgres.c:3356
2023-11-25 13:50:04.126 UTC [148685] LOG:  00000: task jobid/taskid is cancelled: 1450017/1450030
2023-11-25 13:50:04.126 UTC [148685] CONTEXT:  Citus Background Task Queue Monitor: regression
2023-11-25 13:50:04.126 UTC [148685] LOCATION:  TaskConcurrentCancelCheck, background_jobs.c:720
2023-11-25 13:50:04.128 UTC [133093] LOG:  00000: background worker "Citus Background Task Queue Executor: regression/postgres for (1450017/1450030)" (PID 148698) exited with exit code 1
2023-11-25 13:50:04.128 UTC [133093] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 13:50:05.128 UTC [133093] LOG:  00000: received SIGHUP, reloading configuration files
2023-11-25 13:50:05.128 UTC [133093] LOCATION:  SIGHUP_handler, postmaster.c:2769
2023-11-25 13:50:05.137 UTC [133093] LOG:  00000: received SIGHUP, reloading configuration files
2023-11-25 13:50:05.137 UTC [133093] LOCATION:  SIGHUP_handler, postmaster.c:2769
2023-11-25 13:50:05.137 UTC [133093] LOG:  00000: parameter "citus.background_task_queue_interval" removed from configuration file, reset to default
2023-11-25 13:50:05.137 UTC [133093] LOCATION:  ProcessConfigFileInternal, guc-file.l:388
2023-11-25 13:50:05.137 UTC [133093] LOG:  00000: parameter "citus.max_background_task_executors" removed from configuration file, reset to default
2023-11-25 13:50:05.137 UTC [133093] LOCATION:  ProcessConfigFileInternal, guc-file.l:388
2023-11-25 13:50:05.251 UTC [148713] ERROR:  22P02: invalid input syntax for type cluster_clock: "(-1, 100)" at character 39
2023-11-25 13:50:05.251 UTC [148713] LOCATION:  cluster_clock_in_internal, type_utils.c:71
2023-11-25 13:50:05.251 UTC [148713] STATEMENT:  INSERT INTO cluster_clock_type values('(-1, 100)');
2023-11-25 13:50:05.251 UTC [148713] ERROR:  22P02: invalid input syntax for type cluster_clock: "(100, -1)" at character 39
2023-11-25 13:50:05.251 UTC [148713] LOCATION:  cluster_clock_in_internal, type_utils.c:82
2023-11-25 13:50:05.251 UTC [148713] STATEMENT:  INSERT INTO cluster_clock_type values('(100, -1)');
2023-11-25 13:50:05.251 UTC [148713] ERROR:  22P02: invalid input syntax for type cluster_clock: "(4398046511104, 100)" at character 39
2023-11-25 13:50:05.251 UTC [148713] LOCATION:  cluster_clock_in_internal, type_utils.c:71
2023-11-25 13:50:05.251 UTC [148713] STATEMENT:  INSERT INTO cluster_clock_type values('(4398046511104, 100)');
2023-11-25 13:50:05.251 UTC [148713] ERROR:  22P02: invalid input syntax for type cluster_clock: "(0, 4194304)" at character 39
2023-11-25 13:50:05.251 UTC [148713] LOCATION:  cluster_clock_in_internal, type_utils.c:82
2023-11-25 13:50:05.251 UTC [148713] STATEMENT:  INSERT INTO cluster_clock_type values('(0, 4194304)');
2023-11-25 13:50:05.254 UTC [148713] ERROR:  23505: duplicate key value violates unique constraint "cluster_clock_type_cc_key"
2023-11-25 13:50:05.254 UTC [148713] DETAIL:  Key (cc)=((100,1)) already exists.
2023-11-25 13:50:05.254 UTC [148713] LOCATION:  _bt_check_unique, nbtinsert.c:664
2023-11-25 13:50:05.254 UTC [148713] STATEMENT:  INSERT INTO cluster_clock_type values('(100, 1)');
2023-11-25 13:50:05.254 UTC [148713] ERROR:  23505: duplicate key value violates unique constraint "cluster_clock_type_cc_key"
2023-11-25 13:50:05.254 UTC [148713] DETAIL:  Key (cc)=((100,200)) already exists.
2023-11-25 13:50:05.254 UTC [148713] LOCATION:  _bt_check_unique, nbtinsert.c:664
2023-11-25 13:50:05.254 UTC [148713] STATEMENT:  INSERT INTO cluster_clock_type values('(100, 200)');
2023-11-25 13:50:05.254 UTC [148713] ERROR:  23505: duplicate key value violates unique constraint "cluster_clock_type_cc_key"
2023-11-25 13:50:05.254 UTC [148713] DETAIL:  Key (cc)=((100,100)) already exists.
2023-11-25 13:50:05.254 UTC [148713] LOCATION:  _bt_check_unique, nbtinsert.c:664
2023-11-25 13:50:05.254 UTC [148713] STATEMENT:  INSERT INTO cluster_clock_type values('(100, 100)');
2023-11-25 13:50:05.370 UTC [148713] WARNING:  01000: GUC enable_cluster_clock is off
2023-11-25 13:50:05.370 UTC [148713] LOCATION:  PrepareAndSetTransactionClock, causal_clock.c:419
2023-11-25 13:50:05.372 UTC [148713] ERROR:  42501: permission denied for sequence pg_dist_clock_logical_seq
2023-11-25 13:50:05.372 UTC [148713] LOCATION:  do_setval, sequence.c:967
2023-11-25 13:50:05.372 UTC [148713] STATEMENT:  SELECT setval('pg_dist_clock_logical_seq', 100, true);
2023-11-25 13:50:05.483 UTC [148757] LOG:  00000: deferred drop of orphaned resource alter_distributed_table.shard_split_table_362667 on localhost:57637 completed
2023-11-25 13:50:05.483 UTC [148757] LOCATION:  DropOrphanedResourcesForCleanup, shard_cleaner.c:298
2023-11-25 13:50:05.483 UTC [148757] STATEMENT:  CALL citus_cleanup_orphaned_resources()
2023-11-25 13:50:05.952 UTC [148751] WARNING:  0A000: "view pg_source_view" has dependency to "table pg_source" that is not in Citus' metadata
2023-11-25 13:50:05.952 UTC [148751] DETAIL:  "view pg_source_view" will be created only locally
2023-11-25 13:50:05.952 UTC [148751] HINT:  Distribute "table pg_source" first to distribute "view pg_source_view"
2023-11-25 13:50:05.952 UTC [148751] LOCATION:  DeferErrorIfHasUnsupportedDependency, dependency.c:950
2023-11-25 13:50:06.111 UTC [148751] ERROR:  0A000: non-IMMUTABLE functions are not yet supported in MERGE sql with distributed tables 
2023-11-25 13:50:06.111 UTC [148751] LOCATION:  MergeQuerySupported, merge_planner.c:134
2023-11-25 13:50:06.111 UTC [148751] STATEMENT:  MERGE INTO target_serial sda
	USING source_serial sdn
	ON sda.id = sdn.id
	WHEN NOT matched THEN
	       INSERT (id, z) VALUES (id, z);
2023-11-25 13:50:06.124 UTC [148751] ERROR:  0A000: cannot assign to system column "ctid" at character 110
2023-11-25 13:50:06.124 UTC [148751] LOCATION:  transformAssignedExpr, parse_target.c:480
2023-11-25 13:50:06.124 UTC [148751] STATEMENT:  MERGE INTO target_set
	USING source_set AS foo ON target_set.t1 = foo.s1
	WHEN MATCHED THEN
	        UPDATE SET ctid = '(0,100)';
2023-11-25 13:50:06.124 UTC [148751] ERROR:  0A000: cannot pushdown the subquery since not all subqueries in the UNION have the partition column in the same position
2023-11-25 13:50:06.124 UTC [148751] DETAIL:  Each leaf query of the UNION should return the partition column in the same position and all joins must be on the partition column
2023-11-25 13:50:06.124 UTC [148751] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:582
2023-11-25 13:50:06.124 UTC [148751] STATEMENT:  MERGE INTO target_set
	USING (SELECT s1,s2 FROM source_set UNION SELECT s2,s1 FROM source_set) AS foo ON target_set.t1 = foo.s1
	WHEN MATCHED THEN
	        UPDATE SET t2 = t2 + 1;
2023-11-25 13:50:06.124 UTC [148751] ERROR:  0A000: cannot push down this subquery
2023-11-25 13:50:06.124 UTC [148751] DETAIL:  Limit clause is currently unsupported when a subquery references a column from another query
2023-11-25 13:50:06.124 UTC [148751] LOCATION:  DeferErrorIfSubqueryRequiresMerge, query_pushdown_planning.c:1156
2023-11-25 13:50:06.124 UTC [148751] STATEMENT:  MERGE INTO target_set
	USING (SELECT 2 as s3, source_set.* FROM (SELECT * FROM source_set LIMIT 1) as foo LEFT JOIN source_set USING( s1)) AS foo
	ON target_set.t1 = foo.s1
	WHEN MATCHED THEN UPDATE SET t2 = t2 + 1
	WHEN NOT MATCHED THEN INSERT VALUES(s1, s3);
2023-11-25 13:50:06.124 UTC [148751] ERROR:  0A000: MERGE command is only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:50:06.124 UTC [148751] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:50:06.124 UTC [148751] STATEMENT:  EXPLAIN
	WITH cte_1 AS (DELETE FROM target_json)
	MERGE INTO target_json sda
	USING source_json sdn
	ON sda.id = sdn.id
	WHEN NOT matched THEN
		INSERT (id, z) VALUES (sdn.id, 5);
2023-11-25 13:50:06.125 UTC [148751] ERROR:  0A000: cannot push down this subquery
2023-11-25 13:50:06.125 UTC [148751] DETAIL:  could not run distributed query with GROUPING SETS, CUBE, or ROLLUP
2023-11-25 13:50:06.125 UTC [148751] LOCATION:  DeferErrorIfCannotPushdownSubquery, query_pushdown_planning.c:1048
2023-11-25 13:50:06.125 UTC [148751] STATEMENT:  MERGE INTO citus_target t
	USING (SELECT count(*), id FROM citus_source GROUP BY GROUPING SETS (id, val)) subq
	ON subq.id = t.id
	WHEN MATCHED AND t.id > 350 THEN
	    UPDATE SET val = t.val || 'Updated'
	WHEN NOT MATCHED THEN
	        INSERT VALUES (subq.id, 99)
	WHEN MATCHED AND t.id < 350 THEN
	        DELETE;
2023-11-25 13:50:06.125 UTC [148751] ERROR:  0A000: cannot push down this subquery
2023-11-25 13:50:06.125 UTC [148751] DETAIL:  could not run distributed query with GROUPING SETS, CUBE, or ROLLUP
2023-11-25 13:50:06.125 UTC [148751] LOCATION:  DeferErrorIfCannotPushdownSubquery, query_pushdown_planning.c:1048
2023-11-25 13:50:06.125 UTC [148751] STATEMENT:  WITH subq AS
	(
	SELECT count(*), id FROM citus_source GROUP BY GROUPING SETS (id, val)
	)
	MERGE INTO citus_target t
	USING subq
	ON subq.id = t.id
	WHEN MATCHED AND t.id > 350 THEN
	    UPDATE SET val = t.val || 'Updated'
	WHEN NOT MATCHED THEN
	        INSERT VALUES (subq.id, 99)
	WHEN MATCHED AND t.id < 350 THEN
	        DELETE;
2023-11-25 13:50:06.125 UTC [148751] ERROR:  0A000: cannot perform MERGE INSERT with DEFAULTS
2023-11-25 13:50:06.125 UTC [148751] LOCATION:  InsertDistributionColumnMatchesSource, merge_planner.c:546
2023-11-25 13:50:06.125 UTC [148751] STATEMENT:  MERGE INTO citus_target t
	USING citus_source s
	ON t.id = s.id
	WHEN NOT MATCHED THEN
	  INSERT DEFAULT VALUES;
2023-11-25 13:50:06.125 UTC [148751] ERROR:  0A000: MERGE INSERT must refer a source column for distribution column 
2023-11-25 13:50:06.125 UTC [148751] LOCATION:  InsertDistributionColumnMatchesSource, merge_planner.c:583
2023-11-25 13:50:06.125 UTC [148751] STATEMENT:  MERGE INTO citus_target t
	USING citus_source s
	ON t.id = s.id
	WHEN NOT MATCHED THEN
	  INSERT VALUES(10000);
2023-11-25 13:50:06.125 UTC [148751] ERROR:  0A000: MERGE INSERT must refer a source column for distribution column 
2023-11-25 13:50:06.125 UTC [148751] LOCATION:  InsertDistributionColumnMatchesSource, merge_planner.c:583
2023-11-25 13:50:06.125 UTC [148751] STATEMENT:  MERGE INTO citus_target t
	USING citus_source s
	ON t.id = s.id
	WHEN NOT MATCHED THEN
	  INSERT (id) VALUES(1000);
2023-11-25 13:50:06.125 UTC [148751] ERROR:  0A000: MERGE INSERT must use the source table distribution column value
2023-11-25 13:50:06.125 UTC [148751] LOCATION:  InsertDistributionColumnMatchesSource, merge_planner.c:575
2023-11-25 13:50:06.125 UTC [148751] STATEMENT:  MERGE INTO t1 t
	USING s1 s
	ON t.id = s.id
	WHEN NOT MATCHED THEN
	  INSERT (id) VALUES(s.val);
2023-11-25 13:50:06.125 UTC [148751] ERROR:  0A000: MERGE INSERT must have distribution column as value
2023-11-25 13:50:06.125 UTC [148751] LOCATION:  InsertDistributionColumnMatchesSource, merge_planner.c:592
2023-11-25 13:50:06.125 UTC [148751] STATEMENT:  MERGE INTO t1 t
	USING s1 s
	ON t.id = s.id
	WHEN NOT MATCHED THEN
	  INSERT (val) VALUES(s.val);
2023-11-25 13:50:06.126 UTC [148751] ERROR:  0A000: updating the distribution column is not allowed in MERGE actions
2023-11-25 13:50:06.126 UTC [148751] LOCATION:  MergeQualAndTargetListFunctionsSupported, merge_planner.c:651
2023-11-25 13:50:06.126 UTC [148751] STATEMENT:  MERGE INTO target_cj t
	  USING source_cj1 s
	  ON t.tid = s.sid1 AND t.tid = 2
	  WHEN MATCHED THEN
	    UPDATE SET tid = tid + 9, src = src || ' updated by merge'
	  WHEN NOT MATCHED THEN
	    INSERT VALUES (sid1, 'inserted by merge', val1);
2023-11-25 13:50:06.126 UTC [148751] ERROR:  0A000: cannot execute MERGE on relation "foreign_table"
2023-11-25 13:50:06.126 UTC [148751] DETAIL:  This operation is not supported for foreign tables.
2023-11-25 13:50:06.126 UTC [148751] LOCATION:  transformMergeStmt, parse_merge.c:180
2023-11-25 13:50:06.126 UTC [148751] STATEMENT:  MERGE INTO foreign_table
		USING ft_target ON (foreign_table.id = ft_target.id)
		WHEN MATCHED THEN
			DELETE
		WHEN NOT MATCHED THEN
			INSERT (id, user_val) VALUES (ft_target.id, ft_target.user_val);
2023-11-25 13:50:06.146 UTC [148751] ERROR:  0A000: MERGE command is not supported with combination of distributed/local tables yet
2023-11-25 13:50:06.146 UTC [148751] LOCATION:  ErrorIfMergeHasUnsupportedTables, merge_planner.c:456
2023-11-25 13:50:06.146 UTC [148751] STATEMENT:  MERGE INTO t1
		USING s1 ON (s1.id = t1.val) -- val is not a distribution column
		WHEN MATCHED AND s1.val = 0 THEN
			DELETE
		WHEN MATCHED THEN
			UPDATE SET val = t1.val + 1
		WHEN NOT MATCHED THEN
			INSERT (id, val) VALUES (s1.id, s1.val);
2023-11-25 13:50:06.162 UTC [148751] ERROR:  0A000: non-IMMUTABLE functions are not yet supported in MERGE sql with distributed tables 
2023-11-25 13:50:06.162 UTC [148751] LOCATION:  MergeQuerySupported, merge_planner.c:134
2023-11-25 13:50:06.162 UTC [148751] STATEMENT:  MERGE INTO t1 USING (SELECT * FROM s1 WHERE true) s1 ON
	  t1.id = s1.id AND s1.id = 2
	   WHEN matched THEN
	 UPDATE SET id = s1.id, val = random();
2023-11-25 13:50:06.163 UTC [148751] ERROR:  0A000: non-IMMUTABLE functions are not yet supported in MERGE sql with distributed tables 
2023-11-25 13:50:06.163 UTC [148751] LOCATION:  MergeQuerySupported, merge_planner.c:134
2023-11-25 13:50:06.163 UTC [148751] STATEMENT:  MERGE INTO t1
	USING s1 ON t1.id = s1.id
	WHEN NOT MATCHED THEN
		INSERT VALUES(s1.id, add_s(s1.val, 2));
2023-11-25 13:50:06.163 UTC [148751] ERROR:  0A000: non-IMMUTABLE functions are not yet supported in MERGE sql with distributed tables 
2023-11-25 13:50:06.163 UTC [148751] LOCATION:  MergeQuerySupported, merge_planner.c:134
2023-11-25 13:50:06.163 UTC [148751] STATEMENT:  MERGE INTO t1
	USING s1 ON t1.id = s1.id AND t1.id = 2 AND (merge_when_and_write())
	WHEN MATCHED THEN
	        UPDATE SET val = t1.val + s1.val;
2023-11-25 13:50:06.164 UTC [148751] ERROR:  0A000: non-IMMUTABLE functions are not yet supported in MERGE sql with distributed tables 
2023-11-25 13:50:06.164 UTC [148751] LOCATION:  MergeQuerySupported, merge_planner.c:134
2023-11-25 13:50:06.164 UTC [148751] STATEMENT:  MERGE INTO t1
	USING s1 ON t1.id = s1.id AND t1.id = 2
	WHEN MATCHED AND (merge_when_and_write()) THEN
	        UPDATE SET val = t1.val + s1.val;
2023-11-25 13:50:06.164 UTC [148751] ERROR:  0A000: MERGE command is only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:50:06.164 UTC [148751] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:50:06.164 UTC [148751] STATEMENT:  MERGE INTO t1
		USING (SELECT * FROM s1) sub ON (sub.val = t1.id) -- sub.val is not a distribution column
		WHEN MATCHED AND sub.val = 0 THEN
			DELETE
		WHEN MATCHED THEN
			UPDATE SET val = t1.val + 1
		WHEN NOT MATCHED THEN
			INSERT (id, val) VALUES (sub.id, sub.val);
2023-11-25 13:50:06.164 UTC [148751] ERROR:  0A000: MERGE command is only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:50:06.164 UTC [148751] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:50:06.164 UTC [148751] STATEMENT:  WITH s1_res AS (
		SELECT * FROM s1
	)
	MERGE INTO t1
		USING s1_res ON (s1_res.val = t1.id)
		WHEN MATCHED AND s1_res.val = 0 THEN
			DELETE
		WHEN MATCHED THEN
			UPDATE SET val = t1.val + 1
		WHEN NOT MATCHED THEN
			INSERT (id, val) VALUES (s1_res.id, s1_res.val);
2023-11-25 13:50:06.164 UTC [148751] ERROR:  0A000: MERGE command is only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:50:06.164 UTC [148751] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:50:06.164 UTC [148751] STATEMENT:  WITH s1_res AS (
		SELECT * FROM s1
	)
	MERGE INTO t1
		USING s1_res ON (TRUE)
		WHEN MATCHED AND s1_res.val = 0 THEN
			DELETE
		WHEN MATCHED THEN
			UPDATE SET val = t1.val + 1
		WHEN NOT MATCHED THEN
			INSERT (id, val) VALUES (s1_res.id, s1_res.val);
2023-11-25 13:50:06.164 UTC [148751] ERROR:  0A000: MERGE command is only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:50:06.164 UTC [148751] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:50:06.164 UTC [148751] STATEMENT:  WITH s1_res AS (
	     SELECT * FROM s1
	 )
	 MERGE INTO t1 USING s1_res ON (s1_res.id = t1.val)
	 WHEN MATCHED THEN DELETE
		WHEN NOT MATCHED THEN
			INSERT (id, val) VALUES (s1_res.id, s1_res.val);
2023-11-25 13:50:06.186 UTC [148751] ERROR:  0A000: MERGE command is not supported on reference tables yet
2023-11-25 13:50:06.186 UTC [148751] LOCATION:  CheckIfRTETypeIsUnsupported, merge_planner.c:330
2023-11-25 13:50:06.186 UTC [148751] STATEMENT:  MERGE INTO t1
		USING s1 ON (s1.id = t1.id)
		WHEN MATCHED AND s1.val = 0 THEN
			DELETE
		WHEN MATCHED THEN
			UPDATE SET val = t1.val + 1
		WHEN NOT MATCHED THEN
			INSERT (id, val) VALUES (s1.id, s1.val);
2023-11-25 13:50:06.204 UTC [148751] ERROR:  0A000: MERGE command is not supported with combination of distributed/local tables yet
2023-11-25 13:50:06.204 UTC [148751] LOCATION:  ErrorIfMergeHasUnsupportedTables, merge_planner.c:456
2023-11-25 13:50:06.204 UTC [148751] STATEMENT:  MERGE INTO t1
		USING s1 ON (s1.id = t1.id)
		WHEN MATCHED AND s1.val = 0 THEN
			DELETE
		WHEN MATCHED THEN
			UPDATE SET val = t1.val + 1
		WHEN NOT MATCHED THEN
			INSERT (id, val) VALUES (s1.id, s1.val);
2023-11-25 13:50:06.204 UTC [148751] ERROR:  0A000: MERGE command is not supported with combination of distributed/local tables yet
2023-11-25 13:50:06.204 UTC [148751] LOCATION:  ErrorIfMergeHasUnsupportedTables, merge_planner.c:456
2023-11-25 13:50:06.204 UTC [148751] STATEMENT:  MERGE INTO t1
		USING (SELECT * FROM s1) sub ON (sub.id = t1.id)
		WHEN MATCHED AND sub.val = 0 THEN
			DELETE
		WHEN MATCHED THEN
			UPDATE SET val = t1.val + 1
		WHEN NOT MATCHED THEN
			INSERT (id, val) VALUES (sub.id, sub.val);
2023-11-25 13:50:06.214 UTC [148751] ERROR:  0A000: MERGE command is not supported with combination of distributed/local tables yet
2023-11-25 13:50:06.214 UTC [148751] LOCATION:  ErrorIfMergeHasUnsupportedTables, merge_planner.c:456
2023-11-25 13:50:06.214 UTC [148751] STATEMENT:  MERGE INTO t1
		USING (SELECT s1.id, pg.val FROM s1, pg) sub ON (sub.id = t1.id)
		WHEN MATCHED AND sub.val = 0 THEN
			DELETE
		WHEN MATCHED THEN
			UPDATE SET val = t1.val + 1
		WHEN NOT MATCHED THEN
			INSERT (id, val) VALUES (sub.id, sub.val);
2023-11-25 13:50:06.215 UTC [148751] ERROR:  0A000: MERGE command is not supported with combination of distributed/local tables yet
2023-11-25 13:50:06.215 UTC [148751] LOCATION:  ErrorIfMergeHasUnsupportedTables, merge_planner.c:456
2023-11-25 13:50:06.215 UTC [148751] STATEMENT:  WITH pg_res AS (
		SELECT * FROM pg
	)
	MERGE INTO t1
		USING (SELECT s1.id, pg_res.val FROM s1, pg_res) sub ON (sub.id = t1.id)
		WHEN MATCHED AND sub.val = 0 THEN
			DELETE
		WHEN MATCHED THEN
			UPDATE SET val = t1.val + 1
		WHEN NOT MATCHED THEN
			INSERT (id, val) VALUES (sub.id, sub.val);
2023-11-25 13:50:06.230 UTC [148751] ERROR:  21000: MERGE command cannot affect row a second time
2023-11-25 13:50:06.230 UTC [148751] HINT:  Ensure that not more than one source row matches any one target row.
2023-11-25 13:50:06.230 UTC [148751] LOCATION:  ExecMergeMatched, nodeModifyTable.c:2932
2023-11-25 13:50:06.230 UTC [148751] STATEMENT:  MERGE INTO t1
		USING s1 ON (s1.id = t1.id)
		WHEN MATCHED AND s1.val = 0 THEN
			DELETE
		WHEN MATCHED THEN
			UPDATE SET val = t1.val + 1
		WHEN NOT MATCHED THEN
			INSERT (id, val) VALUES (s1.id, s1.val);
2023-11-25 13:50:06.231 UTC [148751] ERROR:  0A000: cannot execute MERGE on relation "mv_source"
2023-11-25 13:50:06.231 UTC [148751] DETAIL:  This operation is not supported for materialized views.
2023-11-25 13:50:06.231 UTC [148751] LOCATION:  transformMergeStmt, parse_merge.c:180
2023-11-25 13:50:06.231 UTC [148751] STATEMENT:  MERGE INTO mv_source
	USING mv_target
	ON mv_source.id = mv_target.id
	WHEN MATCHED THEN
	    DO NOTHING
	WHEN NOT MATCHED THEN
	    INSERT VALUES(mv_source.id, mv_source.val);
2023-11-25 13:50:06.243 UTC [148751] ERROR:  0A000: For MERGE command, all the distributed tables must be colocated
2023-11-25 13:50:06.243 UTC [148751] LOCATION:  ErrorIfDistTablesNotColocated, merge_planner.c:277
2023-11-25 13:50:06.243 UTC [148751] STATEMENT:  MERGE INTO dist_target
	USING dist_source
	ON dist_target.id = dist_source.id
	WHEN MATCHED THEN
	UPDATE SET val = dist_source.val
	WHEN NOT MATCHED THEN
	INSERT VALUES(dist_source.id, dist_source.val);
2023-11-25 13:50:06.248 UTC [148751] ERROR:  0A000: MERGE command is only supported when all distributed tables are co-located and joined on their distribution columns
2023-11-25 13:50:06.248 UTC [148751] LOCATION:  DeferErrorIfUnsupportedSubqueryPushdown, query_pushdown_planning.c:602
2023-11-25 13:50:06.248 UTC [148751] STATEMENT:  MERGE INTO dist_target
	USING dist_colocated
	ON dist_target.id = dist_colocated.val -- val is not the distribution column
	WHEN MATCHED THEN
	UPDATE SET val = dist_colocated.val
	WHEN NOT MATCHED THEN
	INSERT VALUES(dist_colocated.id, dist_colocated.val);
2023-11-25 13:50:06.248 UTC [148751] ERROR:  0A000: For MERGE command, both the source and target must be distributed
2023-11-25 13:50:06.248 UTC [148751] LOCATION:  ErrorIfDistTablesNotColocated, merge_planner.c:269
2023-11-25 13:50:06.248 UTC [148751] STATEMENT:  MERGE INTO dist_target
	USING (SELECT 100 id) AS source
	ON dist_target.id = source.id AND dist_target.val = 'const'
	WHEN MATCHED THEN
	UPDATE SET val = 'source'
	WHEN NOT MATCHED THEN
	INSERT VALUES(source.id, 'source');
2023-11-25 13:50:06.261 UTC [148751] ERROR:  0A000: For MERGE command, all the distributed tables must be colocated, for append/range distribution, colocation is not supported
2023-11-25 13:50:06.261 UTC [148751] HINT:  Consider using hash distribution instead
2023-11-25 13:50:06.261 UTC [148751] LOCATION:  CheckIfRTETypeIsUnsupported, merge_planner.c:339
2023-11-25 13:50:06.261 UTC [148751] STATEMENT:  MERGE INTO dist_target
	USING dist_source
	ON dist_target.id = dist_source.id
	WHEN MATCHED THEN
	UPDATE SET val = dist_source.val
	WHEN NOT MATCHED THEN
	INSERT VALUES(dist_source.id, dist_source.val);
2023-11-25 13:50:06.264 UTC [148751] ERROR:  0A000: For MERGE command, all the distributed tables must be colocated, for append/range distribution, colocation is not supported
2023-11-25 13:50:06.264 UTC [148751] HINT:  Consider using hash distribution instead
2023-11-25 13:50:06.264 UTC [148751] LOCATION:  CheckIfRTETypeIsUnsupported, merge_planner.c:339
2023-11-25 13:50:06.264 UTC [148751] STATEMENT:  MERGE INTO dist_target
	USING dist_source
	ON dist_target.id = dist_source.id
	WHEN MATCHED THEN
	UPDATE SET val = dist_source.val
	WHEN NOT MATCHED THEN
	INSERT VALUES(dist_source.id, dist_source.val);
2023-11-25 13:50:06.274 UTC [148751] ERROR:  0A000: For MERGE command, all the distributed tables must be colocated, for append/range distribution, colocation is not supported
2023-11-25 13:50:06.274 UTC [148751] HINT:  Consider using hash distribution instead
2023-11-25 13:50:06.274 UTC [148751] LOCATION:  CheckIfRTETypeIsUnsupported, merge_planner.c:339
2023-11-25 13:50:06.274 UTC [148751] STATEMENT:  MERGE INTO dist_target
	USING dist_source
	ON dist_target.id = dist_source.id
	WHEN MATCHED THEN
	UPDATE SET val = dist_source.val
	WHEN NOT MATCHED THEN
	INSERT VALUES(dist_source.id, dist_source.val);
2023-11-25 13:50:06.280 UTC [148751] ERROR:  0A000: For MERGE command, all the distributed tables must be colocated, for append/range distribution, colocation is not supported
2023-11-25 13:50:06.280 UTC [148751] HINT:  Consider using hash distribution instead
2023-11-25 13:50:06.280 UTC [148751] LOCATION:  CheckIfRTETypeIsUnsupported, merge_planner.c:339
2023-11-25 13:50:06.280 UTC [148751] STATEMENT:  MERGE INTO dist_target
	USING dist_source
	ON dist_target.id = dist_source.id
	WHEN MATCHED THEN
	UPDATE SET val = dist_source.val
	WHEN NOT MATCHED THEN
	INSERT VALUES(dist_source.id, dist_source.val);
2023-11-25 13:50:06.691 UTC [148841] ERROR:  42601: syntax error at or near "RANDOMWORD" at character 21
2023-11-25 13:50:06.691 UTC [148841] LOCATION:  scanner_yyerror, scan.l:1188
2023-11-25 13:50:06.691 UTC [148841] STATEMENT:  MERGE INTO target t RANDOMWORD
	USING source AS s
	ON t.tid = s.sid
	WHEN MATCHED THEN
		UPDATE SET balance = 0;
2023-11-25 13:50:06.691 UTC [148841] ERROR:  42601: syntax error at or near "INSERT" at character 75
2023-11-25 13:50:06.691 UTC [148841] LOCATION:  scanner_yyerror, scan.l:1188
2023-11-25 13:50:06.691 UTC [148841] STATEMENT:  MERGE INTO target t
	USING source AS s
	ON t.tid = s.sid
	WHEN MATCHED THEN
		INSERT DEFAULT VALUES;
2023-11-25 13:50:06.691 UTC [148841] ERROR:  42601: syntax error at or near "INTO" at character 86
2023-11-25 13:50:06.691 UTC [148841] LOCATION:  scanner_yyerror, scan.l:1188
2023-11-25 13:50:06.691 UTC [148841] STATEMENT:  MERGE INTO target t
	USING source AS s
	ON t.tid = s.sid
	WHEN NOT MATCHED THEN
		INSERT INTO target DEFAULT VALUES;
2023-11-25 13:50:06.691 UTC [148841] ERROR:  42601: syntax error at or near "," at character 98
2023-11-25 13:50:06.691 UTC [148841] LOCATION:  scanner_yyerror, scan.l:1188
2023-11-25 13:50:06.691 UTC [148841] STATEMENT:  MERGE INTO target t
	USING source AS s
	ON t.tid = s.sid
	WHEN NOT MATCHED THEN
		INSERT VALUES (1,1), (2,2);
2023-11-25 13:50:06.691 UTC [148841] ERROR:  42601: syntax error at or near "SELECT" at character 86
2023-11-25 13:50:06.691 UTC [148841] LOCATION:  scanner_yyerror, scan.l:1188
2023-11-25 13:50:06.691 UTC [148841] STATEMENT:  MERGE INTO target t
	USING source AS s
	ON t.tid = s.sid
	WHEN NOT MATCHED THEN
		INSERT SELECT (1, 1);
2023-11-25 13:50:06.691 UTC [148841] ERROR:  42601: syntax error at or near "UPDATE" at character 79
2023-11-25 13:50:06.691 UTC [148841] LOCATION:  scanner_yyerror, scan.l:1188
2023-11-25 13:50:06.691 UTC [148841] STATEMENT:  MERGE INTO target t
	USING source AS s
	ON t.tid = s.sid
	WHEN NOT MATCHED THEN
		UPDATE SET balance = 0;
2023-11-25 13:50:06.691 UTC [148841] ERROR:  42601: syntax error at or near "target" at character 82
2023-11-25 13:50:06.691 UTC [148841] LOCATION:  scanner_yyerror, scan.l:1188
2023-11-25 13:50:06.691 UTC [148841] STATEMENT:  MERGE INTO target t
	USING source AS s
	ON t.tid = s.sid
	WHEN MATCHED THEN
		UPDATE target SET balance = 0;
2023-11-25 13:50:06.691 UTC [148841] ERROR:  42712: name "target" specified more than once
2023-11-25 13:50:06.691 UTC [148841] DETAIL:  The name is used both as MERGE target table and data source.
2023-11-25 13:50:06.691 UTC [148841] LOCATION:  transformMergeStmt, parse_merge.c:206
2023-11-25 13:50:06.691 UTC [148841] STATEMENT:  MERGE INTO target
	USING target
	ON tid = tid
	WHEN MATCHED THEN DO NOTHING;
2023-11-25 13:50:06.691 UTC [148841] ERROR:  0A000: MERGE not supported in WITH query at character 6
2023-11-25 13:50:06.691 UTC [148841] LOCATION:  transformWithClause, parse_cte.c:131
2023-11-25 13:50:06.691 UTC [148841] STATEMENT:  WITH foo AS (
	  MERGE INTO target USING source ON (true)
	  WHEN MATCHED THEN DELETE
	) SELECT * FROM foo;
2023-11-25 13:50:06.691 UTC [148841] ERROR:  0A000: MERGE not supported in COPY
2023-11-25 13:50:06.691 UTC [148841] LOCATION:  DoCopy, copy.c:281
2023-11-25 13:50:06.691 UTC [148841] STATEMENT:  COPY (
	  MERGE INTO target USING source ON (true)
	  WHEN MATCHED THEN DELETE
	) TO stdout;
2023-11-25 13:50:06.698 UTC [148841] ERROR:  0A000: cannot execute MERGE on relation "tv"
2023-11-25 13:50:06.698 UTC [148841] DETAIL:  This operation is not supported for views.
2023-11-25 13:50:06.698 UTC [148841] LOCATION:  transformMergeStmt, parse_merge.c:180
2023-11-25 13:50:06.698 UTC [148841] STATEMENT:  MERGE INTO tv t
	USING source s
	ON t.tid = s.sid
	WHEN NOT MATCHED THEN
		INSERT DEFAULT VALUES;
2023-11-25 13:50:06.704 UTC [148841] ERROR:  0A000: cannot execute MERGE on relation "mv"
2023-11-25 13:50:06.704 UTC [148841] DETAIL:  This operation is not supported for materialized views.
2023-11-25 13:50:06.704 UTC [148841] LOCATION:  transformMergeStmt, parse_merge.c:180
2023-11-25 13:50:06.704 UTC [148841] STATEMENT:  MERGE INTO mv t
	USING source s
	ON t.tid = s.sid
	WHEN NOT MATCHED THEN
		INSERT DEFAULT VALUES;
2023-11-25 13:50:06.705 UTC [148841] ERROR:  42501: permission denied for table source2
2023-11-25 13:50:06.705 UTC [148841] LOCATION:  aclcheck_error, aclchk.c:3650
2023-11-25 13:50:06.705 UTC [148841] STATEMENT:  MERGE INTO target
	USING source2
	ON target.tid = source2.sid
	WHEN MATCHED THEN
		UPDATE SET balance = 0;
2023-11-25 13:50:06.706 UTC [148841] ERROR:  42501: permission denied for table target
2023-11-25 13:50:06.706 UTC [148841] LOCATION:  aclcheck_error, aclchk.c:3650
2023-11-25 13:50:06.706 UTC [148841] STATEMENT:  MERGE INTO target
	USING source2
	ON target.tid = source2.sid
	WHEN MATCHED THEN
		UPDATE SET balance = 0;
2023-11-25 13:50:06.711 UTC [148841] ERROR:  42501: permission denied for table target2
2023-11-25 13:50:06.711 UTC [148841] LOCATION:  aclcheck_error, aclchk.c:3650
2023-11-25 13:50:06.711 UTC [148841] STATEMENT:  MERGE INTO target2
	USING source
	ON target2.tid = source.sid
	WHEN MATCHED THEN
		DELETE;
2023-11-25 13:50:06.711 UTC [148841] ERROR:  42501: permission denied for table target2
2023-11-25 13:50:06.711 UTC [148841] LOCATION:  aclcheck_error, aclchk.c:3650
2023-11-25 13:50:06.711 UTC [148841] STATEMENT:  MERGE INTO target2
	USING source
	ON target2.tid = source.sid
	WHEN NOT MATCHED THEN
		INSERT DEFAULT VALUES;
2023-11-25 13:50:06.712 UTC [148841] ERROR:  42P01: invalid reference to FROM-clause entry for table "t" at character 55
2023-11-25 13:50:06.712 UTC [148841] HINT:  There is an entry for table "t", but it cannot be referenced from this part of the query.
2023-11-25 13:50:06.712 UTC [148841] LOCATION:  errorMissingRTE, parse_relation.c:3597
2023-11-25 13:50:06.712 UTC [148841] STATEMENT:  MERGE INTO target t
	USING (SELECT * FROM source WHERE t.tid > sid) s
	ON t.tid = s.sid
	WHEN NOT MATCHED THEN
		INSERT DEFAULT VALUES;
2023-11-25 13:50:06.728 UTC [148841] ERROR:  21000: MERGE command cannot affect row a second time
2023-11-25 13:50:06.728 UTC [148841] HINT:  Ensure that not more than one source row matches any one target row.
2023-11-25 13:50:06.728 UTC [148841] LOCATION:  ExecMergeMatched, nodeModifyTable.c:2932
2023-11-25 13:50:06.728 UTC [148841] STATEMENT:  MERGE INTO target t
	USING source AS s
	ON t.tid = s.sid
	WHEN MATCHED THEN
		UPDATE SET balance = 0;
2023-11-25 13:50:06.728 UTC [148841] ERROR:  21000: MERGE command cannot affect row a second time
2023-11-25 13:50:06.728 UTC [148841] HINT:  Ensure that not more than one source row matches any one target row.
2023-11-25 13:50:06.728 UTC [148841] LOCATION:  ExecMergeMatched, nodeModifyTable.c:2932
2023-11-25 13:50:06.728 UTC [148841] STATEMENT:  MERGE INTO target t
	USING source AS s
	ON t.tid = s.sid
	WHEN MATCHED THEN
		DELETE;
2023-11-25 13:50:06.729 UTC [148841] ERROR:  23505: duplicate key value violates unique constraint "targetidx_4001000"
2023-11-25 13:50:06.729 UTC [148841] DETAIL:  Key (tid)=(4) already exists.
2023-11-25 13:50:06.729 UTC [148841] LOCATION:  _bt_check_unique, nbtinsert.c:664
2023-11-25 13:50:06.729 UTC [148841] STATEMENT:  MERGE INTO target t
	USING source AS s
	ON t.tid = s.sid
	WHEN NOT MATCHED THEN
	  INSERT VALUES (4, NULL);
2023-11-25 13:50:06.729 UTC [148841] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 13:50:06.729 UTC [148841] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 13:50:06.729 UTC [148841] STATEMENT:  SELECT * FROM target ORDER BY tid;
2023-11-25 13:50:06.737 UTC [148841] ERROR:  42P01: invalid reference to FROM-clause entry for table "t" at character 109
2023-11-25 13:50:06.737 UTC [148841] HINT:  There is an entry for table "t", but it cannot be referenced from this part of the query.
2023-11-25 13:50:06.737 UTC [148841] LOCATION:  errorMissingRTE, parse_relation.c:3597
2023-11-25 13:50:06.737 UTC [148841] STATEMENT:  MERGE INTO target t
	USING source AS s
	ON t.tid = s.sid
	WHEN NOT MATCHED THEN
		INSERT (tid, balance) VALUES (t.tid, s.delta);
2023-11-25 13:50:06.737 UTC [148841] ERROR:  42P01: invalid reference to FROM-clause entry for table "t" at character 109
2023-11-25 13:50:06.737 UTC [148841] HINT:  There is an entry for table "t", but it cannot be referenced from this part of the query.
2023-11-25 13:50:06.737 UTC [148841] LOCATION:  errorMissingRTE, parse_relation.c:3597
2023-11-25 13:50:06.737 UTC [148841] STATEMENT:  MERGE INTO target t
	USING source AS s
	ON (SELECT true)
	WHEN NOT MATCHED THEN
		INSERT (tid, balance) VALUES (t.tid, s.delta);
2023-11-25 13:50:06.737 UTC [148841] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 13:50:06.737 UTC [148841] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 13:50:06.737 UTC [148841] STATEMENT:  SELECT * FROM target ORDER BY tid;
2023-11-25 13:50:06.738 UTC [148841] ERROR:  42601: unreachable WHEN clause specified after unconditional WHEN clause
2023-11-25 13:50:06.738 UTC [148841] LOCATION:  transformMergeStmt, parse_merge.c:159
2023-11-25 13:50:06.738 UTC [148841] STATEMENT:  MERGE INTO target t
	USING source AS s
	ON t.tid = s.sid
	WHEN MATCHED THEN /* Terminal WHEN clause for MATCHED */
		DELETE
	WHEN MATCHED AND s.delta > 0 THEN
		UPDATE SET balance = t.balance - s.delta;
2023-11-25 13:50:06.748 UTC [148841] ERROR:  42P01: invalid reference to FROM-clause entry for table "t" at character 80
2023-11-25 13:50:06.748 UTC [148841] HINT:  There is an entry for table "t", but it cannot be referenced from this part of the query.
2023-11-25 13:50:06.748 UTC [148841] LOCATION:  errorMissingRTE, parse_relation.c:3597
2023-11-25 13:50:06.748 UTC [148841] STATEMENT:  MERGE INTO wq_target t
	USING wq_source s ON t.tid = s.sid
	WHEN NOT MATCHED AND t.balance = 100 THEN
		INSERT (tid) VALUES (s.sid);
2023-11-25 13:50:06.748 UTC [148841] ERROR:  25P02: current transaction is aborted, commands ignored until end of transaction block
2023-11-25 13:50:06.748 UTC [148841] LOCATION:  exec_simple_query, postgres.c:1112
2023-11-25 13:50:06.748 UTC [148841] STATEMENT:  SELECT * FROM wq_target;
2023-11-25 13:50:06.753 UTC [148841] ERROR:  42P10: cannot use system column "xmin" in MERGE WHEN condition at character 76
2023-11-25 13:50:06.753 UTC [148841] LOCATION:  scanNSItemForColumn, parse_relation.c:709
2023-11-25 13:50:06.753 UTC [148841] STATEMENT:  MERGE INTO wq_target t
	USING wq_source s ON t.tid = s.sid
	WHEN MATCHED AND t.xmin = t.xmax THEN
		UPDATE SET balance = t.balance + s.balance;
2023-11-25 13:50:06.756 UTC [148841] ERROR:  0A000: non-IMMUTABLE functions are not yet supported in MERGE sql with distributed tables 
2023-11-25 13:50:06.756 UTC [148841] LOCATION:  MergeQuerySupported, merge_planner.c:134
2023-11-25 13:50:06.756 UTC [148841] STATEMENT:  MERGE INTO wq_target t
	USING wq_source s ON t.tid = s.sid
	WHEN MATCHED AND (merge_when_and_write()) THEN
		UPDATE SET balance = t.balance + s.balance;
2023-11-25 13:50:06.756 UTC [148841] ERROR:  0A000: non-IMMUTABLE functions are not yet supported in MERGE sql with distributed tables 
2023-11-25 13:50:06.756 UTC [148841] LOCATION:  MergeQuerySupported, merge_planner.c:134
2023-11-25 13:50:06.756 UTC [148841] STATEMENT:  MERGE INTO wq_target t
	USING wq_source s ON t.tid = s.sid AND (merge_when_and_write())
	WHEN MATCHED THEN
		UPDATE SET balance = t.balance + s.balance;
2023-11-25 13:50:06.832 UTC [148841] ERROR:  42702: column reference "balance" is ambiguous at character 98
2023-11-25 13:50:06.832 UTC [148841] LOCATION:  colNameToVar, parse_relation.c:898
2023-11-25 13:50:06.832 UTC [148841] STATEMENT:  MERGE INTO sq_target
	USING v
	ON tid = sid
	WHEN MATCHED AND tid > 2 THEN
	    UPDATE SET balance = balance + delta
	WHEN NOT MATCHED THEN
		INSERT (balance, tid) VALUES (balance + delta, sid)
	WHEN MATCHED AND tid < 2 THEN
		DELETE;
2023-11-25 13:50:06.833 UTC [148841] ERROR:  42601: syntax error at or near "RETURNING" at character 231
2023-11-25 13:50:06.833 UTC [148841] LOCATION:  scanner_yyerror, scan.l:1188
2023-11-25 13:50:06.833 UTC [148841] STATEMENT:  MERGE INTO sq_target t
	USING v
	ON tid = sid
	WHEN MATCHED AND tid > 2 THEN
	    UPDATE SET balance = t.balance + delta
	WHEN NOT MATCHED THEN
		INSERT (balance, tid) VALUES (balance + delta, sid)
	WHEN MATCHED AND tid < 2 THEN
		DELETE
	RETURNING *;
2023-11-25 13:50:07.214 UTC [133093] LOG:  00000: received SIGHUP, reloading configuration files
2023-11-25 13:50:07.214 UTC [133093] LOCATION:  SIGHUP_handler, postmaster.c:2769
2023-11-25 13:50:07.214 UTC [133093] LOG:  00000: parameter "citus.max_cached_conns_per_worker" changed to "0"
2023-11-25 13:50:07.214 UTC [133093] LOCATION:  ProcessConfigFileInternal, guc-file.l:453
2023-11-25 13:50:07.214 UTC [133093] LOG:  00000: parameter "citus.distributed_deadlock_detection_factor" changed to "-1"
2023-11-25 13:50:07.214 UTC [133093] LOCATION:  ProcessConfigFileInternal, guc-file.l:453
2023-11-25 13:50:07.214 UTC [133093] LOG:  00000: received SIGHUP, reloading configuration files
2023-11-25 13:50:07.214 UTC [133093] LOCATION:  SIGHUP_handler, postmaster.c:2769
2023-11-25 13:50:07.215 UTC [133093] LOG:  00000: parameter "citus.recover_2pc_interval" changed to "1ms"
2023-11-25 13:50:07.215 UTC [133093] LOCATION:  ProcessConfigFileInternal, guc-file.l:453
2023-11-25 13:50:07.316 UTC [133093] LOG:  00000: received SIGHUP, reloading configuration files
2023-11-25 13:50:07.316 UTC [133093] LOCATION:  SIGHUP_handler, postmaster.c:2769
2023-11-25 13:50:07.316 UTC [133093] LOG:  00000: parameter "citus.recover_2pc_interval" changed to "-1"
2023-11-25 13:50:07.316 UTC [133093] LOCATION:  ProcessConfigFileInternal, guc-file.l:453
2023-11-25 13:50:07.552 UTC [133093] LOG:  00000: received SIGHUP, reloading configuration files
2023-11-25 13:50:07.552 UTC [133093] LOCATION:  SIGHUP_handler, postmaster.c:2769
2023-11-25 13:50:07.552 UTC [133093] LOG:  00000: parameter "citus.distributed_deadlock_detection_factor" removed from configuration file, reset to default
2023-11-25 13:50:07.552 UTC [133093] LOCATION:  ProcessConfigFileInternal, guc-file.l:388
2023-11-25 13:50:07.552 UTC [133093] LOG:  00000: parameter "citus.max_cached_conns_per_worker" removed from configuration file, reset to default
2023-11-25 13:50:07.552 UTC [133093] LOCATION:  ProcessConfigFileInternal, guc-file.l:388
2023-11-25 13:50:07.552 UTC [133093] LOG:  00000: parameter "citus.recover_2pc_interval" removed from configuration file, reset to default
2023-11-25 13:50:07.552 UTC [133093] LOCATION:  ProcessConfigFileInternal, guc-file.l:388
2023-11-25 13:50:07.691 UTC [149025] ERROR:  0A000: cannot complete operation on generated_identities.smallint_identity_column with smallint/int identity column
2023-11-25 13:50:07.691 UTC [149025] HINT:  Use bigint identity column instead.
2023-11-25 13:50:07.691 UTC [149025] LOCATION:  ErrorIfTableHasUnsupportedIdentityColumn, table.c:3997
2023-11-25 13:50:07.691 UTC [149025] STATEMENT:  SELECT create_distributed_table('smallint_identity_column', 'a');
2023-11-25 13:50:07.831 UTC [149025] ERROR:  0A000: cannot complete operation on generated_identities.smallint_identity_column with smallint/int identity column
2023-11-25 13:50:07.831 UTC [149025] HINT:  Use bigint identity column instead.
2023-11-25 13:50:07.831 UTC [149025] LOCATION:  ErrorIfTableHasUnsupportedIdentityColumn, table.c:3997
2023-11-25 13:50:07.831 UTC [149025] STATEMENT:  SELECT create_distributed_table_concurrently('smallint_identity_column', 'a');
2023-11-25 13:50:07.832 UTC [149025] ERROR:  0A000: cannot complete operation on a table with identity column
2023-11-25 13:50:07.832 UTC [149025] LOCATION:  ErrorIfTableHasIdentityColumn, table.c:4026
2023-11-25 13:50:07.832 UTC [149025] STATEMENT:  SELECT create_reference_table('smallint_identity_column');
2023-11-25 13:50:07.839 UTC [149025] ERROR:  0A000: cannot complete operation on generated_identities.int_identity_column with smallint/int identity column
2023-11-25 13:50:07.839 UTC [149025] HINT:  Use bigint identity column instead.
2023-11-25 13:50:07.839 UTC [149025] LOCATION:  ErrorIfTableHasUnsupportedIdentityColumn, table.c:3997
2023-11-25 13:50:07.839 UTC [149025] STATEMENT:  SELECT create_distributed_table('int_identity_column', 'a');
2023-11-25 13:50:07.856 UTC [149025] ERROR:  0A000: cannot complete operation on generated_identities.int_identity_column with smallint/int identity column
2023-11-25 13:50:07.856 UTC [149025] HINT:  Use bigint identity column instead.
2023-11-25 13:50:07.856 UTC [149025] LOCATION:  ErrorIfTableHasUnsupportedIdentityColumn, table.c:3997
2023-11-25 13:50:07.856 UTC [149025] STATEMENT:  SELECT create_distributed_table_concurrently('int_identity_column', 'a');
2023-11-25 13:50:07.856 UTC [149025] ERROR:  0A000: cannot complete operation on a table with identity column
2023-11-25 13:50:07.856 UTC [149025] LOCATION:  ErrorIfTableHasIdentityColumn, table.c:4026
2023-11-25 13:50:07.856 UTC [149025] STATEMENT:  SELECT create_reference_table('int_identity_column');
2023-11-25 13:50:07.939 UTC [149072] ERROR:  0A000: cannot complete operation on a table with identity column
2023-11-25 13:50:07.939 UTC [149072] LOCATION:  ErrorIfTableHasIdentityColumn, table.c:4026
2023-11-25 13:50:07.939 UTC [149072] STATEMENT:  SELECT alter_distributed_table('bigint_identity_column', 'b');
2023-11-25 13:50:07.940 UTC [149072] ERROR:  0A000: cannot complete operation on a table with identity column
2023-11-25 13:50:07.940 UTC [149072] LOCATION:  ErrorIfTableHasIdentityColumn, table.c:4026
2023-11-25 13:50:07.940 UTC [149072] STATEMENT:  SELECT undistribute_table('bigint_identity_column');
2023-11-25 13:50:08.050 UTC [149089] ERROR:  0A000: alter table command is currently unsupported
2023-11-25 13:50:08.050 UTC [149089] DETAIL:  Only ADD|DROP COLUMN, SET|DROP NOT NULL, SET|DROP DEFAULT, ADD|DROP|VALIDATE CONSTRAINT, SET (), RESET (), ENABLE|DISABLE|NO FORCE|FORCE ROW LEVEL SECURITY, ATTACH|DETACH PARTITION and TYPE subcommands are supported.
2023-11-25 13:50:08.050 UTC [149089] LOCATION:  ErrorIfUnsupportedAlterTableStmt, table.c:3506
2023-11-25 13:50:08.050 UTC [149089] STATEMENT:  ALTER TABLE partitioned_table ALTER COLUMN g ADD GENERATED ALWAYS AS IDENTITY;
2023-11-25 13:50:08.050 UTC [149089] ERROR:  XX000: cannot execute ALTER COLUMN command involving identity column
2023-11-25 13:50:08.050 UTC [149089] LOCATION:  ErrorIfUnsupportedAlterTableStmt, table.c:3306
2023-11-25 13:50:08.050 UTC [149089] STATEMENT:  ALTER TABLE partitioned_table ALTER COLUMN b TYPE int;
2023-11-25 13:50:08.131 UTC [149104] ERROR:  42501: must be owner of table color
2023-11-25 13:50:08.131 UTC [149104] LOCATION:  aclcheck_error, aclchk.c:3788
2023-11-25 13:50:08.131 UTC [149104] STATEMENT:  SELECT create_distributed_table('color', 'color_id');
2023-11-25 13:50:08.159 UTC [149104] LOG:  00000: performing non-blocking create_distributed_table_concurrently 
2023-11-25 13:50:08.159 UTC [149104] LOCATION:  SplitShard, shard_split.c:519
2023-11-25 13:50:08.159 UTC [149104] STATEMENT:  SELECT create_distributed_table_concurrently('color', 'color_id');
2023-11-25 13:50:08.164 UTC [149104] LOG:  00000: creating child shards for create_distributed_table_concurrently
2023-11-25 13:50:08.164 UTC [149104] LOCATION:  NonBlockingShardSplit, shard_split.c:1430
2023-11-25 13:50:08.164 UTC [149104] STATEMENT:  SELECT create_distributed_table_concurrently('color', 'color_id');
2023-11-25 13:50:08.191 UTC [149104] LOG:  00000: creating replication artifacts (publications, replication slots, subscriptions for create_distributed_table_concurrently
2023-11-25 13:50:08.191 UTC [149104] LOCATION:  NonBlockingShardSplit, shard_split.c:1462
2023-11-25 13:50:08.191 UTC [149104] STATEMENT:  SELECT create_distributed_table_concurrently('color', 'color_id');
2023-11-25 13:50:08.201 UTC [149114] LOG:  00000: Initializing CDC decoder
2023-11-25 13:50:08.201 UTC [149114] LOCATION:  _PG_output_plugin_init, cdc_decoder.c:82
2023-11-25 13:50:08.201 UTC [149114] STATEMENT:  CREATE_REPLICATION_SLOT citus_shard_split_slot_16_10_24 LOGICAL citus EXPORT_SNAPSHOT;
2023-11-25 13:50:08.201 UTC [149114] LOG:  00000: logical decoding found consistent point at 0/7C3DA10
2023-11-25 13:50:08.201 UTC [149114] DETAIL:  There are no running transactions.
2023-11-25 13:50:08.201 UTC [149114] LOCATION:  SnapBuildFindSnapshot, snapbuild.c:1366
2023-11-25 13:50:08.201 UTC [149114] STATEMENT:  CREATE_REPLICATION_SLOT citus_shard_split_slot_16_10_24 LOGICAL citus EXPORT_SNAPSHOT;
2023-11-25 13:50:08.202 UTC [149114] LOG:  00000: exported logical decoding snapshot: "00000007-000007DC-1" with 0 transaction IDs
2023-11-25 13:50:08.202 UTC [149114] LOCATION:  SnapBuildExportSnapshot, snapbuild.c:687
2023-11-25 13:50:08.202 UTC [149114] STATEMENT:  CREATE_REPLICATION_SLOT citus_shard_split_slot_16_10_24 LOGICAL citus EXPORT_SNAPSHOT;
2023-11-25 13:50:08.209 UTC [149113] LOG:  00000: Initializing CDC decoder
2023-11-25 13:50:08.209 UTC [149113] LOCATION:  _PG_output_plugin_init, cdc_decoder.c:82
2023-11-25 13:50:08.209 UTC [149113] STATEMENT:  SELECT pg_catalog.pg_copy_logical_replication_slot('citus_shard_split_slot_16_10_24', 'citus_shard_split_slot_34_10_24')
2023-11-25 13:50:08.231 UTC [149104] LOG:  00000: performing copy for create_distributed_table_concurrently
2023-11-25 13:50:08.231 UTC [149104] LOCATION:  NonBlockingShardSplit, shard_split.c:1534
2023-11-25 13:50:08.231 UTC [149104] STATEMENT:  SELECT create_distributed_table_concurrently('color', 'color_id');
2023-11-25 13:50:08.232 UTC [149117] LOG:  00000: performing copy from shard generated_identities.color_363052 to [generated_identities.color_363053 (nodeId: 16), generated_identities.color_363054 (nodeId: 34), generated_identities.color_363055 (nodeId: 16), generated_identities.color_363056 (nodeId: 34)]
2023-11-25 13:50:08.232 UTC [149117] LOCATION:  worker_split_copy, worker_split_copy_udf.c:107
2023-11-25 13:50:08.232 UTC [149117] STATEMENT:  SELECT pg_catalog.worker_split_copy(363052, 'color_id', ARRAY[ROW(363053, -2147483648, -1073741825, 16)::pg_catalog.split_copy_info,ROW(363054, -1073741824, -1, 34)::pg_catalog.split_copy_info,ROW(363055, 0, 1073741823, 16)::pg_catalog.split_copy_info,ROW(363056, 1073741824, 2147483647, 34)::pg_catalog.split_copy_info]);
2023-11-25 13:50:08.232 UTC [149104] LOG:  00000: replicating changes for create_distributed_table_concurrently
2023-11-25 13:50:08.232 UTC [149104] LOCATION:  NonBlockingShardSplit, shard_split.c:1541
2023-11-25 13:50:08.232 UTC [149104] STATEMENT:  SELECT create_distributed_table_concurrently('color', 'color_id');
2023-11-25 13:50:08.239 UTC [149122] LOG:  00000: Initializing CDC decoder
2023-11-25 13:50:08.239 UTC [149122] LOCATION:  _PG_output_plugin_init, cdc_decoder.c:82
2023-11-25 13:50:08.239 UTC [149122] STATEMENT:  START_REPLICATION SLOT "citus_shard_split_slot_34_10_24" LOGICAL 0/0 (proto_version '3', publication_names '"citus_shard_split_publication_34_10_24"', binary 'true')
2023-11-25 13:50:08.239 UTC [149123] LOG:  00000: Initializing CDC decoder
2023-11-25 13:50:08.239 UTC [149123] LOCATION:  _PG_output_plugin_init, cdc_decoder.c:82
2023-11-25 13:50:08.239 UTC [149123] STATEMENT:  START_REPLICATION SLOT "citus_shard_split_slot_16_10_24" LOGICAL 0/0 (proto_version '3', publication_names '"citus_shard_split_publication_16_10_24"', binary 'true')
2023-11-25 13:50:08.239 UTC [149122] LOG:  00000: starting logical decoding for slot "citus_shard_split_slot_34_10_24"
2023-11-25 13:50:08.239 UTC [149122] DETAIL:  Streaming transactions committing after 0/7C3DA48, reading WAL from 0/7C3DA10.
2023-11-25 13:50:08.239 UTC [149122] LOCATION:  CreateDecodingContext, logical.c:569
2023-11-25 13:50:08.239 UTC [149122] STATEMENT:  START_REPLICATION SLOT "citus_shard_split_slot_34_10_24" LOGICAL 0/0 (proto_version '3', publication_names '"citus_shard_split_publication_34_10_24"', binary 'true')
2023-11-25 13:50:08.239 UTC [149123] LOG:  00000: starting logical decoding for slot "citus_shard_split_slot_16_10_24"
2023-11-25 13:50:08.239 UTC [149123] DETAIL:  Streaming transactions committing after 0/7C3DA48, reading WAL from 0/7C3DA10.
2023-11-25 13:50:08.239 UTC [149123] LOCATION:  CreateDecodingContext, logical.c:569
2023-11-25 13:50:08.239 UTC [149123] STATEMENT:  START_REPLICATION SLOT "citus_shard_split_slot_16_10_24" LOGICAL 0/0 (proto_version '3', publication_names '"citus_shard_split_publication_16_10_24"', binary 'true')
2023-11-25 13:50:08.239 UTC [149122] LOG:  00000: logical decoding found consistent point at 0/7C3DA10
2023-11-25 13:50:08.239 UTC [149122] DETAIL:  There are no running transactions.
2023-11-25 13:50:08.239 UTC [149122] LOCATION:  SnapBuildFindSnapshot, snapbuild.c:1366
2023-11-25 13:50:08.239 UTC [149122] STATEMENT:  START_REPLICATION SLOT "citus_shard_split_slot_34_10_24" LOGICAL 0/0 (proto_version '3', publication_names '"citus_shard_split_publication_34_10_24"', binary 'true')
2023-11-25 13:50:08.239 UTC [149123] LOG:  00000: logical decoding found consistent point at 0/7C3DA10
2023-11-25 13:50:08.239 UTC [149123] DETAIL:  There are no running transactions.
2023-11-25 13:50:08.239 UTC [149123] LOCATION:  SnapBuildFindSnapshot, snapbuild.c:1366
2023-11-25 13:50:08.239 UTC [149123] STATEMENT:  START_REPLICATION SLOT "citus_shard_split_slot_16_10_24" LOGICAL 0/0 (proto_version '3', publication_names '"citus_shard_split_publication_16_10_24"', binary 'true')
2023-11-25 13:50:09.236 UTC [149104] LOG:  00000: The LSN of the target subscriptions on node localhost:57638 have caught up with the source LSN 
2023-11-25 13:50:09.236 UTC [149104] LOCATION:  WaitForGroupedLogicalRepTargetsToCatchUp, multi_logical_replication.c:1842
2023-11-25 13:50:09.236 UTC [149104] STATEMENT:  SELECT create_distributed_table_concurrently('color', 'color_id');
2023-11-25 13:50:09.237 UTC [149104] LOG:  00000: The LSN of the target subscriptions on node localhost:57637 have caught up with the source LSN 
2023-11-25 13:50:09.237 UTC [149104] LOCATION:  WaitForGroupedLogicalRepTargetsToCatchUp, multi_logical_replication.c:1842
2023-11-25 13:50:09.237 UTC [149104] STATEMENT:  SELECT create_distributed_table_concurrently('color', 'color_id');
2023-11-25 13:50:09.256 UTC [149104] LOG:  00000: The LSN of the target subscriptions on node localhost:57638 have caught up with the source LSN 
2023-11-25 13:50:09.256 UTC [149104] LOCATION:  WaitForGroupedLogicalRepTargetsToCatchUp, multi_logical_replication.c:1842
2023-11-25 13:50:09.256 UTC [149104] STATEMENT:  SELECT create_distributed_table_concurrently('color', 'color_id');
2023-11-25 13:50:09.256 UTC [149104] LOG:  00000: The LSN of the target subscriptions on node localhost:57637 have caught up with the source LSN 
2023-11-25 13:50:09.256 UTC [149104] LOCATION:  WaitForGroupedLogicalRepTargetsToCatchUp, multi_logical_replication.c:1842
2023-11-25 13:50:09.256 UTC [149104] STATEMENT:  SELECT create_distributed_table_concurrently('color', 'color_id');
2023-11-25 13:50:09.257 UTC [149104] LOG:  00000: The LSN of the target subscriptions on node localhost:57638 have caught up with the source LSN 
2023-11-25 13:50:09.257 UTC [149104] LOCATION:  WaitForGroupedLogicalRepTargetsToCatchUp, multi_logical_replication.c:1842
2023-11-25 13:50:09.257 UTC [149104] STATEMENT:  SELECT create_distributed_table_concurrently('color', 'color_id');
2023-11-25 13:50:09.257 UTC [149104] LOG:  00000: The LSN of the target subscriptions on node localhost:57637 have caught up with the source LSN 
2023-11-25 13:50:09.257 UTC [149104] LOCATION:  WaitForGroupedLogicalRepTargetsToCatchUp, multi_logical_replication.c:1842
2023-11-25 13:50:09.257 UTC [149104] STATEMENT:  SELECT create_distributed_table_concurrently('color', 'color_id');
2023-11-25 13:50:09.257 UTC [149104] LOG:  00000: marking deferred cleanup of source shard(s) for create_distributed_table_concurrently
2023-11-25 13:50:09.257 UTC [149104] LOCATION:  NonBlockingShardSplit, shard_split.c:1560
2023-11-25 13:50:09.257 UTC [149104] STATEMENT:  SELECT create_distributed_table_concurrently('color', 'color_id');
2023-11-25 13:50:09.260 UTC [149104] LOG:  00000: creating foreign key constraints (if any) for create_distributed_table_concurrently
2023-11-25 13:50:09.260 UTC [149104] LOCATION:  NonBlockingShardSplit, shard_split.c:1610
2023-11-25 13:50:09.260 UTC [149104] STATEMENT:  SELECT create_distributed_table_concurrently('color', 'color_id');
2023-11-25 13:50:09.381 UTC [149137] ERROR:  0A000: cannot execute ADD COLUMN commands involving identity columns when metadata is synchronized to workers
2023-11-25 13:50:09.381 UTC [149137] LOCATION:  ErrorIfUnsupportedAlterTableStmt, table.c:3182
2023-11-25 13:50:09.381 UTC [149137] STATEMENT:  ALTER TABLE color ADD COLUMN color_id BIGINT GENERATED ALWAYS AS IDENTITY;
2023-11-25 13:50:09.382 UTC [149137] ERROR:  XX000: Altering a distributed sequence is currently not supported.
2023-11-25 13:50:09.382 UTC [149137] LOCATION:  PreprocessAlterSequenceStmt, sequence.c:464
2023-11-25 13:50:09.382 UTC [149137] STATEMENT:  ALTER SEQUENCE color_color_id_seq RESTART WITH 1000;
2023-11-25 13:50:09.382 UTC [149137] ERROR:  428C9: cannot insert a non-DEFAULT value into column "color_id"
2023-11-25 13:50:09.382 UTC [149137] DETAIL:  Column "color_id" is an identity column defined as GENERATED ALWAYS.
2023-11-25 13:50:09.382 UTC [149137] HINT:  Use OVERRIDING SYSTEM VALUE to override.
2023-11-25 13:50:09.382 UTC [149137] LOCATION:  rewriteTargetListIU, rewriteHandler.c:884
2023-11-25 13:50:09.382 UTC [149137] STATEMENT:  INSERT INTO color(color_id, color_name) VALUES (1, 'Red');
2023-11-25 13:50:09.382 UTC [149137] ERROR:  428C9: cannot insert a non-DEFAULT value into column "color_id"
2023-11-25 13:50:09.382 UTC [149137] DETAIL:  Column "color_id" is an identity column defined as GENERATED ALWAYS.
2023-11-25 13:50:09.382 UTC [149137] HINT:  Use OVERRIDING SYSTEM VALUE to override.
2023-11-25 13:50:09.382 UTC [149137] LOCATION:  rewriteTargetListIU, rewriteHandler.c:884
2023-11-25 13:50:09.382 UTC [149137] STATEMENT:  INSERT INTO color(color_id, color_name) VALUES (NULL, 'Red');
2023-11-25 13:50:09.388 UTC [149137] ERROR:  23505: duplicate key value violates unique constraint "color_color_id_key_12400000"
2023-11-25 13:50:09.388 UTC [149137] DETAIL:  Key (color_id)=(1) already exists.
2023-11-25 13:50:09.388 UTC [149137] CONTEXT:  while executing command on localhost:57637
2023-11-25 13:50:09.388 UTC [149137] LOCATION:  ReportResultError, remote_commands.c:317
2023-11-25 13:50:09.388 UTC [149137] STATEMENT:  INSERT INTO color(color_id, color_name) OVERRIDING SYSTEM VALUE VALUES (1, 'Red');
2023-11-25 13:50:09.388 UTC [149137] ERROR:  428C9: column "color_id" can only be updated to DEFAULT
2023-11-25 13:50:09.388 UTC [149137] DETAIL:  Column "color_id" is an identity column defined as GENERATED ALWAYS.
2023-11-25 13:50:09.388 UTC [149137] LOCATION:  rewriteTargetListIU, rewriteHandler.c:950
2023-11-25 13:50:09.388 UTC [149137] STATEMENT:  UPDATE color SET color_id = NULL;
2023-11-25 13:50:09.388 UTC [149137] ERROR:  428C9: column "color_id" can only be updated to DEFAULT
2023-11-25 13:50:09.388 UTC [149137] DETAIL:  Column "color_id" is an identity column defined as GENERATED ALWAYS.
2023-11-25 13:50:09.388 UTC [149137] LOCATION:  rewriteTargetListIU, rewriteHandler.c:950
2023-11-25 13:50:09.388 UTC [149137] STATEMENT:  UPDATE color SET color_id = 1;
2023-11-25 13:50:09.632 UTC [149160] LOG:  00000: starting maintenance daemon on database 33233 user 10
2023-11-25 13:50:09.632 UTC [149160] CONTEXT:  Citus maintenance daemon for database 33233 user 10
2023-11-25 13:50:09.632 UTC [149160] LOCATION:  CitusMaintenanceDaemonMain, maintenanced.c:373
2023-11-25 13:50:09.753 UTC [133094] LOG:  00000: checkpoint starting: immediate force wait
2023-11-25 13:50:09.753 UTC [133094] LOCATION:  LogCheckpointStart, xlog.c:6089
2023-11-25 13:50:09.775 UTC [133094] LOG:  00000: checkpoint complete: wrote 2495 buffers (15.2%); 0 WAL file(s) added, 0 removed, 4 recycled; write=0.010 s, sync=0.001 s, total=0.022 s; sync files=0, longest=0.000 s, average=0.000 s; distance=61560 kB, estimate=61560 kB
2023-11-25 13:50:09.775 UTC [133094] LOCATION:  LogCheckpointEnd, xlog.c:6170
2023-11-25 13:50:09.784 UTC [133094] LOG:  00000: checkpoint starting: immediate force wait
2023-11-25 13:50:09.784 UTC [133094] LOCATION:  LogCheckpointStart, xlog.c:6089
2023-11-25 13:50:09.784 UTC [133094] LOG:  00000: checkpoint complete: wrote 2 buffers (0.0%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0.001 s, sync=0.001 s, total=0.001 s; sync files=0, longest=0.000 s, average=0.000 s; distance=1 kB, estimate=55404 kB
2023-11-25 13:50:09.784 UTC [133094] LOCATION:  LogCheckpointEnd, xlog.c:6170
2023-11-25 13:50:10.683 UTC [133093] LOG:  00000: received fast shutdown request
2023-11-25 13:50:10.683 UTC [133093] LOCATION:  pmdie, postmaster.c:2904
2023-11-25 13:50:10.683 UTC [133093] LOG:  00000: aborting any active transactions
2023-11-25 13:50:10.683 UTC [133093] LOCATION:  pmdie, postmaster.c:2922
2023-11-25 13:50:10.685 UTC [133093] LOG:  00000: background worker "logical replication launcher" (PID 133099) exited with exit code 1
2023-11-25 13:50:10.685 UTC [133093] LOCATION:  LogChildExit, postmaster.c:3737
2023-11-25 13:50:10.685 UTC [133094] LOG:  00000: shutting down
2023-11-25 13:50:10.685 UTC [133094] LOCATION:  ShutdownXLOG, xlog.c:6039
2023-11-25 13:50:10.685 UTC [133094] LOG:  00000: checkpoint starting: shutdown immediate
2023-11-25 13:50:10.685 UTC [133094] LOCATION:  LogCheckpointStart, xlog.c:6089
2023-11-25 13:50:10.686 UTC [133094] LOG:  00000: checkpoint complete: wrote 11 buffers (0.1%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0.001 s, sync=0.001 s, total=0.001 s; sync files=0, longest=0.000 s, average=0.000 s; distance=33 kB, estimate=49867 kB
2023-11-25 13:50:10.686 UTC [133094] LOCATION:  LogCheckpointEnd, xlog.c:6170
2023-11-25 13:50:10.698 UTC [133093] LOG:  00000: database system is shut down
2023-11-25 13:50:10.698 UTC [133093] LOCATION:  UnlinkLockFiles, miscinit.c:977
